title,abstract,token_count,arxiv_id,category,categories,query_field,published_date,collection_timestamp
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.910736
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",artificial intelligence,2024-12-20,2024-12-23T21:06:22.911734
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",artificial intelligence,2024-12-20,2024-12-23T21:06:22.912733
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.912733
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.913729
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.914726
Dual-Polarized Beyond Diagonal RIS,"Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of
RIS architectures more flexible than conventional RIS. While BD-RIS has been
primarily analyzed assuming uni-polarized systems, modern wireless deployments
are dual-polarized. To address this gap, this paper investigates the
fundamental limits of dual-polarized BD-RIS-aided systems. We derive the
scaling laws governing the performance of BD-RIS and the Pareto frontier of the
trade-off between performance and circuit complexity enabled by BD-RIS.
Theoretical results show that the group-connected RIS with group size 2
provides remarkable gains over conventional RIS in both Rayleigh and
line-of-sight (LoS) channels, while maintaining a reduced circuit complexity.",166,2412.16097v1,cs.IT,"cs.IT,eess.SP,math.IT",artificial intelligence,2024-12-20,2024-12-23T21:06:22.915724
The Evolution of LLM Adoption in Industry Data Curation Practices,"As large language models (LLMs) grow increasingly adept at processing
unstructured text data, they offer new opportunities to enhance data curation
workflows. This paper explores the evolution of LLM adoption among
practitioners at a large technology company, evaluating the impact of LLMs in
data curation tasks through participants' perceptions, integration strategies,
and reported usage scenarios. Through a series of surveys, interviews, and user
studies, we provide a timely snapshot of how organizations are navigating a
pivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess
LLM adoption in industry for development tasks (N=84), and facilitated expert
interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we
explored practitioners' current and anticipated LLM usage through a user study
involving two LLM-based prototypes (N=12). While each study addressed distinct
research goals, they revealed a broader narrative about evolving LLM usage in
aggregate. We discovered an emerging shift in data understanding from
heuristic-first, bottom-up approaches to insights-first, top-down workflows
supported by LLMs. Furthermore, to respond to a more complex data landscape,
data practitioners now supplement traditional subject-expert-created 'golden
datasets' with LLM-generated 'silver' datasets and rigorously validated 'super
golden' datasets curated by diverse experts. This research sheds light on the
transformative role of LLMs in large-scale analysis of unstructured data and
highlights opportunities for further tool development.",328,2412.16089v1,cs.HC,"cs.HC,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.915724
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",artificial intelligence,2024-12-20,2024-12-23T21:06:22.916720
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",artificial intelligence,2024-12-20,2024-12-23T21:06:22.917719
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",artificial intelligence,2024-12-20,2024-12-23T21:06:22.917719
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.918715
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.919712
A Framework for Streaming Event-Log Prediction in Business Processes,"We present a Python-based framework for event-log prediction in streaming
mode, enabling predictions while data is being generated by a business process.
The framework allows for easy integration of streaming algorithms, including
language models like n-grams and LSTMs, and for combining these predictors
using ensemble methods.
  Using our framework, we conducted experiments on various well-known
process-mining data sets and compared classical batch with streaming mode.
Though, in batch mode, LSTMs generally achieve the best performance, there is
often an n-gram whose accuracy comes very close. Combining basic models in
ensemble methods can even outperform LSTMs. The value of basic models with
respect to LSTMs becomes even more apparent in streaming mode, where LSTMs
generally lack accuracy in the early stages of a prediction run, while basic
methods make sensible predictions immediately.",173,2412.16032v1,cs.AI,"cs.AI,cs.LG",artificial intelligence,2024-12-20,2024-12-23T21:06:22.920710
The Only Way is Ethics: A Guide to Ethical Research with Large Language Models,"There is a significant body of work looking at the ethical considerations of
large language models (LLMs): critiquing tools to measure performance and
harms; proposing toolkits to aid in ideation; discussing the risks to workers;
considering legislation around privacy and security etc. As yet there is no
work that integrates these resources into a single practical guide that focuses
on LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',
which we provide as an open and living resource for NLP practitioners, and
those tasked with evaluating the ethical implications of others' work. Our goal
is to translate ethics literature into concrete recommendations and
provocations for thinking with clear first steps, aimed at computer scientists.
'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's
and Don'ts, which we present also in this paper. We likewise identify useful
toolkits to support ethical work. We refer the interested reader to the full
LLM Ethics Whitepaper, which provides a succinct discussion of ethical
considerations at each stage in a project lifecycle, as well as citations for
the hundreds of papers from which we drew our recommendations. The present
paper can be thought of as a pocket guide to conducting ethical research with
LLMs.",263,2412.16022v1,cs.CL,"cs.CL,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.920710
Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition,"Explaining machine learning (ML) models using eXplainable AI (XAI) techniques
has become essential to make them more transparent and trustworthy. This is
especially important in high-stakes domains like healthcare, where
understanding model decisions is critical to ensure ethical, sound, and
trustworthy outcome predictions. However, users are often confused about which
explanability method to choose for their specific use case. We present a
comparative analysis of widely used explainability methods, Shapley Additive
Explanations (SHAP) and Gradient-weighted Class Activation Mapping (GradCAM),
within the domain of human activity recognition (HAR) utilizing graph
convolutional networks (GCNs). By evaluating these methods on skeleton-based
data from two real-world datasets, including a healthcare-critical cerebral
palsy (CP) case, this study provides vital insights into both approaches'
strengths, limitations, and differences, offering a roadmap for selecting the
most appropriate explanation method based on specific models and applications.
We quantitatively and quantitatively compare these methods, focusing on feature
importance ranking, interpretability, and model sensitivity through
perturbation experiments. While SHAP provides detailed input feature
attribution, GradCAM delivers faster, spatially oriented explanations, making
both methods complementary depending on the application's requirements. Given
the importance of XAI in enhancing trust and transparency in ML models,
particularly in sensitive environments like healthcare, our research
demonstrates how SHAP and GradCAM could complement each other to provide more
interpretable and actionable model explanations.",315,2412.16003v1,cs.LG,"cs.LG,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.921707
CNN-LSTM Hybrid Deep Learning Model for Remaining Useful Life Estimation,"Remaining Useful Life (RUL) of a component or a system is defined as the
length from the current time to the end of the useful life. Accurate RUL
estimation plays a crucial role in Predictive Maintenance applications.
Traditional regression methods, both linear and non-linear, have struggled to
achieve high accuracy in this domain. While Convolutional Neural Networks
(CNNs) have shown improved accuracy, they often overlook the sequential nature
of the data, relying instead on features derived from sliding windows. Since
RUL prediction inherently involves multivariate time series analysis, robust
sequence learning is essential. In this work, we propose a hybrid approach
combining Convolutional Neural Networks with Long Short-Term Memory (LSTM)
networks for RUL estimation. Although CNN-based LSTM models have been applied
to sequence prediction tasks in financial forecasting, this is the first
attempt to adopt this approach for RUL estimation in prognostics. In this
approach, CNN is first employed to efficiently extract features from the data,
followed by LSTM, which uses these extracted features to predict RUL. This
method effectively leverages sensor sequence information, uncovering hidden
patterns within the data, even under multiple operating conditions and fault
scenarios. Our results demonstrate that the hybrid CNN-LSTM model achieves the
highest accuracy, offering a superior score compared to the other methods.",281,2412.15998v1,cs.LG,"cs.LG,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.922705
Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling,"Conversational assistants are increasingly popular across diverse real-world
applications, highlighting the need for advanced multimodal speech modeling.
Speech, as a natural mode of communication, encodes rich user-specific
characteristics such as speaking rate and pitch, making it critical for
effective interaction. Our work introduces a data-centric customization
approach for efficiently enhancing multimodal understanding in conversational
speech modeling. Central to our contributions is a novel multi-task learning
paradigm that involves designing auxiliary tasks to utilize a small amount of
speech data. Our approach achieves state-of-the-art performance on the
Spoken-SQuAD benchmark, using only 10% of the training data with open-weight
models, establishing a robust and efficient framework for audio-centric
conversational modeling. We also introduce ASK-QA, the first dataset for
multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation
inputs. Code and data forthcoming.",187,2412.15995v1,cs.CL,"cs.CL,cs.AI,cs.SD,eess.AS",artificial intelligence,2024-12-20,2024-12-23T21:06:22.922705
APIRL: Deep Reinforcement Learning for REST API Fuzzing,"REST APIs have become key components of web services. However, they often
contain logic flaws resulting in server side errors or security
vulnerabilities. HTTP requests are used as test cases to find and mitigate such
issues. Existing methods to modify requests, including those using deep
learning, suffer from limited performance and precision, relying on undirected
search or making limited usage of the contextual information. In this paper we
propose APIRL, a fully automated deep reinforcement learning tool for testing
REST APIs. A key novelty of our approach is the use of feedback from a
transformer module pre-trained on JSON-structured data, akin to that used in
API responses. This allows APIRL to learn the subtleties relating to test
outcomes, and generalise to unseen API endpoints. We show APIRL can find
significantly more bugs than the state-of-the-art in real world REST APIs while
minimising the number of required test cases. We also study how reward
functions, and other key design choices, affect learnt policies in a thorough
ablation study.",218,2412.15991v1,cs.SE,"cs.SE,cs.AI,cs.NI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.923702
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",artificial intelligence,2024-12-20,2024-12-23T21:06:22.924699
Never Reset Again: A Mathematical Framework for Continual Inference in Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are widely used for sequential processing
but face fundamental limitations with continual inference due to state
saturation, requiring disruptive hidden state resets. However, reset-based
methods impose synchronization requirements with input boundaries and increase
computational costs at inference. To address this, we propose an adaptive loss
function that eliminates the need for resets during inference while preserving
high accuracy over extended sequences. By combining cross-entropy and
Kullback-Leibler divergence, the loss dynamically modulates the gradient based
on input informativeness, allowing the network to differentiate meaningful data
from noise and maintain stable representations over time. Experimental results
demonstrate that our reset-free approach outperforms traditional reset-based
methods when applied to a variety of RNNs, particularly in continual tasks,
enhancing both the theoretical and practical capabilities of RNNs for streaming
applications.",178,2412.15983v1,cs.LG,"cs.LG,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.924699
Self-Supervised Radiograph Anatomical Region Classification -- How Clean Is Your Real-World Data?,"Modern deep learning-based clinical imaging workflows rely on accurate labels
of the examined anatomical region. Knowing the anatomical region is required to
select applicable downstream models and to effectively generate cohorts of high
quality data for future medical and machine learning research efforts. However,
this information may not be available in externally sourced data or generally
contain data entry errors. To address this problem, we show the effectiveness
of self-supervised methods such as SimCLR and BYOL as well as supervised
contrastive deep learning methods in assigning one of 14 anatomical region
classes in our in-house dataset of 48,434 skeletal radiographs. We achieve a
strong linear evaluation accuracy of 96.6% with a single model and 97.7% using
an ensemble approach. Furthermore, only a few labeled instances (1% of the
training set) suffice to achieve an accuracy of 92.2%, enabling usage in
low-label and thus low-resource scenarios. Our model can be used to correct
data entry mistakes: a follow-up analysis of the test set errors of our
best-performing single model by an expert radiologist identified 35% incorrect
labels and 11% out-of-domain images. When accounted for, the radiograph
anatomical region labelling performance increased -- without and with an
ensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%.",282,2412.15967v1,cs.CV,"cs.CV,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.925696
Optimization of Beyond Diagonal RIS: A Universal Framework Applicable to Arbitrary Architectures,"Reconfigurable intelligent surfaces (RISs) are envisioned as a promising
technology for future wireless communication systems due to their ability to
control the propagation environment in a hardware- and energy-efficient way.
Recently, the concept of RISs has been extended to beyond diagonal RISs
(BD-RISs), which unlock the full potential of RISs thanks to the presence of
tunable interconnections between RIS elements. While various algorithms have
been proposed for specific BD-RIS architectures, a universal optimization
framework applicable to arbitrary architectures is still lacking. In this
paper, we bridge this research gap by proposing an architecture-independent
framework for BD-RIS optimization, with the main focus on sum-rate maximization
and transmit power minimization in multiuser multi-input single-output
(MU-MISO) systems. Specifically, we first incorporate BD-RIS architectures into
the models by connecting the scattering matrix with the admittance matrix and
introducing appropriate constraints to the admittance matrix. The formulated
problems are then solved by our custom-designed partially proximal alternating
direction method of multipliers (pp-ADMM) algorithms. The pp-ADMM algorithms
are computationally efficient, with each subproblem either admitting a
closed-form solution or being easily solvable. We further explore the extension
of the proposed framework to general utility functions and multiuser
multi-input multi-output (MU-MIMO) systems. Simulation results demonstrate that
the proposed approaches achieve a better trade-off between performance and
computational efficiency compared to existing methods. We also compare the
performance of various BD-RIS architectures in MU-MISO systems using the
proposed approach, which has not been explored before due to the lack of an
architecture-independent framework.",366,2412.15965v1,eess.SP,"eess.SP,cs.IT,math.IT,math.OC",artificial intelligence,2024-12-20,2024-12-23T21:06:22.926694
From General to Specific: Tailoring Large Language Models for Personalized Healthcare,"The rapid development of large language models (LLMs) has transformed many
industries, including healthcare. However, previous medical LLMs have largely
focused on leveraging general medical knowledge to provide responses, without
accounting for patient variability and lacking true personalization at the
individual level. To address this, we propose a novel method called
personalized medical language model (PMLM), which explores and optimizes
personalized LLMs through recommendation systems and reinforcement learning
(RL). Specifically, by utilizing self-informed and peer-informed
personalization, PMLM captures changes in behaviors and preferences to design
initial personalized prompts tailored to individual needs. We further refine
these initial personalized prompts through RL, ultimately enhancing the
precision of LLM guidance. Notably, the personalized prompt are hard prompt,
which grants PMLM high adaptability and reusability, allowing it to directly
leverage high-quality proprietary LLMs. We evaluate PMLM using real-world
obstetrics and gynecology data, and the experimental results demonstrate that
PMLM achieves personalized responses, and it provides more refined and
individualized services, offering a potential way for personalized medical
LLMs.",239,2412.15957v1,cs.CL,"cs.CL,cs.AI,cs.IR",artificial intelligence,2024-12-20,2024-12-23T21:06:22.927692
Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring,"In the software industry, the drive to add new features often overshadows the
need to improve existing code. Large Language Models (LLMs) offer a new
approach to improving codebases at an unprecedented scale through AI-assisted
refactoring. However, LLMs come with inherent risks such as braking changes and
the introduction of security vulnerabilities. We advocate for encapsulating the
interaction with the models in IDEs and validating refactoring attempts using
trustworthy safeguards. However, equally important for the uptake of AI
refactoring is research on trust development. In this position paper, we
position our future work based on established models from research on human
factors in automation. We outline action research within CodeScene on
development of 1) novel LLM safeguards and 2) user interaction that conveys an
appropriate level of trust. The industry collaboration enables large-scale
repository analysis and A/B testing to continuously guide the design of our
research interventions.",201,2412.15948v1,cs.SE,"cs.SE,cs.AI,cs.HC",artificial intelligence,2024-12-20,2024-12-23T21:06:22.927692
pyRheo: An open-source Python package for complex rheology,"Mathematical modeling is a powerful tool in rheology, and we present pyRheo,
an open-source package for Python designed to streamline the analysis of creep,
stress relaxation, oscillation, and rotation tests. pyRheo contains a
comprehensive selection of viscoelastic models, including fractional order
approaches. It integrates model selection and fitting features and employs
machine intelligence to suggest a model to describe a given dataset. The
package fits the suggested model or one chosen by the user. An advantage of
using pyRheo is that it addresses challenges associated with sensitivity to
initial guesses in parameter optimization. It allows the user to iteratively
search for the best initial guesses, avoiding convergence to local minima. We
discuss the capabilities of pyRheo and compare them to other tools for
rheological modeling of biological matter. We demonstrate that pyRheo
significantly reduces the computation time required to fit high-performance
viscoelastic models.",198,2412.15941v1,cond-mat.soft,cond-mat.soft,artificial intelligence,2024-12-20,2024-12-23T21:06:22.928688
Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation,"The rise of the generative models quality during the past years enabled the
generation of edited variations of images at an important scale. To counter the
harmful effects of such technology, the Image Difference Captioning (IDC) task
aims to describe the differences between two images. While this task is
successfully handled for simple 3D rendered images, it struggles on real-world
images. The reason is twofold: the training data-scarcity, and the difficulty
to capture fine-grained differences between complex images. To address those
issues, we propose in this paper a simple yet effective framework to both adapt
existing image captioning models to the IDC task and augment IDC datasets. We
introduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational
cost, and show it outperforms two-streams approaches by a significant margin on
real-world IDC datasets. We also propose to use synthetic augmentation to
improve the performance of IDC models in an agnostic fashion. We show that our
synthetic augmentation strategy provides high quality data, leading to a
challenging new dataset well-suited for IDC named Syned1.",244,2412.15939v1,cs.CV,"cs.CV,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.929686
Watertox: The Art of Simplicity in Universal Attacks A Cross-Model Framework for Robust Adversarial Generation,"Contemporary adversarial attack methods face significant limitations in
cross-model transferability and practical applicability. We present Watertox,
an elegant adversarial attack framework achieving remarkable effectiveness
through architectural diversity and precision-controlled perturbations. Our
two-stage Fast Gradient Sign Method combines uniform baseline perturbations
($\epsilon_1 = 0.1$) with targeted enhancements ($\epsilon_2 = 0.4$). The
framework leverages an ensemble of complementary architectures, from VGG to
ConvNeXt, synthesizing diverse perspectives through an innovative voting
mechanism. Against state-of-the-art architectures, Watertox reduces model
accuracy from 70.6% to 16.0%, with zero-shot attacks achieving up to 98.8%
accuracy reduction against unseen architectures. These results establish
Watertox as a significant advancement in adversarial methodologies, with
promising applications in visual security systems and CAPTCHA generation.",205,2412.15924v1,cs.CV,"cs.CV,cs.AI,cs.CR",artificial intelligence,2024-12-20,2024-12-23T21:06:22.929686
Less is More: Towards Green Code Large Language Models via Unified Structural Pruning,"The extensive application of Large Language Models (LLMs) in generative
coding tasks has raised concerns due to their high computational demands and
energy consumption. Unlike previous structural pruning methods designed for
classification models that deal with lowdimensional classification logits,
generative Code LLMs produce high-dimensional token logit sequences, making
traditional pruning objectives inherently limited. Moreover, existing single
component pruning approaches further constrain the effectiveness when applied
to generative Code LLMs. In response, we propose Flab-Pruner, an innovative
unified structural pruning method that combines vocabulary, layer, and
Feed-Forward Network (FFN) pruning. This approach effectively reduces model
parameters while maintaining performance. Additionally, we introduce a
customized code instruction data strategy for coding tasks to enhance the
performance recovery efficiency of the pruned model. Through extensive
evaluations on three state-of-the-art Code LLMs across multiple generative
coding tasks, the results demonstrate that Flab-Pruner retains 97% of the
original performance after pruning 22% of the parameters and achieves the same
or even better performance after post-training. The pruned models exhibit
significant improvements in storage, GPU usage, computational efficiency, and
environmental impact, while maintaining well robustness. Our research provides
a sustainable solution for green software engineering and promotes the
efficient deployment of LLMs in real-world generative coding intelligence
applications.",294,2412.15921v1,cs.SE,"cs.SE,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.930683
Speedup Techniques for Switchable Temporal Plan Graph Optimization,"Multi-Agent Path Finding (MAPF) focuses on planning collision-free paths for
multiple agents. However, during the execution of a MAPF plan, agents may
encounter unexpected delays, which can lead to inefficiencies, deadlocks, or
even collisions. To address these issues, the Switchable Temporal Plan Graph
provides a framework for finding an acyclic Temporal Plan Graph with the
minimum execution cost under delays, ensuring deadlock- and collision-free
execution. Unfortunately, existing optimal algorithms, such as Mixed Integer
Linear Programming and Graph-Based Switchable Edge Search (GSES), are often too
slow for practical use. This paper introduces Improved GSES, which
significantly accelerates GSES through four speedup techniques: stronger
admissible heuristics, edge grouping, prioritized branching, and incremental
implementation. Experiments conducted on four different map types with varying
numbers of agents demonstrate that Improved GSES consistently achieves over
twice the success rate of GSES and delivers up to a 30-fold speedup on
instances where both methods successfully find solutions.",220,2412.15908v1,cs.MA,"cs.MA,cs.AI,cs.RO",artificial intelligence,2024-12-20,2024-12-23T21:06:22.931682
Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model,"Background: Recent advances in large language models highlight the need for
high-quality multilingual medical datasets. While Japan leads globally in CT
scanner deployment and utilization, the lack of large-scale Japanese radiology
datasets has hindered the development of specialized language models for
medical imaging analysis. Objective: To develop a comprehensive Japanese CT
report dataset through machine translation and establish a specialized language
model for structured finding classification. Additionally, to create a
rigorously validated evaluation dataset through expert radiologist review.
Methods: We translated the CT-RATE dataset (24,283 CT reports from 21,304
patients) into Japanese using GPT-4o mini. The training dataset consisted of
22,778 machine-translated reports, while the validation dataset included 150
radiologist-revised reports. We developed CT-BERT-JPN based on
""tohoku-nlp/bert-base-japanese-v3"" architecture for extracting 18 structured
findings from Japanese radiology reports. Results: Translation metrics showed
strong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores
ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression
sections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in
11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular
septal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1
scores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in
four conditions. Conclusions: Our study establishes a robust Japanese CT report
dataset and demonstrates the effectiveness of a specialized language model for
structured finding classification. The hybrid approach of machine translation
and expert validation enables the creation of large-scale medical datasets
while maintaining high quality.",398,2412.15907v1,cs.CL,"cs.CL,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.932679
What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning,"Step-level reward models (SRMs) can significantly enhance mathematical
reasoning performance through process supervision or step-level preference
alignment based on reinforcement learning. The performance of SRMs is pivotal,
as they serve as critical guidelines, ensuring that each step in the reasoning
process is aligned with desired outcomes. Recently, AlphaZero-like methods,
where Monte Carlo Tree Search (MCTS) is employed for automatic step-level
preference annotation, have proven particularly effective. However, the precise
mechanisms behind the success of SRMs remain largely unexplored. To address
this gap, this study delves into the counterintuitive aspects of SRMs,
particularly focusing on MCTS-based approaches. Our findings reveal that the
removal of natural language descriptions of thought processes has minimal
impact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept
at assessing the complex logical coherence present in mathematical language
while having difficulty in natural language. These insights provide a nuanced
understanding of the core elements that drive effective step-level reward
modeling in mathematical reasoning. By shedding light on these mechanisms, this
study offers valuable guidance for developing more efficient and streamlined
SRMs, which can be achieved by focusing on the crucial parts of mathematical
reasoning.",253,2412.15904v1,cs.AI,"cs.AI,cs.LG",artificial intelligence,2024-12-20,2024-12-23T21:06:22.932679
On the Suitability of pre-trained foundational LLMs for Analysis in German Legal Education,"We show that current open-source foundational LLMs possess instruction
capability and German legal background knowledge that is sufficient for some
legal analysis in an educational context. However, model capability breaks down
in very specific tasks, such as the classification of ""Gutachtenstil"" appraisal
style components, or with complex contexts, such as complete legal opinions.
Even with extended context and effective prompting strategies, they cannot
match the Bag-of-Words baseline. To combat this, we introduce a Retrieval
Augmented Generation based prompt example selection method that substantially
improves predictions in high data availability scenarios. We further evaluate
the performance of pre-trained LLMs on two standard tasks for argument mining
and automated essay scoring and find it to be more adequate. Throughout,
pre-trained LLMs improve upon the baseline in scenarios with little or no
labeled data with Chain-of-Thought prompting further helping in the zero-shot
case.",183,2412.15902v1,cs.CL,"cs.CL,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.933677
"TelcoLM: collecting data, adapting, and benchmarking language models for the telecommunication domain","Despite outstanding processes in many tasks, Large Language Models (LLMs)
still lack accuracy when dealing with highly technical domains. Especially,
telecommunications (telco) is a particularly challenging domain due the large
amount of lexical, semantic and conceptual peculiarities. Yet, this domain
holds many valuable use cases, directly linked to industrial needs. Hence, this
paper studies how LLMs can be adapted to the telco domain. It reports our
effort to (i) collect a massive corpus of domain-specific data (800M tokens,
80K instructions), (ii) perform adaptation using various methodologies, and
(iii) benchmark them against larger generalist models in downstream tasks that
require extensive knowledge of telecommunications. Our experiments on
Llama-2-7b show that domain-adapted models can challenge the large generalist
models. They also suggest that adaptation can be restricted to a unique
instruction-tuning step, dicarding the need for any fine-tuning on raw texts
beforehand.",200,2412.15891v1,cs.CL,"cs.CL,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.933677
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",artificial intelligence,2024-12-20,2024-12-23T21:06:22.934673
AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI,"AI is the workhorse of modern data analytics and omnipresent across many
sectors. Large Language Models and multi-modal foundation models are today
capable of generating code, charts, visualizations, etc. How will these massive
developments of AI in data analytics shape future data visualizations and
visual analytics workflows? What is the potential of AI to reshape methodology
and design of future visual analytics applications? What will be our role as
visualization researchers in the future? What are opportunities, open
challenges and threats in the context of an increasingly powerful AI? This
Visualization Viewpoint discusses these questions in the special context of
biomedical data analytics as an example of a domain in which critical decisions
are taken based on complex and sensitive data, with high requirements on
transparency, efficiency, and reliability. We map recent trends and
developments in AI on the elements of interactive visualization and visual
analytics workflows and highlight the potential of AI to transform biomedical
visualization as a research field. Given that agency and responsibility have to
remain with human experts, we argue that it is helpful to keep the focus on
human-centered workflows, and to use visual analytics as a tool for integrating
``AI-in-the-loop''. This is in contrast to the more traditional term
``human-in-the-loop'', which focuses on incorporating human expertise into
AI-based systems.",282,2412.15876v1,cs.HC,"cs.HC,cs.AI,cs.GR,68U01,H.1.2; H.5.2; I.3.6; I.2.1; J.3; D.2.0",artificial intelligence,2024-12-20,2024-12-23T21:06:22.935671
Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback,"Reinforcement learning from human feedback (RLHF) has proven effective in
enhancing the instruction-following capabilities of large language models;
however, it remains underexplored in the cross-modality domain. As the number
of modalities increases, aligning all-modality models with human intentions --
such as instruction following -- becomes a pressing challenge. In this work, we
make the first attempt to fine-tune all-modality models (i.e. input and output
with any modality, also named any-to-any models) using human preference data
across all modalities (including text, image, audio, and video), ensuring its
behavior aligns with human intentions. This endeavor presents several
challenges. First, there is no large-scale all-modality human preference data
in existing open-source resources, as most datasets are limited to specific
modalities, predominantly text and image. Secondly, the effectiveness of binary
preferences in RLHF for post-training alignment in complex all-modality
scenarios remains an unexplored area. Finally, there is a lack of a systematic
framework to evaluate the capabilities of all-modality models, particularly
regarding modality selection and synergy. To address these challenges, we
propose the align-anything framework, which includes meticulously annotated
200k all-modality human preference data. Then, we introduce an alignment method
that learns from unified language feedback, effectively capturing complex
modality-specific human preferences and enhancing the model's
instruction-following capabilities. Furthermore, to assess performance
improvements in all-modality models after post-training alignment, we construct
a challenging all-modality capability evaluation framework -- eval-anything.
All data, models, and code frameworks have been open-sourced for the community.
For more details, please refer to
https://github.com/PKU-Alignment/align-anything.",399,2412.15838v1,cs.AI,"cs.AI,cs.CL",artificial intelligence,2024-12-20,2024-12-23T21:06:22.936668
Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis,"Complying with traffic rules is challenging for automated vehicles, as
numerous rules need to be considered simultaneously. If a planned trajectory
violates traffic rules, it is common to replan a new trajectory from scratch.
We instead propose a trajectory repair technique to save computation time. By
coupling satisfiability modulo theories with set-based reachability analysis,
we determine if and in what manner the initial trajectory can be repaired.
Experiments in high-fidelity simulators and in the real world demonstrate the
benefits of our proposed approach in various scenarios. Even in complex
environments with intricate rules, we efficiently and reliably repair
rule-violating trajectories, enabling automated vehicles to swiftly resume
legally safe operation in real-time.",146,2412.15837v1,cs.RO,"cs.RO,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.936668
AIFS-CRPS: Ensemble forecasting using a model trained with a loss function based on the Continuous Ranked Probability Score,"Over the last three decades, ensemble forecasts have become an integral part
of forecasting the weather. They provide users with more complete information
than single forecasts as they permit to estimate the probability of weather
events by representing the sources of uncertainties and accounting for the
day-to-day variability of error growth in the atmosphere. This paper presents a
novel approach to obtain a weather forecast model for ensemble forecasting with
machine-learning. AIFS-CRPS is a variant of the Artificial Intelligence
Forecasting System (AIFS) developed at ECMWF. Its loss function is based on a
proper score, the Continuous Ranked Probability Score (CRPS). For the loss, the
almost fair CRPS is introduced because it approximately removes the bias in the
score due to finite ensemble size yet avoids a degeneracy of the fair CRPS. The
trained model is stochastic and can generate as many exchangeable members as
desired and computationally feasible in inference. For medium-range forecasts
AIFS-CRPS outperforms the physics-based Integrated Forecasting System (IFS)
ensemble for the majority of variables and lead times. For subseasonal
forecasts, AIFS-CRPS outperforms the IFS ensemble before calibration and is
competitive with the IFS ensemble when forecasts are evaluated as anomalies to
remove the influence of model biases.",282,2412.15832v1,physics.ao-ph,physics.ao-ph,artificial intelligence,2024-12-20,2024-12-23T21:06:22.937665
S$^2$DN: Learning to Denoise Unconvincing Knowledge for Inductive Knowledge Graph Completion,"Inductive Knowledge Graph Completion (KGC) aims to infer missing facts
between newly emerged entities within knowledge graphs (KGs), posing a
significant challenge. While recent studies have shown promising results in
inferring such entities through knowledge subgraph reasoning, they suffer from
(i) the semantic inconsistencies of similar relations, and (ii) noisy
interactions inherent in KGs due to the presence of unconvincing knowledge for
emerging entities. To address these challenges, we propose a Semantic
Structure-aware Denoising Network (S$^2$DN) for inductive KGC. Our goal is to
learn adaptable general semantics and reliable structures to distill consistent
semantic knowledge while preserving reliable interactions within KGs.
Specifically, we introduce a semantic smoothing module over the enclosing
subgraphs to retain the universal semantic knowledge of relations. We
incorporate a structure refining module to filter out unreliable interactions
and offer additional knowledge, retaining robust structure surrounding target
links. Extensive experiments conducted on three benchmark KGs demonstrate that
S$^2$DN surpasses the performance of state-of-the-art models. These results
demonstrate the effectiveness of S$^2$DN in preserving semantic consistency and
enhancing the robustness of filtering out unreliable interactions in
contaminated KGs.",265,2412.15822v1,cs.LG,"cs.LG,cs.AI,cs.CL",artificial intelligence,2024-12-20,2024-12-23T21:06:22.938663
$π$-yalli: un nouveau corpus pour le nahuatl,"The NAHU$^2$ project is a Franco-Mexican collaboration aimed at building the
$\pi$-YALLI corpus adapted to machine learning, which will subsequently be used
to develop computer resources for the Nahuatl language. Nahuatl is a language
with few computational resources, even though it is a living language spoken by
around 2 million people. We have decided to build $\pi$-YALLI, a corpus that
will enable to carry out research on Nahuatl in order to develop Language
Models (LM), whether dynamic or not, which will make it possible to in turn
enable the development of Natural Language Processing (NLP) tools such as: a) a
grapheme unifier, b) a word segmenter, c) a POS grammatical analyser, d) a
content-based Automatic Text Summarization; and possibly, e) a translator
translator (probabilistic or learning-based).",195,2412.15821v1,cs.CL,"cs.CL,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.938663
WebLLM: A High-Performance In-Browser LLM Inference Engine,"Advancements in large language models (LLMs) have unlocked remarkable
capabilities. While deploying these models typically requires server-grade GPUs
and cloud-based inference, the recent emergence of smaller open-source models
and increasingly powerful consumer devices have made on-device deployment
practical. The web browser as a platform for on-device deployment is
universally accessible, provides a natural agentic environment, and
conveniently abstracts out the different backends from diverse device vendors.
To address this opportunity, we introduce WebLLM, an open-source JavaScript
framework that enables high-performance LLM inference entirely within web
browsers. WebLLM provides an OpenAI-style API for seamless integration into web
applications, and leverages WebGPU for efficient local GPU acceleration and
WebAssembly for performant CPU computation. With machine learning compilers
MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming
the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM
can retain up to 80% native performance on the same device, with room to
further close the gap. WebLLM paves the way for universally accessible,
privacy-preserving, personalized, and locally powered LLM applications in web
browsers. The code is available at: https://github.com/mlc-ai/web-llm.",291,2412.15803v1,cs.LG,"cs.LG,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.939660
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.940657
GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning,"The integration of multi-omic data is pivotal for understanding complex
diseases, but its high dimensionality and noise present significant challenges.
Graph Neural Networks (GNNs) offer a robust framework for analyzing large-scale
signaling pathways and protein-protein interaction networks, yet they face
limitations in expressivity when capturing intricate biological relationships.
To address this, we propose Graph Sequence Language Model (GraphSeqLM), a
framework that enhances GNNs with biological sequence embeddings generated by
Large Language Models (LLMs). These embeddings encode structural and biological
properties of DNA, RNA, and proteins, augmenting GNNs with enriched features
for analyzing sample-specific multi-omic data. By integrating topological,
sequence-derived, and biological information, GraphSeqLM demonstrates superior
predictive accuracy and outperforms existing methods, paving the way for more
effective multi-omic data integration in precision medicine.",192,2412.15790v1,q-bio.QM,"q-bio.QM,cs.AI,cs.LG",artificial intelligence,2024-12-20,2024-12-23T21:06:22.940657
AI Apology: A Critical Review of Apology in AI Systems,"Apologies are a powerful tool used in human-human interactions to provide
affective support, regulate social processes, and exchange information
following a trust violation. The emerging field of AI apology investigates the
use of apologies by artificially intelligent systems, with recent research
suggesting how this tool may provide similar value in human-machine
interactions. Until recently, contributions to this area were sparse, and these
works have yet to be synthesised into a cohesive body of knowledge. This
article provides the first synthesis and critical analysis of the state of AI
apology research, focusing on studies published between 2020 and 2023. We
derive a framework of attributes to describe five core elements of apology:
outcome, interaction, offence, recipient, and offender. With this framework as
the basis for our critique, we show how apologies can be used to recover from
misalignment in human-AI interactions, and examine trends and inconsistencies
within the field. Among the observations, we outline the importance of curating
a human-aligned and cross-disciplinary perspective in this research, with
consideration for improved system capabilities and long-term outcomes.",222,2412.15787v1,cs.CY,cs.CY,artificial intelligence,2024-12-20,2024-12-23T21:06:22.941655
Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease Detection based on Spontaneous Speech,"Alzheimer's Disease (AD) is a significant and growing public health concern.
Investigating alterations in speech and language patterns offers a promising
path towards cost-effective and non-invasive early detection of AD on a large
scale. Large language models (LLMs), such as GPT, have enabled powerful new
possibilities for semantic text analysis. In this study, we leverage GPT-4 to
extract five semantic features from transcripts of spontaneous patient speech.
The features capture known symptoms of AD, but they are difficult to quantify
effectively using traditional methods of computational linguistics. We
demonstrate the clinical significance of these features and further validate
one of them (""Word-Finding Difficulties"") against a proxy measure and human
raters. When combined with established linguistic features and a Random Forest
classifier, the GPT-derived features significantly improve the detection of AD.
Our approach proves effective for both manually transcribed and automatically
generated transcripts, representing a novel and impactful use of recent
advancements in LLMs for AD speech analysis.",206,2412.15772v1,cs.CL,"cs.CL,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.942653
Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models,"Background: Despite the current ubiquity of Large Language Models (LLMs)
across the medical domain, there is a surprising lack of studies which address
their reasoning behaviour. We emphasise the importance of understanding
reasoning behaviour as opposed to high-level prediction accuracies, since it is
equivalent to explainable AI (XAI) in this context. In particular, achieving
XAI in medical LLMs used in the clinical domain will have a significant impact
across the healthcare sector. Results: Therefore, we define the concept of
reasoning behaviour in the specific context of medical LLMs. We then categorise
and discuss the current state of the art of methods which evaluate reasoning
behaviour in medical LLMs. Finally, we propose theoretical frameworks which can
empower medical professionals or machine learning engineers to gain insight
into the low-level reasoning operations of these previously obscure models.
Conclusion: The subsequent increased transparency and trust in medical machine
learning models by clinicians as well as patients will accelerate the
integration, application as well as further development of medical AI for the
healthcare system as a whole",216,2412.15748v1,cs.CL,"cs.CL,cs.AI,cs.LG",artificial intelligence,2024-12-20,2024-12-23T21:06:22.943650
Prompt-based Unifying Inference Attack on Graph Neural Networks,"Graph neural networks (GNNs) provide important prospective insights in
applications such as social behavior analysis and financial risk analysis based
on their powerful learning capabilities on graph data. Nevertheless, GNNs'
predictive performance relies on the quality of task-specific node labels, so
it is common practice to improve the model's generalization ability in the
downstream execution of decision-making tasks through pre-training. Graph
prompting is a prudent choice but risky without taking measures to prevent data
leakage. In other words, in high-risk decision scenarios, prompt learning can
infer private information by accessing model parameters trained on private data
(publishing model parameters in pre-training, i.e., without directly leaking
the raw data, is a tacitly accepted trend). However, myriad graph inference
attacks necessitate tailored module design and processing to enhance inference
capabilities due to variations in supervision signals. In this paper, we
propose a novel Prompt-based unifying Inference Attack framework on GNNs, named
ProIA. Specifically, ProIA retains the crucial topological information of the
graph during pre-training, enhancing the background knowledge of the inference
attack model. It then utilizes a unified prompt and introduces additional
disentanglement factors in downstream attacks to adapt to task-relevant
knowledge. Finally, extensive experiments show that ProIA enhances attack
capabilities and demonstrates remarkable adaptability to various inference
attacks.",283,2412.15735v1,cs.LG,cs.LG,artificial intelligence,2024-12-20,2024-12-23T21:06:22.943650
The Role of Recurrency in Image Segmentation for Noisy and Limited Sample Settings,"The biological brain has inspired multiple advances in machine learning.
However, most state-of-the-art models in computer vision do not operate like
the human brain, simply because they are not capable of changing or improving
their decisions/outputs based on a deeper analysis. The brain is recurrent,
while these models are not. It is therefore relevant to explore what would be
the impact of adding recurrent mechanisms to existing state-of-the-art
architectures and to answer the question of whether recurrency can improve
existing architectures. To this end, we build on a feed-forward segmentation
model and explore multiple types of recurrency for image segmentation. We
explore self-organizing, relational, and memory retrieval types of recurrency
that minimize a specific energy function. In our experiments, we tested these
models on artificial and medical imaging data, while analyzing the impact of
high levels of noise and few-shot learning settings. Our results do not
validate our initial hypothesis that recurrent models should perform better in
these settings, suggesting that these recurrent architectures, by themselves,
are not sufficient to surpass state-of-the-art feed-forward versions and that
additional work needs to be done on the topic.",254,2412.15734v1,cs.CV,"cs.CV,cs.LG",artificial intelligence,2024-12-20,2024-12-23T21:06:22.944647
fluke: Federated Learning Utility frameworK for Experimentation and research,"Since its inception in 2016, Federated Learning (FL) has been gaining
tremendous popularity in the machine learning community. Several frameworks
have been proposed to facilitate the development of FL algorithms, but
researchers often resort to implementing their algorithms from scratch,
including all baselines and experiments. This is because existing frameworks
are not flexible enough to support their needs or the learning curve to extend
them is too steep. In this paper, we present \fluke, a Python package designed
to simplify the development of new FL algorithms. fluke is specifically
designed for prototyping purposes and is meant for researchers or practitioners
focusing on the learning components of a federated system. fluke is
open-source, and it can be either used out of the box or extended with new
algorithms with minimal overhead.",160,2412.15728v1,cs.LG,"cs.LG,cs.AI",artificial intelligence,2024-12-20,2024-12-23T21:06:22.944647
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",machine learning,2024-12-20,2024-12-23T21:06:23.902262
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.903259
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,machine learning,2024-12-20,2024-12-23T21:06:23.904257
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",machine learning,2024-12-20,2024-12-23T21:06:23.904257
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",machine learning,2024-12-20,2024-12-23T21:06:23.905254
EF-Net: A Deep Learning Approach Combining Word Embeddings and Feature Fusion for Patient Disposition Analysis,"One of the most urgent problems is the overcrowding in emergency departments
(EDs), caused by an aging population and rising healthcare costs. Patient
dispositions have become more complex as a result of the strain on hospital
infrastructure and the scarcity of medical resources. Individuals with more
dangerous health issues should be prioritized in the emergency room. Thus, our
research aims to develop a prediction model for patient disposition using
EF-Net. This model will incorporate categorical features into the neural
network layer and add numerical features with the embedded categorical
features. We combine the EF-Net and XGBoost models to attain higher accuracy in
our results. The result is generated using the soft voting technique. In
EF-Net, we attained an accuracy of 95.33%, whereas in the Ensemble Model, we
achieved an accuracy of 96%. The experiment's analysis shows that EF-Net
surpasses existing works in accuracy, AUROC, and F1-Score on the MIMIC-IV-ED
dataset, demonstrating its potential as a scalable solution for patient
disposition assessment. Our code is available at
https://github.com/nafisa67/thesis",245,2412.16134v1,cs.LG,cs.LG,machine learning,2024-12-20,2024-12-23T21:06:23.906251
LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical Analysis of Diffeomorphism,"Image registration is a core task in computational anatomy that establishes
correspondences between images. Invertible deformable registration, which
computes a deformation field and handles complex, non-linear transformation, is
essential for tracking anatomical variations, especially in neuroimaging
applications where inter-subject differences and longitudinal changes are key.
Analyzing the deformation fields is challenging due to their non-linearity,
limiting statistical analysis. However, traditional approaches for analyzing
deformation fields are computationally expensive, sensitive to initialization,
and prone to numerical errors, especially when the deformation is far from the
identity. To address these limitations, we propose the Log-Euclidean
Diffeomorphic Autoencoder (LEDA), an innovative framework designed to compute
the principal logarithm of deformation fields by efficiently predicting
consecutive square roots. LEDA operates within a linearized latent space that
adheres to the diffeomorphisms group action laws, enhancing our model's
robustness and applicability. We also introduce a loss function to enforce
inverse consistency, ensuring accurate latent representations of deformation
fields. Extensive experiments with the OASIS-1 dataset demonstrate the
effectiveness of LEDA in accurately modeling and analyzing complex non-linear
deformations while maintaining inverse consistency. Additionally, we evaluate
its ability to capture and incorporate clinical variables, enhancing its
relevance for clinical applications.",271,2412.16129v1,cs.CV,"cs.CV,cs.LG",machine learning,2024-12-20,2024-12-23T21:06:23.907248
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,machine learning,2024-12-20,2024-12-23T21:06:23.907248
Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts,"This study investigates the potential of Large Language Models (LLMs),
particularly GPT-4o, for Optical Character Recognition (OCR) in low-resource
scripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.
Using a meticulously curated dataset of 2,520 images incorporating controlled
variations in text length, font size, background color, and blur, the research
simulates diverse real-world challenges. Results emphasize the limitations of
zero-shot LLM-based OCR, particularly for linguistically complex scripts,
highlighting the need for annotated datasets and fine-tuned models. This work
underscores the urgency of addressing accessibility gaps in text digitization,
paving the way for inclusive and robust OCR solutions for underserved
languages.",164,2412.16119v1,cs.LG,"cs.LG,cs.CV,eess.IV",machine learning,2024-12-20,2024-12-23T21:06:23.908246
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.909244
Weighted nonlocal operators and their applications in labeled learning,"Motivated by problems in machine learning, we study a class of variational
problems characterized by nonlocal operators. These operators are characterized
by power-type weights, which are singular at a portion of the boundary. We
identify a range of exponents on these weights for which the variational
Dirichlet problem is well-posed. This range is determined by the ambient
dimension of the problem, the growth rate of the nonlocal functional, and the
dimension of the boundary portion on which the Dirichlet data is prescribed. We
show the variational convergence of solutions to solutions of local weighted
Sobolev functionals in the event of vanishing nonlocality.",136,2412.16109v1,math.AP,math.AP,machine learning,2024-12-20,2024-12-23T21:06:23.909244
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.910241
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",machine learning,2024-12-20,2024-12-23T21:06:23.911238
Chiral phase-imaging meta-sensors,"Light waves possess multiple degrees of freedom besides intensity, including
phase and polarization, that often contain important information but require
complex and bulky systems for their measurement. Here we report a pair of
compact multifunctional photodetectors that can selectively measure the local
phase gradient of, respectively, the right and left circular-polarization
component of any incident wave. These devices employ a chiral pair of
integrated plasmonic metasurfaces to introduce a sharp dependence of
responsivity on local direction of propagation of the desired polarization
component. An order-of-magnitude polarization selectivity with respect to phase
gradient is demonstrated with both devices. Using the measured device
characteristics, we also describe computationally a pixel array that allows for
the simultaneous separate mapping of the right and left circularly-polarized
incident wavefronts in a particularly simple imaging setup. These unique
capabilities may be exploited to enable new functionalities for applications in
chemical sensing, biomedical microscopy, and machine vision.",201,2412.16084v1,physics.optics,physics.optics,machine learning,2024-12-20,2024-12-23T21:06:23.911238
Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation,"The increasing demand for privacy-preserving data analytics in finance
necessitates solutions for synthetic data generation that rigorously uphold
privacy standards. We introduce DP-Fed-FinDiff framework, a novel integration
of Differential Privacy, Federated Learning and Denoising Diffusion
Probabilistic Models designed to generate high-fidelity synthetic tabular data.
This framework ensures compliance with stringent privacy regulations while
maintaining data utility. We demonstrate the effectiveness of DP-Fed-FinDiff on
multiple real-world financial datasets, achieving significant improvements in
privacy guarantees without compromising data quality. Our empirical evaluations
reveal the optimal trade-offs between privacy budgets, client configurations,
and federated optimization strategies. The results affirm the potential of
DP-Fed-FinDiff to enable secure data sharing and robust analytics in highly
regulated domains, paving the way for further advances in federated learning
and privacy-preserving data synthesis.",186,2412.16083v1,cs.LG,"cs.LG,q-fin.ST",machine learning,2024-12-20,2024-12-23T21:06:23.912235
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",machine learning,2024-12-20,2024-12-23T21:06:23.913232
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",machine learning,2024-12-20,2024-12-23T21:06:23.913232
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",machine learning,2024-12-20,2024-12-23T21:06:23.914230
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",machine learning,2024-12-20,2024-12-23T21:06:23.915227
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,machine learning,2024-12-20,2024-12-23T21:06:23.916224
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.916224
Segmentation of arbitrary features in very high resolution remote sensing imagery,"Very high resolution (VHR) mapping through remote sensing (RS) imagery
presents a new opportunity to inform decision-making and sustainable practices
in countless domains. Efficient processing of big VHR data requires automated
tools applicable to numerous geographic regions and features. Contemporary RS
studies address this challenge by employing deep learning (DL) models for
specific datasets or features, which limits their applicability across
contexts.
  The present research aims to overcome this limitation by introducing
EcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.
EcoMapper fully automates processing of geospatial data, DL model training, and
inference. Models trained with EcoMapper successfully segmented two distinct
features in a real-world UAV dataset, achieving scores competitive with prior
studies which employed context-specific models.
  To evaluate EcoMapper, many additional models were trained on permutations of
principal field survey characteristics (FSCs). A relationship was discovered
allowing derivation of optimal ground sampling distance from feature size,
termed Cording Index (CI). A comprehensive methodology for field surveys was
developed to ensure DL methods can be applied effectively to collected data.
  The EcoMapper code accompanying this work is available at
https://github.com/hcording/ecomapper .",264,2412.16046v1,cs.CV,cs.CV,machine learning,2024-12-20,2024-12-23T21:06:23.917222
Designing Visual Explanations and Learner Controls to Engage Adolescents in AI-Supported Exercise Selection,"E-learning platforms that personalise content selection with AI are often
criticised for lacking transparency and controllability. Researchers have
therefore proposed solutions such as open learner models and letting learners
select from ranked recommendations, which engage learners before or after the
AI-supported selection process. However, little research has explored how
learners - especially adolescents - could engage during such AI-supported
decision-making. To address this open challenge, we iteratively designed and
implemented a control mechanism that enables learners to steer the difficulty
of AI-compiled exercise series before practice, while interactively analysing
their control's impact in a 'what-if' visualisation. We evaluated our
prototypes through four qualitative studies involving adolescents, teachers,
EdTech professionals, and pedagogical experts, focusing on different types of
visual explanations for recommendations. Our findings suggest that 'why'
explanations do not always meet the explainability needs of young learners but
can benefit teachers. Additionally, 'what-if' explanations were well-received
for their potential to boost motivation. Overall, our work illustrates how
combining learner control and visual explanations can be operationalised on
e-learning platforms for adolescents. Future research can build upon our
designs for 'why' and 'what-if' explanations and verify our preliminary
findings.",260,2412.16034v1,cs.HC,cs.HC,machine learning,2024-12-20,2024-12-23T21:06:23.918219
A Framework for Streaming Event-Log Prediction in Business Processes,"We present a Python-based framework for event-log prediction in streaming
mode, enabling predictions while data is being generated by a business process.
The framework allows for easy integration of streaming algorithms, including
language models like n-grams and LSTMs, and for combining these predictors
using ensemble methods.
  Using our framework, we conducted experiments on various well-known
process-mining data sets and compared classical batch with streaming mode.
Though, in batch mode, LSTMs generally achieve the best performance, there is
often an n-gram whose accuracy comes very close. Combining basic models in
ensemble methods can even outperform LSTMs. The value of basic models with
respect to LSTMs becomes even more apparent in streaming mode, where LSTMs
generally lack accuracy in the early stages of a prediction run, while basic
methods make sensible predictions immediately.",173,2412.16032v1,cs.AI,"cs.AI,cs.LG",machine learning,2024-12-20,2024-12-23T21:06:23.918219
Learning sparsity-promoting regularizers for linear inverse problems,"This paper introduces a novel approach to learning sparsity-promoting
regularizers for solving linear inverse problems. We develop a bilevel
optimization framework to select an optimal synthesis operator, denoted as $B$,
which regularizes the inverse problem while promoting sparsity in the solution.
The method leverages statistical properties of the underlying data and
incorporates prior knowledge through the choice of $B$. We establish the
well-posedness of the optimization problem, provide theoretical guarantees for
the learning process, and present sample complexity bounds. The approach is
demonstrated through examples, including compact perturbations of a known
operator and the problem of learning the mother wavelet, showcasing its
flexibility in incorporating prior knowledge into the regularization framework.
This work extends previous efforts in Tikhonov regularization by addressing
non-differentiable norms and proposing a data-driven approach for sparse
regularization in infinite dimensions.",181,2412.16031v1,stat.ML,"stat.ML,cs.LG,math.ST,stat.TH,65J22, 68T05",machine learning,2024-12-20,2024-12-23T21:06:23.919217
Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep Learning,"Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a
cornerstone to assessing the authenticity of the received information and
guaranteeing robust service delivery in several application domains. The
solutions available today for spoofing detection either rely on additional
communication systems, receivers, and antennas, or require mobile deployments.
Detection systems working at the Physical (PHY) layer of the satellite
communication link also require time-consuming and energy-hungry training
processes on all satellites of the constellation, and rely on the availability
of spoofed data, which are often challenging to collect. Moreover, none of such
contributions investigate the feasibility of aerial spoofing attacks launched
via drones operating at various altitudes. In this paper, we propose a new
spoofing detection technique for LEO satellite constellation systems, applying
anomaly detection on the received PHY signal via autoencoders. We validate our
solution through an extensive measurement campaign involving the deployment of
an actual spoofer (Software-Defined Radio) installed on a drone and injecting
rogue IRIDIUM messages while flying at different altitudes with various
movement patterns. Our results demonstrate that the proposed technique can
reliably detect LEO spoofing attacks launched at different altitudes, while
state-of-the-art competing approaches simply fail. We also release the
collected data as open source, fostering further research on satellite
security.",276,2412.16008v1,cs.CR,cs.CR,machine learning,2024-12-20,2024-12-23T21:06:23.920214
Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition,"Explaining machine learning (ML) models using eXplainable AI (XAI) techniques
has become essential to make them more transparent and trustworthy. This is
especially important in high-stakes domains like healthcare, where
understanding model decisions is critical to ensure ethical, sound, and
trustworthy outcome predictions. However, users are often confused about which
explanability method to choose for their specific use case. We present a
comparative analysis of widely used explainability methods, Shapley Additive
Explanations (SHAP) and Gradient-weighted Class Activation Mapping (GradCAM),
within the domain of human activity recognition (HAR) utilizing graph
convolutional networks (GCNs). By evaluating these methods on skeleton-based
data from two real-world datasets, including a healthcare-critical cerebral
palsy (CP) case, this study provides vital insights into both approaches'
strengths, limitations, and differences, offering a roadmap for selecting the
most appropriate explanation method based on specific models and applications.
We quantitatively and quantitatively compare these methods, focusing on feature
importance ranking, interpretability, and model sensitivity through
perturbation experiments. While SHAP provides detailed input feature
attribution, GradCAM delivers faster, spatially oriented explanations, making
both methods complementary depending on the application's requirements. Given
the importance of XAI in enhancing trust and transparency in ML models,
particularly in sensitive environments like healthcare, our research
demonstrates how SHAP and GradCAM could complement each other to provide more
interpretable and actionable model explanations.",315,2412.16003v1,cs.LG,"cs.LG,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.920214
CNN-LSTM Hybrid Deep Learning Model for Remaining Useful Life Estimation,"Remaining Useful Life (RUL) of a component or a system is defined as the
length from the current time to the end of the useful life. Accurate RUL
estimation plays a crucial role in Predictive Maintenance applications.
Traditional regression methods, both linear and non-linear, have struggled to
achieve high accuracy in this domain. While Convolutional Neural Networks
(CNNs) have shown improved accuracy, they often overlook the sequential nature
of the data, relying instead on features derived from sliding windows. Since
RUL prediction inherently involves multivariate time series analysis, robust
sequence learning is essential. In this work, we propose a hybrid approach
combining Convolutional Neural Networks with Long Short-Term Memory (LSTM)
networks for RUL estimation. Although CNN-based LSTM models have been applied
to sequence prediction tasks in financial forecasting, this is the first
attempt to adopt this approach for RUL estimation in prognostics. In this
approach, CNN is first employed to efficiently extract features from the data,
followed by LSTM, which uses these extracted features to predict RUL. This
method effectively leverages sensor sequence information, uncovering hidden
patterns within the data, even under multiple operating conditions and fault
scenarios. Our results demonstrate that the hybrid CNN-LSTM model achieves the
highest accuracy, offering a superior score compared to the other methods.",281,2412.15998v1,cs.LG,"cs.LG,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.921211
Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling,"Conversational assistants are increasingly popular across diverse real-world
applications, highlighting the need for advanced multimodal speech modeling.
Speech, as a natural mode of communication, encodes rich user-specific
characteristics such as speaking rate and pitch, making it critical for
effective interaction. Our work introduces a data-centric customization
approach for efficiently enhancing multimodal understanding in conversational
speech modeling. Central to our contributions is a novel multi-task learning
paradigm that involves designing auxiliary tasks to utilize a small amount of
speech data. Our approach achieves state-of-the-art performance on the
Spoken-SQuAD benchmark, using only 10% of the training data with open-weight
models, establishing a robust and efficient framework for audio-centric
conversational modeling. We also introduce ASK-QA, the first dataset for
multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation
inputs. Code and data forthcoming.",187,2412.15995v1,cs.CL,"cs.CL,cs.AI,cs.SD,eess.AS",machine learning,2024-12-20,2024-12-23T21:06:23.922208
APIRL: Deep Reinforcement Learning for REST API Fuzzing,"REST APIs have become key components of web services. However, they often
contain logic flaws resulting in server side errors or security
vulnerabilities. HTTP requests are used as test cases to find and mitigate such
issues. Existing methods to modify requests, including those using deep
learning, suffer from limited performance and precision, relying on undirected
search or making limited usage of the contextual information. In this paper we
propose APIRL, a fully automated deep reinforcement learning tool for testing
REST APIs. A key novelty of our approach is the use of feedback from a
transformer module pre-trained on JSON-structured data, akin to that used in
API responses. This allows APIRL to learn the subtleties relating to test
outcomes, and generalise to unseen API endpoints. We show APIRL can find
significantly more bugs than the state-of-the-art in real world REST APIs while
minimising the number of required test cases. We also study how reward
functions, and other key design choices, affect learnt policies in a thorough
ablation study.",218,2412.15991v1,cs.SE,"cs.SE,cs.AI,cs.NI",machine learning,2024-12-20,2024-12-23T21:06:23.922208
Never Reset Again: A Mathematical Framework for Continual Inference in Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are widely used for sequential processing
but face fundamental limitations with continual inference due to state
saturation, requiring disruptive hidden state resets. However, reset-based
methods impose synchronization requirements with input boundaries and increase
computational costs at inference. To address this, we propose an adaptive loss
function that eliminates the need for resets during inference while preserving
high accuracy over extended sequences. By combining cross-entropy and
Kullback-Leibler divergence, the loss dynamically modulates the gradient based
on input informativeness, allowing the network to differentiate meaningful data
from noise and maintain stable representations over time. Experimental results
demonstrate that our reset-free approach outperforms traditional reset-based
methods when applied to a variety of RNNs, particularly in continual tasks,
enhancing both the theoretical and practical capabilities of RNNs for streaming
applications.",178,2412.15983v1,cs.LG,"cs.LG,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.923206
iRadar: Synthesizing Millimeter-Waves from Wearable Inertial Inputs for Human Gesture Sensing,"Millimeter-wave (mmWave) radar-based gesture recognition is gaining attention
as a key technology to enable intuitive human-machine interaction.
Nevertheless, the significant challenge lies in obtaining large-scale,
high-quality mmWave gesture datasets. To tackle this problem, we present
iRadar, a novel cross-modal gesture recognition framework that employs Inertial
Measurement Unit (IMU) data to synthesize the radar signals generated by the
corresponding gestures. The key idea is to exploit the IMU signals, which are
commonly available in contemporary wearable devices, to synthesize the radar
signals that would be produced if the same gesture was performed in front of a
mmWave radar. However, several technical obstacles must be overcome due to the
differences between mmWave and IMU signals, the noisy gesture sensing of mmWave
radar, and the dynamics of human gestures. Firstly, we develop a method for
processing IMU and mmWave data to extract critical gesture features. Secondly,
we propose a diffusion-based IMU-to-radar translation model that accurately
transforms IMU data into mmWave data. Lastly, we devise a novel transformer
model to enhance gesture recognition performance. We thoroughly evaluate
iRadar, involving 18 gestures and 30 subjects in three scenarios, using five
wearable devices. Experimental results demonstrate that iRadar consistently
achieves 99.82% Top-3 accuracy across diverse scenarios.",284,2412.15980v1,cs.HC,cs.HC,machine learning,2024-12-20,2024-12-23T21:06:23.924203
MR-GDINO: Efficient Open-World Continual Object Detection,"Open-world (OW) recognition and detection models show strong zero- and
few-shot adaptation abilities, inspiring their use as initializations in
continual learning methods to improve performance. Despite promising results on
seen classes, such OW abilities on unseen classes are largely degenerated due
to catastrophic forgetting. To tackle this challenge, we propose an open-world
continual object detection task, requiring detectors to generalize to old, new,
and unseen categories in continual learning scenarios. Based on this task, we
present a challenging yet practical OW-COD benchmark to assess detection
abilities. The goal is to motivate OW detectors to simultaneously preserve
learned classes, adapt to new classes, and maintain open-world capabilities
under few-shot adaptations. To mitigate forgetting in unseen categories, we
propose MR-GDINO, a strong, efficient and scalable baseline via memory and
retrieval mechanisms within a highly scalable memory pool. Experimental results
show that existing continual detectors suffer from severe forgetting for both
seen and unseen categories. In contrast, MR-GDINO largely mitigates forgetting
with only 0.1% activated extra parameters, achieving state-of-the-art
performance for old, new, and unseen categories.",243,2412.15979v1,cs.CV,cs.CV,machine learning,2024-12-20,2024-12-23T21:06:23.924203
Active Flow Control for Bluff Body under High Reynolds Number Turbulent Flow Conditions Using Deep Reinforcement Learning,"This study employs Deep Reinforcement Learning (DRL) for active flow control
in a turbulent flow field of high Reynolds numbers at $Re=274000$. That is, an
agent is trained to obtain a control strategy that can reduce the drag of a
cylinder while also minimizing the oscillations of the lift. Probes are placed
only around the surface of the cylinder, and a Proximal Policy Optimization
(PPO) agent controls nine zero-net mass flux jets on the downstream side of the
cylinder. The trained PPO agent effectively reduces drag by $29\%$ and
decreases lift oscillations by $18\%$ of amplitude, with the control effect
demonstrating good repeatability. Control tests of this agent within the
Reynolds number range of $Re=260000$ to $288000$ show the agent's control
strategy possesses a certain degree of robustness, with very similar drag
reduction effects under different Reynolds numbers. Analysis using power
spectral energy reveals that the agent learns specific flow frequencies in the
flow field and effectively suppresses low-frequency, large-scale structures.
Graphically visualizing the policy, combined with pressure, vorticity, and
turbulent kinetic energy contours, reveals the mechanism by which jets achieve
drag reduction by influencing reattachment vortices. This study successfully
implements robust active flow control in realistically significant high
Reynolds number turbulent flows, minimizing time costs (using two-dimensional
geometrical models and turbulence models) and maximally considering the
feasibility of future experimental implementation.",315,2412.15975v1,physics.flu-dyn,physics.flu-dyn,machine learning,2024-12-20,2024-12-23T21:06:23.925200
actifpTM: a refined confidence metric of AlphaFold2 predictions involving flexible regions,"One of the main advantages of deep learning models of protein structure, such
as Alphafold2, is their ability to accurately estimate the confidence of a
generated structural model, which allows us to focus on highly confident
predictions.The ipTM score provides a confidence estimate of interchain
contacts in protein-protein interactions. However, interactions, in particular
motif-mediated interactions, often also contain regions that remain flexible
upon binding. These non-interacting flanking regions are assigned low
confidence values and will affect iPTM, as it considers all interchain residue
pairs, and two models of the same motif-domain interaction, but differing in
the length of their flanking regions, would be assigned very different values.
Here we propose actifpTM (actual interface pTM), a modified ipTM measure, that
focuses on the confident region of an interaction, resulting in a more robust
measure of interaction confidence, even when not the full interaction is
structured. actifpTM has been incorporated into ColabFold.",201,2412.15970v1,q-bio.QM,q-bio.QM,machine learning,2024-12-20,2024-12-23T21:06:23.926198
Self-Supervised Radiograph Anatomical Region Classification -- How Clean Is Your Real-World Data?,"Modern deep learning-based clinical imaging workflows rely on accurate labels
of the examined anatomical region. Knowing the anatomical region is required to
select applicable downstream models and to effectively generate cohorts of high
quality data for future medical and machine learning research efforts. However,
this information may not be available in externally sourced data or generally
contain data entry errors. To address this problem, we show the effectiveness
of self-supervised methods such as SimCLR and BYOL as well as supervised
contrastive deep learning methods in assigning one of 14 anatomical region
classes in our in-house dataset of 48,434 skeletal radiographs. We achieve a
strong linear evaluation accuracy of 96.6% with a single model and 97.7% using
an ensemble approach. Furthermore, only a few labeled instances (1% of the
training set) suffice to achieve an accuracy of 92.2%, enabling usage in
low-label and thus low-resource scenarios. Our model can be used to correct
data entry mistakes: a follow-up analysis of the test set errors of our
best-performing single model by an expert radiologist identified 35% incorrect
labels and 11% out-of-domain images. When accounted for, the radiograph
anatomical region labelling performance increased -- without and with an
ensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%.",282,2412.15967v1,cs.CV,"cs.CV,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.926198
Monkey Transfer Learning Can Improve Human Pose Estimation,"In this study, we investigated whether transfer learning from macaque monkeys
could improve human pose estimation. Current state-of-the-art pose estimation
techniques, often employing deep neural networks, can match human annotation in
non-clinical datasets. However, they underperform in novel situations, limiting
their generalisability to clinical populations with pathological movement
patterns. Clinical datasets are not widely available for AI training due to
ethical challenges and a lack of data collection. We observe that data from
other species may be able to bridge this gap by exposing the network to a
broader range of motion cues. We found that utilising data from other species
and undertaking transfer learning improved human pose estimation in terms of
precision and recall compared to the benchmark, which was trained on humans
only. Compared to the benchmark, fewer human training examples were needed for
the transfer learning approach (1,000 vs 19,185). These results suggest that
macaque pose estimation can improve human pose estimation in clinical
situations. Future work should further explore the utility of pose estimation
trained with monkey data in clinical populations.",226,2412.15966v1,cs.CV,cs.CV,machine learning,2024-12-20,2024-12-23T21:06:23.927195
What shall we learn from a future supernova?,"Core-collapse supernovae constitute a unique laboratory for particle physics
and astrophysics. They are powerful neutrino sources of all flavors, emitting
essentially all the gravitational binding energy through neutrinos, at the end
of their life. I will highlight how crucial is the observation of the next
core-collapse supernova and of the diffuse supernova neutrino background, whose
discovery might be imminent.",84,2412.15964v1,astro-ph.SR,"astro-ph.SR,astro-ph.HE,hep-ph",machine learning,2024-12-20,2024-12-23T21:06:23.927195
From General to Specific: Tailoring Large Language Models for Personalized Healthcare,"The rapid development of large language models (LLMs) has transformed many
industries, including healthcare. However, previous medical LLMs have largely
focused on leveraging general medical knowledge to provide responses, without
accounting for patient variability and lacking true personalization at the
individual level. To address this, we propose a novel method called
personalized medical language model (PMLM), which explores and optimizes
personalized LLMs through recommendation systems and reinforcement learning
(RL). Specifically, by utilizing self-informed and peer-informed
personalization, PMLM captures changes in behaviors and preferences to design
initial personalized prompts tailored to individual needs. We further refine
these initial personalized prompts through RL, ultimately enhancing the
precision of LLM guidance. Notably, the personalized prompt are hard prompt,
which grants PMLM high adaptability and reusability, allowing it to directly
leverage high-quality proprietary LLMs. We evaluate PMLM using real-world
obstetrics and gynecology data, and the experimental results demonstrate that
PMLM achieves personalized responses, and it provides more refined and
individualized services, offering a potential way for personalized medical
LLMs.",239,2412.15957v1,cs.CL,"cs.CL,cs.AI,cs.IR",machine learning,2024-12-20,2024-12-23T21:06:23.928192
Black-Box Uniform Stability for Non-Euclidean Empirical Risk Minimization,"We study first-order algorithms that are uniformly stable for empirical risk
minimization (ERM) problems that are convex and smooth with respect to
$p$-norms, $p \geq 1$. We propose a black-box reduction method that, by
employing properties of uniformly convex regularizers, turns an optimization
algorithm for H\""older smooth convex losses into a uniformly stable learning
algorithm with optimal statistical risk bounds on the excess risk, up to a
constant factor depending on $p$. Achieving a black-box reduction for uniform
stability was posed as an open question by (Attia and Koren, 2022), which had
solved the Euclidean case $p=2$. We explore applications that leverage
non-Euclidean geometry in addressing binary classification problems.",158,2412.15956v1,cs.LG,"cs.LG,math.OC,stat.ML,68Q32, 90C25,G.1.6; I.2.6",machine learning,2024-12-20,2024-12-23T21:06:23.928192
Mamba-based Deep Learning Approaches for Sleep Staging on a Wireless Multimodal Wearable System without Electroencephalography,"Study Objectives: We investigate using Mamba-based deep learning approaches
for sleep staging on signals from ANNE One (Sibel Health, Evanston, IL), a
minimally intrusive dual-sensor wireless wearable system measuring chest
electrocardiography (ECG), triaxial accelerometry, and temperature, as well as
finger photoplethysmography (PPG) and temperature.
  Methods: We obtained wearable sensor recordings from 360 adults undergoing
concurrent clinical polysomnography (PSG) at a tertiary care sleep lab. PSG
recordings were scored according to AASM criteria. PSG and wearable sensor data
were automatically aligned using their ECG channels with manual confirmation by
visual inspection. We trained Mamba-based models with both
convolutional-recurrent neural network (CRNN) and the recurrent neural network
(RNN) architectures on these recordings. Ensembling of model variants with
similar architectures was performed.
  Results: Our best approach, after ensembling, attains a 3-class (wake, NREM,
REM) balanced accuracy of 83.50%, F1 score of 84.16%, Cohen's $\kappa$ of
72.68%, and a MCC score of 72.84%; a 4-class (wake, N1/N2, N3, REM) balanced
accuracy of 74.64%, F1 score of 74.56%, Cohen's $\kappa$ of 61.63%, and MCC
score of 62.04%; a 5-class (wake, N1, N2, N3, REM) balanced accuracy of 64.30%,
F1 score of 66.97%, Cohen's $\kappa$ of 53.23%, MCC score of 54.38%.
  Conclusions: Deep learning models can infer major sleep stages from a
wearable system without electroencephalography (EEG) and can be successfully
applied to data from adults attending a tertiary care sleep clinic.",419,2412.15947v1,q-bio.QM,"q-bio.QM,cs.LG",machine learning,2024-12-20,2024-12-23T21:06:23.929190
pyRheo: An open-source Python package for complex rheology,"Mathematical modeling is a powerful tool in rheology, and we present pyRheo,
an open-source package for Python designed to streamline the analysis of creep,
stress relaxation, oscillation, and rotation tests. pyRheo contains a
comprehensive selection of viscoelastic models, including fractional order
approaches. It integrates model selection and fitting features and employs
machine intelligence to suggest a model to describe a given dataset. The
package fits the suggested model or one chosen by the user. An advantage of
using pyRheo is that it addresses challenges associated with sensitivity to
initial guesses in parameter optimization. It allows the user to iteratively
search for the best initial guesses, avoiding convergence to local minima. We
discuss the capabilities of pyRheo and compare them to other tools for
rheological modeling of biological matter. We demonstrate that pyRheo
significantly reduces the computation time required to fit high-performance
viscoelastic models.",198,2412.15941v1,cond-mat.soft,cond-mat.soft,machine learning,2024-12-20,2024-12-23T21:06:23.930187
RiTTA: Modeling Event Relations in Text-to-Audio Generation,"Despite significant advancements in Text-to-Audio (TTA) generation models
achieving high-fidelity audio with fine-grained context understanding, they
struggle to model the relations between audio events described in the input
text. However, previous TTA methods have not systematically explored audio
event relation modeling, nor have they proposed frameworks to enhance this
capability. In this work, we systematically study audio event relation modeling
in TTA generation models. We first establish a benchmark for this task by: 1.
proposing a comprehensive relation corpus covering all potential relations in
real-world scenarios; 2. introducing a new audio event corpus encompassing
commonly heard audios; and 3. proposing new evaluation metrics to assess audio
event relation modeling from various perspectives. Furthermore, we propose a
finetuning framework to enhance existing TTA models ability to model audio
events relation. Code is available at: https://github.com/yuhanghe01/RiTTA",193,2412.15922v1,cs.LG,"cs.LG,cs.SD,eess.AS",machine learning,2024-12-20,2024-12-23T21:06:23.930187
Data Preparation for Fairness-Performance Trade-Offs: A Practitioner-Friendly Alternative?,"As machine learning (ML) systems are increasingly adopted across industries,
addressing fairness and bias has become essential. While many solutions focus
on ethical challenges in ML, recent studies highlight that data itself is a
major source of bias. Pre-processing techniques, which mitigate bias before
training, are effective but may impact model performance and pose integration
difficulties. In contrast, fairness-aware Data Preparation practices are both
familiar to practitioners and easier to implement, providing a more accessible
approach to reducing bias. Objective. This registered report proposes an
empirical evaluation of how optimally selected fairness-aware practices,
applied in early ML lifecycle stages, can enhance both fairness and
performance, potentially outperforming standard pre-processing bias mitigation
methods. Method. To this end, we will introduce FATE, an optimization technique
for selecting 'Data Preparation' pipelines that optimize fairness and
performance. Using FATE, we will analyze the fairness-performance trade-off,
comparing pipelines selected by FATE with results by pre-processing bias
mitigation techniques.",209,2412.15920v1,cs.SE,"cs.SE,cs.LG",machine learning,2024-12-20,2024-12-23T21:06:23.931184
Self-supervised Spatial-Temporal Learner for Precipitation Nowcasting,"Nowcasting, the short-term prediction of weather, is essential for making
timely and weather-dependent decisions. Specifically, precipitation nowcasting
aims to predict precipitation at a local level within a 6-hour time frame. This
task can be framed as a spatial-temporal sequence forecasting problem, where
deep learning methods have been particularly effective. However, despite
advancements in self-supervised learning, most successful methods for
nowcasting remain fully supervised. Self-supervised learning is advantageous
for pretraining models to learn representations without requiring extensive
labeled data. In this work, we leverage the benefits of self-supervised
learning and integrate it with spatial-temporal learning to develop a novel
model, SpaT-SparK. SpaT-SparK comprises a CNN-based encoder-decoder structure
pretrained with a masked image modeling (MIM) task and a translation network
that captures temporal relationships among past and future precipitation maps
in downstream tasks. We conducted experiments on the NL-50 dataset to evaluate
the performance of SpaT-SparK. The results demonstrate that SpaT-SparK
outperforms existing baseline supervised models, such as SmaAt-UNet, providing
more accurate nowcasting predictions.",242,2412.15917v1,cs.LG,cs.LG,machine learning,2024-12-20,2024-12-23T21:06:23.932182
Kernel shape renormalization explains output-output correlations in finite Bayesian one-hidden-layer networks,"Finite-width one hidden layer networks with multiple neurons in the readout
layer display non-trivial output-output correlations that vanish in the
lazy-training infinite-width limit. In this manuscript we leverage recent
progress in the proportional limit of Bayesian deep learning (that is the limit
where the size of the training set $P$ and the width of the hidden layers $N$
are taken to infinity keeping their ratio $\alpha = P/N$ finite) to rationalize
this empirical evidence. In particular, we show that output-output correlations
in finite fully-connected networks are taken into account by a kernel shape
renormalization of the infinite-width NNGP kernel, which naturally arises in
the proportional limit. We perform accurate numerical experiments both to
assess the predictive power of the Bayesian framework in terms of
generalization, and to quantify output-output correlations in finite-width
networks. By quantitatively matching our predictions with the observed
correlations, we provide additional evidence that kernel shape renormalization
is instrumental to explain the phenomenology observed in finite Bayesian one
hidden layer networks.",228,2412.15911v1,cond-mat.dis-nn,cond-mat.dis-nn,machine learning,2024-12-20,2024-12-23T21:06:23.932182
CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR Sequences,"Neural distance fields (NDF) have emerged as a powerful tool for addressing
challenges in 3D computer vision and graphics downstream problems. While
significant progress has been made to learn NDF from various kind of sensor
data, a crucial aspect that demands attention is the supervision of neural
fields during training as the ground-truth NDFs are not available for
large-scale outdoor scenes. Previous works have utilized various forms of
expected signed distance to guide model learning. Yet, these approaches often
need to pay more attention to critical considerations of surface geometry and
are limited to small-scale implementations. To this end, we propose a novel
methodology leveraging second-order derivatives of the signed distance field
for improved neural field learning. Our approach addresses limitations by
accurately estimating signed distance, offering a more comprehensive
understanding of underlying geometry. To assess the efficacy of our
methodology, we conducted comparative evaluations against prevalent methods for
mapping and localization tasks, which are primary application areas of NDF. Our
results demonstrate the superiority of the proposed approach, highlighting its
potential for advancing the capabilities of neural distance fields in computer
vision and graphics applications.",222,2412.15909v1,cs.CV,"cs.CV,cs.GR",machine learning,2024-12-20,2024-12-23T21:06:23.933179
Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model,"Background: Recent advances in large language models highlight the need for
high-quality multilingual medical datasets. While Japan leads globally in CT
scanner deployment and utilization, the lack of large-scale Japanese radiology
datasets has hindered the development of specialized language models for
medical imaging analysis. Objective: To develop a comprehensive Japanese CT
report dataset through machine translation and establish a specialized language
model for structured finding classification. Additionally, to create a
rigorously validated evaluation dataset through expert radiologist review.
Methods: We translated the CT-RATE dataset (24,283 CT reports from 21,304
patients) into Japanese using GPT-4o mini. The training dataset consisted of
22,778 machine-translated reports, while the validation dataset included 150
radiologist-revised reports. We developed CT-BERT-JPN based on
""tohoku-nlp/bert-base-japanese-v3"" architecture for extracting 18 structured
findings from Japanese radiology reports. Results: Translation metrics showed
strong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores
ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression
sections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in
11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular
septal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1
scores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in
four conditions. Conclusions: Our study establishes a robust Japanese CT report
dataset and demonstrates the effectiveness of a specialized language model for
structured finding classification. The hybrid approach of machine translation
and expert validation enables the creation of large-scale medical datasets
while maintaining high quality.",398,2412.15907v1,cs.CL,"cs.CL,cs.AI",machine learning,2024-12-20,2024-12-23T21:06:23.934178
What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning,"Step-level reward models (SRMs) can significantly enhance mathematical
reasoning performance through process supervision or step-level preference
alignment based on reinforcement learning. The performance of SRMs is pivotal,
as they serve as critical guidelines, ensuring that each step in the reasoning
process is aligned with desired outcomes. Recently, AlphaZero-like methods,
where Monte Carlo Tree Search (MCTS) is employed for automatic step-level
preference annotation, have proven particularly effective. However, the precise
mechanisms behind the success of SRMs remain largely unexplored. To address
this gap, this study delves into the counterintuitive aspects of SRMs,
particularly focusing on MCTS-based approaches. Our findings reveal that the
removal of natural language descriptions of thought processes has minimal
impact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept
at assessing the complex logical coherence present in mathematical language
while having difficulty in natural language. These insights provide a nuanced
understanding of the core elements that drive effective step-level reward
modeling in mathematical reasoning. By shedding light on these mechanisms, this
study offers valuable guidance for developing more efficient and streamlined
SRMs, which can be achieved by focusing on the crucial parts of mathematical
reasoning.",253,2412.15904v1,cs.AI,"cs.AI,cs.LG",machine learning,2024-12-20,2024-12-23T21:06:23.935175
A Thorough Investigation into the Application of Deep CNN for Enhancing Natural Language Processing Capabilities,"Natural Language Processing (NLP) is widely used in fields like machine
translation and sentiment analysis. However, traditional NLP models struggle
with accuracy and efficiency. This paper introduces Deep Convolutional Neural
Networks (DCNN) into NLP to address these issues. By integrating DCNN, machine
learning (ML) algorithms, and generative adversarial networks (GAN), the study
improves language understanding, reduces ambiguity, and enhances task
performance. The high-performance NLP model shows a 10% improvement in
segmentation accuracy and a 4% increase in recall rate compared to traditional
models. This integrated approach excels in tasks such as word segmentation,
part-of-speech tagging, machine translation, and text classification, offering
better recognition accuracy and processing efficiency.",160,2412.15900v1,cs.CL,cs.CL,machine learning,2024-12-20,2024-12-23T21:06:23.935175
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.826436
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",computer vision,2024-12-20,2024-12-23T21:06:24.827433
Can Generative Video Models Help Pose Estimation?,"Pairwise pose estimation from images with little or no overlap is an open
challenge in computer vision. Existing methods, even those trained on
large-scale datasets, struggle in these scenarios due to the lack of
identifiable correspondences or visual overlap. Inspired by the human ability
to infer spatial relationships from diverse scenes, we propose a novel
approach, InterPose, that leverages the rich priors encoded within pre-trained
generative video models. We propose to use a video model to hallucinate
intermediate frames between two input images, effectively creating a dense,
visual transition, which significantly simplifies the problem of pose
estimation. Since current video models can still produce implausible motion or
inconsistent geometry, we introduce a self-consistency score that evaluates the
consistency of pose predictions from sampled videos. We demonstrate that our
approach generalizes among three state-of-the-art video models and show
consistent improvements over the state-of-the-art DUSt3R on four diverse
datasets encompassing indoor, outdoor, and object-centric scenes. Our findings
suggest a promising avenue for improving pose estimation models by leveraging
large generative models trained on vast amounts of video data, which is more
readily available than 3D data. See our project page for results:
https://inter-pose.github.io/.",271,2412.16155v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.827433
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",computer vision,2024-12-20,2024-12-23T21:06:24.828430
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.829427
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.829427
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset.",207,2412.16146v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.830425
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",computer vision,2024-12-20,2024-12-23T21:06:24.831422
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",computer vision,2024-12-20,2024-12-23T21:06:24.832419
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.832419
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",computer vision,2024-12-20,2024-12-23T21:06:24.833417
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",computer vision,2024-12-20,2024-12-23T21:06:24.834414
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",computer vision,2024-12-20,2024-12-23T21:06:24.834414
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",computer vision,2024-12-20,2024-12-23T21:06:24.835411
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",computer vision,2024-12-20,2024-12-23T21:06:24.836410
LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical Analysis of Diffeomorphism,"Image registration is a core task in computational anatomy that establishes
correspondences between images. Invertible deformable registration, which
computes a deformation field and handles complex, non-linear transformation, is
essential for tracking anatomical variations, especially in neuroimaging
applications where inter-subject differences and longitudinal changes are key.
Analyzing the deformation fields is challenging due to their non-linearity,
limiting statistical analysis. However, traditional approaches for analyzing
deformation fields are computationally expensive, sensitive to initialization,
and prone to numerical errors, especially when the deformation is far from the
identity. To address these limitations, we propose the Log-Euclidean
Diffeomorphic Autoencoder (LEDA), an innovative framework designed to compute
the principal logarithm of deformation fields by efficiently predicting
consecutive square roots. LEDA operates within a linearized latent space that
adheres to the diffeomorphisms group action laws, enhancing our model's
robustness and applicability. We also introduce a loss function to enforce
inverse consistency, ensuring accurate latent representations of deformation
fields. Extensive experiments with the OASIS-1 dataset demonstrate the
effectiveness of LEDA in accurately modeling and analyzing complex non-linear
deformations while maintaining inverse consistency. Additionally, we evaluate
its ability to capture and incorporate clinical variables, enhancing its
relevance for clinical applications.",271,2412.16129v1,cs.CV,"cs.CV,cs.LG",computer vision,2024-12-20,2024-12-23T21:06:24.837408
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",computer vision,2024-12-20,2024-12-23T21:06:24.838405
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,computer vision,2024-12-20,2024-12-23T21:06:24.839402
Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts,"This study investigates the potential of Large Language Models (LLMs),
particularly GPT-4o, for Optical Character Recognition (OCR) in low-resource
scripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.
Using a meticulously curated dataset of 2,520 images incorporating controlled
variations in text length, font size, background color, and blur, the research
simulates diverse real-world challenges. Results emphasize the limitations of
zero-shot LLM-based OCR, particularly for linguistically complex scripts,
highlighting the need for annotated datasets and fine-tuned models. This work
underscores the urgency of addressing accessibility gaps in text digitization,
paving the way for inclusive and robust OCR solutions for underserved
languages.",164,2412.16119v1,cs.LG,"cs.LG,cs.CV,eess.IV",computer vision,2024-12-20,2024-12-23T21:06:24.839402
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",computer vision,2024-12-20,2024-12-23T21:06:24.840399
PruneVid: Visual Token Pruning for Efficient Video Large Language Models,"In this paper, we introduce PruneVid, a visual token pruning method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing pruning methods. Code:
https://github.com/Visual-AI/PruneVid.",204,2412.16117v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.841398
The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse,"There is an ongoing debate about how to moderate toxic speech on social media
and how content moderation affects online discourse. We propose and validate a
methodology for measuring the content-moderation-induced distortions in online
discourse using text embeddings from computational linguistics. We test our
measure on a representative dataset of 5 million US political Tweets and find
that removing toxic Tweets distorts online content. This finding is consistent
across different embedding models, toxicity metrics, and samples. Importantly,
we demonstrate that content-moderation-induced distortions are not caused by
the toxic language. Instead, we show that, as a side effect, content moderation
shifts the mean and variance of the embedding space, distorting the topic
composition of online content. Finally, we propose an alternative approach to
content moderation that uses generative Large Language Models to rephrase toxic
Tweets to preserve their salvageable content rather than removing them
entirely. We demonstrate that this rephrasing strategy reduces toxicity while
minimizing distortions in online content.",220,2412.16114v1,cs.SI,cs.SI,computer vision,2024-12-20,2024-12-23T21:06:24.841398
CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up,"Diffusion Transformers (DiT) have become a leading architecture in image
generation. However, the quadratic complexity of attention mechanisms, which
are responsible for modeling token-wise relationships, results in significant
latency when generating high-resolution images. To address this issue, we aim
at a linear attention mechanism in this paper that reduces the complexity of
pre-trained DiTs to linear. We begin our exploration with a comprehensive
summary of existing efficient attention mechanisms and identify four key
factors crucial for successful linearization of pre-trained DiTs: locality,
formulation consistency, high-rank attention maps, and feature integrity. Based
on these insights, we introduce a convolution-like local attention strategy
termed CLEAR, which limits feature interactions to a local window around each
query token, and thus achieves linear complexity. Our experiments indicate
that, by fine-tuning the attention layer on merely 10K self-generated samples
for 10K iterations, we can effectively transfer knowledge from a pre-trained
DiT to a student model with linear complexity, yielding results comparable to
the teacher model. Simultaneously, it reduces attention computations by 99.5%
and accelerates generation by 6.3 times for generating 8K-resolution images.
Furthermore, we investigate favorable properties in the distilled attention
layers, such as zero-shot generalization cross various models and plugins, and
improved support for multi-GPU parallel inference. Models and codes are
available here: https://github.com/Huage001/CLEAR.",308,2412.16112v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.842394
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",computer vision,2024-12-20,2024-12-23T21:06:24.843391
Logical Consistency of Large Language Models in Fact-checking,"In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses -- a meaning-preserving change in the input query
results in an inconsistent response and attributes to vulnerabilities of LLMs
such as hallucination, jailbreaking, etc. Consequently, existing research
focuses on simple paraphrasing-based consistency assessment of LLMs, and
ignores complex queries that necessitates an even better understanding of
logical reasoning by an LLM. Our work therefore addresses the logical
inconsistency of LLMs under complex logical queries with primitive logical
operators, e.g., negation, conjunction, and disjunction. As a test bed, we
consider retrieval-augmented LLMs on a fact-checking task involving
propositional logic queries from real-world knowledge graphs (KGs). Our
contributions are three-fold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries as input and demonstrate that existing LLMs lack
logical consistency, specially on complex queries. Improvement: We employ
supervised fine-tuning to improve the logical consistency of LLMs on the
complex fact-checking task with KG contexts.",309,2412.16100v1,cs.CL,cs.CL,computer vision,2024-12-20,2024-12-23T21:06:24.844388
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",computer vision,2024-12-20,2024-12-23T21:06:24.844388
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",computer vision,2024-12-20,2024-12-23T21:06:24.845386
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,computer vision,2024-12-20,2024-12-23T21:06:24.846383
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",computer vision,2024-12-20,2024-12-23T21:06:24.846383
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,computer vision,2024-12-20,2024-12-23T21:06:24.847380
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,computer vision,2024-12-20,2024-12-23T21:06:24.848378
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",computer vision,2024-12-20,2024-12-23T21:06:24.848378
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",computer vision,2024-12-20,2024-12-23T21:06:24.849375
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",computer vision,2024-12-20,2024-12-23T21:06:24.849375
Chiral phase-imaging meta-sensors,"Light waves possess multiple degrees of freedom besides intensity, including
phase and polarization, that often contain important information but require
complex and bulky systems for their measurement. Here we report a pair of
compact multifunctional photodetectors that can selectively measure the local
phase gradient of, respectively, the right and left circular-polarization
component of any incident wave. These devices employ a chiral pair of
integrated plasmonic metasurfaces to introduce a sharp dependence of
responsivity on local direction of propagation of the desired polarization
component. An order-of-magnitude polarization selectivity with respect to phase
gradient is demonstrated with both devices. Using the measured device
characteristics, we also describe computationally a pixel array that allows for
the simultaneous separate mapping of the right and left circularly-polarized
incident wavefronts in a particularly simple imaging setup. These unique
capabilities may be exploited to enable new functionalities for applications in
chemical sensing, biomedical microscopy, and machine vision.",201,2412.16084v1,physics.optics,physics.optics,computer vision,2024-12-20,2024-12-23T21:06:24.850880
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",computer vision,2024-12-20,2024-12-23T21:06:24.850880
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",computer vision,2024-12-20,2024-12-23T21:06:24.851878
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,computer vision,2024-12-20,2024-12-23T21:06:24.852876
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,computer vision,2024-12-20,2024-12-23T21:06:24.852876
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,computer vision,2024-12-20,2024-12-23T21:06:24.853873
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",computer vision,2024-12-20,2024-12-23T21:06:24.854870
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",computer vision,2024-12-20,2024-12-23T21:06:24.855867
Correct implied volatility shapes and reliable pricing in the rough Heston model,"We use modifications of the Adams method and very fast and accurate
sinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and
Levendorski\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for
options of moderate and long maturities and strikes not very far from the spot,
thousands of prices can be calculated in several msec. with relative errors of
the order of 0.5\% and smaller running Matlab on a Mac with moderate
characteristics. We demonstrate that for the calibrated set of parameters in
Euch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility
surface is significantly flatter and fits the data very poorly, hence, the
calibration results in op.cit. is an example of the {\em ghost calibration}
(M.Boyarchenko and Levendorki\u{i}, Quantitative Finance 2015, v. 15): the
errors of the model and numerical method almost cancel one another. We explain
how calibration errors of this sort are generated by each of popular versions
of numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)
with prefixed parameters of a numerical method, resulting in spurious
volatility smiles and skews. We suggest a general {\em Conformal Bootstrap
principle} which allows one to avoid ghost calibration errors. We outline
schemes of application of Conformal Bootstrap principle and the method of the
paper to the design of accurate and fast calibration procedures.",339,2412.16067v1,q-fin.MF,"q-fin.MF,q-fin.CP,60-08, 60E10, 60G10, 60G22, 65C20, 65D30, 65G20, 91G20, 91G60",computer vision,2024-12-20,2024-12-23T21:06:24.856865
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",computer vision,2024-12-20,2024-12-23T21:06:24.857862
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",computer vision,2024-12-20,2024-12-23T21:06:24.857862
Adaptable TeaStore,"Adaptability is a fundamental requirement for modern Cloud software
architectures to ensure robust performance in the face of diverse known and
unforeseen events inherent to distributed systems. State-of-the-art Cloud
systems frequently adopt microservices or serverless architectures. Among
these, TeaStore is a recognised microservice reference architecture that offers
a benchmarking framework for modelling and resource management techniques.
However, TeaStore's original configuration lacks the flexibility necessary to
address the varied scenarios encountered in real-world applications. To
overcome this limitation, we propose an enhanced variant of TeaStore that
distinguishes between mandatory and optional services while incorporating
third-party service integration. Core services such as WebUI, Image Provider,
and Persistence are designated as mandatory to maintain essential
functionality, whereas optional services, such as Recommender and Auth, extend
the architecture's feature set. We outline the design and configuration
possibilities of this adaptable TeaStore variant, aimed at enabling a broader
spectrum of configurability and operational resilience.",213,2412.16060v1,cs.DC,cs.DC,computer vision,2024-12-20,2024-12-23T21:06:24.858859
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",computer vision,2024-12-20,2024-12-23T21:06:24.859857
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,computer vision,2024-12-20,2024-12-23T21:06:24.860854
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",computer vision,2024-12-20,2024-12-23T21:06:24.860854
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",computer vision,2024-12-20,2024-12-23T21:06:24.861851
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:25.285789
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",natural language processing,2024-12-20,2024-12-23T21:06:28.630383
A vector logic for extensional formal semantics,"This paper proves a homomorphism between extensional formal semantics and
distributional vector space semantics, demonstrating structural compatibility.
Formal semantics models meaning as reference, using logical structures to map
linguistic expressions to truth conditions, while distributional semantics
represents meaning through word vectors derived from contextual usage. By
constructing injective mappings that preserve semantic relationships, we show
that every semantic function in an extensional model corresponds to a
compatible vector space operation. This result respects compositionality and
extends to function compositions, constant interpretations, and $n$-ary
relations. Rather than pursuing unification, we highlight a mathematical
foundation for hybrid cognitive models that integrate symbolic and sub-symbolic
reasoning and semantics. These findings support multimodal language processing,
aligning `meaning as reference' (Frege, Tarski) with `meaning as use'
(Wittgenstein, Firth).",174,2412.16152v1,math.LO,"math.LO,03C55, 03B38, 91F20, 68T50, 03B65,F.3.2; F.4.1",natural language processing,2024-12-20,2024-12-23T21:06:28.630383
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:28.631381
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:28.632378
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset.",207,2412.16146v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:28.632378
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",natural language processing,2024-12-20,2024-12-23T21:06:32.107566
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",natural language processing,2024-12-20,2024-12-23T21:06:32.109561
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",natural language processing,2024-12-20,2024-12-23T21:06:32.109561
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",natural language processing,2024-12-20,2024-12-23T21:06:32.110558
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",natural language processing,2024-12-20,2024-12-23T21:06:39.602253
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",natural language processing,2024-12-20,2024-12-23T21:06:39.603250
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",natural language processing,2024-12-20,2024-12-23T21:06:39.603250
A Scenario-Based Assessment of the Long-Term Funding Adequacy of the German Nuclear Waste Fund KENFO,"The German site selection process for high-level nuclear waste was initially
planned to conclude in 2031, with the deep geological repository sealed around
the year 2080. However, in 2022, substantial delays were announced by the
responsible federal agency, pushing the site selection to 2046 or even 2068.
With this delay come uncertainties regarding the duration, consequential
knowledge management, and funding. German nuclear waste management activities
are funded by the external segregated fund KENFO, which is designed to ensure
sufficient funding via generating returns on investments (ROI) in the coming
decades. Given recent developments, we assess the adequacy of the fund volume
based on seven scenarios depicting potential process delays. We find that the
target ROI of 3.7% will not suffice in any case, even if the site selection
concludes in 2031, and that cash injections of up to EUR31.07 billion are
necessary today to ensure that the fund volume will suffice. We conclude that
cost estimations must be updated, KENFO must increase its target ROIs,
potential capital injections must be openly discussed by policymakers, and a
procedural acceleration should be implemented to ensure that financial
liabilities for nuclear waste management are minimized for future taxpayers.",254,2412.16126v1,econ.GN,"econ.GN,q-fin.EC",natural language processing,2024-12-20,2024-12-23T21:06:39.604248
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",natural language processing,2024-12-20,2024-12-23T21:06:39.605245
Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli,"As humans perceive and actively engage with the world, we adjust our
decisions in response to shifting group dynamics and are influenced by social
interactions. This study aims to identify which aspects of interaction affect
cooperation-defection choices. Specifically, we investigate human cooperation
within the Prisoner's Dilemma game, using the Drift-Diffusion Model to describe
the decision-making process. We introduce a novel Bayesian model for the
evolution of the model's parameters based on the nature of interactions
experienced with other players. This approach enables us to predict the
evolution of the population's expected cooperation rate. We successfully
validate our model using an unseen test dataset and apply it to explore three
strategic scenarios: co-player manipulation, use of rewards and punishments,
and time pressure. These results support the potential of our model as a
foundational tool for developing and testing strategies aimed at enhancing
cooperation, ultimately contributing to societal welfare.",182,2412.16121v1,physics.soc-ph,physics.soc-ph,natural language processing,2024-12-20,2024-12-23T21:06:39.606242
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,natural language processing,2024-12-20,2024-12-23T21:06:39.606242
Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts,"This study investigates the potential of Large Language Models (LLMs),
particularly GPT-4o, for Optical Character Recognition (OCR) in low-resource
scripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.
Using a meticulously curated dataset of 2,520 images incorporating controlled
variations in text length, font size, background color, and blur, the research
simulates diverse real-world challenges. Results emphasize the limitations of
zero-shot LLM-based OCR, particularly for linguistically complex scripts,
highlighting the need for annotated datasets and fine-tuned models. This work
underscores the urgency of addressing accessibility gaps in text digitization,
paving the way for inclusive and robust OCR solutions for underserved
languages.",164,2412.16119v1,cs.LG,"cs.LG,cs.CV,eess.IV",natural language processing,2024-12-20,2024-12-23T21:06:39.607241
PruneVid: Visual Token Pruning for Efficient Video Large Language Models,"In this paper, we introduce PruneVid, a visual token pruning method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing pruning methods. Code:
https://github.com/Visual-AI/PruneVid.",204,2412.16117v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:39.607241
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",natural language processing,2024-12-20,2024-12-23T21:06:39.608238
The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse,"There is an ongoing debate about how to moderate toxic speech on social media
and how content moderation affects online discourse. We propose and validate a
methodology for measuring the content-moderation-induced distortions in online
discourse using text embeddings from computational linguistics. We test our
measure on a representative dataset of 5 million US political Tweets and find
that removing toxic Tweets distorts online content. This finding is consistent
across different embedding models, toxicity metrics, and samples. Importantly,
we demonstrate that content-moderation-induced distortions are not caused by
the toxic language. Instead, we show that, as a side effect, content moderation
shifts the mean and variance of the embedding space, distorting the topic
composition of online content. Finally, we propose an alternative approach to
content moderation that uses generative Large Language Models to rephrase toxic
Tweets to preserve their salvageable content rather than removing them
entirely. We demonstrate that this rephrasing strategy reduces toxicity while
minimizing distortions in online content.",220,2412.16114v1,cs.SI,cs.SI,natural language processing,2024-12-20,2024-12-23T21:06:39.609235
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",natural language processing,2024-12-20,2024-12-23T21:06:39.610234
Interleaved Speech-Text Language Models are Simple Streaming Text to Speech Synthesizers,"This paper introduces Interleaved Speech-Text Language Model (IST-LM) for
streaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,
IST-LM is directly trained on interleaved sequences of text and speech tokens
with a fixed ratio, eliminating the need for additional efforts in duration
prediction and grapheme-to-phoneme alignment. The ratio of text chunk size to
speech chunk size is crucial for the performance of IST-LM. To explore this, we
conducted a comprehensive series of statistical analyses on the training data
and performed correlation analysis with the final performance, uncovering
several key factors: 1) the distance between speech tokens and their
corresponding text tokens, 2) the number of future text tokens accessible to
each speech token, and 3) the frequency of speech tokens precedes their
corresponding text tokens. Experimental results demonstrate how to achieve an
optimal streaming TTS system without complicated engineering, which we show has
a limited gap with the non-streaming system. IST-LM is conceptually simple and
empirically powerful, paving the way for streaming TTS with minimal overhead
while largely maintaining performance.",239,2412.16102v1,eess.AS,eess.AS,natural language processing,2024-12-20,2024-12-23T21:06:39.611230
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",natural language processing,2024-12-20,2024-12-23T21:06:39.611230
Logical Consistency of Large Language Models in Fact-checking,"In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses -- a meaning-preserving change in the input query
results in an inconsistent response and attributes to vulnerabilities of LLMs
such as hallucination, jailbreaking, etc. Consequently, existing research
focuses on simple paraphrasing-based consistency assessment of LLMs, and
ignores complex queries that necessitates an even better understanding of
logical reasoning by an LLM. Our work therefore addresses the logical
inconsistency of LLMs under complex logical queries with primitive logical
operators, e.g., negation, conjunction, and disjunction. As a test bed, we
consider retrieval-augmented LLMs on a fact-checking task involving
propositional logic queries from real-world knowledge graphs (KGs). Our
contributions are three-fold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries as input and demonstrate that existing LLMs lack
logical consistency, specially on complex queries. Improvement: We employ
supervised fine-tuning to improve the logical consistency of LLMs on the
complex fact-checking task with KG contexts.",309,2412.16100v1,cs.CL,cs.CL,natural language processing,2024-12-20,2024-12-23T21:06:39.612227
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",natural language processing,2024-12-20,2024-12-23T21:06:39.613224
Dual-Polarized Beyond Diagonal RIS,"Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of
RIS architectures more flexible than conventional RIS. While BD-RIS has been
primarily analyzed assuming uni-polarized systems, modern wireless deployments
are dual-polarized. To address this gap, this paper investigates the
fundamental limits of dual-polarized BD-RIS-aided systems. We derive the
scaling laws governing the performance of BD-RIS and the Pareto frontier of the
trade-off between performance and circuit complexity enabled by BD-RIS.
Theoretical results show that the group-connected RIS with group size 2
provides remarkable gains over conventional RIS in both Rayleigh and
line-of-sight (LoS) channels, while maintaining a reduced circuit complexity.",166,2412.16097v1,cs.IT,"cs.IT,eess.SP,math.IT",natural language processing,2024-12-20,2024-12-23T21:06:39.614221
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,natural language processing,2024-12-20,2024-12-23T21:06:39.614221
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,natural language processing,2024-12-20,2024-12-23T21:06:39.615218
The Evolution of LLM Adoption in Industry Data Curation Practices,"As large language models (LLMs) grow increasingly adept at processing
unstructured text data, they offer new opportunities to enhance data curation
workflows. This paper explores the evolution of LLM adoption among
practitioners at a large technology company, evaluating the impact of LLMs in
data curation tasks through participants' perceptions, integration strategies,
and reported usage scenarios. Through a series of surveys, interviews, and user
studies, we provide a timely snapshot of how organizations are navigating a
pivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess
LLM adoption in industry for development tasks (N=84), and facilitated expert
interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we
explored practitioners' current and anticipated LLM usage through a user study
involving two LLM-based prototypes (N=12). While each study addressed distinct
research goals, they revealed a broader narrative about evolving LLM usage in
aggregate. We discovered an emerging shift in data understanding from
heuristic-first, bottom-up approaches to insights-first, top-down workflows
supported by LLMs. Furthermore, to respond to a more complex data landscape,
data practitioners now supplement traditional subject-expert-created 'golden
datasets' with LLM-generated 'silver' datasets and rigorously validated 'super
golden' datasets curated by diverse experts. This research sheds light on the
transformative role of LLMs in large-scale analysis of unstructured data and
highlights opportunities for further tool development.",328,2412.16089v1,cs.HC,"cs.HC,cs.AI",natural language processing,2024-12-20,2024-12-23T21:06:39.615218
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",natural language processing,2024-12-20,2024-12-23T21:06:39.616216
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",natural language processing,2024-12-20,2024-12-23T21:06:39.617213
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",natural language processing,2024-12-20,2024-12-23T21:06:39.617213
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,natural language processing,2024-12-20,2024-12-23T21:06:39.618210
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",natural language processing,2024-12-20,2024-12-23T21:06:39.619208
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",natural language processing,2024-12-20,2024-12-23T21:06:39.620205
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",natural language processing,2024-12-20,2024-12-23T21:06:39.620205
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",natural language processing,2024-12-20,2024-12-23T21:06:39.621202
Self-organized critical characteristics of TeV-photons from GRB 221009A,"The very high-energy afterglow in GRB 221009A, known as the `Brightest Of All
Time' (B.O.A.T.), has been thoroughly analyzed in previous studies. In this
paper, we conducted a statistical analysis of the waiting time behavior of 172
TeV photons from the B.O.A.T. observed by LHAASO-KM2A. The following results
were obtained: (I) The waiting time distribution (WTD) of these photons
deviates from the exponential distribution. (II) The behavior of these photons
exhibits characteristics resembling those of a self-organized critical system,
such as power-law distribution and scale-invariance features in the waiting
time distribution. The power-law distribution of waiting times is consistent
with the prediction of a non-stationary process. (III) The relationship between
the power-law slopes of the WTD and the scale-invariant characteristics of the
Tsallis q-Gaussian distribution deviates from existing theory. We suggest that
this deviation is due to the photons not being completely independent of each
other. In summary, the power-law and scale-free characteristics observed in
these photons imply a self-organized critical process in the generation of TeV
photons from GRB 221009A. Based on other relevant research, we propose that the
involvement of a partially magnetically dominated component and the continuous
energy injection from the central engine can lead to deviations in the
generation of TeV afterglow from the simple external shock-dominated process,
thereby exhibiting the self-organized critical characteristics mentioned above.",335,2412.16052v1,astro-ph.HE,astro-ph.HE,natural language processing,2024-12-20,2024-12-23T21:06:39.622200
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",natural language processing,2024-12-20,2024-12-23T21:06:39.623198
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",natural language processing,2024-12-20,2024-12-23T21:06:39.624194
Rare multi-nucleon decays with the full data sets of the Majorana Demonstrator,"The Majorana Demonstrator was an ultra-low-background experiment designed for
neutrinoless double-beta decay ($0\nu\beta\beta$) investigation in $^{76}$Ge.
Located at the Sanford Underground Research Facility in Lead, South Dakota, the
Demonstrator utilized modular high-purity Ge detector arrays within shielded
vacuum cryostats, operating deep underground. The arrays, with a capacity of up
to 40.4 kg (27.2 kg enriched to $\sim 88\%$ in $^{76}$Ge), have accumulated the
full data set, totaling 64.5 kg yr of enriched active exposure and 27.4 kg yr
of exposure for natural detectors. Our updated search improves previously
explored three-nucleon decay modes in Ge isotopes, setting new half-life limits
of $1.27\times10^{26}$ years (90\% confidence level) for $^{76}$Ge($ppp$)
$\rightarrow$ $^{73}$Cu e$^+\pi^+\pi^+$ and $^{76}$Ge($ppn$) $\rightarrow$
$^{73}$Zn e$^+\pi^+$. The half-life limit for the invisible tri-proton decay
mode of $^{76}$Ge is found to be $1.4\times10^{25}$ yr. Furthermore, we have
updated limits for corresponding multi-nucleon decays.",330,2412.16047v1,nucl-ex,nucl-ex,natural language processing,2024-12-20,2024-12-23T21:06:39.625192
Segmentation of arbitrary features in very high resolution remote sensing imagery,"Very high resolution (VHR) mapping through remote sensing (RS) imagery
presents a new opportunity to inform decision-making and sustainable practices
in countless domains. Efficient processing of big VHR data requires automated
tools applicable to numerous geographic regions and features. Contemporary RS
studies address this challenge by employing deep learning (DL) models for
specific datasets or features, which limits their applicability across
contexts.
  The present research aims to overcome this limitation by introducing
EcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.
EcoMapper fully automates processing of geospatial data, DL model training, and
inference. Models trained with EcoMapper successfully segmented two distinct
features in a real-world UAV dataset, achieving scores competitive with prior
studies which employed context-specific models.
  To evaluate EcoMapper, many additional models were trained on permutations of
principal field survey characteristics (FSCs). A relationship was discovered
allowing derivation of optimal ground sampling distance from feature size,
termed Cording Index (CI). A comprehensive methodology for field surveys was
developed to ensure DL methods can be applied effectively to collected data.
  The EcoMapper code accompanying this work is available at
https://github.com/hcording/ecomapper .",264,2412.16046v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:39.626191
Integrability versus chaos in the steady state of many-body open quantum systems,"The Lindblad description of an open quantum system gives rise to two types of
integrability, since the nonequilibrium steady state can be integrable
independently of the Liouvillian. Taking boundary-driven and dephasing spin
chains as a representative example, we discriminate Liouvillian and
steady-state chaos by combining level spacing statistics and an extension of
the eigenstate thermalization hypothesis to open quantum systems. Moreover, we
analyze the structure of the steady states by expanding it in the basis of
Pauli strings and comparing the weight of strings of different lengths. We show
that the natural expectation that integrable steady states are ""simple"" (i.e.,
built from few-body local operators) does not hold: the steady states of both
chaotic and integrable models have relevant contributions coming from Pauli
strings of all possible lengths, including long-range and many-body
interactions. Nevertheless, we show that one can effectively use the
operator-size distribution to distinguish chaotic and integrable steady states.",217,2412.16041v1,cond-mat.stat-mech,"cond-mat.stat-mech,cond-mat.mes-hall,nlin.CD,quant-ph",natural language processing,2024-12-20,2024-12-23T21:06:39.626191
SafeCFG: Redirecting Harmful Classifier-Free Guidance for Safe Generation,"Diffusion models (DMs) have demonstrated exceptional performance in
text-to-image (T2I) tasks, leading to their widespread use. With the
introduction of classifier-free guidance (CFG), the quality of images generated
by DMs is improved. However, DMs can generate more harmful images by
maliciously guiding the image generation process through CFG. Some safe
guidance methods aim to mitigate the risk of generating harmful images but
often reduce the quality of clean image generation. To address this issue, we
introduce the Harmful Guidance Redirector (HGR), which redirects harmful CFG
direction while preserving clean CFG direction during image generation,
transforming CFG into SafeCFG and achieving high safety and quality generation.
We train HGR to redirect multiple harmful CFG directions simultaneously,
demonstrating its ability to eliminate various harmful elements while
preserving high-quality generation. Additionally, we find that HGR can detect
image harmfulness, allowing for unsupervised fine-tuning of safe diffusion
models without pre-defined clean or harmful labels. Experimental results show
that by incorporating HGR, images generated by diffusion models achieve both
high quality and strong safety, and safe DMs trained through unsupervised
methods according to the harmfulness detected by HGR also exhibit good safety
performance. The codes will be publicly available.",275,2412.16039v1,cs.CV,cs.CV,natural language processing,2024-12-20,2024-12-23T21:06:39.627187
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",natural language processing,2024-12-20,2024-12-23T21:06:39.628184
X-ray polarization of the magnetar 1E 1841-045 in outburst,"We report on IXPE and NuSTAR observations that began forty days following the
onset of the 2024 outburst of the magnetar 1E 1841-045, marking the first ever
IXPE observation of a magnetar in an enhanced state. Our spectropolarimetric
analysis indicates that a non-thermal double power-law (PL) spectral model can
fit the phase-averaged intensity data well, with the soft and hard components
dominating below and above around 5 keV, respectively. We find that the soft PL
exhibits a polarization degree (PD) of about 20% while the hard X-ray PL
displays a PD of about 50%; both components have a polarization angle (PA)
compatible with 0 degree. These results are supported through model-independent
polarization analysis which shows an increasing PD from about 15% to 70% in the
2-3 keV and 6-8 keV ranges, respectively, while the PA remains consistent with
0 degree. We find marginal evidence for variability in the polarization
properties with pulse phase, namely a higher PD at spin phases coinciding with
the peak in the hard X-ray pulse. We compare the hard X-ray PL to the
expectation from direct resonant inverse Compton scattering (RICS) and
secondary pair cascade synchrotron radiation from primary high-energy RICS
photons, finding that both can provide reasonable spectropolarimetric agreement
with the data, yet, the latter more naturally. Finally, we suggest that the
soft power law X-ray component may be emission emanating from a Comptonized
corona in the inner magnetosphere.",327,2412.16036v1,astro-ph.HE,astro-ph.HE,natural language processing,2024-12-20,2024-12-23T21:06:39.629181
A moment approach for the convergence of spatial branching processes to the Continuum Random Tree,"We consider a general class of branching processes in discrete time, where
particles have types belonging to a Polish space and reproduce independently
according to their type. If the process is critical and the mean distribution
of types converges for large times, we prove that the tree structure of the
process converges to the Brownian Continuum Random Tree, under a moment
assumption. We provide a general approach to prove similar invariance
principles for branching processes, which relies on deducing the convergence of
the genealogy from computing its moments. These are obtained using a new
many-to-few formula, which provides an expression for the moments of order $k$
of a branching process in terms of a Markov chain indexed by a uniform tree
with $k$ leaves.",151,2412.16035v1,math.PR,math.PR,natural language processing,2024-12-20,2024-12-23T21:06:39.630178
Designing Visual Explanations and Learner Controls to Engage Adolescents in AI-Supported Exercise Selection,"E-learning platforms that personalise content selection with AI are often
criticised for lacking transparency and controllability. Researchers have
therefore proposed solutions such as open learner models and letting learners
select from ranked recommendations, which engage learners before or after the
AI-supported selection process. However, little research has explored how
learners - especially adolescents - could engage during such AI-supported
decision-making. To address this open challenge, we iteratively designed and
implemented a control mechanism that enables learners to steer the difficulty
of AI-compiled exercise series before practice, while interactively analysing
their control's impact in a 'what-if' visualisation. We evaluated our
prototypes through four qualitative studies involving adolescents, teachers,
EdTech professionals, and pedagogical experts, focusing on different types of
visual explanations for recommendations. Our findings suggest that 'why'
explanations do not always meet the explainability needs of young learners but
can benefit teachers. Additionally, 'what-if' explanations were well-received
for their potential to boost motivation. Overall, our work illustrates how
combining learner control and visual explanations can be operationalised on
e-learning platforms for adolescents. Future research can build upon our
designs for 'why' and 'what-if' explanations and verify our preliminary
findings.",260,2412.16034v1,cs.HC,cs.HC,natural language processing,2024-12-20,2024-12-23T21:06:39.630178
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",natural language processing,2024-12-20,2024-12-23T21:06:39.631176
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,cybersecurity,2024-12-20,2024-12-23T21:06:40.434158
Autonomous Vehicle Security: A Deep Dive into Threat Modeling,"Autonomous vehicles (AVs) are poised to revolutionize modern transportation,
offering enhanced safety, efficiency, and convenience. However, the increasing
complexity and connectivity of AV systems introduce significant cybersecurity
challenges. This paper provides a comprehensive survey of AV security with a
focus on threat modeling frameworks, including STRIDE, DREAD, and MITRE
ATT\&CK, to systematically identify and mitigate potential risks. The survey
examines key components of AV architectures, such as sensors, communication
modules, and electronic control units (ECUs), and explores common attack
vectors like wireless communication exploits, sensor spoofing, and firmware
vulnerabilities. Through case studies of real-world incidents, such as the Jeep
Cherokee and Tesla Model S exploits, the paper highlights the critical need for
robust security measures. Emerging technologies, including blockchain for
secure Vehicle-to-Everything (V2X) communication, AI-driven threat detection,
and secure Over-The-Air (OTA) updates, are discussed as potential solutions to
mitigate evolving threats. The paper also addresses legal and ethical
considerations, emphasizing data privacy, user safety, and regulatory
compliance. By combining threat modeling frameworks, multi-layered security
strategies, and proactive defenses, this survey offers insights and
recommendations for enhancing the cybersecurity of autonomous vehicles.",270,2412.15348v1,eess.SY,"eess.SY,cs.SY",cybersecurity,2024-12-19,2024-12-23T21:06:40.435156
Security and Privacy of Digital Twins for Advanced Manufacturing: A Survey,"In Industry 4.0, the digital twin is one of the emerging technologies,
offering simulation abilities to predict, refine, and interpret conditions and
operations, where it is crucial to emphasize a heightened concentration on the
associated security and privacy risks. To be more specific, the adoption of
digital twins in the manufacturing industry relies on integrating technologies
like cyber-physical systems, the Industrial Internet of Things, virtualization,
and advanced manufacturing. The interactions of these technologies give rise to
numerous security and privacy vulnerabilities that remain inadequately
explored. Towards that end, this paper analyzes the cybersecurity threats of
digital twins for advanced manufacturing in the context of data collection,
data sharing, machine learning and deep learning, and system-level security and
privacy. We also provide several solutions to the threats in those four
categories that can help establish more trust in digital twins.",175,2412.13939v1,eess.SY,"eess.SY,cs.SY",cybersecurity,2024-12-18,2024-12-23T21:06:40.435156
A Review of the Duality of Adversarial Learning in Network Intrusion: Attacks and Countermeasures,"Deep learning solutions are instrumental in cybersecurity, harnessing their
ability to analyze vast datasets, identify complex patterns, and detect
anomalies. However, malevolent actors can exploit these capabilities to
orchestrate sophisticated attacks, posing significant challenges to defenders
and traditional security measures. Adversarial attacks, particularly those
targeting vulnerabilities in deep learning models, present a nuanced and
substantial threat to cybersecurity. Our study delves into adversarial learning
threats such as Data Poisoning, Test Time Evasion, and Reverse Engineering,
specifically impacting Network Intrusion Detection Systems. Our research
explores the intricacies and countermeasures of attacks to deepen understanding
of network security challenges amidst adversarial threats. In our study, we
present insights into the dynamic realm of adversarial learning and its
implications for network intrusion. The intersection of adversarial attacks and
defenses within network traffic data, coupled with advances in machine learning
and deep learning techniques, represents a relatively underexplored domain. Our
research lays the groundwork for strengthening defense mechanisms to address
the potential breaches in network security and privacy posed by adversarial
attacks. Through our in-depth analysis, we identify domain-specific research
gaps, such as the scarcity of real-life attack data and the evaluation of
AI-based solutions for network traffic. Our focus on these challenges aims to
stimulate future research efforts toward the development of resilient network
defense strategies.",302,2412.13880v1,cs.CR,"cs.CR,cs.ET",cybersecurity,2024-12-18,2024-12-23T21:06:40.436153
BotSim: LLM-Powered Malicious Social Botnet Simulation,"Social media platforms like X(Twitter) and Reddit are vital to global
communication. However, advancements in Large Language Model (LLM) technology
give rise to social media bots with unprecedented intelligence. These bots
adeptly simulate human profiles, conversations, and interactions, disseminating
large amounts of false information and posing significant challenges to
platform regulation. To better understand and counter these threats, we
innovatively design BotSim, a malicious social botnet simulation powered by
LLM. BotSim mimics the information dissemination patterns of real-world social
networks, creating a virtual environment composed of intelligent agent bots and
real human users. In the temporal simulation constructed by BotSim, these
advanced agent bots autonomously engage in social interactions such as posting
and commenting, effectively modeling scenarios of information flow and user
interaction. Building on the BotSim framework, we construct a highly
human-like, LLM-driven bot dataset called BotSim-24 and benchmark multiple bot
detection strategies against it. The experimental results indicate that
detection methods effective on traditional bot datasets perform worse on
BotSim-24, highlighting the urgent need for new detection strategies to address
the cybersecurity threats posed by these advanced bots.",253,2412.13420v1,cs.SI,cs.SI,cybersecurity,2024-12-18,2024-12-23T21:06:40.437150
Quantum community detection via deterministic elimination,"We propose a quantum algorithm for calculating the structural properties of
complex networks and graphs. The corresponding protocol -- deteQt -- is
designed to perform large-scale community and botnet detection, where a
specific subgraph of a larger graph is identified based on its properties. We
construct a workflow relying on ground state preparation of the network
modularity matrix or graph Laplacian. The corresponding maximum modularity
vector is encoded into a $\log(N)$-qubit register that contains community
information. We develop a strategy for ``signing'' this vector via quantum
signal processing, such that it closely resembles a hypergraph state, and
project it onto a suitable linear combination of such states to detect botnets.
As part of the workflow, and of potential independent interest, we present a
readout technique that allows filtering out the incorrect solutions
deterministically. This can reduce the scaling for the number of samples from
exponential to polynomial. The approach serves as a building block for graph
analysis with quantum speed up and enables the cybersecurity of large-scale
networks.",222,2412.13160v1,quant-ph,"quant-ph,cond-mat.dis-nn",cybersecurity,2024-12-17,2024-12-23T21:06:40.437150
Automated Penetration Testing: Formalization and Realization,"Recent changes in standards and regulations, driven by the increasing
importance of software systems in meeting societal needs, mandate increased
security testing of software systems. Penetration testing has been shown to be
a reliable method to asses software system security. However, manual
penetration testing is labor-intensive and requires highly skilled
practitioners. Given the shortage of cybersecurity experts and current societal
needs, increasing the degree of automation involved in penetration testing can
aid in fulfilling the demands for increased security testing. In this work, we
formally express the penetration testing problem at the architectural level and
suggest a general self-organizing architecture that can be instantiated to
automate penetration testing of real systems. We further describe and implement
a specialization of the architecture in the ADAPT tool, targeting systems
composed of hosts and services. We evaluate and demonstrate the feasibility of
ADAPT by automatically performing penetration tests with success against:
Metasploitable2, Metasploitable3, and a realistic virtual network used as a lab
environment for penetration tester training.",203,2412.12745v1,cs.CR,"cs.CR,cs.SE",cybersecurity,2024-12-17,2024-12-23T21:06:40.438148
"Exploring AI-Enabled Cybersecurity Frameworks: Deep-Learning Techniques, GPU Support, and Future Enhancements","Traditional rule-based cybersecurity systems have proven highly effective
against known malware threats. However, they face challenges in detecting novel
threats. To address this issue, emerging cybersecurity systems are
incorporating AI techniques, specifically deep-learning algorithms, to enhance
their ability to detect incidents, analyze alerts, and respond to events. While
these techniques offer a promising approach to combating dynamic security
threats, they often require significant computational resources. Therefore,
frameworks that incorporate AI-based cybersecurity mechanisms need to support
the use of GPUs to ensure optimal performance.
  Many cybersecurity framework vendors do not provide sufficiently detailed
information about their implementation, making it difficult to assess the
techniques employed and their effectiveness. This study aims to overcome this
limitation by providing an overview of the most used cybersecurity frameworks
that utilize AI techniques, specifically focusing on frameworks that provide
comprehensive information about their implementation. Our primary objective is
to identify the deep-learning techniques employed by these frameworks and
evaluate their support for GPU acceleration. We have identified a total of
\emph{two} deep-learning algorithms that are utilized by \emph{three} out of 38
selected cybersecurity frameworks. Our findings aim to assist in selecting
open-source cybersecurity frameworks for future research and assessing any
discrepancies between deep-learning techniques used in theory and practice.",285,2412.12648v1,cs.CR,"cs.CR,cs.AI,cs.DC,cs.LG",cybersecurity,2024-12-17,2024-12-23T21:06:40.439145
"Comprehensive Survey on Adversarial Examples in Cybersecurity: Impacts, Challenges, and Mitigation Strategies","Deep learning (DL) has significantly transformed cybersecurity, enabling
advancements in malware detection, botnet identification, intrusion detection,
user authentication, and encrypted traffic analysis. However, the rise of
adversarial examples (AE) poses a critical challenge to the robustness and
reliability of DL-based systems. These subtle, crafted perturbations can
deceive models, leading to severe consequences like misclassification and
system vulnerabilities. This paper provides a comprehensive review of the
impact of AE attacks on key cybersecurity applications, highlighting both their
theoretical and practical implications. We systematically examine the methods
used to generate adversarial examples, their specific effects across various
domains, and the inherent trade-offs attackers face between efficacy and
resource efficiency. Additionally, we explore recent advancements in defense
mechanisms, including gradient masking, adversarial training, and detection
techniques, evaluating their potential to enhance model resilience. By
summarizing cutting-edge research, this study aims to bridge the gap between
adversarial research and practical security applications, offering insights to
fortify the adoption of DL solutions in cybersecurity.",241,2412.12217v1,cs.CR,"cs.CR,cs.LG",cybersecurity,2024-12-16,2024-12-23T21:06:40.440142
"A technical solution for the rule of law, peace, security, and evolvability of global cyberspace -- solve the three genetic defects of IP network","Since its inception in the 1960s, the internet has profoundly transformed
human life. However, its original design now struggles to meet the evolving
demands of modern society. Three primary defects have emerged: First, the
concentration of power among a few dominant entities has intensified
international conflicts and widened the technological divide. Second, the
Internet Protocol (IP)-based system lacks inherent security, leading to
frequent global cybersecurity incidents. Third, the rigidity of the IP protocol
has hindered the sustainable development of cyberspace, as it resists necessary
adaptations and innovations. Addressing these issues is crucial for the future
resilience and security of the global digital landscape.
  To address these challenges, we propose the Co-governed Multi-Identifier
Network (CoG-MIN briefly as MIN), a novel network architecture that leverages
blockchain technology to ensure equal participation of countries worldwide in
cyberspace governance and the rule of law. As a next-generation network system,
CoG-MIN integrates mechanisms such as user authentication, data signatures, and
encryption to significantly enhance network security. In testing environments,
CoG-MIN has consistently withstood extensive attacks during various
international cybersecurity competitions. Additionally, CoG-MIN supports the
evolution and interoperability of different identifier systems, remains
IP-compatible, and facilitates a gradual transition away from IP, providing an
adaptable ecosystem for diverse network architectures. This adaptability
fosters the development and evolution of diverse network architectures within
CoG-MIN, making it a natural progression for the internet's future development.
  We further introduce a trilogy of cyberspace security theorems... (Due to
character limitations, the full abstract is available in the paper PDF.)",351,2412.10722v1,cs.CR,"cs.CR,cs.NI",cybersecurity,2024-12-14,2024-12-23T21:06:40.441140
algoTRIC: Symmetric and asymmetric encryption algorithms for Cryptography -- A comparative analysis in AI era,"The increasing integration of artificial intelligence (AI) within
cybersecurity has necessitated stronger encryption methods to ensure data
security. This paper presents a comparative analysis of symmetric (SE) and
asymmetric encryption (AE) algorithms, focusing on their role in securing
sensitive information in AI-driven environments. Through an in-depth study of
various encryption algorithms such as AES, RSA, and others, this research
evaluates the efficiency, complexity, and security of these algorithms within
modern cybersecurity frameworks. Utilizing both qualitative and quantitative
analysis, this research explores the historical evolution of encryption
algorithms and their growing relevance in AI applications. The comparison of SE
and AE algorithms focuses on key factors such as processing speed, scalability,
and security resilience in the face of evolving threats. Special attention is
given to how these algorithms are integrated into AI systems and how they
manage the challenges posed by large-scale data processing in multi-agent
environments. Our results highlight that while SE algorithms demonstrate
high-speed performance and lower computational demands, AE algorithms provide
superior security, particularly in scenarios requiring enhanced encryption for
AI-based networks. The paper concludes by addressing the security concerns that
encryption algorithms must tackle in the age of AI and outlines future research
directions aimed at enhancing encryption techniques for cybersecurity.",263,2412.15237v1,cs.CR,cs.CR,cybersecurity,2024-12-12,2024-12-23T21:06:40.441140
Enhancing Cybersecurity in IoT Networks: A Deep Learning Approach to Anomaly Detection,"With the proliferation of the Internet and smart devices, IoT technology has
seen significant advancements and has become an integral component of smart
homes, urban security, smart logistics, and other sectors. IoT facilitates
real-time monitoring of critical production indicators, enabling businesses to
detect potential quality issues, anticipate equipment malfunctions, and refine
processes, thereby minimizing losses and reducing costs. Furthermore, IoT
enhances real-time asset tracking, optimizing asset utilization and management.
However, the expansion of IoT has also led to a rise in cybercrimes, with
devices increasingly serving as vectors for malicious attacks. As the number of
IoT devices grows, there is an urgent need for robust network security measures
to counter these escalating threats. This paper introduces a deep learning
model incorporating LSTM and attention mechanisms, a pivotal strategy in
combating cybercrime in IoT networks. Our experiments, conducted on datasets
including IoT-23, BoT-IoT, IoT network intrusion, MQTT, and MQTTset,
demonstrate that our proposed method outperforms existing baselines.",234,2412.08301v1,cs.CR,"cs.CR,cs.LG",cybersecurity,2024-12-11,2024-12-23T21:06:40.442137
Intelligent Electric Power Steering: Artificial Intelligence Integration Enhances Vehicle Safety and Performance,"Electric Power Steering (EPS) systems utilize electric motors to aid users in
steering their vehicles, which provide additional precise control and reduced
energy consumption compared to traditional hydraulic systems. EPS technology
provides safety,control and efficiency.. This paper explains the integration of
Artificial Intelligence (AI) into Electric Power Steering (EPS) systems,
focusing on its role in enhancing the safety, and adaptability across diverse
driving conditions. We explore significant development in AI-driven EPS,
including predictive control algorithms, adaptive torque management systems,
and data-driven diagnostics. The paper presents case studies of AI applications
in EPS, such as Lane centering control (LCC), Automated Parking Systems, and
Autonomous Vehicle Steering, while considering the challenges, limitations, and
future prospects of this technology. This article discusses current
developments in AI-driven EPS, emphasizing on the benefits of improved safety,
adaptive control, and predictive maintenance. Challenges in integrating AI in
EPS systems. This paper addresses cybersecurity risks, ethical concerns, and
technical limitations,, along with next steps for research and implementation
in autonomous, and connected vehicles.",222,2412.08133v1,cs.RO,"cs.RO,cs.AI,cs.SY,eess.SY",cybersecurity,2024-12-11,2024-12-23T21:06:40.443136
Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education,"Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.",256,2412.14191v1,cs.CY,"cs.CY,cs.AI",cybersecurity,2024-12-10,2024-12-23T21:06:40.444132
Digital Transformation in the Water Distribution System based on the Digital Twins Concept,"Digital Twins have emerged as a disruptive technology with great potential;
they can enhance WDS by offering real-time monitoring, predictive maintenance,
and optimization capabilities. This paper describes the development of a
state-of-the-art DT platform for WDS, introducing advanced technologies such as
the Internet of Things, Artificial Intelligence, and Machine Learning models.
This paper provides insight into the architecture of the proposed
platform-CAUCCES-that, informed by both historical and meteorological data,
effectively deploys AI/ML models like LSTM networks, Prophet, LightGBM, and
XGBoost in trying to predict water consumption patterns. Furthermore, we delve
into how optimization in the maintenance of WDS can be achieved by formulating
a Constraint Programming problem for scheduling, hence minimizing the
operational cost efficiently with reduced environmental impacts. It also
focuses on cybersecurity and protection to ensure the integrity and reliability
of the DT platform. In this view, the system will contribute to improvements in
decision-making capabilities, operational efficiency, and system reliability,
with reassurance being drawn from the important role it can play toward
sustainable management of water resources.",235,2412.06694v1,cs.CY,"cs.CY,cs.AI",cybersecurity,2024-12-09,2024-12-23T21:06:40.444132
The Fusion of Large Language Models and Formal Methods for Trustworthy AI Agents: A Roadmap,"Large Language Models (LLMs) have emerged as a transformative AI paradigm,
profoundly influencing daily life through their exceptional language
understanding and contextual generation capabilities. Despite their remarkable
performance, LLMs face a critical challenge: the propensity to produce
unreliable outputs due to the inherent limitations of their learning-based
nature. Formal methods (FMs), on the other hand, are a well-established
computation paradigm that provides mathematically rigorous techniques for
modeling, specifying, and verifying the correctness of systems. FMs have been
extensively applied in mission-critical software engineering, embedded systems,
and cybersecurity. However, the primary challenge impeding the deployment of
FMs in real-world settings lies in their steep learning curves, the absence of
user-friendly interfaces, and issues with efficiency and adaptability.
  This position paper outlines a roadmap for advancing the next generation of
trustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs.
First, we illustrate how FMs, including reasoning and certification techniques,
can help LLMs generate more reliable and formally certified outputs.
Subsequently, we highlight how the advanced learning capabilities and
adaptability of LLMs can significantly enhance the usability, efficiency, and
scalability of existing FM tools. Finally, we show that unifying these two
computation paradigms -- integrating the flexibility and intelligence of LLMs
with the rigorous reasoning abilities of FMs -- has transformative potential
for the development of trustworthy AI software systems. We acknowledge that
this integration has the potential to enhance both the trustworthiness and
efficiency of software engineering practices while fostering the development of
intelligent FM tools capable of addressing complex yet real-world challenges.",342,2412.06512v1,cs.AI,"cs.AI,cs.CL,cs.SE",cybersecurity,2024-12-09,2024-12-23T21:06:40.445129
Towards a Comprehensive Framework for Cyber-Incident Response Decision Support in Smart Grids,"The modernization of power grid infrastructures necessitates the
incorporation of decision support systems to effectively mitigate cybersecurity
threats. This paper presents a comprehensive framework based on integrating
Attack-Defense Trees and the Multi-Criteria Decision Making method to enhance
smart grid cybersecurity. By analyzing risk attributes and optimizing defense
strategies, this framework enables grid operators to prioritize critical
security measures. Additionally, this paper incorporates findings on
decision-making processes in intelligent power systems to present a
comprehensive approach to grid cybersecurity. The proposed model aims to
optimize the effectiveness and efficiency of grid cybersecurity efforts while
offering insights into future grid management challenges.",138,2412.06254v1,cs.CR,cs.CR,cybersecurity,2024-12-09,2024-12-23T21:06:40.446126
Applications of Positive Unlabeled (PU) and Negative Unlabeled (NU) Learning in Cybersecurity,"This paper explores the relatively underexplored application of Positive
Unlabeled (PU) Learning and Negative Unlabeled (NU) Learning in the
cybersecurity domain. While these semi-supervised learning methods have been
applied successfully in fields like medicine and marketing, their potential in
cybersecurity remains largely untapped. The paper identifies key areas of
cybersecurity--such as intrusion detection, vulnerability management, malware
detection, and threat intelligence--where PU/NU learning can offer significant
improvements, particularly in scenarios with imbalanced or limited labeled
data. We provide a detailed problem formulation for each subfield, supported by
mathematical reasoning, and highlight the specific challenges and research gaps
in scaling these methods to real-time systems, addressing class imbalance, and
adapting to evolving threats. Finally, we propose future directions to advance
the integration of PU/NU learning in cybersecurity, offering solutions that can
better detect, manage, and mitigate emerging cyber threats.",204,2412.06203v1,cs.CR,"cs.CR,cs.LG",cybersecurity,2024-12-09,2024-12-23T21:06:40.446126
Trust No AI: Prompt Injection Along The CIA Security Triad,"The CIA security triad - Confidentiality, Integrity, and Availability - is a
cornerstone of data and cybersecurity. With the emergence of large language
model (LLM) applications, a new class of threat, known as prompt injection, was
first identified in 2022. Since then, numerous real-world vulnerabilities and
exploits have been documented in production LLM systems, including those from
leading vendors like OpenAI, Microsoft, Anthropic and Google. This paper
compiles real-world exploits and proof-of concept examples, based on the
research conducted and publicly documented by the author, demonstrating how
prompt injection undermines the CIA triad and poses ongoing risks to
cybersecurity and AI systems at large.",149,2412.06090v1,cs.CR,"cs.CR,cs.AI,cs.LG",cybersecurity,2024-12-08,2024-12-23T21:06:40.447124
siForest: Detecting Network Anomalies with Set-Structured Isolation Forest,"As cyber threats continue to evolve in sophistication and scale, the ability
to detect anomalous network behavior has become critical for maintaining robust
cybersecurity defenses. Modern cybersecurity systems face the overwhelming
challenge of analyzing billions of daily network interactions to identify
potential threats, making efficient and accurate anomaly detection algorithms
crucial for network defense. This paper investigates the use of variations of
the Isolation Forest (iForest) machine learning algorithm for detecting
anomalies in internet scan data. In particular, it presents the Set-Partitioned
Isolation Forest (siForest), a novel extension of the iForest method designed
to detect anomalies in set-structured data. By treating instances such as sets
of multiple network scans with the same IP address as cohesive units, siForest
effectively addresses some challenges of analyzing complex, multidimensional
datasets. Extensive experiments on synthetic datasets simulating diverse
anomaly scenarios in network traffic demonstrate that siForest has the
potential to outperform traditional approaches on some types of internet scan
data.",215,2412.06015v1,cs.LG,"cs.LG,cs.CR",cybersecurity,2024-12-08,2024-12-23T21:06:40.447124
OCEAN: Open-World Contrastive Authorship Identification,"In an era where cyberattacks increasingly target the software supply chain,
the ability to accurately attribute code authorship in binary files is critical
to improving cybersecurity measures. We propose OCEAN, a contrastive
learning-based system for function-level authorship attribution. OCEAN is the
first framework to explore code authorship attribution on compiled binaries in
an open-world and extreme scenario, where two code samples from unknown authors
are compared to determine if they are developed by the same author. To evaluate
OCEAN, we introduce new realistic datasets: CONAN, to improve the performance
of authorship attribution systems in real-world use cases, and SNOOPY, to
increase the robustness of the evaluation of such systems. We use CONAN to
train our model and evaluate on SNOOPY, a fully unseen dataset, resulting in an
AUROC score of 0.86 even when using high compiler optimizations. We further
show that CONAN improves performance by 7% compared to the previously used
Google Code Jam dataset. Additionally, OCEAN outperforms previous methods in
their settings, achieving a 10% improvement over state-of-the-art SCS-Gan in
scenarios analyzing source code. Furthermore, OCEAN can detect code injections
from an unknown author in a software update, underscoring its value for
securing software supply chains.",271,2412.05049v1,cs.AI,"cs.AI,cs.CR",cybersecurity,2024-12-06,2024-12-23T21:06:40.448121
On Process Awareness in Detecting Multi-stage Cyberattacks in Smart Grids,"This study delves into the role of process awareness in enhancing intrusion
detection within Smart Grids, considering the increasing fusion of ICT in power
systems and the associated emerging threats. The research harnesses a
co-simulation environment, encapsulating IT, OT, and ET layers, to model
multi-stage cyberattacks and evaluate machine learning-based IDS strategies.
The key observation is that process-aware IDS demonstrate superior detection
capabilities, especially in scenarios closely tied to operational processes, as
opposed to IT-only IDS. This improvement is notable in distinguishing complex
cyber threats from regular IT activities. The findings underscore the
significance of further developing sophisticated IDS benchmarks and digital
twin datasets in Smart Grid environments, paving the way for more resilient
cybersecurity infrastructures.",165,2412.04902v1,cs.CR,cs.CR,cybersecurity,2024-12-06,2024-12-23T21:06:40.449118
ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models,"The increasing frequency and sophistication of cybersecurity vulnerabilities
in software systems underscore the urgent need for robust and effective methods
of vulnerability assessment. However, existing approaches often rely on highly
technical and abstract frameworks, which hinders understanding and increases
the likelihood of exploitation, resulting in severe cyberattacks. Given the
growing adoption of Large Language Models (LLMs) across diverse domains, this
paper explores their potential application in cybersecurity, specifically for
enhancing the assessment of software vulnerabilities. We propose ChatNVD, an
LLM-based cybersecurity vulnerability assessment tool leveraging the National
Vulnerability Database (NVD) to provide context-rich insights and streamline
vulnerability analysis for cybersecurity professionals, developers, and
non-technical users. We develop three variants of ChatNVD, utilizing three
prominent LLMs: GPT-4o mini by OpenAI, Llama 3 by Meta, and Gemini 1.5 Pro by
Google. To evaluate their efficacy, we conduct a comparative analysis of these
models using a comprehensive questionnaire comprising common security
vulnerability questions, assessing their accuracy in identifying and analyzing
software vulnerabilities. This study provides valuable insights into the
potential of LLMs to address critical challenges in understanding and
mitigation of software vulnerabilities.",269,2412.04756v1,cs.CR,"cs.CR,cs.CL",cybersecurity,2024-12-06,2024-12-23T21:06:40.449118
Game-Theoretic Foundations for Cyber Resilience Against Deceptive Information Attacks in Intelligent Transportation Systems,"The growing complexity and interconnectivity of Intelligent Transportation
Systems (ITS) make them increasingly vulnerable to advanced cyber threats,
particularly deceptive information attacks. These sophisticated threats exploit
vulnerabilities to manipulate data integrity and decision-making processes
through techniques such as data poisoning, spoofing, and phishing. They target
multiple ITS domains, including intra-vehicle systems, inter-vehicle
communications, transportation infrastructure, and human interactions, creating
cascading effects across the ecosystem. This chapter introduces a
game-theoretic framework, enhanced by control and learning theories, to
systematically analyze and mitigate these risks. By modeling the strategic
interactions among attackers, users, and system operators, the framework
facilitates comprehensive risk assessment and the design of adaptive, scalable
resilience mechanisms. A prime example of this approach is the Proactive Risk
Assessment and Mitigation of Misinformed Demand Attacks (PRADA) system, which
integrates trust mechanisms, dynamic learning processes, and multi-layered
defense strategies to counteract deceptive attacks on navigational
recommendation systems. In addition, the chapter explores the broader
applicability of these methodologies to address various ITS threats, including
spoofing, Advanced Persistent Threats (APTs), and denial-of-service attacks. It
highlights cross-domain resilience strategies, offering actionable insights to
bolster the security, reliability, and adaptability of ITS. By providing a
robust game-theoretic foundation, this work advances the development of
comprehensive solutions to the evolving challenges in ITS cybersecurity.",314,2412.04627v1,cs.GT,cs.GT,cybersecurity,2024-12-05,2024-12-23T21:06:40.450116
SCADE: Scalable Framework for Anomaly Detection in High-Performance System,"As command-line interfaces remain integral to high-performance computing
environments, the risk of exploitation through stealthy and complex
command-line abuse grows. Conventional security solutions struggle to detect
these anomalies due to their context-specific nature, lack of labeled data, and
the prevalence of sophisticated attacks like Living-off-the-Land (LOL). To
address this gap, we introduce the Scalable Command-Line Anomaly Detection
Engine (SCADE), a framework that combines global statistical models with local
context-specific analysis for unsupervised anomaly detection. SCADE leverages
novel statistical methods, including BM25 and Log Entropy, alongside dynamic
thresholding to adaptively detect rare, malicious command-line patterns in low
signal-to-noise ratio (SNR) environments. Experimental results show that SCADE
achieves above 98% SNR in identifying anomalous behavior while minimizing false
positives. Designed for scalability and precision, SCADE provides an
innovative, metadata-enriched approach to anomaly detection, offering a robust
solution for cybersecurity in high-computation environments. This work presents
SCADE's architecture, detection methodology, and its potential for enhancing
anomaly detection in enterprise systems. We argue that SCADE represents a
significant advancement in unsupervised anomaly detection, offering a robust,
adaptive framework for security analysts and researchers seeking to enhance
detection accuracy in high-computation environments.",287,2412.04259v2,cs.CR,"cs.CR,cs.LG",cybersecurity,2024-12-05,2024-12-23T21:06:40.451113
Digital Twin for Evaluating Detective Countermeasures in Smart Grid Cybersecurity,"As the integration of digital technologies and communication systems
continues within distribution grids, new avenues emerge to tackle energy
transition challenges. Nevertheless, this deeper technological immersion
amplifies the necessity for resilience against threats, encompassing both
systemic outages and targeted cyberattacks. To ensure the robustness and
safeguarding of vital infrastructure, a thorough examination of potential smart
grid vulnerabilities and subsequent countermeasure development is essential.
This study delves into the potential of digital twins, replicating a smart
grid's cyber-physical laboratory environment, thereby enabling focused
cybersecurity assessments. Merging the nuances of communication network
emulation and power network simulation, we introduce a flexible, comprehensive
digital twin model equipped for hardware-in-the-loop evaluations. Through this
innovative framework, we not only verify and refine security countermeasures
but also underscore their role in maintaining grid stability and
trustworthiness.",190,2412.03973v1,cs.CR,cs.CR,cybersecurity,2024-12-05,2024-12-23T21:06:40.452110
PBP: Post-training Backdoor Purification for Malware Classifiers,"In recent years, the rise of machine learning (ML) in cybersecurity has
brought new challenges, including the increasing threat of backdoor poisoning
attacks on ML malware classifiers. For instance, adversaries could inject
malicious samples into public malware repositories, contaminating the training
data and potentially misclassifying malware by the ML model. Current
countermeasures predominantly focus on detecting poisoned samples by leveraging
disagreements within the outputs of a diverse set of ensemble models on
training data points. However, these methods are not suitable for scenarios
where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove
backdoors from a model after it has been trained. Addressing this scenario, we
introduce PBP, a post-training defense for malware classifiers that mitigates
various types of backdoor embeddings without assuming any specific backdoor
embedding mechanism. Our method exploits the influence of backdoor attacks on
the activation distribution of neural networks, independent of the
trigger-embedding method. In the presence of a backdoor attack, the activation
distribution of each layer is distorted into a mixture of distributions. By
regulating the statistics of the batch normalization layers, we can guide a
backdoored model to perform similarly to a clean one. Our method demonstrates
substantial advantages over several state-of-the-art methods, as evidenced by
experiments on two datasets, two types of backdoor methods, and various attack
configurations. Notably, our approach requires only a small portion of the
training data -- only 1\% -- to purify the backdoor and reduce the attack
success rate from 100\% to almost 0\%, a 100-fold improvement over the baseline
methods. Our code is available at
\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}.",400,2412.03441v3,cs.LG,"cs.LG,cs.AI,cs.CR",cybersecurity,2024-12-04,2024-12-23T21:06:40.453108
Out-of-Distribution Detection for Neurosymbolic Autonomous Cyber Agents,"Autonomous agents for cyber applications take advantage of modern defense
techniques by adopting intelligent agents with conventional and
learning-enabled components. These intelligent agents are trained via
reinforcement learning (RL) algorithms, and can learn, adapt to, reason about
and deploy security rules to defend networked computer systems while
maintaining critical operational workflows. However, the knowledge available
during training about the state of the operational network and its environment
may be limited. The agents should be trustworthy so that they can reliably
detect situations they cannot handle, and hand them over to cyber experts. In
this work, we develop an out-of-distribution (OOD) Monitoring algorithm that
uses a Probabilistic Neural Network (PNN) to detect anomalous or OOD situations
of RL-based agents with discrete states and discrete actions. To demonstrate
the effectiveness of the proposed approach, we integrate the OOD monitoring
algorithm with a neurosymbolic autonomous cyber agent that uses behavior trees
with learning-enabled components. We evaluate the proposed approach in a
simulated cyber environment under different adversarial strategies.
Experimental results over a large number of episodes illustrate the overall
efficiency of our proposed approach.",238,2412.02875v1,cs.LG,"cs.LG,cs.AI,cs.CR",cybersecurity,2024-12-03,2024-12-23T21:06:40.453108
Analyzing Computing Undergraduate Majors from Job Market Perspective,"The demand for computing education increases due to the rapid development of
technology and its involvement in most daily activities. Academic institutes
offer a variety of computing majors, such as Computer Engineering, Computer
Science, Information Systems, Information Technology, Software Engineering,
Cybersecurity, and Data Science. Since a major objective of earning a
bachelor's degree is to improve career opportunities, it is crucial to
understand how the job market perceives these computing majors. This study
analyzed the relationships between various computing majors and the job market
in Saudi Arabia, using LinkedIn public profile data, discovering insights into
the strong relationship between the focus of certain computing majors and the
employment of relevant job positions. Moreover, job category trends were
analyzed over the past ten years, observing that demands for System Admin and
Technical Support positions declined while demands for Business Analysis and
Artificial Intelligence and Data Science inclined. This study also compared
earned professional certifications between different computing major graduates
that correspond to job position findings.",193,2412.15219v1,cs.CY,cs.CY,cybersecurity,2024-12-03,2024-12-23T21:06:40.454105
Hacking CTFs with Plain Agents,"We saturate a high-school-level hacking benchmark with plain LLM agent
design. Concretely, we obtain 95% performance on InterCode-CTF, a popular
offensive security benchmark, using prompting, tool use, and multiple attempts.
This beats prior work by Phuong et al. 2024 (29%) and Abramovich et al. 2024
(72%).
  Our results suggest that current LLMs have surpassed the high school level in
offensive cybersecurity. Their hacking capabilities remain underelicited: our
ReAct&Plan prompting strategy solves many challenges in 1-2 turns without
complex engineering or advanced harnessing.",137,2412.02776v1,cs.CR,"cs.CR,cs.AI",cybersecurity,2024-12-03,2024-12-23T21:06:40.454105
Explore Reinforced: Equilibrium Approximation with Reinforcement Learning,"Current approximate Coarse Correlated Equilibria (CCE) algorithms struggle
with equilibrium approximation for games in large stochastic environments but
are theoretically guaranteed to converge to a strong solution concept. In
contrast, modern Reinforcement Learning (RL) algorithms provide faster training
yet yield weaker solutions. We introduce Exp3-IXrl - a blend of RL and
game-theoretic approach, separating the RL agent's action selection from the
equilibrium computation while preserving the integrity of the learning process.
We demonstrate that our algorithm expands the application of equilibrium
approximation algorithms to new environments. Specifically, we show the
improved performance in a complex and adversarial cybersecurity network
environment - the Cyber Operations Research Gym - and in the classical
multi-armed bandit settings.",156,2412.02016v1,cs.LG,"cs.LG,cs.AI,cs.GT",cybersecurity,2024-12-02,2024-12-23T21:06:40.455102
HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing,"We introduce HackSynth, a novel Large Language Model (LLM)-based agent
capable of autonomous penetration testing. HackSynth's dual-module architecture
includes a Planner and a Summarizer, which enable it to generate commands and
process feedback iteratively. To benchmark HackSynth, we propose two new
Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms
PicoCTF and OverTheWire. These benchmarks include two hundred challenges across
diverse domains and difficulties, providing a standardized framework for
evaluating LLM-based penetration testing agents. Based on these benchmarks,
extensive experiments are presented, analyzing the core parameters of
HackSynth, including creativity (temperature and top-p) and token utilization.
Multiple open source and proprietary LLMs were used to measure the agent's
capabilities. The experiments show that the agent performed best with the
GPT-4o model, better than what the GPT-4o's system card suggests. We also
discuss the safety and predictability of HackSynth's actions. Our findings
indicate the potential of LLM-based agents in advancing autonomous penetration
testing and the importance of robust safeguards. HackSynth and the benchmarks
are publicly available to foster research on autonomous cybersecurity
solutions.",268,2412.01778v1,cs.CR,"cs.CR,cs.AI,68M25,I.2.1; K.6.5",cybersecurity,2024-12-02,2024-12-23T21:06:40.455102
Towards Type Agnostic Cyber Defense Agents,"With computing now ubiquitous across government, industry, and education,
cybersecurity has become a critical component for every organization on the
planet. Due to this ubiquity of computing, cyber threats have continued to grow
year over year, leading to labor shortages and a skills gap in cybersecurity.
As a result, many cybersecurity product vendors and security organizations have
looked to artificial intelligence to shore up their defenses. This work
considers how to characterize attackers and defenders in one approach to the
automation of cyber defense -- the application of reinforcement learning.
Specifically, we characterize the types of attackers and defenders in the sense
of Bayesian games and, using reinforcement learning, derive empirical findings
about how to best train agents that defend against multiple types of attackers.",157,2412.01542v1,cs.CR,"cs.CR,cs.AI,cs.GT,cs.LG",cybersecurity,2024-12-02,2024-12-23T21:06:40.456100
Multi-Agent Collaboration in Incident Response with Large Language Models,"Incident response (IR) is a critical aspect of cybersecurity, requiring rapid
decision-making and coordinated efforts to address cyberattacks effectively.
Leveraging large language models (LLMs) as intelligent agents offers a novel
approach to enhancing collaboration and efficiency in IR scenarios. This paper
explores the application of LLM-based multi-agent collaboration using the
Backdoors & Breaches framework, a tabletop game designed for cybersecurity
training. We simulate real-world IR dynamics through various team structures,
including centralized, decentralized, and hybrid configurations. By analyzing
agent interactions and performance across these setups, we provide insights
into optimizing multi-agent collaboration for incident response. Our findings
highlight the potential of LLMs to enhance decision-making, improve
adaptability, and streamline IR processes, paving the way for more effective
and coordinated responses to cyber threats.",180,2412.00652v1,cs.CL,"cs.CL,cs.CR",cybersecurity,2024-12-01,2024-12-23T21:06:40.456100
ACTISM: Threat-informed Dynamic Security Modelling for Automotive Systems,"Cybersecurity threats in automotive systems pose significant risks to safety
and reliability. This article introduces a methodology integrating
threat-informed dynamic security modelling with a Threat Analysis and Risk
Assessment workflow. Using the example of an In-Vehicle Infotainment system, we
demonstrate the methodology's application in risk management to strengthen
automotive resiliency.",70,2412.00416v1,cs.CR,cs.CR,cybersecurity,2024-11-30,2024-12-23T21:06:40.457097
Artificial intelligence and cybersecurity in banking sector: opportunities and risks,"The rapid advancements in artificial intelligence (AI) have presented new
opportunities for enhancing efficiency and economic competitiveness across
various industries, espcially in banking. Machine learning (ML), as a subset of
artificial intelligence, enables systems to adapt and learn from vast datasets,
revolutionizing decision-making processes, fraud detection, and customer
service automation. However, these innovations also introduce new challenges,
particularly in the realm of cybersecurity. Adversarial attacks, such as data
poisoning and evasion attacks, represent critical threats to machine learning
models, exploiting vulnerabilities to manipulate outcomes or compromise
sensitive information. Furthermore, this study highlights the dual-use nature
of AI tools, which can be used by malicious users. To address these challenges,
the paper emphasizes the importance of developing machine learning models with
key characteristics such as security, trust, resilience and robustness. These
features are essential to mitigating risks and ensuring the secure deployment
of AI technologies in banking sectors, where the protection of financial data
is paramount. The findings underscore the urgent need for enhanced
cybersecurity frameworks and continuous improvements in defensive mechanisms.
By exploring both opportunities and risks, this paper aims to guide the
responsible integration of AI in the banking sector, paving the way for
innovation while safeguarding against emerging threats.",270,2412.04495v1,cs.CR,cs.CR,cybersecurity,2024-11-28,2024-12-23T21:06:40.457097
"A Comparative Analysis of Vulnerability Management Tools: Evaluating Nessus, Acunetix, and Nikto for Risk Based Security Solutions","The evolving threat landscape in cybersecurity necessitates the adoption of
advanced tools for effective vulnerability management. This paper presents a
comprehensive comparative analysis of three widely used tools: Nessus,
Acunetix, and Nikto. Each tool is assessed based on its detection accuracy,
risk scoring using the Common Vulnerability Scoring System (CVSS), ease of use,
automation and reporting capabilities, performance metrics, and cost
effectiveness. The research addresses the challenges faced by organizations in
selecting the most suitable tool for their unique security requirements.",110,2411.19123v1,cs.CR,cs.CR,cybersecurity,2024-11-28,2024-12-23T21:06:40.458094
Swarm Intelligence-Driven Client Selection for Federated Learning in Cybersecurity applications,"This study addresses a critical gap in the literature regarding the use of
Swarm Intelligence Optimization (SI) algorithms for client selection in
Federated Learning (FL), with a focus on cybersecurity applications. Existing
research primarily explores optimization techniques for centralized machine
learning, leaving the unique challenges of client diveristy, non-IID data
distributions, and adversarial noise in decentralized FL largely unexamined. To
bridge this gap, we evaluate nine SI algorithms-Grey Wolf Optimization (GWO),
Particle Swarm Optimization (PSO), Cuckoo Search, Bat Algorithm, Bee Colony,
Ant Colony Optimization, Fish Swarm, Glow Worm, and Intelligent Water
Droplet-across four experimental scenarios: fixed client participation, dynamic
participation patterns, hetergeneous non-IID data distributions, and
adversarial noise conditions. Results indicate that GWO exhibits superior
adaptability and robustness, achieving the highest accuracy, recall and
F1-scoress across all configurations, while PSO and Cuckoo Search also
demonstrate strong performance. These findings underscore the potential of SI
algorithms to address decentralized and adversarial FL challenges, offereing
scalable and resilient solutions for cybersecurity applications, including
intrusion detection in IoT and large-scale networks.",262,2411.18877v1,cs.LG,"cs.LG,68T20, 68M25,I.2.8; C.2.0; K.6.5",cybersecurity,2024-11-28,2024-12-23T21:06:40.459093
Contrasting the optimal resource allocation to cybersecurity and cyber insurance using prospect theory versus expected utility theory,"Protecting against cyber-threats is vital for every organization and can be
done by investing in cybersecurity controls and purchasing cyber insurance.
However, these are interlinked since insurance premiums could be reduced by
investing more in cybersecurity controls. The expected utility theory and the
prospect theory are two alternative theories explaining decision-making under
risk and uncertainty, which can inform strategies for optimizing resource
allocation. While the former is considered a rational approach, research has
shown that most people make decisions consistent with the latter, including on
insurance uptakes. We compare and contrast these two approaches to provide
important insights into how the two approaches could lead to different optimal
allocations resulting in differing risk exposure as well as financial costs. We
introduce the concept of a risk curve and show that identifying the nature of
the risk curve is a key step in deriving the optimal resource allocation.",178,2411.18838v1,econ.EM,"econ.EM,math.OC,stat.OT",cybersecurity,2024-11-28,2024-12-23T21:06:40.460092
Exploring the Impact of Rewards on Developers' Proactive AI Accountability Behavior,"The rapid integration of Artificial Intelligence (AI)-based systems offers
benefits for various domains of the economy and society but simultaneously
raises concerns due to emerging scandals. These scandals have led to the
increasing importance of AI accountability to ensure that actors provide
justification and victims receive compensation. However, AI accountability has
a negative connotation due to its emphasis on penalizing sanctions, resulting
in reactive approaches to emerging concerns. To counteract the prevalent
negative view and offer a proactive approach to facilitate the AI
accountability behavior of developers, we explore rewards as an alternative
mechanism to sanctions. We develop a theoretical model grounded in
Self-Determination Theory to uncover the potential impact of rewards and
sanctions on AI developers. We further identify typical sanctions and bug
bounties as potential reward mechanisms by surveying related research from
various domains, including cybersecurity.",166,2411.18393v1,cs.CY,cs.CY,cybersecurity,2024-11-27,2024-12-23T21:06:40.460092
A Practical Approach to Formal Methods: An Eclipse Integrated Development Environment (IDE) for Security Protocols,"To develop trustworthy distributed systems, verification techniques and
formal methods, including lightweight and practical approaches, have been
employed to certify the design or implementation of security protocols.
Lightweight formal methods offer a more accessible alternative to traditional
fully formalised techniques by focusing on simplified models and tool support,
making them more applicable in practical settings. The technical advantages of
formal verification over manual testing are increasingly recognised in the
cybersecurity community. However, for practitioners, formal modelling and
verification are often too complex and unfamiliar to be used routinely. In this
paper, we present an Eclipse IDE for the design, verification, and
implementation of security protocols and evaluate its effectiveness, including
feedback from users in educational settings. It offers user-friendly assistance
in the formalisation process as part of a Model-Driven Development approach.
This IDE centres around the Alice & Bob (AnB) notation, the AnBx Compiler and
Code Generator, the OFMC model checker, and the ProVerif cryptographic protocol
verifier. For the evaluation, we identify the six most prominent limiting
factors for formal method adoption, based on relevant literature in this field,
and we consider the IDE's effectiveness against those criteria. Additionally,
we conducted a structured survey to collect feedback from university students
who have used the toolkit for their projects. The findings demonstrate that
this contribution is valuable as a workflow aid and helps users grasp essential
cybersecurity concepts, even for those with limited knowledge of formal methods
or cryptography. Crucially, users reported that the IDE has been an important
component to complete their projects and that they would use again in the
future, given the opportunity.",336,2411.17926v1,cs.CR,"cs.CR,cs.SE,D.2.6; D.2.4; D.4.6",cybersecurity,2024-11-26,2024-12-23T21:06:40.462084
AI-Augmented Ethical Hacking: A Practical Examination of Manual Exploitation and Privilege Escalation in Linux Environments,"This study explores the application of generative AI (GenAI) within manual
exploitation and privilege escalation tasks in Linux-based penetration testing
environments, two areas critical to comprehensive cybersecurity assessments.
Building on previous research into the role of GenAI in the ethical hacking
lifecycle, this paper presents a hands-on experimental analysis conducted in a
controlled virtual setup to evaluate the utility of GenAI in supporting these
crucial, often manual, tasks. Our findings demonstrate that GenAI can
streamline processes, such as identifying potential attack vectors and parsing
complex outputs for sensitive data during privilege escalation. The study also
identifies key benefits and challenges associated with GenAI, including
enhanced efficiency and scalability, alongside ethical concerns related to data
privacy, unintended discovery of vulnerabilities, and potential for misuse.
This work contributes to the growing field of AI-assisted cybersecurity by
emphasising the importance of human-AI collaboration, especially in contexts
requiring careful decision-making, rather than the complete replacement of
human input.",211,2411.17539v1,cs.CR,"cs.CR,cs.AI,cs.NI",cybersecurity,2024-11-26,2024-12-23T21:06:40.462084
ThreatModeling-LLM: Automating Threat Modeling using Large Language Models for Banking System,"Threat modeling is a crucial component of cybersecurity, particularly for
industries such as banking, where the security of financial data is paramount.
Traditional threat modeling approaches require expert intervention and manual
effort, often leading to inefficiencies and human error. The advent of Large
Language Models (LLMs) offers a promising avenue for automating these
processes, enhancing both efficiency and efficacy. However, this transition is
not straightforward due to three main challenges: (1) the lack of publicly
available, domain-specific datasets, (2) the need for tailored models to handle
complex banking system architectures, and (3) the requirement for real-time,
adaptive mitigation strategies that align with compliance standards like NIST
800-53.
  In this paper, we introduce ThreatModeling-LLM, a novel and adaptable
framework that automates threat modeling for banking systems using LLMs.
ThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt
engineering and 3) model fine-tuning. We first generate a benchmark dataset
using Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought
(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize
the initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation
(LoRA) based on the benchmark dataset and the optimized prompt to improve the
threat identification and mitigation generation capabilities of pre-trained
LLMs.",315,2411.17058v1,cs.CR,"cs.CR,cs.AI",cybersecurity,2024-11-26,2024-12-23T21:06:40.463082
Unsupervised Quantum Anomaly Detection on Noisy Quantum Processors,"Whether in fundamental physics, cybersecurity or finance, the detection of
anomalies with machine learning techniques is a highly relevant and active
field of research, as it potentially accelerates the discovery of novel physics
or criminal activities. We provide a systematic analysis of the generalization
properties of the One-Class Support Vector Machine (OCSVM) algorithm, using
projected quantum kernels for a realistic dataset of the latter application.
These results were both theoretically simulated and experimentally validated on
trapped-ion and superconducting quantum processors, by leveraging partial state
tomography to obtain precise approximations of the quantum states that are used
to estimate the quantum kernels. Moreover, we analyzed both platforms
respective hardware-efficient feature maps over a wide range of anomaly ratios
and showed that for our financial dataset in all anomaly regimes, the
quantum-enhanced OCSVMs lead to better generalization properties compared to
the purely classical approach. As such our work bridges the gap between theory
and practice in the noisy intermediate scale quantum (NISQ) era and paves the
path towards useful quantum applications.",224,2411.16970v1,quant-ph,"quant-ph,cond-mat.quant-gas,cond-mat.supr-con,physics.app-ph",cybersecurity,2024-11-25,2024-12-23T21:06:40.464079
Strengthening Power System Resilience to Extreme Weather Events Through Grid Enhancing Technologies,"Climate change significantly increases risks to power systems, exacerbating
issues such as aging infrastructure, evolving regulations, cybersecurity
threats, and fluctuating demand. This paper focuses on the utilization of Grid
Enhancing Technologies (GETs) to strengthen power system resilience in the face
of extreme weather events. GETs are pivotal in optimizing energy distribution,
enabling predictive maintenance, ensuring reliable electricity supply,
facilitating renewable energy integration, and automating responses to power
instabilities and outages. Drawing insights from resilience theory, the paper
reviews recent grid resilience literature, highlighting increasing
vulnerabilities due to severe weather events. It demonstrates how GETs are
crucial in optimizing smart grid operations, thereby not only mitigating
climate-related impacts but also promoting industrial transformation.
  Keywords: Climate change, power systems, grid enhancing technologies (GETs),
power system resilience, extreme weather",187,2411.16962v1,eess.SY,"eess.SY,cs.ET,cs.SY,math.DS",cybersecurity,2024-11-25,2024-12-23T21:06:40.464079
Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective,"Jailbreak prompts pose a significant threat in AI and cybersecurity, as they
are crafted to bypass ethical safeguards in large language models, potentially
enabling misuse by cybercriminals. This paper analyzes jailbreak prompts from a
cyber defense perspective, exploring techniques like prompt injection and
context manipulation that allow harmful content generation, content filter
evasion, and sensitive information extraction. We assess the impact of
successful jailbreaks, from misinformation and automated social engineering to
hazardous content creation, including bioweapons and explosives. To address
these threats, we propose strategies involving advanced prompt analysis,
dynamic safety protocols, and continuous model fine-tuning to strengthen AI
resilience. Additionally, we highlight the need for collaboration among AI
researchers, cybersecurity experts, and policymakers to set standards for
protecting AI systems. Through case studies, we illustrate these cyber defense
approaches, promoting responsible AI practices to maintain system integrity and
public trust. \textbf{\color{red}Warning: This paper contains content which the
reader may find offensive.}",218,2411.16642v1,cs.CR,"cs.CR,cs.CL",cybersecurity,2024-11-25,2024-12-23T21:06:40.465076
Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions (Extended Abstract),"This paper explores the intersection of privacy, cybersecurity, and
environmental impacts, specifically energy consumption and carbon emissions, in
cloud-based office solutions. We hypothesise that solutions that emphasise
privacy and security are typically ""greener"" than solutions that are financed
through data collection and advertising. To test our hypothesis, we first
investigate how the underlying architectures and business models of these
services, e.g., monetisation through (personalised) advertising, contribute to
the services' environmental impact. We then explore commonly used methodologies
and identify tools that facilitate environmental assessments of software
systems. By combining these tools, we develop an approach to systematically
assess the environmental footprint of the user-side of online services, which
we apply to investigate and compare the influence of service design and
ad-blocking technology on the emissions of common web-mail services. Our
measurements of a limited selection of such services does not yet conclusively
support or falsify our hypothesis regarding primary impacts. However, we are
already able to identify the greener web-mail services on the user-side and
continue the investigation towards conclusive assessment strategies for online
office solutions.",233,2411.16340v2,cs.SE,"cs.SE,cs.CY",cybersecurity,2024-11-25,2024-12-23T21:06:40.466074
CS-Eval: A Comprehensive Large Language Model Benchmark for CyberSecurity,"Over the past year, there has been a notable rise in the use of large
language models (LLMs) for academic research and industrial practices within
the cybersecurity field. However, it remains a lack of comprehensive and
publicly accessible benchmarks to evaluate the performance of LLMs on
cybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly
accessible, comprehensive and bilingual LLM benchmark specifically designed for
cybersecurity. CS-Eval synthesizes the research hotspots from academia and
practical applications from industry, curating a diverse set of high-quality
questions across 42 categories within cybersecurity, systematically organized
into three cognitive levels: knowledge, ability, and application. Through an
extensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered
valuable insights. For instance, while GPT-4 generally excels overall, other
models may outperform it in certain specific subcategories. Additionally, by
conducting evaluations over several months, we observed significant
improvements in many LLMs' abilities to solve cybersecurity tasks. The
benchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.",256,2411.16239v1,cs.CR,cs.CR,cybersecurity,2024-11-25,2024-12-23T21:06:40.466074
Stealth Attacks Against Moving Target Defense for Smart Grid,"Data injection attacks (DIAs) pose a significant cybersecurity threat to the
Smart Grid by enabling an attacker to compromise the integrity of data
acquisition and manipulate estimated states without triggering bad data
detection procedures. To mitigate this vulnerability, the moving target defense
(MTD) alters branch admittances to mismatch the system information that is
available to an attacker, thereby inducing an imperfect DIA construction that
results in degradation of attack performance. In this paper, we first analyze
the existence of stealth attacks for the case in which the MTD strategy only
changes the admittance of a single branch. Equipped with this initial insight,
we then extend the results to the case in which multiple branches are protected
by the MTD strategy. Remarkably, we show that stealth attacks can be
constructed with information only about which branches are protected, without
knowledge about the particular admittance value changes. Furthermore, we
provide a sufficient protection condition for the MTD strategy via
graph-theoretic tools that guarantee that the system is not vulnerable to DIAs.
Numerical simulations are implemented on IEEE test systems to validate the
obtained results.",224,2411.16024v1,eess.SY,"eess.SY,cs.SY,eess.SP,math.OC",cybersecurity,2024-11-25,2024-12-23T21:06:40.467071
Feasibility Study for Supporting Static Malware Analysis Using LLM,"Large language models (LLMs) are becoming more advanced and widespread and
have shown their applicability to various domains, including cybersecurity.
Static malware analysis is one of the most important tasks in cybersecurity;
however, it is time-consuming and requires a high level of expertise.
Therefore, we conducted a demonstration experiment focusing on whether an LLM
can be used to support static analysis. First, we evaluated the ability of the
LLM to explain malware functionality. The results showed that the LLM can
generate descriptions that cover functions with an accuracy of up to 90.9\%. In
addition, we asked six static analysts to perform a pseudo static analysis task
using LLM explanations to verify that the LLM can be used in practice. Through
subsequent questionnaires and interviews with the participants, we also
demonstrated the practical applicability of LLMs. Lastly, we summarized the
problems and required functions when using an LLM as static analysis support,
as well as recommendations for future research opportunities.",208,2411.14905v1,cs.CR,cs.CR,cybersecurity,2024-11-22,2024-12-23T21:06:40.467071
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.965863
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",quantum computing,2024-12-20,2024-12-23T21:06:41.965863
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",quantum computing,2024-12-20,2024-12-23T21:06:41.966861
Can Generative Video Models Help Pose Estimation?,"Pairwise pose estimation from images with little or no overlap is an open
challenge in computer vision. Existing methods, even those trained on
large-scale datasets, struggle in these scenarios due to the lack of
identifiable correspondences or visual overlap. Inspired by the human ability
to infer spatial relationships from diverse scenes, we propose a novel
approach, InterPose, that leverages the rich priors encoded within pre-trained
generative video models. We propose to use a video model to hallucinate
intermediate frames between two input images, effectively creating a dense,
visual transition, which significantly simplifies the problem of pose
estimation. Since current video models can still produce implausible motion or
inconsistent geometry, we introduce a self-consistency score that evaluates the
consistency of pose predictions from sampled videos. We demonstrate that our
approach generalizes among three state-of-the-art video models and show
consistent improvements over the state-of-the-art DUSt3R on four diverse
datasets encompassing indoor, outdoor, and object-centric scenes. Our findings
suggest a promising avenue for improving pose estimation models by leveraging
large generative models trained on vast amounts of video data, which is more
readily available than 3D data. See our project page for results:
https://inter-pose.github.io/.",271,2412.16155v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.967858
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",quantum computing,2024-12-20,2024-12-23T21:06:41.967858
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.968855
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.969853
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset.",207,2412.16146v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.969853
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",quantum computing,2024-12-20,2024-12-23T21:06:41.970850
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",quantum computing,2024-12-20,2024-12-23T21:06:41.971847
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",quantum computing,2024-12-20,2024-12-23T21:06:41.972844
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",quantum computing,2024-12-20,2024-12-23T21:06:41.972844
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.973842
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",quantum computing,2024-12-20,2024-12-23T21:06:41.973842
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",quantum computing,2024-12-20,2024-12-23T21:06:41.974839
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",quantum computing,2024-12-20,2024-12-23T21:06:41.975837
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",quantum computing,2024-12-20,2024-12-23T21:06:41.976834
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",quantum computing,2024-12-20,2024-12-23T21:06:41.977831
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",quantum computing,2024-12-20,2024-12-23T21:06:41.977831
LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical Analysis of Diffeomorphism,"Image registration is a core task in computational anatomy that establishes
correspondences between images. Invertible deformable registration, which
computes a deformation field and handles complex, non-linear transformation, is
essential for tracking anatomical variations, especially in neuroimaging
applications where inter-subject differences and longitudinal changes are key.
Analyzing the deformation fields is challenging due to their non-linearity,
limiting statistical analysis. However, traditional approaches for analyzing
deformation fields are computationally expensive, sensitive to initialization,
and prone to numerical errors, especially when the deformation is far from the
identity. To address these limitations, we propose the Log-Euclidean
Diffeomorphic Autoencoder (LEDA), an innovative framework designed to compute
the principal logarithm of deformation fields by efficiently predicting
consecutive square roots. LEDA operates within a linearized latent space that
adheres to the diffeomorphisms group action laws, enhancing our model's
robustness and applicability. We also introduce a loss function to enforce
inverse consistency, ensuring accurate latent representations of deformation
fields. Extensive experiments with the OASIS-1 dataset demonstrate the
effectiveness of LEDA in accurately modeling and analyzing complex non-linear
deformations while maintaining inverse consistency. Additionally, we evaluate
its ability to capture and incorporate clinical variables, enhancing its
relevance for clinical applications.",271,2412.16129v1,cs.CV,"cs.CV,cs.LG",quantum computing,2024-12-20,2024-12-23T21:06:41.978830
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",quantum computing,2024-12-20,2024-12-23T21:06:41.979827
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,quantum computing,2024-12-20,2024-12-23T21:06:41.980825
Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts,"This study investigates the potential of Large Language Models (LLMs),
particularly GPT-4o, for Optical Character Recognition (OCR) in low-resource
scripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.
Using a meticulously curated dataset of 2,520 images incorporating controlled
variations in text length, font size, background color, and blur, the research
simulates diverse real-world challenges. Results emphasize the limitations of
zero-shot LLM-based OCR, particularly for linguistically complex scripts,
highlighting the need for annotated datasets and fine-tuned models. This work
underscores the urgency of addressing accessibility gaps in text digitization,
paving the way for inclusive and robust OCR solutions for underserved
languages.",164,2412.16119v1,cs.LG,"cs.LG,cs.CV,eess.IV",quantum computing,2024-12-20,2024-12-23T21:06:41.980825
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",quantum computing,2024-12-20,2024-12-23T21:06:41.981822
PruneVid: Visual Token Pruning for Efficient Video Large Language Models,"In this paper, we introduce PruneVid, a visual token pruning method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing pruning methods. Code:
https://github.com/Visual-AI/PruneVid.",204,2412.16117v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.982819
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",quantum computing,2024-12-20,2024-12-23T21:06:41.982819
The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse,"There is an ongoing debate about how to moderate toxic speech on social media
and how content moderation affects online discourse. We propose and validate a
methodology for measuring the content-moderation-induced distortions in online
discourse using text embeddings from computational linguistics. We test our
measure on a representative dataset of 5 million US political Tweets and find
that removing toxic Tweets distorts online content. This finding is consistent
across different embedding models, toxicity metrics, and samples. Importantly,
we demonstrate that content-moderation-induced distortions are not caused by
the toxic language. Instead, we show that, as a side effect, content moderation
shifts the mean and variance of the embedding space, distorting the topic
composition of online content. Finally, we propose an alternative approach to
content moderation that uses generative Large Language Models to rephrase toxic
Tweets to preserve their salvageable content rather than removing them
entirely. We demonstrate that this rephrasing strategy reduces toxicity while
minimizing distortions in online content.",220,2412.16114v1,cs.SI,cs.SI,quantum computing,2024-12-20,2024-12-23T21:06:41.983817
CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up,"Diffusion Transformers (DiT) have become a leading architecture in image
generation. However, the quadratic complexity of attention mechanisms, which
are responsible for modeling token-wise relationships, results in significant
latency when generating high-resolution images. To address this issue, we aim
at a linear attention mechanism in this paper that reduces the complexity of
pre-trained DiTs to linear. We begin our exploration with a comprehensive
summary of existing efficient attention mechanisms and identify four key
factors crucial for successful linearization of pre-trained DiTs: locality,
formulation consistency, high-rank attention maps, and feature integrity. Based
on these insights, we introduce a convolution-like local attention strategy
termed CLEAR, which limits feature interactions to a local window around each
query token, and thus achieves linear complexity. Our experiments indicate
that, by fine-tuning the attention layer on merely 10K self-generated samples
for 10K iterations, we can effectively transfer knowledge from a pre-trained
DiT to a student model with linear complexity, yielding results comparable to
the teacher model. Simultaneously, it reduces attention computations by 99.5%
and accelerates generation by 6.3 times for generating 8K-resolution images.
Furthermore, we investigate favorable properties in the distilled attention
layers, such as zero-shot generalization cross various models and plugins, and
improved support for multi-GPU parallel inference. Models and codes are
available here: https://github.com/Huage001/CLEAR.",308,2412.16112v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.984814
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",quantum computing,2024-12-20,2024-12-23T21:06:41.984814
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",quantum computing,2024-12-20,2024-12-23T21:06:41.984814
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",quantum computing,2024-12-20,2024-12-23T21:06:41.985811
Logical Consistency of Large Language Models in Fact-checking,"In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses -- a meaning-preserving change in the input query
results in an inconsistent response and attributes to vulnerabilities of LLMs
such as hallucination, jailbreaking, etc. Consequently, existing research
focuses on simple paraphrasing-based consistency assessment of LLMs, and
ignores complex queries that necessitates an even better understanding of
logical reasoning by an LLM. Our work therefore addresses the logical
inconsistency of LLMs under complex logical queries with primitive logical
operators, e.g., negation, conjunction, and disjunction. As a test bed, we
consider retrieval-augmented LLMs on a fact-checking task involving
propositional logic queries from real-world knowledge graphs (KGs). Our
contributions are three-fold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries as input and demonstrate that existing LLMs lack
logical consistency, specially on complex queries. Improvement: We employ
supervised fine-tuning to improve the logical consistency of LLMs on the
complex fact-checking task with KG contexts.",309,2412.16100v1,cs.CL,cs.CL,quantum computing,2024-12-20,2024-12-23T21:06:41.986809
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",quantum computing,2024-12-20,2024-12-23T21:06:41.986809
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",quantum computing,2024-12-20,2024-12-23T21:06:41.987806
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,quantum computing,2024-12-20,2024-12-23T21:06:41.988803
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",quantum computing,2024-12-20,2024-12-23T21:06:41.988803
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,quantum computing,2024-12-20,2024-12-23T21:06:41.989801
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,quantum computing,2024-12-20,2024-12-23T21:06:41.990798
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",quantum computing,2024-12-20,2024-12-23T21:06:41.990798
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",quantum computing,2024-12-20,2024-12-23T21:06:41.991795
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",quantum computing,2024-12-20,2024-12-23T21:06:41.991795
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,quantum computing,2024-12-20,2024-12-23T21:06:41.993301
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",quantum computing,2024-12-20,2024-12-23T21:06:41.993301
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",quantum computing,2024-12-20,2024-12-23T21:06:41.994300
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,quantum computing,2024-12-20,2024-12-23T21:06:41.995297
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,quantum computing,2024-12-20,2024-12-23T21:06:41.995297
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,quantum computing,2024-12-20,2024-12-23T21:06:41.996294
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",quantum computing,2024-12-20,2024-12-23T21:06:41.997292
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",quantum computing,2024-12-20,2024-12-23T21:06:41.998289
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",quantum computing,2024-12-20,2024-12-23T21:06:41.998289
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",robotics,2024-12-20,2024-12-23T21:06:42.750907
Allocation for Omnidirectional Aerial Robots: Incorporating Power Dynamics,"Tilt-rotor aerial robots are more dynamic and versatile than their
fixed-rotor counterparts, since the thrust vector and body orientation are
decoupled. However, the coordination of servomotors and propellers (the
allocation problem) is not trivial, especially accounting for overactuation and
actuator dynamics. We present and compare different methods of actuator
allocation for tilt-rotor platforms, evaluating them on a real aerial robot
performing dynamic trajectories. We extend the state-of-the-art geometric
allocation into a differential allocation, which uses the platform's redundancy
and does not suffer from singularities typical of the geometric solution. We
expand it by incorporating actuator dynamics and introducing propeller limit
curves. These improve the modeling of propeller limits, automatically balancing
their usage and allowing the platform to selectively activate and deactivate
propellers during flight. We show that actuator dynamics and limits make the
tuning of the allocation not only easier, but also allow it to track more
dynamic oscillating trajectories with angular velocities up to 4 rad/s,
compared to 2.8 rad/s of geometric methods.",238,2412.16107v1,cs.RO,cs.RO,robotics,2024-12-20,2024-12-23T21:06:42.750907
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,robotics,2024-12-20,2024-12-23T21:06:42.751904
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",robotics,2024-12-20,2024-12-23T21:06:42.752902
Speedup Techniques for Switchable Temporal Plan Graph Optimization,"Multi-Agent Path Finding (MAPF) focuses on planning collision-free paths for
multiple agents. However, during the execution of a MAPF plan, agents may
encounter unexpected delays, which can lead to inefficiencies, deadlocks, or
even collisions. To address these issues, the Switchable Temporal Plan Graph
provides a framework for finding an acyclic Temporal Plan Graph with the
minimum execution cost under delays, ensuring deadlock- and collision-free
execution. Unfortunately, existing optimal algorithms, such as Mixed Integer
Linear Programming and Graph-Based Switchable Edge Search (GSES), are often too
slow for practical use. This paper introduces Improved GSES, which
significantly accelerates GSES through four speedup techniques: stronger
admissible heuristics, edge grouping, prioritized branching, and incremental
implementation. Experiments conducted on four different map types with varying
numbers of agents demonstrate that Improved GSES consistently achieves over
twice the success rate of GSES and delivers up to a 30-fold speedup on
instances where both methods successfully find solutions.",220,2412.15908v1,cs.MA,"cs.MA,cs.AI,cs.RO",robotics,2024-12-20,2024-12-23T21:06:42.752902
Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis,"Complying with traffic rules is challenging for automated vehicles, as
numerous rules need to be considered simultaneously. If a planned trajectory
violates traffic rules, it is common to replan a new trajectory from scratch.
We instead propose a trajectory repair technique to save computation time. By
coupling satisfiability modulo theories with set-based reachability analysis,
we determine if and in what manner the initial trajectory can be repaired.
Experiments in high-fidelity simulators and in the real world demonstrate the
benefits of our proposed approach in various scenarios. Even in complex
environments with intricate rules, we efficiently and reliably repair
rule-violating trajectories, enabling automated vehicles to swiftly resume
legally safe operation in real-time.",146,2412.15837v1,cs.RO,"cs.RO,cs.AI",robotics,2024-12-20,2024-12-23T21:06:42.753899
FTISS Adaptive Bearing-Only Formation Tracking Control with Unknown Disturbance Rejection,"This paper proposes a finite-time input-to-state stable (FTISS) bearing-only
formation control law that rejects unknown constant disturbances. Unlike
existing finite-time bearing-based formation control laws, which typically rely
on the availability of a global coordinate frame and some information about the
disturbances, our approach requires only local bearing vector measurements and
does not necessitate the alignment of agent coordinate frames. The proposed
control law guarantees that formation control errors converge to a neighborhood
of zero in finite time, and subsequently converge to zero asymptotically. We
first address the scenario where leaders are stationary and then extend the
results to leaders moving with a constant velocity. Simulation and experimental
results are presented to validate the effectiveness of the proposed control
law.",151,2412.15757v1,eess.SY,"eess.SY,cs.SY",robotics,2024-12-20,2024-12-23T21:06:42.753899
Probabilistic Latent Variable Modeling for Dynamic Friction Identification and Estimation,"Precise identification of dynamic models in robotics is essential to support
control design, friction compensation, output torque estimation, etc. A
longstanding challenge remains in the identification of friction models for
robotic joints, given the numerous physical phenomena affecting the underlying
friction dynamics which result into nonlinear characteristics and hysteresis
behaviour in particular. These phenomena proof difficult to be modelled and
captured accurately using physical analogies alone. This has motivated
researchers to shift from physics-based to data-driven models. Currently, these
methods are still limited in their ability to generalize effectively to typical
industrial robot deployement, characterized by high- and low-velocity
operations and frequent direction reversals. Empirical observations motivate
the use of dynamic friction models but these remain particulary challenging to
establish. To address the current limitations, we propose to account for
unidentified dynamics in the robot joints using latent dynamic states. The
friction model may then utilize both the dynamic robot state and additional
information encoded in the latent state to evaluate the friction torque. We
cast this stochastic and partially unsupervised identification problem as a
standard probabilistic representation learning problem. In this work both the
friction model and latent state dynamics are parametrized as neural networks
and integrated in the conventional lumped parameter dynamic robot model. The
complete dynamics model is directly learned from the noisy encoder measurements
in the robot joints. We use the Expectation-Maximisation (EM) algorithm to find
a Maximum Likelihood Estimate (MLE) of the model parameters. The effectiveness
of the proposed method is validated in terms of open-loop prediction accuracy
in comparison with baseline methods, using the Kuka KR6 R700 as a test
platform.",341,2412.15756v1,cs.RO,"cs.RO,cs.LG,cs.SY,eess.SY",robotics,2024-12-20,2024-12-23T21:06:42.755894
Dexterous Manipulation Based on Prior Dexterous Grasp Pose Knowledge,"Dexterous manipulation has received considerable attention in recent
research. Predominantly, existing studies have concentrated on reinforcement
learning methods to address the substantial degrees of freedom in hand
movements. Nonetheless, these methods typically suffer from low efficiency and
accuracy. In this work, we introduce a novel reinforcement learning approach
that leverages prior dexterous grasp pose knowledge to enhance both efficiency
and accuracy. Unlike previous work, they always make the robotic hand go with a
fixed dexterous grasp pose, We decouple the manipulation process into two
distinct phases: initially, we generate a dexterous grasp pose targeting the
functional part of the object; after that, we employ reinforcement learning to
comprehensively explore the environment. Our findings suggest that the majority
of learning time is expended in identifying the appropriate initial position
and selecting the optimal manipulation viewpoint. Experimental results
demonstrate significant improvements in learning efficiency and success rates
across four distinct tasks.",177,2412.15587v1,cs.RO,"cs.RO,cs.LG",robotics,2024-12-20,2024-12-23T21:06:42.755894
SaliencyI2PLoc: saliency-guided image-point cloud localization using contrastive learning,"Image to point cloud global localization is crucial for robot navigation in
GNSS-denied environments and has become increasingly important for multi-robot
map fusion and urban asset management. The modality gap between images and
point clouds poses significant challenges for cross-modality fusion. Current
cross-modality global localization solutions either require modality
unification, which leads to information loss, or rely on engineered training
schemes to encode multi-modality features, which often lack feature alignment
and relation consistency. To address these limitations, we propose,
SaliencyI2PLoc, a novel contrastive learning based architecture that fuses the
saliency map into feature aggregation and maintains the feature relation
consistency on multi-manifold spaces. To alleviate the pre-process of data
mining, the contrastive learning framework is applied which efficiently
achieves cross-modality feature mapping. The context saliency-guided local
feature aggregation module is designed, which fully leverages the contribution
of the stationary information in the scene generating a more representative
global feature. Furthermore, to enhance the cross-modality feature alignment
during contrastive learning, the consistency of relative relationships between
samples in different manifold spaces is also taken into account. Experiments
conducted on urban and highway scenario datasets demonstrate the effectiveness
and robustness of our method. Specifically, our method achieves a Recall@1 of
78.92% and a Recall@20 of 97.59% on the urban scenario evaluation dataset,
showing an improvement of 37.35% and 18.07%, compared to the baseline method.
This demonstrates that our architecture efficiently fuses images and point
clouds and represents a significant step forward in cross-modality global
localization. The project page and code will be released.",351,2412.15577v1,cs.CV,"cs.CV,cs.LG,cs.RO",robotics,2024-12-20,2024-12-23T21:06:42.756891
QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning,"This paper addresses the inherent inference latency challenges associated
with deploying multimodal large language models (MLLM) in quadruped
vision-language-action (QUAR-VLA) tasks. Our investigation reveals that
conventional parameter reduction techniques ultimately impair the performance
of the language foundation model during the action instruction tuning phase,
making them unsuitable for this purpose. We introduce a novel latency-free
quadruped MLLM model, dubbed QUART-Online, designed to enhance inference
efficiency without degrading the performance of the language foundation model.
By incorporating Action Chunk Discretization (ACD), we compress the original
action representation space, mapping continuous action values onto a smaller
set of discrete representative vectors while preserving critical information.
Subsequently, we fine-tune the MLLM to integrate vision, language, and
compressed actions into a unified semantic space. Experimental results
demonstrate that QUART-Online operates in tandem with the existing MLLM system,
achieving real-time inference in sync with the underlying controller frequency,
significantly boosting the success rate across various tasks by 65\%. Our
project page is
\href{https://quart-online.github.io}https://quart-online.github.io.",258,2412.15576v1,cs.RO,"cs.RO,cs.CV",robotics,2024-12-20,2024-12-23T21:06:42.757888
Multi Agent Reinforcement Learning for Sequential Satellite Assignment Problems,"Assignment problems are a classic combinatorial optimization problem in which
a group of agents must be assigned to a group of tasks such that maximum
utility is achieved while satisfying assignment constraints. Given the utility
of each agent completing each task, polynomial-time algorithms exist to solve a
single assignment problem in its simplest form. However, in many modern-day
applications such as satellite constellations, power grids, and mobile robot
scheduling, assignment problems unfold over time, with the utility for a given
assignment depending heavily on the state of the system. We apply multi-agent
reinforcement learning to this problem, learning the value of assignments by
bootstrapping from a known polynomial-time greedy solver and then learning from
further experience. We then choose assignments using a distributed optimal
assignment mechanism rather than by selecting them directly. We demonstrate
that this algorithm is theoretically justified and avoids pitfalls experienced
by other RL algorithms in this setting. Finally, we show that our algorithm
significantly outperforms other methods in the literature, even while scaling
to realistic scenarios with hundreds of agents and tasks.",214,2412.15573v1,cs.MA,"cs.MA,cs.LG",robotics,2024-12-20,2024-12-23T21:06:42.757888
VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving,"In recent years, reinforcement learning (RL)-based methods for learning
driving policies have gained increasing attention in the autonomous driving
community and have achieved remarkable progress in various driving scenarios.
However, traditional RL approaches rely on manually engineered rewards, which
require extensive human effort and often lack generalizability. To address
these limitations, we propose \textbf{VLM-RL}, a unified framework that
integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward
signals using image observation and natural language goals. The core of VLM-RL
is the contrasting language goal (CLG)-as-reward paradigm, which uses positive
and negative language goals to generate semantic rewards. We further introduce
a hierarchical reward synthesis approach that combines CLG-based semantic
rewards with vehicle state information, improving reward stability and offering
a more comprehensive reward signal. Additionally, a batch-processing technique
is employed to optimize computational efficiency during training. Extensive
experiments in the CARLA simulator demonstrate that VLM-RL outperforms
state-of-the-art baselines, achieving a 10.5\% reduction in collision rate, a
104.6\% increase in route completion rate, and robust generalization to unseen
driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any
standard RL algorithms, potentially revolutionizing the existing RL paradigm
that relies on manual reward engineering and enabling continuous performance
improvements. The demo video and code can be accessed at:
https://zilin-huang.github.io/VLM-RL-website.",329,2412.15544v1,cs.RO,"cs.RO,cs.AI,cs.CV",robotics,2024-12-20,2024-12-23T21:06:42.759885
Enhancing Large-scale UAV Route Planing with Global and Local Features via Reinforcement Graph Fusion,"Numerous remarkable advancements have been made in accuracy, speed, and
parallelism for solving the Unmanned Aerial Vehicle Route Planing (UAVRP).
However, existing UAVRP solvers face challenges when attempting to scale
effectively and efficiently for larger instances. In this paper, we present a
generalization framework that enables current UAVRP solvers to robustly extend
their capabilities to larger instances, accommodating up to 10,000 points,
using widely recognized test sets. The UAVRP under a large number of patrol
points is a typical large-scale TSP problem.Our proposed framework comprises
three distinct steps. Firstly, we employ Delaunay triangulation to extract
subgraphs from large instances while preserving global features. Secondly, we
utilize an embedded TSP solver to obtain sub-results, followed by graph fusion.
Finally, we implement a decoding strategy customizable to the user's
requirements, resulting in high-quality solutions, complemented by a warming-up
process for the heatmap. To demonstrate the flexibility of our approach, we
integrate two representative TSP solvers into our framework and conduct a
comprehensive comparative analysis against existing algorithms using large TSP
benchmark datasets. The results unequivocally demonstrate that our framework
efficiently scales existing TSP solvers to handle large instances and
consistently outperforms state-of-the-art (SOTA) methods. Furthermore, since
our proposed framework does not necessitate additional training or fine-tuning,
we believe that its generality can significantly advance research on end-to-end
UAVRP solvers, enabling the application of a broader range of methods to
real-world scenarios.",342,2412.15537v1,cs.AI,"cs.AI,cs.RO",robotics,2024-12-20,2024-12-23T21:06:42.760881
Analyzing Fundamental Diagrams of Mixed Traffic Control at Unsignalized Intersections,"This report examines the effect of mixed traffic, specifically the variation
in robot vehicle (RV) penetration rates, on the fundamental diagrams at
unsignalized intersections. Through a series of simulations across four
distinct intersections, the relationship between traffic flow characteristics
were analyzed. The RV penetration rates were varied from 0% to 100% in
increments of 25%. The study reveals that while the presence of RVs influences
traffic dynamics, the impact on flow and speed is not uniform across different
levels of RV penetration. The fundamental diagrams indicate that intersections
may experience an increase in capacity with varying levels of RVs, but this
trend does not consistently hold as RV penetration approaches 100%. The
variability observed across intersections suggests that local factors possibly
influence the traffic flow characteristics. These findings highlight the
complexity of integrating RVs into the existing traffic system and underscore
the need for intersection-specific traffic management strategies to accommodate
the transition towards increased RV presence.",188,2412.15508v1,cs.RO,cs.RO,robotics,2024-12-20,2024-12-23T21:06:42.760881
Toward Appearance-based Autonomous Landing Site Identification for Multirotor Drones in Unstructured Environments,"A remaining challenge in multirotor drone flight is the autonomous
identification of viable landing sites in unstructured environments. One
approach to solve this problem is to create lightweight, appearance-based
terrain classifiers that can segment a drone's RGB images into safe and unsafe
regions. However, such classifiers require data sets of images and masks that
can be prohibitively expensive to create. We propose a pipeline to
automatically generate synthetic data sets to train these classifiers,
leveraging modern drones' ability to survey terrain automatically and the
ability to automatically calculate landing safety masks from terrain models
derived from such surveys. We then train a U-Net on the synthetic data set,
test it on real-world data for validation, and demonstrate it on our drone
platform in real-time.",157,2412.15486v1,cs.CV,"cs.CV,cs.LG,cs.RO",robotics,2024-12-20,2024-12-23T21:06:42.761879
TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through Large/Vision Language Models,"TalkWithMachines aims to enhance human-robot interaction by contributing to
interpretable industrial robotic systems, especially for safety-critical
applications. The presented paper investigates recent advancements in Large
Language Models (LLMs) and Vision Language Models (VLMs), in combination with
robotic perception and control. This integration allows robots to understand
and execute commands given in natural language and to perceive their
environment through visual and/or descriptive inputs. Moreover, translating the
LLM's internal states and reasoning into text that humans can easily understand
ensures that operators gain a clearer insight into the robot's current state
and intentions, which is essential for effective and safe operation. Our paper
outlines four LLM-assisted simulated robotic control workflows, which explore
(i) low-level control, (ii) the generation of language-based feedback that
describes the robot's internal states, (iii) the use of visual information as
additional input, and (iv) the use of robot structure information for
generating task plans and feedback, taking the robot's physical capabilities
and limitations into account. The proposed concepts are presented in a set of
experiments, along with a brief discussion. Project description, videos, and
supplementary materials will be available on the project website:
https://talk-machines.github.io.",266,2412.15462v1,cs.RO,"cs.RO,cs.AI,cs.CL,cs.HC,cs.LG",robotics,2024-12-19,2024-12-23T21:06:42.762876
LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction,"Photorealistic 3D scene reconstruction plays an important role in autonomous
driving, enabling the generation of novel data from existing datasets to
simulate safety-critical scenarios and expand training data without additional
acquisition costs. Gaussian Splatting (GS) facilitates real-time,
photorealistic rendering with an explicit 3D Gaussian representation of the
scene, providing faster processing and more intuitive scene editing than the
implicit Neural Radiance Fields (NeRFs). While extensive GS research has
yielded promising advancements in autonomous driving applications, they
overlook two critical aspects: First, existing methods mainly focus on
low-speed and feature-rich urban scenes and ignore the fact that highway
scenarios play a significant role in autonomous driving. Second, while LiDARs
are commonplace in autonomous driving platforms, existing methods learn
primarily from images and use LiDAR only for initial estimates or without
precise sensor modeling, thus missing out on leveraging the rich depth
information LiDAR offers and limiting the ability to synthesize LiDAR data. In
this paper, we propose a novel GS method for dynamic scene synthesis and
editing with improved scene reconstruction through LiDAR supervision and
support for LiDAR rendering. Unlike prior works that are tested mostly on urban
datasets, to the best of our knowledge, we are the first to focus on the more
challenging and highly relevant highway scenes for autonomous driving, with
sparse sensor views and monotone backgrounds.",281,2412.15447v1,cs.CV,"cs.CV,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.763873
An Environment-Adaptive Position/Force Control Based on Physical Property Estimation,"The technology for generating robot actions has significantly contributed to
the automation and efficiency of tasks. However, the ability to adapt to
objects of different shapes and hardness remains a challenge for general
industrial robots. Motion reproduction systems (MRS) replicate previously
acquired actions using position and force control, but generating actions for
significantly different environments is difficult. Furthermore, methods based
on machine learning require the acquisition of a large amount of motion data.
This paper proposes a new method that matches the impedance of two pre-recorded
action data with the current environmental impedance to generate highly
adaptable actions. This method recalculates the command values for position and
force based on the current impedance to improve reproducibility in different
environments. Experiments conducted under conditions of extreme action
impedance, such as position control and force control, confirmed the
superiority of the proposed method over MRS. The advantages of this method
include using only two sets of motion data, significantly reducing the burden
of data acquisition compared to machine learning-based methods, and eliminating
concerns about stability by using existing stable control systems. This study
contributes to improving robots' environmental adaptability while simplifying
the action generation method.",232,2412.15430v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.764870
AdaCred: Adaptive Causal Decision Transformers with Feature Crediting,"Reinforcement learning (RL) can be formulated as a sequence modeling problem,
where models predict future actions based on historical state-action-reward
sequences. Current approaches typically require long trajectory sequences to
model the environment in offline RL settings. However, these models tend to
over-rely on memorizing long-term representations, which impairs their ability
to effectively attribute importance to trajectories and learned representations
based on task-specific relevance. In this work, we introduce AdaCred, a novel
approach that represents trajectories as causal graphs built from short-term
action-reward-state sequences. Our model adaptively learns control policy by
crediting and pruning low-importance representations, retaining only those most
relevant for the downstream task. Our experiments demonstrate that
AdaCred-based policies require shorter trajectory sequences and consistently
outperform conventional methods in both offline reinforcement learning and
imitation learning environments.",188,2412.15427v1,cs.LG,"cs.LG,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.764870
"Tabletop Object Rearrangement: Structure, Complexity, and Efficient Combinatorial Search-Based Solutions","This thesis provides an in-depth structural analysis and efficient
algorithmic solutions for tabletop object rearrangement with overhand grasps
(TORO), a foundational task in advancing intelligent robotic manipulation.
Rearranging multiple objects in a confined workspace presents two primary
challenges: sequencing actions to minimize pick-and-place operations - an
NP-hard problem in TORO - and determining temporary object placements (""buffer
poses"") within a cluttered environment, which is essential yet highly complex.
For TORO with available external free space, this work investigates the minimum
buffer space, or ""running buffer size,"" required for temporary relocations,
presenting both theoretical insights and exact algorithms. For TORO without
external free space, the concept of lazy buffer verification is introduced,
with its efficiency evaluated across various manipulator configurations,
including single-arm, dual-arm, and mobile manipulators.",182,2412.15398v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.765868
Scalable and low-cost remote lab platforms: Teaching industrial robotics using open-source tools and understanding its social implications,"With recent advancements in industrial robots, educating students in new
technologies and preparing them for the future is imperative. However, access
to industrial robots for teaching poses challenges, such as the high cost of
acquiring these robots, the safety of the operator and the robot, and
complicated training material. This paper proposes two low-cost platforms built
using open-source tools like Robot Operating System (ROS) and its latest
version ROS 2 to help students learn and test algorithms on remotely connected
industrial robots. Universal Robotics (UR5) arm and a custom mobile rover were
deployed in different life-size testbeds, a greenhouse, and a warehouse to
create an Autonomous Agricultural Harvester System (AAHS) and an Autonomous
Warehouse Management System (AWMS). These platforms were deployed for a period
of 7 months and were tested for their efficacy with 1,433 and 1,312 students,
respectively. The hardware used in AAHS and AWMS was controlled remotely for
160 and 355 hours, respectively, by students over a period of 3 months.",211,2412.15369v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.765868
OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving,"Since the advent of Multimodal Large Language Models (MLLMs), they have made
a significant impact across a wide range of real-world applications,
particularly in Autonomous Driving (AD). Their ability to process complex
visual data and reason about intricate driving scenarios has paved the way for
a new paradigm in end-to-end AD systems. However, the progress of developing
end-to-end models for AD has been slow, as existing fine-tuning methods demand
substantial resources, including extensive computational power, large-scale
datasets, and significant funding. Drawing inspiration from recent advancements
in inference computing, we propose OpenEMMA, an open-source end-to-end
framework based on MLLMs. By incorporating the Chain-of-Thought reasoning
process, OpenEMMA achieves significant improvements compared to the baseline
when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates
effectiveness, generalizability, and robustness across a variety of challenging
driving scenarios, offering a more efficient and effective approach to
autonomous driving. We release all the codes in
https://github.com/taco-group/OpenEMMA.",240,2412.15208v1,cs.CV,"cs.CV,cs.LG,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.766865
AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving,"Recent advancements in large vision language models (VLMs) tailored for
autonomous driving (AD) have shown strong scene understanding and reasoning
capabilities, making them undeniable candidates for end-to-end driving systems.
However, limited work exists on studying the trustworthiness of DriveVLMs -- a
critical factor that directly impacts public transportation safety. In this
paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for
large vision-language models in autonomous driving (DriveVLMs), considering
diverse perspectives -- including trustfulness, safety, robustness, privacy,
and fairness. We constructed the largest visual question-answering dataset for
investigating trustworthiness issues in driving scenarios, comprising over 10k
unique scenes and 18k queries. We evaluated six publicly available VLMs,
spanning from generalist to specialist, from open-source to commercial models.
Our exhaustive evaluations have unveiled previously undiscovered
vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found
that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform
specialized models fine-tuned for driving in terms of overall trustworthiness.
DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing
sensitive information. Additionally, both generalist and specialist VLMs remain
susceptible to adversarial attacks and struggle to ensure unbiased
decision-making across diverse environments and populations. Our findings call
for immediate and decisive action to address the trustworthiness of DriveVLMs
-- an issue of critical importance to public safety and the welfare of all
citizens relying on autonomous transportation systems. Our benchmark is
publicly available at \url{https://github.com/taco-group/AutoTrust}, and the
leaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.",408,2412.15206v1,cs.CV,"cs.CV,cs.LG,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.767862
LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation,"This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.",279,2412.15199v1,cs.CV,"cs.CV,cs.LG,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.768861
STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning,"Robot learning is witnessing a significant increase in the size, diversity,
and complexity of pre-collected datasets, mirroring trends in domains such as
natural language processing and computer vision. Many robot learning methods
treat such datasets as multi-task expert data and learn a multi-task,
generalist policy by training broadly across them. Notably, while these
generalist policies can improve the average performance across many tasks, the
performance of generalist policies on any one task is often suboptimal due to
negative transfer between partitions of the data, compared to task-specific
specialist policies. In this work, we argue for the paradigm of training
policies during deployment given the scenarios they encounter: rather than
deploying pre-trained policies to unseen problems in a zero-shot manner, we
non-parametrically retrieve and train models directly on relevant data at test
time. Furthermore, we show that many robotics tasks share considerable amounts
of low-level behaviors and that retrieval at the ""sub""-trajectory granularity
enables significantly improved data utilization, generalization, and robustness
in adapting policies to novel problems. In contrast, existing full-trajectory
retrieval methods tend to underutilize the data and miss out on shared
cross-task content. This work proposes STRAP, a technique for leveraging
pre-trained vision foundation models and dynamic time warping to retrieve
sub-sequences of trajectories from large training corpora in a robust fashion.
STRAP outperforms both prior retrieval algorithms and multi-task learning
methods in simulated and real experiments, showing the ability to scale to much
larger offline datasets in the real world as well as the ability to learn
robust control policies with just a handful of real-world demonstrations.",350,2412.15182v1,cs.RO,"cs.RO,cs.LG,cs.SY,eess.SY",robotics,2024-12-19,2024-12-23T21:06:42.769857
Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration,"Humanoid robots are envisioned as embodied intelligent agents capable of
performing a wide range of human-level loco-manipulation tasks, particularly in
scenarios requiring strenuous and repetitive labor. However, learning these
skills is challenging due to the high degrees of freedom of humanoid robots,
and collecting sufficient training data for humanoid is a laborious process.
Given the rapid introduction of new humanoid platforms, a cross-embodiment
framework that allows generalizable skill transfer is becoming increasingly
critical. To address this, we propose a transferable framework that reduces the
data bottleneck by using a unified digital human model as a common prototype
and bypassing the need for re-training on every new robot platform. The model
learns behavior primitives from human demonstrations through adversarial
imitation, and the complex robot structures are decomposed into functional
components, each trained independently and dynamically coordinated. Task
generalization is achieved through a human-object interaction graph, and skills
are transferred to different robots via embodiment-specific kinematic motion
retargeting and dynamic fine-tuning. Our framework is validated on five
humanoid robots with diverse configurations, demonstrating stable
loco-manipulation and highlighting its effectiveness in reducing data
requirements and increasing the efficiency of skill transfer across platforms.",250,2412.15166v1,cs.RO,"cs.RO,cs.AI",robotics,2024-12-19,2024-12-23T21:06:42.770855
Measuring DNA Microswimmer Locomotion in Complex Flow Environments,"Microswimmers are sub-millimeter swimming microrobots that show potential as
a platform for controllable locomotion in applications including targeted cargo
delivery and minimally invasive surgery. To be viable for these target
applications, microswimmers will eventually need to be able to navigate in
environments with dynamic fluid flows and forces. Experimental studies with
microswimmers towards this goal are currently rare because of the difficulty
isolating intentional microswimmer motion from environment-induced motion. In
this work, we present a method for measuring microswimmer locomotion within a
complex flow environment using fiducial microspheres. By tracking the particle
motion of ferromagnetic and non-magnetic polystyrene fiducial microspheres, we
capture the effect of fluid flow and field gradients on microswimmer
trajectories. We then determine the field-driven translation of these
microswimmers relative to fluid flow and demonstrate the effectiveness of this
method by illustrating the motion of multiple microswimmers through different
flows.",216,2412.15152v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.770855
Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation,"Current efforts to learn scalable policies in robotic manipulation primarily
fall into two categories: one focuses on ""action,"" which involves behavior
cloning from extensive collections of robotic data, while the other emphasizes
""vision,"" enhancing model generalization by pre-training representations or
generative models, also referred to as world models, using large-scale visual
datasets. This paper presents an end-to-end paradigm that predicts actions
using inverse dynamics models conditioned on the robot's forecasted visual
states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop
between vision and action, the end-to-end PIDM can be a better scalable action
learner. In practice, we use Transformers to process both visual states and
actions, naming the model Seer. It is initially pre-trained on large-scale
robotic datasets, such as DROID, and can be adapted to realworld scenarios with
a little fine-tuning data. Thanks to large-scale, end-to-end training and the
synergy between vision and action, Seer significantly outperforms previous
methods across both simulation and real-world experiments. It achieves
improvements of 13% on the LIBERO-LONG benchmark, 21% on CALVIN ABC-D, and 43%
in real-world tasks. Notably, Seer sets a new state-of-the-art on CALVIN ABC-D
benchmark, achieving an average length of 4.28, and exhibits superior
generalization for novel objects, lighting conditions, and environments under
high-intensity disturbances on real-world scenarios. Code and models are
publicly available at https://github.com/OpenRobotLab/Seer/.",352,2412.15109v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.771852
Noise Analysis and Modeling of the PMD Flexx2 Depth Camera for Robotic Applications,"Time of Flight ToF cameras renowned for their ability to capture realtime 3D
information have become indispensable for agile mobile robotics These cameras
utilize light signals to accurately measure distances enabling robots to
navigate complex environments with precision Innovative depth cameras
characterized by their compact size and lightweight design such as the recently
released PMD Flexx2 are particularly suited for mobile robots Capable of
achieving high frame rates while capturing depth information this innovative
sensor is suitable for tasks such as robot navigation and terrain mapping
Operating on the ToF measurement principle the sensor offers multiple benefits
over classic stereobased depth cameras However the depth images produced by the
camera are subject to noise from multiple sources complicating their simulation
This paper proposes an accurate quantification and modeling of the
nonsystematic noise of the PMD Flexx2 We propose models for both axial and
lateral noise across various camera modes assuming Gaussian distributions Axial
noise modeled as a function of distance and incidence angle demonstrated a low
average KullbackLeibler KL divergence of 0015 nats reflecting precise noise
characterization Lateral noise deviating from a Gaussian distribution was
modeled conservatively yielding a satisfactory KL divergence of 0868 nats These
results validate our noise models crucial for accurately simulating sensor
behavior in virtual environments and reducing the simtoreal gap in
learningbased control approaches",268,2412.15040v1,eess.IV,"eess.IV,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.772849
Autonomous Navigation in Dynamic Human Environments with an Embedded 2D LiDAR-based Person Tracker,"In the rapidly evolving landscape of autonomous mobile robots, the emphasis
on seamless human-robot interactions has shifted towards autonomous
decision-making. This paper delves into the intricate challenges associated
with robotic autonomy, focusing on navigation in dynamic environments shared
with humans. It introduces an embedded real-time tracking pipeline, integrated
into a navigation planning framework for effective person tracking and
avoidance, adapting a state-of-the-art 2D LiDAR-based human detection network
and an efficient multi-object tracker. By addressing the key components of
detection, tracking, and planning separately, the proposed approach highlights
the modularity and transferability of each component to other applications. Our
tracking approach is validated on a quadruped robot equipped with 270{\deg}
2D-LiDAR against motion capture system data, with the preferred configuration
achieving an average MOTA of 85.45% in three newly recorded datasets, while
reliably running in real-time at 20 Hz on the NVIDIA Jetson Xavier NX embedded
GPU-accelerated platform. Furthermore, the integrated tracking and avoidance
system is evaluated in real-world navigation experiments, demonstrating how
accurate person tracking benefits the planner in optimizing the generated
trajectories, enhancing its collision avoidance capabilities. This paper
contributes to safer human-robot cohabitation, blending recent advances in
human detection with responsive planning to navigate shared spaces effectively
and securely.",283,2412.15000v1,cs.RO,"cs.RO,cs.SY,eess.SY",robotics,2024-12-19,2024-12-23T21:06:42.773847
RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots using Foundation Models for Perception and Planning,"We present the approaches and contributions of the winning team NimbRo@Home
at the RoboCup@Home 2024 competition in the Open Platform League held in
Eindhoven, NL. Further, we describe our hardware setup and give an overview of
the results for the task stages and the final demonstration. For this year's
competition, we put a special emphasis on open-vocabulary object segmentation
and grasping approaches that overcome the labeling overhead of supervised
vision approaches, commonly used in RoboCup@Home. We successfully demonstrated
that we can segment and grasp non-labeled objects by text descriptions.
Further, we extensively employed LLMs for natural language understanding and
task planning. Throughout the competition, our approaches showed robustness and
generalization capabilities. A video of our performance can be found online.",160,2412.14989v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.773847
Efficient Motion Sickness Assessment: Recreation of On-Road Driving on a Compact Test Track,"The ability to engage in other activities during the ride is considered by
consumers as one of the key reasons for the adoption of automated vehicles.
However, engagement in non-driving activities will provoke occupants' motion
sickness, deteriorating their overall comfort and thereby risking acceptance of
automated driving. Therefore, it is critical to extend our understanding of
motion sickness and unravel the modulating factors that affect it through
experiments with participants. Currently, most experiments are conducted on
public roads (realistic but not reproducible) or test tracks (feasible with
prototype automated vehicles). This research study develops a method to design
an optimal path and speed reference to efficiently replicate on-road motion
sickness exposure on a small test track. The method uses model predictive
control to replicate the longitudinal and lateral accelerations collected from
on-road drives on a test track of 70 m by 175 m. A within-subject experiment
(47 participants) was conducted comparing the occupants' motion sickness
occurrence in test-track and on-road conditions, with the conditions being
cross-randomized. The results illustrate no difference and no effect of the
condition on the occurrence of the average motion sickness across the
participants. Meanwhile, there is an overall correspondence of individual
sickness levels between on-road and test-track. This paves the path for the
employment of our method for a simpler, safer and more replicable assessment of
motion sickness.",278,2412.14982v1,cs.RO,"cs.RO,cs.ET,cs.HC",robotics,2024-12-19,2024-12-23T21:06:42.774844
Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations,"The acquisition of substantial volumes of 3D articulated object data is
expensive and time-consuming, and consequently the scarcity of 3D articulated
object data becomes an obstacle for deep learning methods to achieve remarkable
performance in various articulated object understanding tasks. Meanwhile,
pairing these object data with detailed annotations to enable training for
various tasks is also difficult and labor-intensive to achieve. In order to
expeditiously gather a significant number of 3D articulated objects with
comprehensive and detailed annotations for training, we propose Articulated
Object Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox
consists of i) descriptions of articulated objects by means of a generalized
structure program along with their analytic correspondence to the objects'
point cloud, ii) procedural rules about manipulations on the structure program
to synthesize large-scale and diverse new articulated objects, and iii)
mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to
provide annotations to the synthesized object. Arti-PG has two appealing
properties for providing training data for articulated object understanding
tasks: i) objects are created with unlimited variations in shape through
program-oriented structure manipulation, ii) Arti-PG is widely applicable to
diverse tasks by easily providing comprehensive and detailed annotations.
Arti-PG now supports the procedural generation of 26 categories of articulate
objects and provides annotations across a wide range of both vision and
manipulation tasks, and we provide exhaustive experiments which fully
demonstrate its advantages. We will make Arti-PG toolbox publicly available for
the community to use.",326,2412.14974v1,cs.CV,"cs.CV,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.775841
TDCNet: Transparent Objects Depth Completion with CNN-Transformer Dual-Branch Parallel Network,"The sensing and manipulation of transparent objects present a critical
challenge in industrial and laboratory robotics. Conventional sensors face
challenges in obtaining the full depth of transparent objects due to the
refraction and reflection of light on their surfaces and their lack of visible
texture. Previous research has attempted to obtain complete depth maps of
transparent objects from RGB and damaged depth maps (collected by depth sensor)
using deep learning models. However, existing methods fail to fully utilize the
original depth map, resulting in limited accuracy for deep completion. To solve
this problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel
network for transparent object depth completion. The proposed framework
consists of two different branches: one extracts features from partial depth
maps, while the other processes RGB-D images. Experimental results demonstrate
that our model achieves state-of-the-art performance across multiple public
datasets. Our code and the pre-trained model are publicly available at
https://github.com/XianghuiFan/TDCNet.",209,2412.14961v1,cs.CV,cs.CV,robotics,2024-12-19,2024-12-23T21:06:42.776838
Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination,"A world model provides an agent with a representation of its environment,
enabling it to predict the causal consequences of its actions. Current world
models typically cannot directly and explicitly imitate the actual environment
in front of a robot, often resulting in unrealistic behaviors and
hallucinations that make them unsuitable for real-world applications. In this
paper, we introduce a new paradigm for constructing world models that are
explicit representations of the real world and its dynamics. By integrating
cutting-edge advances in real-time photorealism with Gaussian Splatting and
physics simulators, we propose the first compositional manipulation world
model, which we call DreMa. DreMa replicates the observed world and its
dynamics, allowing it to imagine novel configurations of objects and predict
the future consequences of robot actions. We leverage this capability to
generate new data for imitation learning by applying equivariant
transformations to a small set of demonstrations. Our evaluations across
various settings demonstrate significant improvements in both accuracy and
robustness by incrementing actions and object distributions, reducing the data
needed to learn a policy and improving the generalization of the agents. As a
highlight, we show that a real Franka Emika Panda robot, powered by DreMa's
imagination, can successfully learn novel physical tasks from just a single
example per task variation (one-shot policy learning). Our project page and
source code can be found in https://leobarcellona.github.io/DreamToManipulate/",305,2412.14957v1,cs.RO,"cs.RO,cs.CV",robotics,2024-12-19,2024-12-23T21:06:42.777836
Vibration-based Full State In-Hand Manipulation of Thin Objects,"Robotic hands offer advanced manipulation capabilities, while their
complexity and cost often limit their real-world applications. In contrast,
simple parallel grippers, though affordable, are restricted to basic tasks like
pick-and-place. Recently, a vibration-based mechanism was proposed to augment
parallel grippers and enable in-hand manipulation capabilities for thin
objects. By utilizing the stick-slip phenomenon, a simple controller was able
to drive a grasped object to a desired position. However, due to the
underactuated nature of the mechanism, direct control of the object's
orientation was not possible. In this letter, we address the challenge of
manipulating the entire state of the object. Hence, we present the excitation
of a cyclic phenomenon where the object's center-of-mass rotates in a constant
radius about the grasping point. With this cyclic motion, we propose an
algorithm for manipulating the object to desired states. In addition to a full
analytical analysis of the cyclic phenomenon, we propose the use of duty cycle
modulation in operating the vibration actuator to provide more accurate
manipulation. Finite element analysis, experiments and task demonstrations
validate the proposed algorithm.",233,2412.14899v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.778832
Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning,"In dynamic domains such as autonomous robotics and video game simulations,
agents must continuously adapt to new tasks while retaining previously acquired
skills. This ongoing process, known as Continual Reinforcement Learning,
presents significant challenges, including the risk of forgetting past
knowledge and the need for scalable solutions as the number of tasks increases.
To address these issues, we introduce HIerarchical LOW-rank Subspaces of
Policies (HILOW), a novel framework designed for continual learning in offline
navigation settings. HILOW leverages hierarchical policy subspaces to enable
flexible and efficient adaptation to new tasks while preserving existing
knowledge. We demonstrate, through a careful experimental study, the
effectiveness of our method in both classical MuJoCo maze environments and
complex video game-like simulations, showcasing competitive performance and
satisfying adaptability according to classical continual learning metrics, in
particular regarding memory usage. Our work provides a promising framework for
real-world applications where continuous learning from pre-collected data is
essential.",192,2412.14865v1,cs.LG,cs.LG,robotics,2024-12-19,2024-12-23T21:06:42.778832
Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations,"Recent advancements in robotics have focused on developing generalist
policies capable of performing multiple tasks. Typically, these policies
utilize pre-trained vision encoders to capture crucial information from current
observations. However, previous vision encoders, which trained on two-image
contrastive learning or single-image reconstruction, can not perfectly capture
the sequential information essential for embodied tasks. Recently, video
diffusion models (VDMs) have demonstrated the capability to accurately predict
future image sequences, exhibiting a good understanding of physical dynamics.
Motivated by the strong visual prediction capabilities of VDMs, we hypothesize
that they inherently possess visual representations that reflect the evolution
of the physical world, which we term predictive visual representations.
Building on this hypothesis, we propose the Video Prediction Policy (VPP), a
generalist robotic policy conditioned on the predictive visual representations
from VDMs. To further enhance these representations, we incorporate diverse
human or robotic manipulation datasets, employing unified video-generation
training objectives. VPP consistently outperforms existing methods across two
simulated and two real-world benchmarks. Notably, it achieves a 28.1\% relative
improvement in the Calvin ABC-D benchmark compared to the previous
state-of-the-art and delivers a 28.8\% increase in success rates for complex
real-world dexterous manipulation tasks.",276,2412.14803v1,cs.CV,"cs.CV,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.779830
DCL-Sparse: Distributed Range-only Cooperative Localization of Multi-Robots in Noisy and Sparse Sensing Graphs,"This paper presents a novel approach to range-based cooperative localization
for robot swarms in GPS-denied environments, addressing the limitations of
current methods in noisy and sparse settings. We propose a robust multi-layered
localization framework that combines shadow edge localization techniques with
the strategic deployment of UAVs. This approach not only addresses the
challenges associated with nonrigid and poorly connected graphs but also
enhances the convergence rate of the localization process. We introduce two key
concepts: the S1-Edge approach in our distributed protocol to address the
rigidity problem of sparse graphs and the concept of a powerful UAV node to
increase the sensing and localization capability of the multi-robot system. Our
approach leverages the advantages of the distributed localization methods,
enhancing scalability and adaptability in large robot networks. We establish
theoretical conditions for the new S1-Edge that ensure solutions exist even in
the presence of noise, thereby validating the effectiveness of shadow edge
localization. Extensive simulation experiments confirm the superior performance
of our method compared to state-of-the-art techniques, resulting in up to 95\%
reduction in localization error, demonstrating substantial improvements in
localization accuracy and robustness to sparse graphs. This work provides a
decisive advancement in the field of multi-robot localization, offering a
powerful tool for high-performance and reliable operations in challenging
environments.",277,2412.14793v1,cs.RO,"cs.RO,cs.MA",robotics,2024-12-19,2024-12-23T21:06:42.780828
Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning,"In multi-agent environments, agents often struggle to learn optimal policies
due to sparse or delayed global rewards, particularly in long-horizon tasks
where it is challenging to evaluate actions at intermediate time steps. We
introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach
designed to address the agent-temporal credit assignment problem by
redistributing sparse rewards both temporally and across agents. TAR$^2$
decomposes sparse global rewards into time-step-specific rewards and calculates
agent-specific contributions to these rewards. We theoretically prove that
TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the
optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$
stabilizes and accelerates the learning process. Additionally, we show that
when TAR$^2$ is integrated with single-agent reinforcement learning algorithms,
it performs as well as or better than traditional multi-agent reinforcement
learning methods.",194,2412.14779v1,cs.MA,"cs.MA,cs.AI,cs.GT,cs.LG,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.781826
A General Control Method for Human-Robot Integration,"This paper introduces a new generalized control method designed for
multi-degrees-of-freedom devices to help people with limited motion
capabilities in their daily activities. The challenge lies in finding the most
adapted strategy for the control interface to effectively map user's motions in
a low-dimensional space to complex robotic assistive devices, such as
prostheses, supernumerary limbs, up to remote robotic avatars. The goal is a
system which integrates the human and the robotic parts into a unique system,
moving so as to reach the targets decided by the human while autonomously
reducing the user's effort and discomfort. We present a framework to control
general multi DoFs assistive systems, which translates user-performed
compensatory motions into the necessary robot commands for reaching targets
while canceling or reducing compensation. The framework extends to prostheses
of any number of DoF up to full robotic avatars, regarded here as a sort of
whole-body prosthesis of the person who sees the robot as an artificial
extension of their own body without a physical link but with a sensory-motor
integration. We have validated and applied this control strategy through tests
encompassing simulated scenarios and real-world trials involving a virtual twin
of the robotic parts (prosthesis and robot) and a physical humanoid avatar.",266,2412.14762v1,cs.RO,"cs.RO,Robotics (cs.RO)",robotics,2024-12-19,2024-12-23T21:06:42.782823
A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space,"Open-set object detection (OSOD) is highly desirable for robotic manipulation
in unstructured environments. However, existing OSOD methods often fail to meet
the requirements of robotic applications due to their high computational burden
and complex deployment. To address this issue, this paper proposes a
light-weight framework called Decoupled OSOD (DOSOD), which is a practical and
highly efficient solution to support real-time OSOD tasks in robotic systems.
Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a
vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)
adaptor is developed to transform text embeddings extracted by the VLM into a
joint space, within which the detector learns the region representations of
class-agnostic proposals. Cross-modality features are directly aligned in the
joint space, avoiding the complex feature interactions and thereby improving
computational efficiency. DOSOD operates like a traditional closed-set detector
during the testing phase, effectively bridging the gap between closed-set and
open-set detection. Compared to the baseline YOLO-World, the proposed DOSOD
significantly enhances real-time performance while maintaining comparable
accuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\%$, compared to
$26.2\%$ for YOLO-World-v1-S and $22.7\%$ for YOLO-World-v2-S, using similar
backbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is
$57.1\%$ higher than YOLO-World-v1-S and $29.6\%$ higher than YOLO-World-v2-S.
Meanwhile, we demonstrate that the DOSOD model facilitates the deployment of
edge devices. The codes and models are publicly available at
https://github.com/D-Robotics-AI-Lab/DOSOD.",412,2412.14680v1,cs.CV,"cs.CV,cs.AI,cs.RO",robotics,2024-12-19,2024-12-23T21:06:42.783821
Optimization of Collective Bayesian Decision-Making in a Swarm of Miniaturized Vibration-Sensing Robots,"Inspection of infrastructure using static sensor nodes has become a well
established approach in recent decades. In this work, we present an
experimental setup to address a binary inspection task using mobile sensor
nodes. The objective is to identify the predominant tile type in a 1mx1m tiled
surface composed of vibrating and non-vibrating tiles. A swarm of miniaturized
robots, equipped with onboard IMUs for sensing and IR sensors for collision
avoidance, performs the inspection. The decision-making approach leverages a
Bayesian algorithm, updating robots' belief using inference. The original
algorithm uses one of two information sharing strategies. We introduce a novel
information sharing strategy, aiming to accelerate the decision-making. To
optimize the algorithm parameters, we develop a simulation framework calibrated
to our real-world setup in the high-fidelity Webots robotic simulator. We
evaluate the three information sharing strategies through simulations and
real-world experiments. Moreover, we test the effectiveness of our optimization
by placing swarms with optimized and non-optimized parameters in increasingly
complex environments with varied spatial correlation and fill ratios. Results
show that our proposed information sharing strategy consistently outperforms
previously established information-sharing strategies in decision time.
Additionally, optimized parameters yield robust performance across different
environments. Conversely, non-optimized parameters perform well in simpler
scenarios but show reduced accuracy in complex settings.",277,2412.14646v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.783821
"Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities","With the increasing demand for seamless connectivity and intelligent
communication, the integration of artificial intelligence (AI) and
communication for sixth-generation (6G) network is emerging as a revolutionary
architecture. This paper presents a comprehensive overview of AI and
communication for 6G networks, emphasizing their foundational principles,
inherent challenges, and future research opportunities. We commence with a
retrospective analysis of AI and the evolution of large-scale AI models,
underscoring their pivotal roles in shaping contemporary communication
technologies. The discourse then transitions to a detailed exposition of the
envisioned integration of AI within 6G networks, delineated across three
progressive developmental stages. The initial stage, AI for Network, focuses on
employing AI to augment network performance, optimize efficiency, and enhance
user service experiences. The subsequent stage, Network for AI, highlights the
role of the network in facilitating and buttressing AI operations and presents
key enabling technologies, including digital twins for AI and semantic
communication. In the final stage, AI as a Service, it is anticipated that
future 6G networks will innately provide AI functions as services and support
application scenarios like immersive communication and intelligent industrial
robots. Specifically, we have defined the quality of AI service, which refers
to the measurement framework system of AI services within the network. In
addition to these developmental stages, we thoroughly examine the
standardization processes pertinent to AI in network contexts, highlighting key
milestones and ongoing efforts. Finally, we outline promising future research
opportunities that could drive the evolution and refinement of AI and
communication for 6G, positioning them as a cornerstone of next-generation
communication infrastructure.",325,2412.14538v2,cs.NI,"cs.NI,cs.AI,eess.SP",robotics,2024-12-19,2024-12-23T21:06:42.784817
Embedding high-resolution touch across robotic hands enables adaptive human-like grasping,"Developing robotic hands that adapt to real-world dynamics remains a
fundamental challenge in robotics and machine intelligence. Despite significant
advances in replicating human hand kinematics and control algorithms, robotic
systems still struggle to match human capabilities in dynamic environments,
primarily due to inadequate tactile feedback. To bridge this gap, we present
F-TAC Hand, a biomimetic hand featuring high-resolution tactile sensing (0.1mm
spatial resolution) across 70% of its surface area. Through optimized hand
design, we overcome traditional challenges in integrating high-resolution
tactile sensors while preserving the full range of motion. The hand, powered by
our generative algorithm that synthesizes human-like hand configurations,
demonstrates robust grasping capabilities in dynamic real-world conditions.
Extensive evaluation across 600 real-world trials demonstrates that this
tactile-embodied system significantly outperforms non-tactile alternatives in
complex manipulation tasks (p<0.0001). These results provide empirical evidence
for the critical role of rich tactile embodiment in developing advanced robotic
intelligence, offering new perspectives on the relationship between physical
sensing capabilities and intelligent behavior.",236,2412.14482v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.785815
GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering,"In Embodied Question Answering (EQA), agents must explore and develop a
semantic understanding of an unseen environment in order to answer a situated
question with confidence. This remains a challenging problem in robotics, due
to the difficulties in obtaining useful semantic representations, updating
these representations online, and leveraging prior world knowledge for
efficient exploration and planning. Aiming to address these limitations, we
propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic
scene graphs (3DSGs) and task relevant images as multi-modal memory for
grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen
environments. We employ a hierarchical planning approach that exploits the
hierarchical nature of 3DSGs for structured planning and semantic-guided
exploration. Through experiments in simulation on the HM-EQA dataset and in the
real world in home and office environments, we demonstrate that our method
outperforms key baselines by completing EQA tasks with higher success rates and
fewer planning steps.",203,2412.14480v1,cs.RO,"cs.RO,cs.CL,cs.CV,cs.LG",robotics,2024-12-19,2024-12-23T21:06:42.786812
EPN: An Ego Vehicle Planning-Informed Network for Target Trajectory Prediction,"Trajectory prediction plays a crucial role in improving the safety and
reliability of autonomous vehicles, serving as an intermediate link between
perception and planning. However, due to the highly dynamic and multimodal
nature of the task, accurately predicting the future trajectory of a target
vehicle remains a significant challenge. To address these challenges, we
propose an Ego vehicle Planning-informed Network (EPN) for multimodal
trajectory prediction. Current trajectory prediction methods typically use the
historical trajectory and vehicle attributes as inputs, focusing primarily on
how historical information influences the future trajectory of the target
vehicle. In real-world driving scenarios, however, the future trajectory of a
vehicle is influenced not only by its own historical data but also by the
behavior of other vehicles on the road. To address this, we incorporate the
future planned trajectory of the ego vehicle as an additional input to simulate
the mutual influence between the ego vehicle's planned trajectory and the
predicted trajectory of the target vehicle. Furthermore, to tackle the
challenges of intention ambiguity and large prediction errors often encountered
in methods based on driving intentions, we propose a target's endpoint
prediction module. This module first predicts the possible endpoints of the
target vehicle, then refines these predictions through a correction mechanism,
and finally generates a complete multimodal predicted trajectory based on the
corrected endpoints. Experimental results demonstrate that, compared to other
trajectory prediction methods, EPN achieves an average reduction of 34.9%,
30.7%, and 30.4% in RMSE, ADE, and FDE evaluation metrics on the NGSIM dataset,
and an average reduction of 64.6%, 64.5%, and 64.3% in RMSE, ADE, and FDE on
the HighD dataset. These results highlight the strong performance of EPN in
trajectory prediction.",369,2412.14442v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.787810
Enhancing Diffusion Models for High-Quality Image Generation,"This report presents the comprehensive implementation, evaluation, and
optimization of Denoising Diffusion Probabilistic Models (DDPMs) and Denoising
Diffusion Implicit Models (DDIMs), which are state-of-the-art generative
models. During inference, these models take random noise as input and
iteratively generate high-quality images as output. The study focuses on
enhancing their generative capabilities by incorporating advanced techniques
such as Classifier-Free Guidance (CFG), Latent Diffusion Models with
Variational Autoencoders (VAE), and alternative noise scheduling strategies.
The motivation behind this work is the growing demand for efficient and
scalable generative AI models that can produce realistic images across diverse
datasets, addressing challenges in applications such as art creation, image
synthesis, and data augmentation. Evaluations were conducted on datasets
including CIFAR-10 and ImageNet-100, with a focus on improving inference speed,
computational efficiency, and image quality metrics like Frechet Inception
Distance (FID). Results demonstrate that DDIM + CFG achieves faster inference
and superior image quality. Challenges with VAE and noise scheduling are also
highlighted, suggesting opportunities for future optimization. This work lays
the groundwork for developing scalable, efficient, and high-quality generative
AI systems to benefit industries ranging from entertainment to robotics.",280,2412.14422v1,cs.CV,"cs.CV,cs.AI,cs.LG",robotics,2024-12-19,2024-12-23T21:06:42.787810
Cutting Sequence Diffuser: Sim-to-Real Transferable Planning for Object Shaping by Grinding,"Automating object shaping by grinding with a robot is a crucial industrial
process that involves removing material with a rotating grinding belt. This
process generates removal resistance depending on such process conditions as
material type, removal volume, and robot grinding posture, all of which
complicate the analytical modeling of shape transitions. Additionally, a
data-driven approach based on real-world data is challenging due to high data
collection costs and the irreversible nature of the process. This paper
proposes a Cutting Sequence Diffuser (CSD) for object shaping by grinding. The
CSD, which only requires simple simulation data for model learning, offers an
efficient way to plan long-horizon action sequences transferable to the real
world. Our method designs a smooth action space with constrained small removal
volumes to suppress the complexity of the shape transitions caused by removal
resistance, thus reducing the reality gap in simulations. Moreover, by using a
diffusion model to generate long-horizon action sequences, our approach reduces
the planning time and allows for grinding the target shape while adhering to
the constraints of a small removal volume per step. Through evaluations in both
simulation and real robot experiments, we confirmed that our CSD was effective
for grinding to different materials and various target shapes in a short time.",252,2412.14417v1,cs.RO,cs.RO,robotics,2024-12-19,2024-12-23T21:06:42.788807
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",biomedical engineering,2024-12-20,2024-12-23T21:06:43.535918
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.536916
Interleaved Speech-Text Language Models are Simple Streaming Text to Speech Synthesizers,"This paper introduces Interleaved Speech-Text Language Model (IST-LM) for
streaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,
IST-LM is directly trained on interleaved sequences of text and speech tokens
with a fixed ratio, eliminating the need for additional efforts in duration
prediction and grapheme-to-phoneme alignment. The ratio of text chunk size to
speech chunk size is crucial for the performance of IST-LM. To explore this, we
conducted a comprehensive series of statistical analyses on the training data
and performed correlation analysis with the final performance, uncovering
several key factors: 1) the distance between speech tokens and their
corresponding text tokens, 2) the number of future text tokens accessible to
each speech token, and 3) the frequency of speech tokens precedes their
corresponding text tokens. Experimental results demonstrate how to achieve an
optimal streaming TTS system without complicated engineering, which we show has
a limited gap with the non-streaming system. IST-LM is conceptually simple and
empirically powerful, paving the way for streaming TTS with minimal overhead
while largely maintaining performance.",239,2412.16102v1,eess.AS,eess.AS,biomedical engineering,2024-12-20,2024-12-23T21:06:43.537913
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",biomedical engineering,2024-12-20,2024-12-23T21:06:43.537913
Chiral phase-imaging meta-sensors,"Light waves possess multiple degrees of freedom besides intensity, including
phase and polarization, that often contain important information but require
complex and bulky systems for their measurement. Here we report a pair of
compact multifunctional photodetectors that can selectively measure the local
phase gradient of, respectively, the right and left circular-polarization
component of any incident wave. These devices employ a chiral pair of
integrated plasmonic metasurfaces to introduce a sharp dependence of
responsivity on local direction of propagation of the desired polarization
component. An order-of-magnitude polarization selectivity with respect to phase
gradient is demonstrated with both devices. Using the measured device
characteristics, we also describe computationally a pixel array that allows for
the simultaneous separate mapping of the right and left circularly-polarized
incident wavefronts in a particularly simple imaging setup. These unique
capabilities may be exploited to enable new functionalities for applications in
chemical sensing, biomedical microscopy, and machine vision.",201,2412.16084v1,physics.optics,physics.optics,biomedical engineering,2024-12-20,2024-12-23T21:06:43.538910
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",biomedical engineering,2024-12-20,2024-12-23T21:06:43.539908
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",biomedical engineering,2024-12-20,2024-12-23T21:06:43.539908
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,biomedical engineering,2024-12-20,2024-12-23T21:06:43.540905
Self-organized critical characteristics of TeV-photons from GRB 221009A,"The very high-energy afterglow in GRB 221009A, known as the `Brightest Of All
Time' (B.O.A.T.), has been thoroughly analyzed in previous studies. In this
paper, we conducted a statistical analysis of the waiting time behavior of 172
TeV photons from the B.O.A.T. observed by LHAASO-KM2A. The following results
were obtained: (I) The waiting time distribution (WTD) of these photons
deviates from the exponential distribution. (II) The behavior of these photons
exhibits characteristics resembling those of a self-organized critical system,
such as power-law distribution and scale-invariance features in the waiting
time distribution. The power-law distribution of waiting times is consistent
with the prediction of a non-stationary process. (III) The relationship between
the power-law slopes of the WTD and the scale-invariant characteristics of the
Tsallis q-Gaussian distribution deviates from existing theory. We suggest that
this deviation is due to the photons not being completely independent of each
other. In summary, the power-law and scale-free characteristics observed in
these photons imply a self-organized critical process in the generation of TeV
photons from GRB 221009A. Based on other relevant research, we propose that the
involvement of a partially magnetically dominated component and the continuous
energy injection from the central engine can lead to deviations in the
generation of TeV afterglow from the simple external shock-dominated process,
thereby exhibiting the self-organized critical characteristics mentioned above.",335,2412.16052v1,astro-ph.HE,astro-ph.HE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.541902
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",biomedical engineering,2024-12-20,2024-12-23T21:06:43.542901
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.543898
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.544895
Fuzzy-Space Engineering,"The techniques developed for matrix models and fuzzy geometry are powerful
tools for representing strings and membranes in quantum physics. We study the
representation of fuzzy surfaces using these techniques. This involves
constructing graphs and writing their coordinates and connectivity into
matrices. To construct arbitrary graphs and quickly change them, we use 3D
software. A script generates the three matrices from the graphs. These matrices
are then processed in Wolfram Mathematica to calculate the zero modes of the
Dirac operator. Our first result shows the quantization of a two-dimensional
Trefoil knot. Additional examples illustrate various properties and behaviors
of this process. This helps us to gain a deeper understanding of fuzzy spaces
and zero-mode surfaces. This work contributes to advancing the understanding of
visualization aspects in fuzzy geometry.",153,2412.16011v1,hep-th,hep-th,biomedical engineering,2024-12-20,2024-12-23T21:06:43.545893
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.545893
APIRL: Deep Reinforcement Learning for REST API Fuzzing,"REST APIs have become key components of web services. However, they often
contain logic flaws resulting in server side errors or security
vulnerabilities. HTTP requests are used as test cases to find and mitigate such
issues. Existing methods to modify requests, including those using deep
learning, suffer from limited performance and precision, relying on undirected
search or making limited usage of the contextual information. In this paper we
propose APIRL, a fully automated deep reinforcement learning tool for testing
REST APIs. A key novelty of our approach is the use of feedback from a
transformer module pre-trained on JSON-structured data, akin to that used in
API responses. This allows APIRL to learn the subtleties relating to test
outcomes, and generalise to unseen API endpoints. We show APIRL can find
significantly more bugs than the state-of-the-art in real world REST APIs while
minimising the number of required test cases. We also study how reward
functions, and other key design choices, affect learnt policies in a thorough
ablation study.",218,2412.15991v1,cs.SE,"cs.SE,cs.AI,cs.NI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.546890
Adding interferometric lightning detection to the Pierre Auger Observatory,"The Pierre Auger Observatory has detected downward terrestrial gamma-ray
flashes (TGFs) with its Surface Detector. A key to understanding this
high-energy radiation in thunderstorms is to combine such measurements with
measurements of lightning processes in their earliest stages. With eleven
modified Auger Engineering Radio Array (AERA) stations we can build an
interferometric lightning detection array working in the bandwidth between 30 -
80 MHz inside the Surface Detector array to precisely measure lightning stepped
leaders in 3D. These measurements allow us to decipher the cause of TGFs and
clarify the reason for the observed high-energy particles in thunderstorms. We
will present the current status of the detection plans including the
configuration of the interferometric lightning detection array and the steps to
take as well as the reconstruction characteristics obtained with AERA.",166,2412.15972v1,astro-ph.IM,"astro-ph.IM,astro-ph.HE",biomedical engineering,2024-12-20,2024-12-23T21:06:43.546890
Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring,"In the software industry, the drive to add new features often overshadows the
need to improve existing code. Large Language Models (LLMs) offer a new
approach to improving codebases at an unprecedented scale through AI-assisted
refactoring. However, LLMs come with inherent risks such as braking changes and
the introduction of security vulnerabilities. We advocate for encapsulating the
interaction with the models in IDEs and validating refactoring attempts using
trustworthy safeguards. However, equally important for the uptake of AI
refactoring is research on trust development. In this position paper, we
position our future work based on established models from research on human
factors in automation. We outline action research within CodeScene on
development of 1) novel LLM safeguards and 2) user interaction that conveys an
appropriate level of trust. The industry collaboration enables large-scale
repository analysis and A/B testing to continuously guide the design of our
research interventions.",201,2412.15948v1,cs.SE,"cs.SE,cs.AI,cs.HC",biomedical engineering,2024-12-20,2024-12-23T21:06:43.547887
Large Language Model assisted Hybrid Fuzzing,"Greybox fuzzing is one of the most popular methods for detecting software
vulnerabilities, which conducts a biased random search within the program input
space. To enhance its effectiveness in achieving deep coverage of program
behaviors, greybox fuzzing is often combined with concolic execution, which
performs a path-sensitive search over the domain of program inputs. In hybrid
fuzzing, conventional greybox fuzzing is followed by concolic execution in an
iterative loop, where reachability roadblocks encountered by greybox fuzzing
are tackled by concolic execution. However, such hybrid fuzzing still suffers
from difficulties conventionally faced by symbolic execution, such as the need
for environment modeling and system call support. In this work, we show how to
achieve the effect of concolic execution without having to compute and solve
symbolic path constraints. When coverage-based greybox fuzzing reaches a
roadblock in terms of reaching certain branches, we conduct a slicing on the
execution trace and suggest modifications of the input to reach the relevant
branches. A Large Language Model (LLM) is used as a solver to generate the
modified input for reaching the desired branches. Compared with both the
vanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based
hybrid fuzzer HyLLfuzz (pronounced ""hill fuzz"") demonstrates superior coverage.
Furthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is
4-19 times faster than the concolic execution running in existing hybrid
fuzzing tools. This experience shows that LLMs can be effectively inserted into
the iterative loop of hybrid fuzzers, to efficiently expose more program
behaviors.",361,2412.15931v1,cs.SE,"cs.SE,cs.CR",biomedical engineering,2024-12-20,2024-12-23T21:06:43.548884
Less is More: Towards Green Code Large Language Models via Unified Structural Pruning,"The extensive application of Large Language Models (LLMs) in generative
coding tasks has raised concerns due to their high computational demands and
energy consumption. Unlike previous structural pruning methods designed for
classification models that deal with lowdimensional classification logits,
generative Code LLMs produce high-dimensional token logit sequences, making
traditional pruning objectives inherently limited. Moreover, existing single
component pruning approaches further constrain the effectiveness when applied
to generative Code LLMs. In response, we propose Flab-Pruner, an innovative
unified structural pruning method that combines vocabulary, layer, and
Feed-Forward Network (FFN) pruning. This approach effectively reduces model
parameters while maintaining performance. Additionally, we introduce a
customized code instruction data strategy for coding tasks to enhance the
performance recovery efficiency of the pruned model. Through extensive
evaluations on three state-of-the-art Code LLMs across multiple generative
coding tasks, the results demonstrate that Flab-Pruner retains 97% of the
original performance after pruning 22% of the parameters and achieves the same
or even better performance after post-training. The pruned models exhibit
significant improvements in storage, GPU usage, computational efficiency, and
environmental impact, while maintaining well robustness. Our research provides
a sustainable solution for green software engineering and promotes the
efficient deployment of LLMs in real-world generative coding intelligence
applications.",294,2412.15921v1,cs.SE,"cs.SE,cs.AI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.549882
Data Preparation for Fairness-Performance Trade-Offs: A Practitioner-Friendly Alternative?,"As machine learning (ML) systems are increasingly adopted across industries,
addressing fairness and bias has become essential. While many solutions focus
on ethical challenges in ML, recent studies highlight that data itself is a
major source of bias. Pre-processing techniques, which mitigate bias before
training, are effective but may impact model performance and pose integration
difficulties. In contrast, fairness-aware Data Preparation practices are both
familiar to practitioners and easier to implement, providing a more accessible
approach to reducing bias. Objective. This registered report proposes an
empirical evaluation of how optimally selected fairness-aware practices,
applied in early ML lifecycle stages, can enhance both fairness and
performance, potentially outperforming standard pre-processing bias mitigation
methods. Method. To this end, we will introduce FATE, an optimization technique
for selecting 'Data Preparation' pipelines that optimize fairness and
performance. Using FATE, we will analyze the fairness-performance trade-off,
comparing pipelines selected by FATE with results by pre-processing bias
mitigation techniques.",209,2412.15920v1,cs.SE,"cs.SE,cs.LG",biomedical engineering,2024-12-20,2024-12-23T21:06:43.549882
Vulnerability Detection in Popular Programming Languages with Language Models,"Vulnerability detection is crucial for maintaining software security, and
recent research has explored the use of Language Models (LMs) for this task.
While LMs have shown promising results, their performance has been inconsistent
across datasets, particularly when generalizing to unseen code. Moreover, most
studies have focused on the C/C++ programming language, with limited attention
given to other popular languages. This paper addresses this gap by
investigating the effectiveness of LMs for vulnerability detection in
JavaScript, Java, Python, PHP, and Go, in addition to C/C++ for comparison. We
utilize the CVEFixes dataset to create a diverse collection of
language-specific vulnerabilities and preprocess the data to ensure quality and
integrity. We fine-tune and evaluate state-of-the-art LMs across the selected
languages and find that the performance of vulnerability detection varies
significantly. JavaScript exhibits the best performance, with considerably
better and more practical detection capabilities compared to C/C++. We also
examine the relationship between code complexity and detection performance
across the six languages and find only a weak correlation between code
complexity metrics and the models' F1 scores.",240,2412.15905v1,cs.CR,"cs.CR,cs.SE",biomedical engineering,2024-12-20,2024-12-23T21:06:43.550880
AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI,"AI is the workhorse of modern data analytics and omnipresent across many
sectors. Large Language Models and multi-modal foundation models are today
capable of generating code, charts, visualizations, etc. How will these massive
developments of AI in data analytics shape future data visualizations and
visual analytics workflows? What is the potential of AI to reshape methodology
and design of future visual analytics applications? What will be our role as
visualization researchers in the future? What are opportunities, open
challenges and threats in the context of an increasingly powerful AI? This
Visualization Viewpoint discusses these questions in the special context of
biomedical data analytics as an example of a domain in which critical decisions
are taken based on complex and sensitive data, with high requirements on
transparency, efficiency, and reliability. We map recent trends and
developments in AI on the elements of interactive visualization and visual
analytics workflows and highlight the potential of AI to transform biomedical
visualization as a research field. Given that agency and responsibility have to
remain with human experts, we argue that it is helpful to keep the focus on
human-centered workflows, and to use visual analytics as a tool for integrating
``AI-in-the-loop''. This is in contrast to the more traditional term
``human-in-the-loop'', which focuses on incorporating human expertise into
AI-based systems.",282,2412.15876v1,cs.HC,"cs.HC,cs.AI,cs.GR,68U01,H.1.2; H.5.2; I.3.6; I.2.1; J.3; D.2.0",biomedical engineering,2024-12-20,2024-12-23T21:06:43.551877
User Modeling in Model-Driven Engineering: A Systematic Literature Review,"In software applications, user models can be used to specify the profile of
the typical users of the application, including personality traits,
preferences, skills, etc. In theory, this would enable an adaptive application
behavior that could lead to a better user experience. Nevertheless, user models
do not seem to be part of standard modeling languages nor common in current
model-driven engineering (MDE) approaches. In this paper, we conduct a
systematic literature review to analyze existing proposals for user modeling in
MDE and identify their limitations. The results showcase that there is a lack
of a unified and complete user modeling perspective. Instead, we observe a lot
of fragmented and partial proposals considering only simple user dimensions and
with lack of proper tool support. This limits the implementation of richer user
interfaces able to better support the user-specific needs. Therefore, we hope
this analysis triggers a discussion on the importance of user models and their
inclusion in MDE pipelines. Especially in a context where, thanks to the rise
of AI techniques, personalization, based on a rich number of user dimensions,
is becoming more and more of a possibility.",223,2412.15871v1,cs.SE,cs.SE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.551877
Revealing spin-flip two-level systems using ultra-thin film superconducting resonators,"Material disorders are one of the major sources of noise and loss in
solid-state quantum devices, whose behaviors are often modeled as two-level
systems (TLSs) formed by charge tunneling between neighboring sites. However,
the role of their spins in tunneling and its impact on device performance
remain highly unexplored. In this work, employing ultra-thin TiN
superconducting resonators, we reveal anomalous TLS behaviors by demonstrating
an unexpected increase in resonant frequency at low magnetic fields.
Furthermore, a spin-flip TLS model is proposed, in which an effective
spin-orbit coupling is generated by inhomogeneous local magnetic fields from
defect spins. This mechanism mixes charge tunnelings and spin flips,
quantitatively reproducing the observed frequency-field relationship and its
temperature dependence. This work deepens the understanding of spin-dependent
TLS behaviors, offering the possibility of magnetically engineering noise and
loss in solid-state quantum devices.",201,2412.15856v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",biomedical engineering,2024-12-20,2024-12-23T21:06:43.552874
WebLLM: A High-Performance In-Browser LLM Inference Engine,"Advancements in large language models (LLMs) have unlocked remarkable
capabilities. While deploying these models typically requires server-grade GPUs
and cloud-based inference, the recent emergence of smaller open-source models
and increasingly powerful consumer devices have made on-device deployment
practical. The web browser as a platform for on-device deployment is
universally accessible, provides a natural agentic environment, and
conveniently abstracts out the different backends from diverse device vendors.
To address this opportunity, we introduce WebLLM, an open-source JavaScript
framework that enables high-performance LLM inference entirely within web
browsers. WebLLM provides an OpenAI-style API for seamless integration into web
applications, and leverages WebGPU for efficient local GPU acceleration and
WebAssembly for performant CPU computation. With machine learning compilers
MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming
the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM
can retain up to 80% native performance on the same device, with room to
further close the gap. WebLLM paves the way for universally accessible,
privacy-preserving, personalized, and locally powered LLM applications in web
browsers. The code is available at: https://github.com/mlc-ai/web-llm.",291,2412.15803v1,cs.LG,"cs.LG,cs.AI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.553871
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.553871
Combinatorial Optimization with Quantum Computers,"Quantum computers leverage the principles of quantum mechanics to do
computation with a potential advantage over classical computers. While a single
classical computer transforms one particular binary input into an output after
applying one operator to the input, a quantum computer can apply the operator
to a superposition of binary strings to provide a superposition of binary
outputs, doing computation apparently in parallel. This feature allows quantum
computers to speed up the computation compared to classical algorithms.
Unsurprisingly, quantum algorithms have been proposed to solve optimization
problems in quantum computers. Furthermore, a family of quantum machines called
quantum annealers are specially designed to solve optimization problems. In
this paper, we provide an introduction to quantum optimization from a practical
point of view. We introduce the reader to the use of quantum annealers and
quantum gate-based machines to solve optimization problems.",166,2412.15778v1,cs.ET,"cs.ET,quant-ph",biomedical engineering,2024-12-20,2024-12-23T21:06:43.554869
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",biomedical engineering,2024-12-20,2024-12-23T21:06:43.555866
Understanding the Effect of Agile Practice Quality on Software Product Quality,"Agile methods and associated practices have been held to deliver value to
software developers and customers. Research studies have reported team
productivity and software quality benefits. While such insights are helpful for
understanding how agile methods add value during software development, there is
need for understanding the intersection of useful practices and outcomes over
project duration. This study addresses this opportunity and conducted an
observation study of student projects that was complemented by the analysis of
demographics data and open responses about the challenges encountered during
the use of agile practices. Data from 22 student teams comprising 85 responses
were analyzed using quantitative and qualitative approaches, where among our
findings we observed that the use of good coding practices and quality
management techniques were positively correlated with all dimensions of product
quality (e.g., functionality scope and software packaging). Outcomes also
reveal that software product quality was predicted by requirements scoping,
team planning and communication, and coding practice. However, high levels of
team planning and communication were not necessary for all software development
activities. When examining project challenges, it was observed that lack of
technical skills and poor time management present most challenges to project
success. While these challenges may be mitigated by agile practices, such
practices may themselves create unease, requiring balance during project
implementation.",246,2412.15761v1,cs.SE,"cs.SE,D.2; D.2.9",biomedical engineering,2024-12-20,2024-12-23T21:06:43.556863
Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models,"Background: Despite the current ubiquity of Large Language Models (LLMs)
across the medical domain, there is a surprising lack of studies which address
their reasoning behaviour. We emphasise the importance of understanding
reasoning behaviour as opposed to high-level prediction accuracies, since it is
equivalent to explainable AI (XAI) in this context. In particular, achieving
XAI in medical LLMs used in the clinical domain will have a significant impact
across the healthcare sector. Results: Therefore, we define the concept of
reasoning behaviour in the specific context of medical LLMs. We then categorise
and discuss the current state of the art of methods which evaluate reasoning
behaviour in medical LLMs. Finally, we propose theoretical frameworks which can
empower medical professionals or machine learning engineers to gain insight
into the low-level reasoning operations of these previously obscure models.
Conclusion: The subsequent increased transparency and trust in medical machine
learning models by clinicians as well as patients will accelerate the
integration, application as well as further development of medical AI for the
healthcare system as a whole",216,2412.15748v1,cs.CL,"cs.CL,cs.AI,cs.LG",biomedical engineering,2024-12-20,2024-12-23T21:06:43.556863
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.557860
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",biomedical engineering,2024-12-20,2024-12-23T21:06:43.558857
PoisonCatcher: Revealing and Identifying LDP Poisoning Attacks in IIoT,"Local Differential Privacy (LDP) is widely adopted in the Industrial Internet
of Things (IIoT) for its lightweight, decentralized, and scalable nature.
However, its perturbation-based privacy mechanism makes it difficult to
distinguish between uncontaminated and tainted data, encouraging adversaries to
launch poisoning attacks. While LDP provides some resilience against minor
poisoning, it lacks robustness in IIoT with dynamic networks and substantial
real-time data flows. Effective countermeasures for such attacks are still
underdeveloped. This work narrows the critical gap by revealing and identifying
LDP poisoning attacks in IIoT. We begin by deepening the understanding of such
attacks, revealing novel threats that arise from the interplay between LDP
indistinguishability and IIoT complexity. This exploration uncovers a novel
rule-poisoning attack, and presents a general attack formulation by unifying it
with input-poisoning and output-poisoning. Furthermore, two key attack impacts,
i.e., Statistical Query Result (SQR) accuracy degradation and inter-dataset
correlations disruption, along with two characteristics: attack patterns
unstable and poisoned data stealth are revealed. From this, we propose
PoisonCatcher, a four-stage solution that detects LDP poisoning attacks and
identifies specific contaminated data points. It utilizes temporal similarity,
attribute correlation, and time-series stability analysis to detect datasets
exhibiting SQR accuracy degradation, inter-dataset disruptions, and unstable
patterns. Enhanced feature engineering is used to extract subtle poisoning
signatures, enabling machine learning models to identify specific
contamination. Experimental evaluations show the effectiveness, achieving
state-of-the-art performance with average precision and recall rates of 86.17%
and 97.5%, respectively, across six representative attack scenarios.",365,2412.15704v1,cs.CR,cs.CR,biomedical engineering,2024-12-20,2024-12-23T21:06:43.559856
Cracking the Code: Evaluating Zero-Shot Prompting Methods for Providing Programming Feedback,"Despite the growing use of large language models (LLMs) for providing
feedback, limited research has explored how to achieve high-quality feedback.
This case study introduces an evaluation framework to assess different
zero-shot prompt engineering methods. We varied the prompts systematically and
analyzed the provided feedback on programming errors in R. The results suggest
that prompts suggesting a stepwise procedure increase the precision, while
omitting explicit specifications about which provided data to analyze improves
error identification.",95,2412.15702v1,cs.SE,cs.SE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.560852
"Revealing the Black Box of Device Search Engine: Scanning Assets, Strategies, and Ethical Consideration","In the digital age, device search engines such as Censys and Shodan play
crucial roles by scanning the internet to catalog online devices, aiding in the
understanding and mitigation of network security risks. While previous research
has used these tools to detect devices and assess vulnerabilities, there
remains uncertainty regarding the assets they scan, the strategies they employ,
and whether they adhere to ethical guidelines. This study presents the first
comprehensive examination of these engines' operational and ethical dimensions.
We developed a novel framework to trace the IP addresses utilized by these
engines and collected 1,407 scanner IPs. By uncovering their IPs, we gain deep
insights into the actions of device search engines for the first time and gain
original findings. By employing 28 honeypots to monitor their scanning
activities extensively in one year, we demonstrate that users can hardly evade
scans by blocklisting scanner IPs or migrating service ports. Our findings
reveal significant ethical concerns, including a lack of transparency,
harmlessness, and anonymity. Notably, these engines often fail to provide
transparency and do not allow users to opt out of scans. Further, the engines
send malformed requests, attempt to access excessive details without
authorization, and even publish personally identifiable information (PII) and
screenshots on search results. These practices compromise user privacy and
expose devices to further risks by potentially aiding malicious entities. This
paper emphasizes the urgent need for stricter ethical standards and enhanced
transparency in the operations of device search engines, offering crucial
insights into safeguarding against invasive scanning practices and protecting
digital infrastructures.",315,2412.15696v1,cs.CR,cs.CR,biomedical engineering,2024-12-20,2024-12-23T21:06:43.561849
Effect of three-orifice baffles orientation on the flow and thermal-hydraulic performance: experimental analysis for net and oscillatory flows,"Three-orifice baffles equally spaced along a circular tube are investigated
as a means for heat transfer enhancement under net, oscillatory and compound
flows. An unprecedented, systematic analysis of the relative orientation of
consecutive baffles -- aligned or opposed -- is accomplished to assess the
changes induced on the flow structure and their impact on the thermal-hydraulic
performance. The results cover the Nusselt number, the net and oscillatory
friction factors and the instantaneous velocity fields using PIV in an
experimental campaign with a 32 mm tube diameter. The study is conducted in the
range of net Reynolds numbers $50 < Re_n < 1000$ and oscillatory Reynolds
numbers $0 < Re_{osc}< 750$, for a dimensionless amplitude $x_0/D = 0.5$ and
$Pr=65$. In absence of oscillatory flow, opposed baffles advance the transition
to turbulence from $Re_n = 100$ to $50$, increasing the net friction factor (40
%) for $Re_n > 50$ and the Nusselt number (maximum of 27 %) for $Re_n < 150$.
When an oscillatory flow is applied, augmentations caused by opposed baffles
are only observed for $Re_n < 150$ and $Re_{osc} < 150$. Above $Re_n$,
$Re_{osc}>150$, opposed baffles are not recommended for the promotion of heat
transfer, owing to friction penalties. However, the chaotic mixing and lack of
short-circuiting between baffles observed with flow velocimetry over a wide
range of operational conditions point out the interest of this configuration to
achieve plug flow.",368,2412.15682v1,physics.flu-dyn,physics.flu-dyn,biomedical engineering,2024-12-20,2024-12-23T21:06:43.562847
High-Dimensional Bayesian Optimisation with Large-Scale Constraints via Latent Space Gaussian Processes,"Design optimisation offers the potential to develop lightweight aircraft
structures with reduced environmental impact. Due to the high number of design
variables and constraints, these challenges are typically addressed using
gradient-based optimisation methods to maintain efficiency. However, this
approach often results in a local solution, overlooking the global design
space. Moreover, gradients are frequently unavailable. Bayesian Optimisation
presents a promising alternative, enabling sample-efficient global optimisation
through probabilistic surrogate models that do not depend on gradients.
Although Bayesian Optimisation has shown its effectiveness for problems with a
small number of design variables, it struggles to scale to high-dimensional
problems, particularly when incorporating large-scale constraints. This
challenge is especially pronounced in aeroelastic tailoring, where directional
stiffness properties are integrated into the structural design to manage
aeroelastic deformations and enhance both aerodynamic and structural
performance. Ensuring the safe operation of the system requires simultaneously
addressing constraints from various analysis disciplines, making global design
space exploration even more complex. This study seeks to address this issue by
employing high-dimensional Bayesian Optimisation combined with a dimensionality
reduction technique to tackle the optimisation challenges in aeroelastic
tailoring. The proposed approach is validated through experiments on a
well-known benchmark case with black-box constraints, as well as its
application to the aeroelastic tailoring problem, demonstrating the feasibility
of Bayesian Optimisation for high-dimensional problems with large-scale
constraints.",301,2412.15679v1,cs.CE,cs.CE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.563844
Code Review Automation Via Multi-task Federated LLM -- An Empirical Study,"Code review is a crucial process before deploying code to production, as it
validates the code, provides suggestions for improvements, and identifies
errors such as missed edge cases. In projects with regular production releases,
the effort required for peer code-reviews remains high. Consequently, there has
been significant interest from software engineering (SE) researchers in
automating the code review process. Previous research on code review automation
has typically approached the task as three independent sub-tasks: review
necessity prediction, review comment generation, and code refinement. Our study
attempts to (i) leverage the relationships between the sub-tasks of code review
automation, by developing a multi-task model that addresses all tasks in an
integrated manner, and (ii) increase model robustness on unseen data via
collaborative large language model (LLM) modeling, while retaining the
proprietary nature of code, by using federated learning (FL). The study
explores five simple techniques for multi-task training, including two
sequential methods, one parallel method, and two cumulative methods. The
results indicate that sequentially training a federated LLM (FedLLM) for our
code review multi-task use case is less efficient in terms of time,
computation, and performance metrics, compared to training separate models for
each task. Because sequential training demonstrates catastrophic forgetting,
alternatively cumulative fine-tuning for multi-task training performs better
than training models for individual tasks. This study highlights the need for
research focused on effective fine-tuning of multi-task FedLLMs for SE tasks.",310,2412.15676v1,cs.SE,cs.SE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.563844
"Gender Disparities in Contributions, Leadership, and Collaboration: An Exploratory Study on Software Systems Research","Gender diversity enhances research by bringing diverse perspectives and
innovative approaches. It ensures equitable solutions that address the needs of
diverse populations. However, gender disparity persists in research where women
remain underrepresented, which might limit diversity and innovation. Many even
leave scientific careers as their contributions often go unnoticed and
undervalued. Therefore, understanding gender-based contributions and
collaboration dynamics is crucial to addressing this gap and creating a more
inclusive research environment. In this study, we analyzed 2,000 articles
published over the past decade in the Journal of Systems and Software (JSS).
From these, we selected 384 articles that detailed authors' contributions and
contained both female and male authors to investigate gender-based
contributions. Our contributions are fourfold. First, we analyzed women's
engagement in software systems research. Our analysis showed that only 32.74%
of the total authors are women and female-led or supervised studies were fewer
than those of men. Second, we investigated female authors' contributions across
14 major roles. Interestingly, we found that women contributed comparably to
men in most roles, with more contributions in conceptualization, writing, and
reviewing articles. Third, we explored the areas of software systems research
and found that female authors are more actively involved in human-centric
research domains. Finally, we analyzed gender-based collaboration dynamics. Our
findings revealed that female supervisors tended to collaborate locally more
often than national-level collaborations. Our study highlights that females'
contributions to software systems research are comparable to those of men.
Therefore, the barriers need to be addressed to enhance female participation
and ensure equity and inclusivity in research.",334,2412.15661v1,cs.SE,cs.SE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.564842
Adaptable and Precise: Enterprise-Scenario LLM Function-Calling Capability Training Pipeline,"Enterprises possess a vast array of API assets scattered across various
functions, forming the backbone of existing business processes. By leveraging
these APIs as functional tools, enterprises can design diverse,
scenario-specific agent applications, driven by on-premise function-calling
models as the core engine. However, generic models often fail to meet
enterprise requirements in terms of computational efficiency, output accuracy,
and stability, necessitating scenario-specific adaptation. In this paper, we
propose a training pipeline for function-calling capabilities tailored to
real-world business scenarios. This pipeline includes the synthesis and
augmentation of scenario-specific function-calling data, model fine-tuning, and
performance evaluation and analysis. Using this pipeline, we generated 1,260
fully AI-generated samples and 1,035 augmented manually-labeled samples in
digital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as
the base model and fine-tuned using the LoRA method on four GPUs with 24GB
VRAM. Our fine-tuned model demonstrated outstanding performance in evaluations
and practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test
set. These results validate the reliability of the proposed pipeline for
training scenario-specific function-calling models.",263,2412.15660v1,cs.AI,"cs.AI,cs.CL,cs.SE",biomedical engineering,2024-12-20,2024-12-23T21:06:43.565839
Unified real-space construction scheme for flat bands based on symmetric compact localized states,"Flat band (FB) systems provide ideal playgrounds for studying correlation
physics, whereas multi-orbital characteristics in real materials are
distinguished from most simple FB models. Here, we propose a systematic and
versatile framework for FB constructions in tight-binding (TB) models based on
symmetric compact localized states (CLSs), integrating lattice and orbital
degrees of freedom. We first demonstrate that any CLS can be symmetrized into a
representation of the point group, which remains valid for high orbitals with
finite spin-orbit coupling (SOC). Second, we determine the candidate CLS sites
according to lattice symmetry, and simplify the hopping as a linear mapping
between two Hilbert spaces: one of CLS sites and another of their adjacent
sites. The existence of FBs depends on a non-empty kernel of the mapping.
Finally, we distinguish eigenstates in the kernel to qualify as a CLS. To
illustrate the versatility of our framework, we construct three representative
FB models: one in two dimensions (2D) and the rest in three dimensions (3D).
All of them lack special lattice structures and incorporate high orbitals.
Notably, the 3D FBs can exhibit not only band touchings at points but also
along lines, a feature of significant physical interest. For a comprehensive
understanding, we derive a concise criterion for determining band touchings,
which provides a natural explanation for the occurrence of both gapped and
gapless FBs. By unifying symmetry principles in real space, our work offers a
systematic approach to constructing FBs across diverse lattice systems. This
framework opens new avenues for understanding and engineering FB systems, with
potential implications for correlated quantum phenomena and exotic phases of
matter.",346,2412.15653v1,cond-mat.mes-hall,cond-mat.mes-hall,biomedical engineering,2024-12-20,2024-12-23T21:06:43.566836
Darkit: A User-Friendly Software Toolkit for Spiking Large Language Model,"Large language models (LLMs) have been widely applied in various practical
applications, typically comprising billions of parameters, with inference
processes requiring substantial energy and computational resources. In
contrast, the human brain, employing bio-plausible spiking mechanisms, can
accomplish the same tasks while significantly reducing energy consumption, even
with a similar number of parameters. Based on this, several pioneering
researchers have proposed and implemented various large language models that
leverage spiking neural networks. They have demonstrated the feasibility of
these models, validated their performance, and open-sourced their frameworks
and partial source code. To accelerate the adoption of brain-inspired large
language models and facilitate secondary development for researchers, we are
releasing a software toolkit named DarwinKit (Darkit). The toolkit is designed
specifically for learners, researchers, and developers working on spiking large
models, offering a suite of highly user-friendly features that greatly simplify
the learning, deployment, and development processes.",194,2412.15634v1,cs.SE,cs.SE,biomedical engineering,2024-12-20,2024-12-23T21:06:43.567834
Insights from the Frontline: GenAI Utilization Among Software Engineering Students,"Generative AI (genAI) tools (e.g., ChatGPT, Copilot) have become ubiquitous
in software engineering (SE). As SE educators, it behooves us to understand the
consequences of genAI usage among SE students and to create a holistic view of
where these tools can be successfully used. Through 16 reflective interviews
with SE students, we explored their academic experiences of using genAI tools
to complement SE learning and implementations. We uncover the contexts where
these tools are helpful and where they pose challenges, along with examining
why these challenges arise and how they impact students. We validated our
findings through member checking and triangulation with instructors. Our
findings provide practical considerations of where and why genAI should (not)
be used in the context of supporting SE students.",164,2412.15624v1,cs.HC,"cs.HC,cs.SE",biomedical engineering,2024-12-20,2024-12-23T21:06:43.567834
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",biomedical engineering,2024-12-20,2024-12-23T21:06:43.568831
On-Demand Magnon Resonance Isolation in Cavity Magnonics,"Cavity magnonics is a promising field focusing the interaction between spin
waves (magnons) and other types of signals. In cavity magnonics, the function
of isolating magnons from the cavity to allow signal storage and processing
fully in the magnonic domain is highly desired, but its realization is often
hindered by the lack of necessary tunability on the interaction. This work
shows that by utilizing the collective mode of two YIG spheres and adopting
Floquet engineering, magnonic signals can be switched on-demand to a magnon
dark mode that is protected from the environment, enabling a variety of
manipulation over the magnon dynamics. Our demonstration can be scaled up to
systems with an array of magnonic resonators, paving the way for large-scale
programmable hybrid magnonic circuits.",168,2412.15600v1,cond-mat.mes-hall,"cond-mat.mes-hall,physics.app-ph",biomedical engineering,2024-12-20,2024-12-23T21:06:43.569828
SaliencyI2PLoc: saliency-guided image-point cloud localization using contrastive learning,"Image to point cloud global localization is crucial for robot navigation in
GNSS-denied environments and has become increasingly important for multi-robot
map fusion and urban asset management. The modality gap between images and
point clouds poses significant challenges for cross-modality fusion. Current
cross-modality global localization solutions either require modality
unification, which leads to information loss, or rely on engineered training
schemes to encode multi-modality features, which often lack feature alignment
and relation consistency. To address these limitations, we propose,
SaliencyI2PLoc, a novel contrastive learning based architecture that fuses the
saliency map into feature aggregation and maintains the feature relation
consistency on multi-manifold spaces. To alleviate the pre-process of data
mining, the contrastive learning framework is applied which efficiently
achieves cross-modality feature mapping. The context saliency-guided local
feature aggregation module is designed, which fully leverages the contribution
of the stationary information in the scene generating a more representative
global feature. Furthermore, to enhance the cross-modality feature alignment
during contrastive learning, the consistency of relative relationships between
samples in different manifold spaces is also taken into account. Experiments
conducted on urban and highway scenario datasets demonstrate the effectiveness
and robustness of our method. Specifically, our method achieves a Recall@1 of
78.92% and a Recall@20 of 97.59% on the urban scenario evaluation dataset,
showing an improvement of 37.35% and 18.07%, compared to the baseline method.
This demonstrates that our architecture efficiently fuses images and point
clouds and represents a significant step forward in cross-modality global
localization. The project page and code will be released.",351,2412.15577v1,cs.CV,"cs.CV,cs.LG,cs.RO",biomedical engineering,2024-12-20,2024-12-23T21:06:43.570825
MORTAR: Metamorphic Multi-turn Testing for LLM-based Dialogue Systems,"With the widespread application of LLM-based dialogue systems in daily life,
quality assurance has become more important than ever. Recent research has
successfully introduced methods to identify unexpected behaviour in single-turn
scenarios. However, multi-turn dialogue testing remains underexplored, with the
Oracle problem in multi-turn testing posing a persistent challenge for dialogue
system developers and researchers. In this paper, we propose MORTAR, a
MetamORphic multi-TuRn diAlogue testing appRoach, which mitigates the test
oracle problem in the assessment of LLM-based dialogue systems. MORTAR
automates the generation of follow-up question-answer (QA) dialogue test cases
with multiple dialogue-level perturbations and metamorphic relations. MORTAR
employs a novel knowledge graph-based dialogue information model which
effectively generates perturbed dialogue test datasets and detects bugs of
multi-turn dialogue systems in a low-cost manner. The proposed approach does
not require an LLM as a judge, eliminating potential of any biases in the
evaluation step. According to the experiment results on multiple LLM-based
dialogue systems and comparisons with single-turn metamorphic testing
approaches, MORTAR explores more unique bugs in LLM-based dialogue systems,
especially for severe bugs that MORTAR detects up to four times more unique
bugs than the most effective existing metamorphic testing approach.",270,2412.15557v1,cs.SE,"cs.SE,cs.CL",biomedical engineering,2024-12-20,2024-12-23T21:06:43.570825
VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving,"In recent years, reinforcement learning (RL)-based methods for learning
driving policies have gained increasing attention in the autonomous driving
community and have achieved remarkable progress in various driving scenarios.
However, traditional RL approaches rely on manually engineered rewards, which
require extensive human effort and often lack generalizability. To address
these limitations, we propose \textbf{VLM-RL}, a unified framework that
integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward
signals using image observation and natural language goals. The core of VLM-RL
is the contrasting language goal (CLG)-as-reward paradigm, which uses positive
and negative language goals to generate semantic rewards. We further introduce
a hierarchical reward synthesis approach that combines CLG-based semantic
rewards with vehicle state information, improving reward stability and offering
a more comprehensive reward signal. Additionally, a batch-processing technique
is employed to optimize computational efficiency during training. Extensive
experiments in the CARLA simulator demonstrate that VLM-RL outperforms
state-of-the-art baselines, achieving a 10.5\% reduction in collision rate, a
104.6\% increase in route completion rate, and robust generalization to unseen
driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any
standard RL algorithms, potentially revolutionizing the existing RL paradigm
that relies on manual reward engineering and enabling continuous performance
improvements. The demo video and code can be accessed at:
https://zilin-huang.github.io/VLM-RL-website.",329,2412.15544v1,cs.RO,"cs.RO,cs.AI,cs.CV",biomedical engineering,2024-12-20,2024-12-23T21:06:43.571823
Mid-Air Single-Sided Acoustic Levitation in High-Pressure Regions,"Acoustic levitation is essential in many scientific and engineering
applications to realize non-contact manipulation using sound waves. Traditional
methods use standing waves from opposing sound sources, which limits
manipulation to enclosed spaces. Single-sided levitation with twin traps has
limited reach near phased array of transducers (\textless 66.7 mm). Herein, we
introduce a new levitation mode and demonstrate stable levitation and
translation between 141 and 397 mm from a phased array of transducers using a
Bessel beam. The non-diffractive propagation characteristics of the Bessel
beam, combined with its high central acoustic pressure, enable particles to be
stably entrapped and manipulated within the high-pressure region of the beam.
This work extends the reach of acoustic manipulation using a single-sided
levitator.",167,2412.15539v1,physics.app-ph,physics.app-ph,biomedical engineering,2024-12-20,2024-12-23T21:06:43.572820
XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent
data with the generative capabilities of Large Language Models (LLMs), ensuring
that the generated output is not only contextually relevant but also accurate
and current.We introduce XRAG, an open-source, modular codebase that
facilitates exhaustive evaluation of the performance of foundational components
of advanced RAG modules. These components are systematically categorized into
four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We
systematically analyse them across reconfigured datasets, providing a
comprehensive benchmark for their effectiveness. Given the escalating
complexity of RAG systems, we underscore the necessity of identifying potential
failure points of RAG modules. We formulate a suite of experimental
methodologies and diagnostic testing protocols to dissect the failure points
inherent in the engineering of RAG modules. Subsequently, we proffer bespoke
solutions that are designed to augment the validation processes and bolster the
overall performance of these modules. Our work thoroughly evaluates the
performance of core advanced components in RAG systems, providing insights into
optimizations for prevalent failure points.",234,2412.15529v1,cs.CL,"cs.CL,cs.AI",biomedical engineering,2024-12-20,2024-12-23T21:06:43.572820
Cross-feeding percolation phase transitions of inter-cellular metabolic networks,"Intercellular exchange networks are essential for the adaptive capabilities
of populations of cells. While diffusional exchanges have traditionally been
difficult to map, recent advances in nanotechnology enable precise probing of
exchange fluxes with the medium at single-cell resolution. Here we introduce a
tiling-based method to reconstruct the dynamic unfolding of exchange networks
from flux data, subsequently applying it to an experimental mammalian
co-culture system where lactate exchanges affect the acidification of the
environment. We observe that the network, which initially exhibits a dense
matrix of exchanges, progressively breaks up into small disconnected clusters
of cells. To explain this behaviour, we develop a two-parameter Maximum-Entropy
multicellular metabolic model that incorporates diffusion-driven exchanges
through a set of global constraints that couple cellular behaviors. The model
predicts a transition from a densely interconnected network to a sparse,
motif-dominated state as glucose and oxygen consumption levels shift. We
characterize such a crossover both numerically, revealing a power-law decay in
the cluster-size distribution at the critical transition, and analytically, by
computing the critical line through a mean-field approximation based on
percolation theory. By comparing empirical data with theoretical predictions,
we find that populations evolve towards the sparse phase by remaining near the
crossover point between these two regimes. These findings offer new insights
into the collective organization driving the adaptive dynamics of cell
populations.",279,2412.09088v1,cond-mat.stat-mech,"cond-mat.stat-mech,q-bio.CB",nanotechnology,2024-12-12,2024-12-23T21:06:44.680538
Engineering of self-bending surface plasmon polaritons through Hermite-Gaussian mode expansion,"Surface plasmon polaritons have received much attention over the last decades
in fields such as photonics or nanotechnology due to their inherent high
sensitivity to metal surface variations (e.g., presence of adsorbates or
changes in the roughness). Because of this important property, it is expected
that they will find promising major applications in widely cross-disciplinary
areas ranging from material science to medicine. Motivated by this ever-growing
interest, here we introduce a novel theoretical framework suitable to design
new types of structured paraxial surface plasmon beams and to control their
propagation along the interface between a dielectric and a lossy metal. This is
possible by virtue of a convenient Hermite-Gaussian mode expansion, which
constitutes a complete basis set upon which new types of structured paraxial
plasmon beams can be generated. The beams obtained from this method present a
rather peculiar feature, as they exhibit local intensity maxima at different
propagation distances. This way of creating tightly confined structured light
allows us control where to place the beam energy. This, thus, opens up worth
exploring pathways to manipulate light propagation along metal surfaces at the
nanoscale. As a proof-of-concept, we provide numerical evidence of the
feasibility of the method by considering the analysis of the propagation of
Airy-based surface plasmon polaritons along an air-silver interface.",283,2412.08706v1,physics.optics,"physics.optics,quant-ph",nanotechnology,2024-12-11,2024-12-23T21:06:44.681536
Anomalous hardening of spin waves in cobalt/molecular-semiconductor heterostructures reveals strongly anisotropic spinterface magnetism,"The interface between a ferromagnetic metal and an organic molecular
semiconductor, commonly referred to as a spinterface, is an important component
for advancing spintronic technologies. Hybridization of the ferromagnetic-metal
surface d orbitals with the molecular-semiconductor p orbitals induces profound
modifications not only in the interfacial molecular layer, but also in the
surface ferromagnetic-metal atomic layer. These effects are particularly
pronounced at low temperatures, manifesting as substantial modifications in the
magnetic properties of thin-film magnetic-metal/organic heterostructures.
Despite extensive research and interest, the magnetic-ordering and
magnetic-properties of the spinterface remain poorly understood. Using
ultrafast time-resolved magneto-optical spectroscopy, to investigate the
magnetic dynamics in such heterostructures, we unveil the unique
spinterface-magnetism and its universality for a broad variety of
cobalt/molecular-semiconductor interfaces. In particular, our findings
demonstrate the presence of highly anisotropic low-temperature
superparamagnetism at the cobalt/molecular-semiconductor spinterface. This
anisotropic interfacial superparamagnetism is likely driven by strong chemical
modifications in the cobalt interfacial layer caused by the chemisorbed
molecular layer. These results highlight the pivotal role of molecular
chemisorption in tuning the magnetic properties at spinterfaces, paving the way
for future spintronic applications.",300,2412.08677v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",nanotechnology,2024-12-11,2024-12-23T21:06:44.681536
Surface-plasmon polaritons in multilayer jellium systems: dispersion and spatial description,"Surface-plasmon polaritons (SPPs) are electromagnetic waves that propagate
along metal-dielectric interfaces, with important applications in sensing,
energy, and nanotechnology. While the behavior of SPPs in single metal slabs is
well understood, the coupling between plasmon modes in multilayer systems has
received less attention. In this paper, we explore the response functions of
SPPs in single-slab, double-slab, and two-different-slab systems using the
jellium model. Thanks to a comparison with classical models, our study reveals
how quantum effects influence the resonance frequencies of these modes. It also
details the spatial description of the different SPP modes and unveils how
their coupling occurs in two-different-slab systems. These findings provide new
insights into the behavior of SPPs, especially in complex nanostructures.",177,2412.05057v1,physics.optics,"physics.optics,cond-mat.mes-hall",nanotechnology,2024-12-06,2024-12-23T21:06:44.682533
Ferroelectric domain walls for environmental sensors,"Domain walls in ferroelectric oxides provide fertile ground for the
development of next-generation nanotechnology. Examples include
domain-wall-based memory, memristors, and diodes, where the unusual electronic
properties and the quasi-2D nature of the walls are leveraged to emulate the
behavior of electronic components at ultra-small length scales. Here, we
demonstrate atmosphere-related reversible changes in the electronic conduction
at neutral ferroelectric domain walls in Er(Mn,Ti)O$_3$. By exposing the system
to reducing and oxidizing conditions, we drive the domain walls from insulating
to conducting, and vice versa, translating the environmental changes into
current signals. Density functional theory calculations show that the effect is
predominately caused by charge carrier density modulations, which arise as
oxygen interstitials accumulate at the domain walls. The work introduces an
innovative concept for domain-wall based environmental sensors, giving an
additional dimension to the field of domain wall nanoelectronics and sensor
technology in general.",217,2412.03691v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,nanotechnology,2024-12-04,2024-12-23T21:06:44.683530
Memristive Nanowire Network for Energy Efficient Audio Classification: Pre-Processing-Free Reservoir Computing with Reduced Latency,"Speech recognition is a key challenge in natural language processing,
requiring low latency, efficient computation, and strong generalization for
real-time applications. While software-based artificial neural networks (ANNs)
excel at this task, they are computationally intensive and depend heavily on
data pre-processing. Neuromorphic computing, with its low-latency and
energy-efficient advantages, holds promise for audio classification. Memristive
nanowire networks, combined with pre-processing techniques like Mel-Frequency
Cepstrum Coefficient extraction, have been widely used for associative
learning, but such pre-processing can be power-intensive, undermining latency
benefits. This study pioneers the use of memristive and spatio-temporal
properties of nanowire networks for audio signal classification without
pre-processing. A nanowire network simulation is paired with three linear
classifiers for 10-class MNIST audio classification and binary speaker
generalization tests. The hybrid system achieves significant benefits:
excellent data compression with only 3% of nanowire output utilized, a 10-fold
reduction in computational latency, and up to 28.5% improved classification
accuracy (using a logistic regression classifier). Precision and recall improve
by 10% and 17% for multispeaker datasets, and by 24% and 17% for individual
speaker datasets, compared to raw data classifiers. This work provides a
foundational proof of concept for utilizing memristive nanowire networks (NWN)
in edge-computing devices, showcasing their potential for efficient, real-time
audio signal processing with reduced computational overhead and power
consumption, and enabling the development of advanced neuromorphic computing
solutions.",344,2411.19611v1,cs.SD,"cs.SD,cond-mat.dis-nn,eess.AS,physics.app-ph,stat.CO",nanotechnology,2024-11-29,2024-12-23T21:06:44.683530
Temporal synthesis of optical nonlinearity through synergy of spectrally-tuneable electron and phonon dynamics in a metamaterial,"Manipulating intensity, phase and polarization of the electromagnetic fields
on ultrafast timescales is essential for all-optical switching, optical
information processing and development of novel time-variant media. Noble metal
based plasmonics has provided numerous platforms for optical switching and
control, enabled by strong local field enhancement, artificially engineered
dispersion and strong Kerr-type free-electron nonlinearities. However, precise
control over switching times and spectrum remains challenging, commonly limited
by the relaxation of hot-electron gas on picosecond time scales and the band
structure of materials. Here we experimentally demonstrate the strong and
tuneable nonlinearity in a metamaterial on a mirror geometry, controlled by the
wavelength of excitation, which imprints a specific non-uniform hot-electron
population distribution and drives targeted electron and lattice dynamics. The
interplay of electromagnetic, electronic and mechanical energy exchange allows
us to achieve sub-300~fs timescales in the recovery of optical constants in the
selected spectral domains, where the modulation surpasses the limitations
imposed by the inherent material response of metamaterial components, owing to
emergence of a Fano-type destructive interference with acoustic vibrations of
the metamaterial, featured in reflection but not in transmission. The observed
effects are highly spectrally selective and sensitive to the polarisation
properties of light and the Fabry-Perot modes of the metamaterial, opening a
pathway for controlling the switching rates by spectral selection and
nanostructure design. The capability to manipulate temporal, spectral and
mechanical aspects of light-matter interactions underscores new potential
nonlinear applications where polarisation diversity, spectral selectivity and
fast modulation are important.",338,2411.16265v1,physics.optics,physics.optics,nanotechnology,2024-11-25,2024-12-23T21:06:44.684528
Synchronized motion of gold nanoparticles in an optothermal trap,"Optical tweezers have revolutionized particle manipulation at the micro- and
nanoscale, playing a critical role in fields such as plasmonics, biophysics,
and nanotechnology. While traditional optical trapping methods primarily rely
on optical forces to manipulate and organize particles, recent studies suggest
that optothermal traps in surfactant solutions can induce unconventional
effects such as enhanced trapping stiffness and increased diffusion. Thus,
there is a need for further exploration of this system to gain a deeper
understanding of the forces involved. This work investigates the behaviour of
gold nanoparticles confined in an optothermal trap around a heated anchor
particle in a surfactant (CTAC) solution. We observe unexpected radial
confinement and synchronized rotational diffusion of particles at
micrometre-scale separations from the anchor particle. These dynamics differ
from known optical binding and thermophoretic effects, suggesting unexplored
forces facilitated by the surfactant environment. This study expands the
understanding of optothermal trapping driven by anchor plasmonic particles and
introduces new possibilities for nanoparticle assembly, offering insights with
potential applications in nanoscale fabrication and materials science.",239,2411.15512v2,physics.optics,"physics.optics,cond-mat.mes-hall,cond-mat.soft",nanotechnology,2024-11-23,2024-12-23T21:06:44.685526
Simulation of Nanorobots with Artificial Intelligence and Reinforcement Learning for Advanced Cancer Cell Detection and Tracking,"Nanorobots are a promising development in targeted drug delivery and the
treatment of neurological disorders, with potential for crossing the
blood-brain barrier (BBB). These small devices leverage advancements in
nanotechnology and bioengineering for precise navigation and targeted payload
delivery, particularly for conditions like brain tumors, Alzheimer's disease,
and Parkinson's disease. Recent progress in artificial intelligence (AI) and
machine learning (ML) has improved the navigation and effectiveness of
nanorobots, allowing them to detect and interact with cancer cells through
biomarker analysis. This study presents a new reinforcement learning (RL)
framework for optimizing nanorobot navigation in complex biological
environments, focusing on cancer cell detection by analyzing the concentration
gradients of surrounding biomarkers. We utilize a computer simulation model to
explore the behavior of nanorobots in a three-dimensional space with cancer
cells and biological barriers. The proposed method uses Q-learning to refine
movement strategies based on real-time biomarker concentration data, enabling
nanorobots to autonomously navigate to cancerous tissues for targeted drug
delivery. This research lays the groundwork for future laboratory experiments
and clinical applications, with implications for personalized medicine and less
invasive cancer treatments. The integration of intelligent nanorobots could
revolutionize therapeutic strategies, reducing side effects and enhancing
treatment effectiveness for cancer patients. Further research will investigate
the practical deployment of these technologies in medical settings, aiming to
unlock the full potential of nanorobotics in healthcare.",302,2411.02345v1,cs.RO,"cs.RO,cs.AI,physics.med-ph,q-bio.OT,Artificial intelligence",nanotechnology,2024-11-04,2024-12-23T21:06:44.686523
Defects in graphite engineered by ion implantation for the self-assembly of gold nanoparticles,"Defect engineering in two-dimensional (2D) materials is essential for
advancing applications such as gas sensing, single-atom catalysis, and guided
nanoparticle self-assembly, enabling the creation of materials with tailored
functionalities. This study investigates ion implantation effects on highly
ordered pyrolytic graphite (HOPG) surfaces, using scanning tunneling microscopy
(STM) and density functional theory (DFT) simulations to identify distinct
defect structures. High-energy heavy ions cause inelastic scattering,
increasing surface damage, while gold atoms deposited onto defect sites
preferentially form atomic clusters. Through focused ion beam techniques,
spatially distributed defects were engineered, guiding the self-assembly of
nanoparticles. This research highlights the precision of ion irradiation for
modifying HOPG surfaces, with significant implications for catalysis,
nanotechnology, and the development of functional materials with controlled
nanoscale properties.",189,2411.02204v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",nanotechnology,2024-11-04,2024-12-23T21:06:44.686523
Ultrafast Entropy Production in Non-Equilibrium Magnets,"We present an ultrafast thermodynamics framework to model heat generation and
entropy production in laser-driven ferromagnetic systems. By establishing a
connection between the magnetic field strength of the laser pulse and
magnetization dynamics we model time-dependent entropy production rates and
deduce the associated heat dissipation in epitaxial and polycrystalline FeNi
and CoFeB thin films. Our theoretical predictions are validated by comparison
to experimental magnetization dynamics data, shedding light on thermodynamic
processes on picosecond timescales. Crucially, we incorporate recently observed
inertial spin dynamics, to describe their impact on heat generation in
pump-probe experiments. As such, this formalism provides novel insights into
controlling heat production in magnetic systems, and contributes to advancing
the understanding of non-equilibrium thermodynamics in magnetic systems, with
implications for future experimental protocols in spintronics and
nanotechnology.",198,2410.23205v2,cond-mat.stat-mech,cond-mat.stat-mech,nanotechnology,2024-10-30,2024-12-23T21:06:44.687521
The mutual dynamics of a resonant particle inside rectangular cavity: Collective polarizability calculation via a ladder-type alternative Green's functions approach,"Light-matter interaction plays a pivotal role in pushing forward
nanotechnology. A particularly important setup involves a resonating particle,
say an emitting molecule or a macroscopic quasi-statically resonating
ferromagnetic sphere, that is located inside a cavity. In this paper, we
provide an analytic formulation for the exact calculation of the mutual
dynamics between a resonant particle and a rectangular cavity in which it is
located. The particle is assumed to be small on the wavelength, and thus, its
excitation is dominated by a dipolar response that can be described using the
discrete dipole approximation and polarizability theory. The dipolar response
depends on the particle's polarizability function which encapsulates the
particle's materials and geometry, and on the local field acting on the
particle. In principle, the latter is nothing more than the backscattering of
the particle field by the cavity walls. However, it may be challenging since it
involves a three-dimensional singularity subtraction of the Green's function in
the cavity and the Green's function in the free space that are differently
represented. In this paper, we suggest the use of a ladder-type process
involving alternative Green's function representations to calculate the local
field in a computationally efficient and numerically stable manner. Using this
approach, we solve several strongly coupled particle-cavity systems and
calculate the collective resonance frequencies of the system with isotropic,
gyrotropic, and chiral particles.",308,2410.20845v1,physics.app-ph,physics.app-ph,nanotechnology,2024-10-28,2024-12-23T21:06:44.688518
Security and RAS in the Computing Continuum,"Security and RAS are two non-functional requirements under focus for current
systems developed for the computing continuum. Due to the increased number of
interconnected computer systems across the continuum, security becomes
especially pervasive at all levels, from the smallest edge device to the
high-performance cloud at the other end. Similarly, RAS (Reliability,
Availability, and Serviceability) ensures the robustness of a system towards
hardware defects. Namely, making them reliable, with high availability and
design for easy service. In this paper and as a result of the Vitamin-V EU
project, the authors detail the comprehensive approach to malware and hardware
attack detection; as well as, the RAS features envisioned for future systems
across the computing continuum.",145,2410.17116v1,cs.CR,"cs.CR,cs.DC",nanotechnology,2024-10-22,2024-12-23T21:06:44.688518
Backbone Mediated Electrical Transport in a Double-Stranded DNA,"In the field of DNA nanotechnology, it is common wisdom that charge transport
occurs through the {\pi} stacked bases in a double-stranded DNA. However,
recent experimental findings by Roman Zhuravel et. al. [Nat. Nanotech. 15, 836
(2020)] suggest that it is the backbone channels through which transport
happens, not through the nitrogen bases. These new experimental studies call
for a detail investigation of these biomolecules from a different perspective.
In this article we examine charge transport properties of a double-stranded DNA
within a tight-binding framework where the backbones form the main conduction
channels. Using techniques based on the Green's function method, we inspect
changes in the density of states (DOS) and localization properties of DNA in
presence of discontinuities (nicks) along the backbone channels. We also
investigate the effect of backbone channel discontinuities on current-voltage
(I-V) responses using the Landauer-Buttiker formalism. We study three
characteristic DNA sequences, two periodic and one random. We observe that, in
all cases, the effects of nicks on the transport properties are similar.
Irrespective of the I-V responses of pristine sequences (be it metallic or
semiconducting), as soon as we introduce two nicks at two different strands,
current is cut-off. Hence we claim that the backbone channel supported charge
conduction in DNA is an universal phenomena irrespective of the sequence and
its pristine I-V characteristics.",317,2410.15845v2,cond-mat.mes-hall,cond-mat.mes-hall,nanotechnology,2024-10-21,2024-12-23T21:06:44.689515
Effect of Grafting Density on the Two-dimensional Assembly of Nanoparticles,"Employing grazing-incidence small-angle X-ray scattering (GISAXS) and X-ray
reflectivity (XRR), we demonstrate that films composed of polyethylene glycol
(PEG)-grafted silver nanoparticles (AgNP) and gold nanoparticles (AuNP), as
well as their binary mixtures, form highly stable hexagonal structures at the
vapor-liquid interface. These nanoparticles exhibit remarkable stability under
varying environmental conditions, including changes in pH, mixing
concentration, and PEG chain length. Short-chain PEG grafting produces dense,
well-ordered films, while longer chains produce more complex, less dense
quasi-bilayer structures. AuNPs exhibit higher grafting densities than AgNPs,
leading to more ordered in-plane arrangements. In binary mixtures, AuNPs
dominate the population at the surface, while AgNPs integrate into the system,
expanding the lattice without forming a distinct binary superstructure. These
results offer valuable insights into the structural behavior of PEG-grafted
nanoparticles and provide a foundation for optimizing binary nanoparticle
assemblies for advanced nanotechnology applications.",242,2410.05456v1,cond-mat.soft,cond-mat.soft,nanotechnology,2024-10-07,2024-12-23T21:06:44.690513
Energy-efficient picosecond spin-orbit torque magnetization switching in ferro- and ferrimagnetic films,"Electrical current pulses can be used to manipulate magnetization efficiently
via spin-orbit torques (SOTs). Pulse durations as short as a few picoseconds
have been used to switch the magnetization of ferromagnetic films, reaching the
THz regime. However, little is known about the reversal mechanisms and energy
requirements in the ultrafast switching regime. In this work, we quantify the
energy cost for magnetization reversal over 7 orders of magnitude in pulse
duration, in both ferromagnetic and ferrimagnetic samples, bridging
quasi-static spintronics and femtomagnetism. To this end, we develop a method
to stretch picosecond pulses generated by a photoconductive switch by an order
of magnitude. Thereby, we can create current pulses from picoseconds to
durations approaching pulse width available with commercial instruments. We
show that the energy cost for SOT switching decreases by more than an order of
magnitude in all samples when the pulse duration enters the picosecond range.
We project an energy cost of 9 fJ for a 100 x 100 nm 2 ferrimagnetic device.
Micromagnetic and macrospin simulations unveil a transition from a non-coherent
to a coherent magnetization reversal with a strong modification of the
magnetization dynamical trajectories as pulse duration is reduced. Our results
cement the potential for high-speed magnetic spin-orbit torque memories and
highlights alternative magnetization reversal pathways at fast time scales.",310,2410.00474v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,nanotechnology,2024-10-01,2024-12-23T21:06:44.690513
Single-molecule Automata: Harnessing Kinetic-Thermodynamic Discrepancy for Temporal Pattern Recognition,"Molecular-scale computation is crucial for smart materials and nanoscale
devices, yet creating single-molecule systems capable of complex computations
remains challenging. We present a theoretical framework for a single-molecule
computer that performs temporal pattern recognition and complex information
processing. Our approach introduces the concept of an energy seascape,
extending traditional energy landscapes by incorporating control parameter
degrees of freedom. By engineering a kinetic-thermodynamic discrepancy in
folding dynamics, we demonstrate that a linear polymer with $N$ binary-state
foldable units can function as a deterministic finite automaton, processing
$2^N$ configurations. The molecule's dominant configuration evolves
deterministically in response to mechanical signals, enabling recognition of
complex temporal patterns. This design allows complete state controllability
through non-equilibrium driving protocols. Our model opens avenues for
molecular-scale computation with applications in biosensing, smart drug
delivery, and adaptive materials. We discuss potential experimental
realizations using DNA nanotechnology. This work bridges the gap between
information processing devices and stochastic molecular systems, paving the way
for sophisticated molecular computers rivaling biological systems in complexity
and adaptability.",238,2409.19803v1,cond-mat.stat-mech,"cond-mat.stat-mech,physics.bio-ph",nanotechnology,2024-09-29,2024-12-23T21:06:44.691510
Microscopic Geared Mechanisms,"The miniaturization of mechanical machines is critical for advancing
nanotechnology and reducing device footprints. Traditional efforts to downsize
gears and micromotors have faced limitations at around 0.1 mm for over thirty
years due to the complexities of constructing drives and coupling systems at
such scales. Here, we present an alternative approach utilizing optical
metasurfaces to locally drive microscopic machines, which can then be
fabricated using standard lithography techniques and seamlessly integrated on
the chip, achieving sizes down to tens of micrometers with movements precise to
the sub-micrometer scale. As a proof of principle, we demonstrate the
construction of microscopic gear trains powered by a single driving gear with a
metasurface activated by a plane light wave. Additionally, we develop a
versatile pinion and rack micromachine capable of transducing rotational
motion, performing periodic motion, and controlling microscopic mirrors for
light deflection. Our on-chip fabrication process allows for straightforward
parallelization and integration. Using light as a widely available and easily
controllable energy source, these miniaturized metamachines offer precise
control and movement, unlocking new possibilities for micro- and nanoscale
systems.",240,2409.17284v1,physics.optics,"physics.optics,cond-mat.soft",nanotechnology,2024-09-25,2024-12-23T21:06:44.692508
Size-dependent multiexciton dynamics governs scintillation from perovskite quantum dots,"The recent emergence of quantum confined nanomaterials in the field of
radiation detection, in particular lead halide perovskite nanocrystals, offers
potentially revolutionary scalability and performance advantages over
conventional materials. This development raises fundamental questions about the
mechanism of scintillation itself at the nanoscale and the role of particle
size, arguably the most defining parameter of quantum dots. Understanding this
is crucial for the design and optimisation of future nanotechnology
scintillators. In this work, we address these open questions by theoretically
and experimentally studying the size-dependent scintillation of CsPbBr3
nanocrystals using a combination of Monte Carlo simulations, spectroscopic, and
radiometric techniques. The results reveal and unravel a complex parametric
space where the fine balance between the simultaneous effects of size-dependent
energy deposition, (multi-)exciton population, and light emission under
ionizing excitation, typical of confined particles, combine to maximize the
scintillation efficiency and time performance of larger nanocrystals due to
greater stopping power and reduced Auger decay. The remarkable agreement
between theory and experiment produces a fully validated descriptive model that
unprecedentedly predicts the scintillation yield and kinetics of nanocrystals
without free parameters, providing the first fundamental guide for the rational
design of nanoscale scintillators.",282,2409.16994v1,physics.app-ph,"physics.app-ph,cond-mat.mes-hall",nanotechnology,2024-09-25,2024-12-23T21:06:44.693506
How we simulate DNA origami,"DNA origami consists of a long scaffold strand and short staple strands that
self-assemble into a target 2D or 3D shape. It is a widely used construct in
nucleic acid nanotechnology, offering a cost-effective way to design and create
diverse nanoscale shapes. With promising applications in areas such as
nanofabrication, diagnostics, and therapeutics, DNA origami has become a key
tool in the bionanotechnology field. Simulations of these structures can offer
insight into their shape and function, thus speeding up and simplifying the
design process. However, simulating these structures, often comprising
thousands of base pairs, poses challenges due to their large size. OxDNA, a
coarse-grained model specifically designed for DNA nanotechnology, offers
powerful simulation capabilities. Its associated ecosystem of visualization and
analysis tools can complement experimental work with in silico
characterization. This tutorial provides a general approach to simulating DNA
origami structures using the oxDNA ecosystem, tailored for experimentalists
looking to integrate computational analysis into their design workflow.",221,2409.13206v1,cond-mat.soft,"cond-mat.soft,physics.comp-ph,physics.ed-ph",nanotechnology,2024-09-20,2024-12-23T21:06:44.694502
Simulation of charged nanotubes self-assembly during evaporation of a sessile droplet on a substrate,"The ability to control the morphology of the nanotube deposit formed during
the evaporation of a sessile droplet on a substrate is of theoretical and
practical interest. Such deposits is required for various applications
including nanotechnology, medicine, biotechnology, and optronics. In the
experiment of Zhao et al. [J. Colloid Interface Sci. 440, 68 (2015)], an
annular deposit was formed near the contact line. The deposition geometry is
caused by the coffee-ring effect. This deposit is unusual in its morphology. It
changes gradually in space from a disordered structure in the inner part of the
ring to an aligned structure of nanotubes close to the periphery. To understand
the mechanisms that lead to this, we have developed a mathematical model that
takes into account the effects of advection, diffusion, and electrostatic
interactions on particle transport. Results of numerical calculations have
confirmed that all these factors together have an influence on the formation of
such a variable morphology. Qualitative agreement with the experiment is shown
for some values of the model parameters.",219,2409.12647v1,cond-mat.soft,cond-mat.soft,nanotechnology,2024-09-19,2024-12-23T21:06:44.695499
Programmable multifunctional integrated microwave photonic circuit on thin-film lithium niobate,"Microwave photonics, with its advanced high-frequency signal processing
capabilities, is expected to play a crucial role in next-generation wireless
communications and radar systems. The realization of highly integrated,
high-performance, and multifunctional microwave photonic links will pave the
way for its widespread deployment in practical applications, which is a
significant challenge. Here, leveraging thin-film lithium niobate intensity
modulator and programmable cascaded microring resonators, we demonstrate for
the first time a tunable microwave photonic notch filter that simultaneously
achieves high level of integration along with high dynamic range, high link
gain, low noise figure, and ultra-high rejection ratio. Additionally, this
programmable on-chip system is multifunctional, allowing for the dual-band
notch filter and the suppression of the high-power interference signal. This
work demonstrates the potential applications of the thin-film lithium niobate
platform in the field of high-performance integrated microwave photonic
filtering and signal processing, facilitating the advancement of microwave
photonic system towards practical applications.",217,2409.10227v2,physics.optics,"physics.optics,physics.app-ph",nanotechnology,2024-09-16,2024-12-23T21:06:44.695499
Tuned ionic mobility by Ultrafast-laser pulses in Black Silicon,"Highly non-equilibrium conditions in femtosecond-laser excited solids cause a
variety of ultrafast phenomena that are not accessible by thermal conditions,
like sub-picosecond solid-to-liquid or solid-to-solid phase transitions. In
recent years the microscopic pathways of various laser-induced crystal
rearrangements could be identified and led to novel applications and/or
improvements in optoelectronics, photonics, and nanotechnology. However, it
remains unclear what effect a femtosecond-laser excitation has on ionic
impurities within an altered crystal environment, in particular on the atomic
mobility. Here, we performed ab-initio molecular dynamics (AIMD) simulations on
laser-excited black silicon, a promising material for high-efficient solar
cells, using the Code for Highly excIted Valence Electron Systems (CHIVES). By
computing time-dependent Bragg peak intensities for doping densities of 0.16%
and 2.31% we could identify the overall weakening of the crystal environment
with increasing impurity density. The analysis of Si-S bond angles and lengths
after different excitation densities, as well as computing interatomic forces
allowed to identify a change in ion mobility with increasing impurity density
and excitation strength. Our results indicate the importance of impurity
concentrations for ionic mobility in laser-excited black silicon and could give
significant insight for semiconductor device optimization and materials science
advancement.",298,2409.07659v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.other",nanotechnology,2024-09-11,2024-12-23T21:06:44.696497
Dissipative self-assembly of patchy particles under nonequilibrium drive: a computational study,"Inspired by biology and implemented using nanotechnology, the self-assembly
of patchy particles has emerged as a pivotal mechanism for constructing complex
structures that mimic natural systems with diverse functionalities. Here, we
explore the dissipative self-assembly of patchy particles under nonequilibrium
conditions, with the aim of overcoming the constraints imposed by equilibrium
assembly. Utilizing extensive Monte Carlo (MC) and Molecular Dynamics (MD)
simulations, we provide insight into the effects of external forces that mirror
natural and chemical processes on the assembly rates and the stability of the
resulting assemblies comprising $8$, $10$, and $13$ patchy particles.
Implemented by a favorable bond-promoting drive in MC or a pulsed square wave
potential in MD, our simulations reveal the role these external drives play in
accelerating assembly kinetics and enhancing structural stability, evidenced by
a decrease in the time to first assembly and an increase in the duration the
system remains in an assembled state. Through the analysis of an order
parameter, entropy production, bond dynamics, and interparticle forces, we
unravel the underlying mechanisms driving these advancements. We also validated
our key findings by simulating a larger system of $100$ patchy particles. Our
comprehensive results not only shed light on the impact of external stimuli on
self-assembly processes but also open a promising pathway for expanding the
application by leveraging patchy particles for novel nanostructures.",290,2409.04748v1,cond-mat.soft,"cond-mat.soft,cond-mat.stat-mech,physics.comp-ph",nanotechnology,2024-09-07,2024-12-23T21:06:44.697494
AI and Machine Learning Approaches for Predicting Nanoparticles Toxicity The Critical Role of Physiochemical Properties,"This research investigates the use of artificial intelligence and machine
learning techniques to predict the toxicity of nanoparticles, a pressing
concern due to their pervasive use in various industries and the inherent
challenges in assessing their biological interactions. Employing models such as
Decision Trees, Random Forests, and XGBoost, the study focuses on analyzing
physicochemical properties like size, shape, surface charge, and chemical
composition to determine their influence on toxicity. Our findings highlight
the significant role of oxygen atoms, particle size, surface area, dosage, and
exposure duration in affecting toxicity levels. The use of machine learning
allows for a nuanced understanding of the intricate patterns these properties
form in biological contexts, surpassing traditional analysis methods in
efficiency and predictive power. These advancements aid in developing safer
nanomaterials through computational chemistry, reducing reliance on costly and
time-consuming experimental methods. This approach not only enhances our
understanding of nanoparticle behavior in biological systems but also
streamlines the safety assessment process, marking a significant stride towards
integrating computational techniques in nanotoxicology.",218,2409.15322v1,physics.chem-ph,"physics.chem-ph,cs.LG,I.2.6, I.2.8 I.2.6, I.2.8,I.2.6; I.2.8",nanotechnology,2024-09-06,2024-12-23T21:06:44.698491
On chip high-dimensional entangled photon sources,"High-dimensional quantum entanglement is an important resource for emerging
quantum technologies such as quantum communication and quantum computation. The
scalability of metres-long experimental setups limits high-dimensional
entanglement in bulk optics. Advancements in quantum technology hinge on
reproducible, and reconfigurable quantum devices -- including photon sources,
which are challenging to achieve in a scalable manner using bulk optics.
Advances in nanotechnology and CMOS-compatible integration techniques have
enabled the generation of entangled photons on millimeter-scale chips,
significantly enhancing scalability, stability, replicability, and
miniaturization for real-world quantum applications. In recent years we have
seen several chip-scale demonstrations with different degrees of freedom
including path, frequency-bin, time-bin, and transverse modes, on many material
platforms. A complete quantum photonic integrated circuit requires the
generation, manipulation, and detection of qudits, involving various active and
passive quantum photonic components which further increase the degree of
complexity. Here, we review and introduce the nonlinear optical processes that
facilitate on-chip high-dimensional entangled photon sources and the currently
used material platforms. We discuss a range of current implementations of
on-chip high-dimensional entangled photon sources and demonstrated
applications. We comment on the current challenges due to the limitations of
individual material platforms and present future opportunities in hybrid and
heterogeneous integration strategies for the next generation of integrated
quantum photonic chips.",298,2409.03224v1,quant-ph,quant-ph,nanotechnology,2024-09-05,2024-12-23T21:06:44.698491
Ultrathin natural biotite crystals as a dielectric layer for van der Waals heterostructure applications,"Biotite, an iron-rich mineral belonging to the trioctahedral mica group, is a
naturally abundant layered material (LM) exhibiting attractive electronic
properties for application in nanodevices. Biotite stands out as a
non-degradable LM under ambient conditions, featuring high-quality basal
cleavage, a significant advantage for van der Waals heterostructure (vdWH)
applications. In this work, we present the micro-mechanical exfoliation of
biotite down to monolayers (1Ls), yielding ultrathin flakes with large areas
and atomically flat surfaces. To identify and characterize the mineral, we
conducted a multi-elemental analysis of biotite using energy-dispersive
spectroscopy mapping. Additionally, synchrotron infrared nano-spectroscopy was
employed to probe its vibrational signature in few-layer form, with sensitivity
to the layer number. We have also observed good morphological and structural
stability in time (up to 12 months) and no important changes in their physical
properties after thermal annealing processes in ultrathin biotite flakes.
Conductive atomic force microscopy evaluated its electrical capacity, revealing
an electrical breakdown strength of approximately 1 V/nm. Finally, we explore
the use of biotite as a substrate and encapsulating LM in vdWH applications. We
have performed optical and magneto-optical measurements at low temperatures. We
find that ultrathin biotite flakes work as a good substrate for 1L-MoSe2,
comparable to hexagonal boron nitride flakes, but it induces a small change of
the 1L-MoSe2 g-factor values, most likely due to natural impurities on its
crystal structure. Furthermore, our results show that biotite flakes are useful
systems to protect sensitive LMs such as black phosphorus from degradation for
up to 60 days in ambient air. Our study introduces biotite as a promising,
cost-effective LM for the advancement of future ultrathin nanotechnologies.",413,2408.16697v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.app-ph",nanotechnology,2024-08-29,2024-12-23T21:06:44.699996
Theory of High-Temperature Superfluorescence in Hybrid Perovskite Thin Films,"The recent discovery of high-temperature superfluorescence in hybrid
perovskite thin films has opened new possibilities for harnessing macroscopic
quantum phenomena in nanotechnology. This study aimed to elucidate the
mechanism that enables high-temperature superfluorescence in these systems. The
proposed model describes a quasi-2D Wannier exciton in a thin film that
interacts with phonons via the longitudinal optical phonon-exciton Frohlich
interaction. We show that the superradiant properties of the coherent state in
hybrid perovskites are stable against perturbations caused by the longitudinal
optical phonon-exciton Frohlich interaction. Using the multiconfiguration
Hartree approach, we derive semiclassical equations of motion for a
single-exciton wavefunction, where the vibrational degrees of freedom interact
with the Wannier exciton through a mean-field Hartree term. Superradiance is
effectively described by a non-Hermitian term in the Hamiltonian. The analysis
was then extended to multiple excited states using the semiclassical
Hamiltonian as the basic model. We demonstrate that the ground state of the
model exciton Hamiltonian with long-range interactions is a symmetric Dicke
superradiant state, where the Frohlich interaction is nullified. The additional
density matrix-based consideration draws an analogy between this system and
stable systems, where the conservation laws determine the nullification of the
constant (momentum-independent) decay rate part. In the exciton-phonon system,
nullification is associated with the absence of a momentum-independent
component in the Wannier exciton-phonon interaction coupling function.",354,2408.15169v1,physics.optics,"physics.optics,cond-mat.mes-hall",nanotechnology,2024-08-27,2024-12-23T21:06:44.700994
Collective bistability of pyridine-furan nanosprings coupled by a graphene plate,"Nanometer-sized molecular structures exhibiting mechanical-like switching
between discrete states are of great interest for their potential uses in
nanotechnology and materials science. Designing such structures and
understanding how they can be combined to operate synchronously is a key to
creating nanoscale functional units. Notable examples recently discovered using
atomistic simulations are pyridine-furan and pyridine-pyrrole nanosprings. When
slightly stretched in aqueous or organic solutions, these nanosprings exhibit
bistable dynamics akin to Duffing nonlinear oscillators. Based on these
findings, we designed a hybrid system consisting of several pyridine furan
nanosprings attached to a graphene plate in organic solvent and simulated the
molecular dynamics of the construct. Our focus is on how the nanosprings
coupled by a graphene plate work together, and whether such a design enables
the nanosprings to respond synchronously to random perturbations and weak
external stimuli. Molecular dynamics simulations of this specific construct are
complemented by a theoretical model of coupled bistable systems to understand
how the synchronization depends on coupling of bistable units.",243,2408.13371v1,cond-mat.mes-hall,cond-mat.mes-hall,nanotechnology,2024-08-23,2024-12-23T21:06:44.700994
Current rectification via Photosystem I monolayers induced by their orientation on hydrophilic self-assembled monolayers on titanium nitride,"Photosystem I (PSI) is a photosynthetic protein which evolved to efficiently
transfer electrons through the thylakoid membrane. This remarkable process
attracted the attention of the biomolecular electronics community, which aims
to study and understand the underlying electronic transport through these
proteins by contacting ensembles of PSI with solid-state metallic contacts.
This paper extends published work of immobilizing monolayers of PSI with a
specific orientation, by using organophosphonate self-assembled molecules with
hydrophilic heads on ultra-flat titanium nitride. Electrical measurements
carried out with eutectic GaIn top contacts showed current rectification ratios
of up to ~200. The previously proposed rectification mechanism, relying on the
protein's internal electric dipole, was inquired by measuring shifts in the
work function. Our straightforward bottom-up fabrication method may allow for
further experimental studies on PSI molecules, such as embedding them in
solid-state, transparent top contact schemes for optoelectronic measurements.",208,2408.09276v1,physics.bio-ph,"physics.bio-ph,cond-mat.mtrl-sci,cond-mat.soft",nanotechnology,2024-08-17,2024-12-23T21:06:44.701992
Nanometric dual-comb ranging using photon-level microcavity solitons,"Absolute distance measurement with low return power, fast measurement speed,
high precision, and immunity to intensity fluctuations is highly demanded in
nanotechnology. However, achieving all these objectives simultaneously remains
a significant challenge for miniaturized systems. Here, we demonstrate
dual-comb ranging (DCR) that encompasses all these capabilities by using
counter-propagating (CP) solitons generated in an integrated Si$_3$N$_4$
microresonator. We derive equations linking the DCR precision with comb line
powers, revealing the advantage of microcomb's large line spacing in precise
ranging. Leveraging the advantage, our system reaches 1-nm-precision and
measures nm-scale vibration at frequencies up to 0.9 MHz. We also show that
precise DCR is possible even in the presence of strong intensity noise and
loss, using a mean received photon number as low as 5.5$\times$10$^{-4}$ per
pulse. Our work establishes an optimization principle for dual-comb systems and
bridges high performance ranging with foundry-manufactured photonic chips.",224,2408.05739v1,physics.optics,physics.optics,nanotechnology,2024-08-11,2024-12-23T21:06:44.702989
Multiscale modeling framework of a constrained fluid with complex boundaries using twin neural networks,"The properties of constrained fluids have increasingly gained relevance for
applications ranging from materials to biology. In this work, we propose a
multiscale model using twin neural networks to investigate the properties of a
fluid constrained between solid surfaces with complex shapes. The atomic scale
model and the mesoscale model are connected by the coarse-grained potential
which is represented by the first neural network. Then we train the second
neural network model as a surrogate to predict the velocity profile of the
constrained fluid with complex boundary conditions at the mesoscale. The effect
of complex boundary conditions on the fluid dynamics properties and the
accuracy of the neural network model prediction are systematically
investigated. We demonstrate that the neural network-enhanced multiscale
framework can connect simulations at atomic scale and mesoscale and reproduce
the properties of a constrained fluid at mesoscale. This work provides insight
into multiscale model development with the aid of machine learning techniques
and the developed model can be used for modern nanotechnology applications such
as enhanced oil recovery and porous materials design.",212,2408.03263v1,physics.chem-ph,"physics.chem-ph,physics.comp-ph",nanotechnology,2024-08-06,2024-12-23T21:06:44.702989
Scientific Exploration with Expert Knowledge (SEEK) in Autonomous Scanning Probe Microscopy with Active Learning,"Microscopy techniques have played vital roles in materials science, biology,
and nanotechnology, offering high-resolution imaging and detailed insights into
properties at nanoscale and atomic level. The automation of microscopy
experiments, in combination with machine learning approaches, is a
transformative advancement, offering increased efficiency, reproducibility, and
the capability to perform complex experiments. Our previous work on autonomous
experimentation with scanning probe microscopy (SPM) demonstrated an active
learning framework using deep kernel learning (DKL) for structure-property
relationship discovery. This approach has demonstrated broad applications in
various microscopy techniques. Here, to address limitations of workflows based
on DKL, we developed methods to incorporate prior knowledge and human interest
into DKL-based workflows and implemented these workflows in SPM. By integrating
expected rewards from structure libraries or spectroscopic features, we
enhanced the exploration efficiency of autonomous microscopy, demonstrating
more efficient and targeted exploration in autonomous microscopy. We
demonstrated the application of these methods in SPM, but we suggest that these
methods can be seamlessly applied to other microscopy and imaging techniques.
Furthermore, the concept can be adapted for general Bayesian optimization in
material discovery across a broad range of autonomous experimental fields.",248,2408.02071v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.app-ph",nanotechnology,2024-08-04,2024-12-23T21:06:44.703986
Hyperspectral near infrared imaging using a tunable spectral phasor,"Hyperspectral imaging captures both spectral and spatial information from a
sample. The near infrared (NIR, > 800 nm) is advantageous for biomedical
imaging as it falls into the tissue transparency window but also contains
vibrational overtone and combination modes useful for molecular fingerprinting.
Here, we demonstrate hyperspectral NIR imaging using a spectral phasor
transformation (HyperNIR). This method employs a liquid crystal variable
retarder (LCVR) for tunable, wavelength-dependent sine-, cosine and no
filtering that transforms optical signals into phasor space. Spectral
information is thus obtained with just three images. The LCVR can be adjusted
to cover a spectral range from 900 nm to 1600 nm in windows tunable from 50 nm
to 700 nm. This approach enables distinguishing NIR fluorophores with emission
peaks less than 5 nm apart. Furthermore, we demonstrate label-free
hyperspectral NIR reflectance imaging to identify plastic polymers and to
monitor in vivo plant health. The approach uses the full camera resolution and
reaches hyperspectral frame rates of 0.2 per second, limited only by the
switching rate of the LCVR. HyperNIR facilitates straightforward hyperspectral
imaging with standard NIR cameras for applications in biomedical imaging and
environmental monitoring.",264,2407.21684v1,physics.optics,"physics.optics,physics.app-ph",nanotechnology,2024-07-31,2024-12-23T21:06:44.704984
Automating Blueprints for Colloidal Quasicrystal Assembly,"One of the frontiers of nanotechnology is advancing beyond the periodic
self-assembly of materials. Icosahedral quasicrystals, aperiodic in all
directions, represent one of the most challenging targets that have yet to be
experimentally realized at the colloidal scale. Previous attempts have required
meticulous human-designed building blocks and often resulted in interactions
beyond current experimental capabilities. In this work, we introduce a
framework for generating experimentally accessible designs that self-assemble
into quasicrystalline arrangements. We present a design for icosahedral
DNA-origami building blocks and demonstrate, through molecular simulations,
their successful assembly into the target quasicrystalline structure. Our
results highlight the feasibility of using automated design protocols to
achieve complex quasicrystalline patterns, paving the way for new applications
in material science and nanotechnology.",175,2407.19968v1,cond-mat.soft,cond-mat.soft,nanotechnology,2024-07-29,2024-12-23T21:06:44.704984
Giant Purcell broadening and Lamb shift for DNA-assembled near-infrared quantum emitters,"Controlling the light emitted by individual molecules is instrumental to a
number of novel nanotechnologies ranging from super-resolution bio-imaging and
molecular sensing to quantum nanophotonics. Molecular emission can be tailored
by modifying the local photonic environment, for example by precisely placing a
single molecule inside a plasmonic nanocavity with the help of DNA origami.
Here, using this scalable approach, we show that commercial fluorophores
experience giant Purcell factors and Lamb shifts, reaching values on par with
those recently reported in scanning tip experiments. Engineering of plasmonic
modes enables cavity-mediated fluorescence far detuned from the
zero-phonon-line (ZPL) - at detunings that are up to two orders of magnitude
larger than the fluorescence linewidth of the bare emitter and reach into the
near-infrared. Our results evidence a regime where the emission linewidth is
dominated by the excited state lifetime, as required for indistinguishable
photon emission, baring relevance to the development of nanoscale, ultrafast
quantum light sources and to the quest toward single-molecule cavity-QED. In
the future, this approach may also allow to design efficient quantum emitters
at infrared wavelengths, where standard organic sources have a reduced
performance.",269,2407.19513v1,physics.optics,"physics.optics,cond-mat.mes-hall,quant-ph",nanotechnology,2024-07-28,2024-12-23T21:06:44.705981
"Comment on ""Ballistic Majorana Nanowire Devices"" by Gul et al. Nature Nanotechnology 2018","This work re-analyzes Gul et al. Nature Nanotechnology 2018 ""Ballistic
Majorana nanowire devices"" using fuller data from the original experiments
released in 2023 on Zenodo. The authors have prepared a correction to their
article that appeared in Nature Nanotechnology in 2024. However, the correction
does not address the concerns we identify here. We demonstrate that the fuller
data contain extensive evidence for quantum dots and disorder that are
completely inconsistent with the authors' conclusion that they have achieved
ballistic devices containing zero bias peaks of likely Majorana origin. We show
how data selection, data cropping and undisclosed data processing played a role
in composing the figures of the final published paper.",140,2407.18623v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,cond-mat.supr-con",nanotechnology,2024-07-26,2024-12-23T21:06:44.705981
Quadrupolar Excitons in MoSe2 Bilayers,"The quest for platforms to generate and control exotic excitonic states has
greatly benefited from the advent of transition metal dichalcogenide (TMD)
monolayers and their heterostructures. Among the unconventional excitonic
states, quadrupolar excitons - a hybridized combination of two dipolar excitons
with anti-aligned dipole moments - are of great interest for applications in
quantum simulations and for the investigation of many-body physics. Here, we
unambiguously demonstrate for the first time in natural MoSe$_2$ homobilayers
the emergence of quadrupolar excitons, whose energy shifts quadratically in
electric field. In contrast to, so far reported trilayer systems hosting
quadrupolar excitons, MoSe$_2$ homobilayers have many advantages, a stronger
interlayer hybridization, cleaner potential landscapes and inherent stability
with respect to moir\'e potentials or post-stacking reconstruction. Our
experimental observations are complemented by many-particle theory calculations
offering microscopic insights in the formation of quadrupole excitons. Our
results suggest TMD homobilayers as ideal platform for the engineering of
excitonic states and their interaction with light and thus candidate for
carrying out on-chip simulations.",264,2407.18040v1,cond-mat.mes-hall,cond-mat.mes-hall,nanotechnology,2024-07-25,2024-12-23T21:06:44.706978
End-to-end simulations of photonic phase correctors for adaptive optics systems,"Optical beams and starlight distorted by atmospheric turbulence can be
corrected with adaptive optics systems to enable efficient coupling into
single-mode fibers. Deformable mirrors, used to flatten the wavefront in
astronomical telescopes, are costly, sensitive, and complex mechanical
components that require careful calibration to enable high-quality imaging in
astronomy, microscopy, and vision science. They are also impractical to deploy
in large numbers for non-imaging applications like free-space optical
communication. Here, we propose a photonic integrated c rcuit capable of
spatially sampling the wavefront collected by the telescope and co-phasing the
subapertures to maximize the flux delivered to an output single-mode fiber as
the integrated photonic implementation of a deformable mirror. We present the
results of end-to-end simulations to quantify the performance of the proposed
photonic solution under varying atmospheric conditions toward realizing an
adaptive optics system without a deformable mirror for free-space optical
receivers.",201,2407.11171v1,physics.optics,"physics.optics,astro-ph.IM",nanotechnology,2024-07-15,2024-12-23T21:06:44.707976
Roadmap for Animate Matter,"Humanity has long sought inspiration from nature to innovate materials and
devices. As science advances, nature-inspired materials are becoming part of
our lives. Animate materials, characterized by their activity, adaptability,
and autonomy, emulate properties of living systems. While only biological
materials fully embody these principles, artificial versions are advancing
rapidly, promising transformative impacts across various sectors. This roadmap
presents authoritative perspectives on animate materials across different
disciplines and scales, highlighting their interdisciplinary nature and
potential applications in diverse fields including nanotechnology, robotics and
the built environment. It underscores the need for concerted efforts to address
shared challenges such as complexity management, scalability, evolvability,
interdisciplinary collaboration, and ethical and environmental considerations.
The framework defined by classifying materials based on their level of animacy
can guide this emerging field encouraging cooperation and responsible
development. By unravelling the mysteries of living matter and leveraging its
principles, we can design materials and systems that will transform our world
in a more sustainable manner.",210,2407.10623v3,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.soft,physics.app-ph",nanotechnology,2024-07-15,2024-12-23T21:06:44.708975
Selective area epitaxy of in-plane HgTe nanostrcutures on CdTe(001) substrate,"Semiconductor nanowires are believed to play a crucial role for future
applications in electronics, spintronics and quantum technologies. A potential
candidate is HgTe but its sensitivity to nanofabrication processes restrain its
development. A way to circumvent this obstacle is the selective area growth
technique. Here, in-plane HgTe nanostructures are grown thanks to selective
area molecular beam epitaxy on a semi-insulating CdTe substrate covered with a
patterned SiO$_{\mathrm{2}}$ mask. The shape of these nanostructures is defined
by the in-plane orientation of the mask aperture along the <$110$>,
<$1\bar{\mathrm{1}}0$>, or <$100$> direction, the deposited thickness, and the
growth temperature. Several micron long in-plane nanowires can be achieved as
well as more complex nanostructures such as networks, diamond structures or
rings. A good selectivity is achieved with very little parasitic growth on the
mask even for a growth temperature as low as $140${\deg}C and growth rate up to
$0.5$ ML/s. For <$110$> oriented nanowires, the center of the nanostructure
exhibits a trapezoidal shape with {$111$}B facets and two grains on the sides,
while <$1\bar{\mathrm{1}}0$> oriented nanowires show {$111$}A facets with
adatoms accumulation on the sides of the top surface. Transmission electron
microscopy observations reveal a continuous epitaxial relation between the CdTe
substrate and the HgTe nanowire. Measurements of the resistance with fourpoint
scanning tunneling microscopy indicates a good electrical homogeneity along the
main NW axis and a thermally activated transport. This growth method paves the
way toward the fabrication of complex HgTe-based nanostructures for electronic
transport measurements.",422,2407.08402v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,nanotechnology,2024-07-11,2024-12-23T21:06:44.709973
Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced Acceleration via Neural Topology Optimization,"The Starshot Breakthrough Initiative aims to send one-gram microchip probes
to Alpha Centauri within 20 years, using gram-scale lightsails propelled by
laser-based radiation pressure, reaching velocities nearing a fifth of light
speed. This mission requires lightsail materials that challenge the
fundamentals of nanotechnology, requiring innovations in optics, material
science and structural engineering. Unlike the microchip payload, which must be
minimized in every dimension, such lightsails need meter-scale dimensions with
nanoscale thickness and billions of nanoscale holes to enhance reflectivity and
reduce mass. Our study employs neural topology optimization, revealing a novel
pentagonal lattice-based photonic crystal (PhC) reflector. The optimized
designs shorten acceleration times, therefore lowering launch costs
significantly. Crucially, these designs also enable lightsail material
fabrication with orders-of-magnitude reduction in costs. We have fabricated a
60 x 60 mm$^2$, 200nm thick, single-layer reflector perforated with over a
billion nanoscale features; the highest aspect-ratio nanophotonic element to
date. We achieve this with nearly 9,000 times cost reduction per m$^2$.
Starshot lightsails will have several stringent requirements but will
ultimately be driven by costs to build at scale. Here we highlight challenges
and possible solutions in developing lightsail materials - showcasing the
potential of scaling nanophotonics for cost-effective next-generation space
exploration.",309,2407.07896v1,physics.optics,"physics.optics,cond-mat.mes-hall,cs.LG,physics.app-ph,physics.space-ph",nanotechnology,2024-07-10,2024-12-23T21:06:44.710969
Electronic Correlations in Multielectron Silicon Quantum Dots,"Silicon quantum computing has the potential to revolutionize technology with
capabilities to solve real-life problems that are computationally complex or
even intractable for modern computers [1] by offering sufficient high quality
qubits to perform complex error-corrected calculations. Silicon
metal-oxide-semiconductor based quantum dots present a promising pathway for
realizing practical quantum computers. To improve certain qubit properties, it
is a common strategy to incorporate multiple electrons in the same dot in order
to form qubits in higher confined orbital states. Theoretical modelling is an
essential part of understanding the quantum behaviour of these electrons,
providing a basis for validating the physical working of device models as well
as providing insights into experimental data.
  Hartree-Fock theory is an imperative tool for the electronic structure
modelling of multi-electron quantum dots due to its ability to simulate a large
number of electrons with manageable computation load. However, an efficient
calculation of the self-consistent field becomes hard because dot formations in
silicon are characterized by strong electron-electron interactions and
conduction band valleys, besides the relatively high comparative effective
mass, which add to create a behaviour dominated by repulsion between electrons
rather than a well established shell structure. In this paper, we present a
Hartree-Fock-based method that accounts for these complexities for the
modelling of silicon quantum dots. With this method, we first establish the
significance of including electron-electron interactions and valley degree of
freedom and their implications. We then explore a simple case of anisotropic
dots and observe the impact of anisotropy on dot formations.",315,2407.04289v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",nanotechnology,2024-07-05,2024-12-23T21:06:44.711965
Strengthening of the superconductivity by real space decimation of the flat band states,"In contrast to standard BCS superconductivity, that in flat bands (FBs)
possesses an interesting degree of freedom that enables the control of the
superfluid weight (SFW), referred to as the quantum metric (QM). In the present
work, we consider the stub lattice and study the impact of the dilution of FB
eigenstates on superconductivity. Among the most remarkable results, it is
revealed that the SFW can be boosted by the decimation of the FB eigenstates.
In addition, it is shown that the widely used uniform pairing hypothesis
systematically predicts the suppression of the SFW, appears misleading and
qualitatively incorrect. With the great progress in nanotechnologies, we
believe that our findings could be realised and tested experimentally in
covalent organic frameworks or in decorated structures in which
defects/vacancies/ad-atoms are created/deposited in a controlled manner and
even in multilayered structures with intercalated atoms.",213,2406.18153v1,cond-mat.supr-con,cond-mat.supr-con,nanotechnology,2024-06-26,2024-12-23T21:06:44.712962
Near-field Strong Coupling and Entanglement of Quantum Emitters for Room-temperature Quantum Technologies,"In recent years, quantum nanophotonics has forged a rich nexus of
nanotechnology with photonic quantum information processing, offering
remarkable prospects for advancing quantum technologies beyond their current
technical limits in terms of physical compactness, energy efficiency, operation
speed, temperature robustness and scalability. In this perspective, we
highlight a number of recent studies that reveal the especially compelling
potential of nanoplasmonic cavity quantum electrodynamics for driving quantum
technologies down to nanoscale spatial and ultrafast temporal regimes, whilst
elevating them to ambient temperatures. Our perspective encompasses innovative
proposals for quantum plasmonic biosensing, driving ultrafast single-photon
emission and achieving near-field multipartite entanglement in the strong
coupling regime, with a notable emphasis on the use of industry-grade devices.
We conclude with an outlook emphasizing how the bespoke characteristics and
functionalities of plasmonic devices are shaping contemporary research
directives in ultrafast and room-temperature quantum nanotechnologies.",205,2406.15171v2,physics.optics,"physics.optics,quant-ph",nanotechnology,2024-06-21,2024-12-23T21:06:44.712962
Highly Sensitive Label-free Biomolecular Detection Using Au-WS2 Nanohybrid Based SERS Substrates,"Recent advancements in nanotechnology have led to the development of
surface-enhanced Raman spectroscopy (SERS) based rapid and low-cost
technologies for ultra-sensitive label-free detection and identification of
molecular analytes. Herein, we utilized the synergistic plasmonic and chemical
enhancement effects of Au-WS2 nanohybrids to attain the high-intensity Raman
signals of targeted analytes. To develop these nanohybrids, a series of
monodispersed Au nanoparticles (NPs) of varying diameters from 20 to 80 nm was
chemically synthesized and successively blended with liquid-phase exfoliated
WS2 nano-flakes of average lateral size 90 nm. They provided a maximum
enhancement factor (EF) of ~1.80 109 corresponding to the characteristic peaks
at 1364 cm-1 and 1512 cm-1 for R6G analyte molecules. Theoretical studies based
on the finite-difference time-domain simulations on Au-WS2 nanohybrid systems
revealed a huge field-intensity enhancement with an EF of more than 1000 at the
plasmonic hotspots, which was induced by the strong coupling of individual
plasmon oscillations of the adjacent Au NPs upon light interactions. These
electromagnetic effects along with the chemical enhancement effects of WS2
nanoflakes were found to be mainly responsible for such huge enhancement in
Raman signals. Furthermore, these hybrids were successfully employed for
achieving highly sensitive detection of the E. coli ATCC 35218 bacterial strain
with a concentration of 104 CFU/mL in phosphate-buffered saline media,
indicating their real capabilities for practical scenarios. The findings of the
present study will indeed provide vital information in the development of
innovative nanomaterial-based biosensors, that will offer new possibilities for
addressing critical public health concerns.",384,2406.13591v1,physics.app-ph,"physics.app-ph,cond-mat.mtrl-sci",nanotechnology,2024-06-19,2024-12-23T21:06:44.713960
Photoinduced Patterning of Oxygen Vacancies to Promote the Ferroelectric Phase of $\mathrm{Hf_{0.5}Zr_{0.5}O_2}$,"Photoinduced reductions in the oxygen vacancy concentration were leveraged to
increase the ferroelectric phase fraction of $\mathrm{Hf_{0.5}Zr_{0.5}O_2}$
(HZO) thin-films. Modest ($\sim 0.02-0.77~\mathrm{mJ/\mu m^2}$) laser doses of
visible light (488 nm, 2.54 eV) spatially patterned the concentration of oxygen
vacancies as monitored by photoluminescence imaging. Local, tip-based,
near-field, nanoFTIR measurements showed that the photoinduced oxygen vacancy
concentration reduction promoted formation of the ferroelectric phase (space
group $Pca2_1$) resulting in an increase in the piezoelectric response measured
by piezoresponse force microscopy. Photoinduced vacancy tailoring provides,
therefore, a spatially prescriptive, post-synthesis, and low-entry method to
modify phase in \hfo-based materials.",225,2406.11763v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",nanotechnology,2024-06-17,2024-12-23T21:06:44.714957
A hybrid graphene-siliconnitride nanomembrane as a versatile and ultra-widely tunable mechanical device,"Integration of 2D materials in nanoelectromechanical systems (NEMS) marries
the robustness of silicon-based materials with exceptional electrical
controllability in 2D materials, drastically enhancing system performance which
now is the key for many advanced applications in nanotechnology. Here, we
experimentally demonstrate and theoretically analyze a powerful on-chip
graphene integrated NEMS device consisting of a hybrid graphene/silicon-nitride
membrane with metallic leads that enables an extremely large static and dynamic
parameter regulation. When a static voltage is applied to the leads, the force
induced by the thermal expansion difference between the leads and the membrane
results in ultra-wide frequency tuning, deformation (post-buckling transition)
and regulation of mechanical properties. Moreover, by injecting an alternating
voltage to the leads, we can excite the resonator vibrating even far beyond its
linear regime without a complex and space consuming actuation system. Our
results prove that the device is a compact integrated system possessing
mechanical robustness, high controllability, and fast response. It not only
expands the limit of the application range of NEMS devices but also pushes
multidimensional nanomechanical resonators into working in the nonlinear
regime.",247,2406.11596v2,cond-mat.mes-hall,"cond-mat.mes-hall,physics.app-ph",nanotechnology,2024-06-17,2024-12-23T21:06:44.714957
Long-Range Quantum Tunneling via Matter Wave,"Quantum tunneling refers to a phenomenon that a microscopic object can pass
through a potential barrier even it does not have enough energy to overcome the
barrier. It has led to many modern applications and nanotechnologies. A general
belief is that quantum tunneling, as a manifestation of the wave-particle
duality, occurs only when the width of the barrier is comparable to or smaller
than the de Broglie's wavelength of the object. Here, via studying the
tunneling of an ultracold atom among $N$ far-separated trapping potentials in a
state-selective optical lattice, we discover a mechanism to realize a
long-range quantum tunneling. It is found that, by the mediation role of the
propagating matter wave emitted from the excited-state atom, a coherent
tunneling of the tightly confined atom to the remote trapping potentials can
occur as long as bound states are present in the energy spectrum of the total
system formed by the atom and its matter wave. Breaking through the generally
believed distance constraint of quantum tunneling, our result opens another
avenue to realize quantum tunneling and gives a guideline to develop tunneling
devices.",233,2406.06162v1,quant-ph,"quant-ph,cond-mat.quant-gas",nanotechnology,2024-06-10,2024-12-23T21:06:44.715955
A comprehensive approach to incorporating intermolecular dispersion into the openCOSMO-RS model. Part 1: Halocarbons,"The COSMO-RS (Conductor-like Screening Model for Real Solvents) is a
predictive thermodynamic model that has found diverse applications in various
domains like chemical engineering, environmental chemistry, nanotechnology,
material science, and biotechnology. Its core concept involves calculating the
screening charge density on the surface of each molecule and letting these
surface patches interact with each other to calculate thermodynamic properties.
In this study, we aim to enhance the performance of the open-source
implementation openCOSMO-RS by incorporating dispersive interactions between
the paired segments. Several parametrizations were systematically evaluated
through the extensive regression analysis using a comprehensive database of
Vapor-Liquid Equilibrium (VLE), Liquid-Liquid Equilibrium (LLE) and Infinite
Dilution Activity Coefficients (IDACs). Furthermore, the influence of different
combinatorial terms on the model performance was investigated. Our findings
indicate that incorporating dispersive interactions significantly improves the
accuracy of phase equilibrium predictions for halocarbons and refrigerant
mixtures.",210,2406.05244v1,cond-mat.soft,"cond-mat.soft,physics.chem-ph,physics.comp-ph",nanotechnology,2024-06-07,2024-12-23T21:06:44.716952
Role of the ratio of tangential to normal stiffness coefficient on the behaviour of vibrofluidised particles,"The selection of parameters in the contact law for inter-particle
interactions affects the results of simulations of flowing granular materials.
The present study aims to understand the effect of the ratio of tangential to
normal spring stiffness coefficient ($\kappa$) on inter-particle contact
behaviour in terms of the rotational coefficient of restitution determined
using data obtained from multi-particle simulations. The effect of $\kappa$ on
the profiles of the micro- and macroscopic properties of particles in a
vibrofluidised bed is also investigated. The Discrete Element Method (DEM) is
used to simulate a vertically vibrated fluidised bed using the open-source
software LAMMPS. The inter-particle and wall-particle contact forces are
determined using the linear spring-dashpot (LSD) model. The distribution of the
mean co-ordination number, force during the contact, contact regimes, and
rotational coefficient of restitution are determined from the data obtained
from simulations. It was shown that $\kappa$ plays a significant role in the
distribution of inter-particle contacts between different regimes and, thereby,
the velocity distribution and profiles of statistically averaged properties of
the vibrofluidised particles. Our results show that for particles with surface
friction coefficient $\mu>0.1$, the commonly used value $\kappa=\frac{2}{7}$
results in quantitatively different results from those obtained using $0.67 \le
\kappa < 1$, a range consistent with the realistic values of Poisson ratios for
simple materials.",315,2412.16133v1,cond-mat.soft,cond-mat.soft,materials science,2024-12-20,2024-12-23T21:06:46.310967
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",materials science,2024-12-20,2024-12-23T21:06:46.311964
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials science,2024-12-20,2024-12-23T21:06:46.312961
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",materials science,2024-12-20,2024-12-23T21:06:46.313959
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",materials science,2024-12-20,2024-12-23T21:06:46.313959
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",materials science,2024-12-20,2024-12-23T21:06:46.314956
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",materials science,2024-12-20,2024-12-23T21:06:46.315953
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials science,2024-12-20,2024-12-23T21:06:46.316950
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",materials science,2024-12-20,2024-12-23T21:06:46.316950
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",materials science,2024-12-20,2024-12-23T21:06:46.317948
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",materials science,2024-12-20,2024-12-23T21:06:46.318945
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",materials science,2024-12-20,2024-12-23T21:06:46.319943
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,materials science,2024-12-20,2024-12-23T21:06:46.320940
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",materials science,2024-12-20,2024-12-23T21:06:46.320940
Integral representation for a relaxed optimal design problem for non-simple grade two materials,"A measure representation result for a functional modelling optimal design
problems for plastic deformations, under linear growth conditions, is obtained.
  Departing from an energy with a bulk term depending on the second gradient,
as well as a perimeter term, the functional in question corresponds to the
relaxation of this energy with respect to a pair $(\chi,u)$, where $\chi$ is
the characteristic function of a set of finite perimeter and $u$ is a function
of bounded hessian.",99,2412.16027v1,math.AP,"math.AP,math.OC,49J45, 49Q20, 26B25",materials science,2024-12-20,2024-12-23T21:06:46.321937
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,materials science,2024-12-20,2024-12-23T21:06:46.321937
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",materials science,2024-12-20,2024-12-23T21:06:46.322934
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,materials science,2024-12-20,2024-12-23T21:06:46.323932
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",materials science,2024-12-20,2024-12-23T21:06:46.323932
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",materials science,2024-12-20,2024-12-23T21:06:46.324929
Identifying and quantifying Su-Schrieffer-Heeger-like interactions with RIXS,"Su-Schrieffer-Heeger (SSH)-like electron-phonon (e-ph) interactions can drive
the formation of light (bi)polarons and several novel states of matter. It is,
therefore, prudent to develop experimental protocols for identifying such
couplings in real materials and quantifying their strength. Here, we
investigate how resonant inelastic x-ray scattering (RIXS) probes e-ph
interactions in the one-dimensional half-filled Hubbard-SSH model with onsite
phonons. Using the density matrix renormalization group method, we compute the
full RIXS response and find that the lattice excitations generated during the
scattering process inevitably couple to the system's charge and magnetic
sectors, resulting in combined multi-particle excitations that cannot be easily
disentangled from one another. While this aspect complicates the interpretation
of RIXS experiments, we outline how it can be leveraged to identify and
quantify SSH-like interactions in quantum materials.",220,2412.15981v1,cond-mat.str-el,cond-mat.str-el,materials science,2024-12-20,2024-12-23T21:06:46.325928
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials science,2024-12-20,2024-12-23T21:06:46.325928
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",materials science,2024-12-20,2024-12-23T21:06:46.326925
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",materials science,2024-12-20,2024-12-23T21:06:46.326925
First Constraint on the Diffuse Supernova Neutrino Background through the CE$ν$NS process from the LZ experiment,"We report the limits on the diffuse supernova neutrino background (DSNB) flux
and the fundamental DSNB parameters measured from the first science run of the
LUX-ZEPLIN (LZ) experiment, a dual-phase xenon detector located at the Sanford
Underground Research Facility in Lead, South Dakota, USA. This is the first
time the DSNB limit is measured through the process of the coherent elastic
neutrino-nucleus scattering (CE$\nu$NS). Using an exposure of 60~live days and
a fiducial mass of 5.5~t, the upper limit on the DSNB $\nu_x$ (each of
$\nu_\mu$, $\nu_\tau$, $\bar\nu_\mu$, $\bar\nu_\tau$) flux is
$686-826$~cm$^{-2}$s$^{-1}$ at the 90\% confidence level for neutrino energies
E$>$19.3~MeV, assuming the flux for each $\nu_x$ flavor is the same. The
interval accounts for the uncertainty in existing DSNB models. The present
result is comparable to the existing best limit and further improvements are
expected after collecting data from an estimated 1,000-day exposure in the
future.",281,2412.15886v1,hep-ex,hep-ex,materials science,2024-12-20,2024-12-23T21:06:46.327921
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials science,2024-12-20,2024-12-23T21:06:46.328918
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",materials science,2024-12-20,2024-12-23T21:06:46.328918
On the Power of Strategic Corpus Enrichment in Content Creation Games,"Search and recommendation ecosystems exhibit competition among content
creators. This competition has been tackled in a variety of game-theoretic
frameworks. Content creators generate documents with the aim of being
recommended by a content ranker for various information needs. In order for the
ecosystem, modeled as a content ranking game, to be effective and maximize user
welfare, it should guarantee stability, where stability is associated with the
existence of pure Nash equilibrium in the corresponding game. Moreover, if the
contents' ranking algorithm possesses a game in which any best-response
learning dynamics of the content creators converge to equilibrium of high
welfare, the system is considered highly attractive. However, as classical
content ranking algorithms, employed by search and recommendation systems, rank
documents by their distance to information needs, it has been shown that they
fail to provide such stability properties. As a result, novel content ranking
algorithms have been devised. In this work, we offer an alternative approach:
corpus enrichment with a small set of fixed dummy documents. It turns out that,
with the right design, such enrichment can lead to pure Nash equilibrium and
even to the convergence of any best-response dynamics to a high welfare result,
where we still employ the classical/current content ranking approach. We show
two such corpus enrichment techniques with tight bounds on the number of
documents needed to obtain the desired results. Interestingly, our study is a
novel extension of Borel's Colonel Blotto game.",287,2412.15878v1,cs.GT,cs.GT,materials science,2024-12-20,2024-12-23T21:06:46.329916
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",materials science,2024-12-20,2024-12-23T21:06:46.330913
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials science,2024-12-20,2024-12-23T21:06:46.331910
IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing,"In inverse rendering, accurately modeling visibility and indirect radiance
for incident light is essential for capturing secondary effects. Due to the
absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have
either adopted a simplified rendering equation or used learnable parameters to
approximate incident light, resulting in inaccurate material and lighting
estimations. To this end, we introduce inter-reflective Gaussian splatting
(IRGS) for inverse rendering. To capture inter-reflection, we apply the full
rendering equation without simplification and compute incident radiance on the
fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we
present an efficient optimization scheme to handle the computational demands of
Monte Carlo sampling for rendering equation evaluation. Furthermore, we
introduce a novel strategy for querying the indirect radiance of incident light
when relighting the optimized scenes. Extensive experiments on multiple
standard benchmarks validate the effectiveness of IRGS, demonstrating its
capability to accurately model complex inter-reflection effects.",207,2412.15867v1,cs.CV,cs.CV,materials science,2024-12-20,2024-12-23T21:06:46.331910
Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science,"Understanding how transportation networks work is important for improving
connectivity, efficiency, and safety. In Brazil, where road transport is a
significant portion of freight and passenger movement, network science can
provide valuable insights into the structural properties of the infrastructure,
thus helping decision makers responsible for proposing improvements to the
system. This paper models the federal road network as weighted networks, with
the intent to unveil its topological characteristics and identify key locations
(cities) that play important roles for the country through 75,000 kilometres of
roads. We start with a simple network to examine basic connectivity and
topology, where weights are the distance of the road segment. We then
incorporate other weights representing number of incidents, population, and
number of cities in-between each segment. We then focus on community detection
as a way to identify clusters of cities that form cohesive groups within a
network. Our findings aim to bring clarity to the overall structure of federal
roads in Brazil, thus providing actionable insights for improving
infrastructure planning and prioritising resources to enhance network
resilience.",208,2412.15865v1,physics.soc-ph,"physics.soc-ph,cs.CY",materials science,2024-12-20,2024-12-23T21:06:46.332908
Revealing spin-flip two-level systems using ultra-thin film superconducting resonators,"Material disorders are one of the major sources of noise and loss in
solid-state quantum devices, whose behaviors are often modeled as two-level
systems (TLSs) formed by charge tunneling between neighboring sites. However,
the role of their spins in tunneling and its impact on device performance
remain highly unexplored. In this work, employing ultra-thin TiN
superconducting resonators, we reveal anomalous TLS behaviors by demonstrating
an unexpected increase in resonant frequency at low magnetic fields.
Furthermore, a spin-flip TLS model is proposed, in which an effective
spin-orbit coupling is generated by inhomogeneous local magnetic fields from
defect spins. This mechanism mixes charge tunnelings and spin flips,
quantitatively reproducing the observed frequency-field relationship and its
temperature dependence. This work deepens the understanding of spin-dependent
TLS behaviors, offering the possibility of magnetically engineering noise and
loss in solid-state quantum devices.",201,2412.15856v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",materials science,2024-12-20,2024-12-23T21:06:46.332908
Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis,"Complying with traffic rules is challenging for automated vehicles, as
numerous rules need to be considered simultaneously. If a planned trajectory
violates traffic rules, it is common to replan a new trajectory from scratch.
We instead propose a trajectory repair technique to save computation time. By
coupling satisfiability modulo theories with set-based reachability analysis,
we determine if and in what manner the initial trajectory can be repaired.
Experiments in high-fidelity simulators and in the real world demonstrate the
benefits of our proposed approach in various scenarios. Even in complex
environments with intricate rules, we efficiently and reliably repair
rule-violating trajectories, enabling automated vehicles to swiftly resume
legally safe operation in real-time.",146,2412.15837v1,cs.RO,"cs.RO,cs.AI",materials science,2024-12-20,2024-12-23T21:06:46.333905
Enriching Social Science Research via Survey Item Linking,"Questions within surveys, called survey items, are used in the social
sciences to study latent concepts, such as the factors influencing life
satisfaction. Instead of using explicit citations, researchers paraphrase the
content of the survey items they use in-text. However, this makes it
challenging to find survey items of interest when comparing related work.
Automatically parsing and linking these implicit mentions to survey items in a
knowledge base can provide more fine-grained references. We model this task,
called Survey Item Linking (SIL), in two stages: mention detection and entity
disambiguation. Due to an imprecise definition of the task, existing datasets
used for evaluating the performance for SIL are too small and of low-quality.
We argue that latent concepts and survey item mentions should be
differentiated. To this end, we create a high-quality and richly annotated
dataset consisting of 20,454 English and German sentences. By benchmarking deep
learning systems for each of the two stages independently and sequentially, we
demonstrate that the task is feasible, but observe that errors propagate from
the first stage, leading to a lower overall task performance. Moreover,
mentions that require the context of multiple sentences are more challenging to
identify for models in the first stage. Modeling the entire context of a
document and combining the two stages into an end-to-end system could mitigate
these problems in future work, and errors could additionally be reduced by
collecting more diverse data and by improving the quality of the knowledge
base. The data and code are available at https://github.com/e-tornike/SIL .",338,2412.15831v1,cs.DL,"cs.DL,cs.CL",materials science,2024-12-20,2024-12-23T21:06:46.334903
SUBMASSIVE: Resolving Subclass Cycles in Very Large Knowledge Graphs,"Large knowledge graphs capture information of a large number of entities and
their relations. Among the many relations they capture, class subsumption
assertions are usually present and expressed using the \texttt{rdfs:subClassOf}
construct. From our examination, publicly available knowledge graphs contain
many potentially erroneous cyclic subclass relations, a problem that can be
exacerbated when different knowledge graphs are integrated as Linked Open Data.
In this paper, we present an automatic approach for resolving such cycles at
scale using automated reasoning by encoding the problem of cycle-resolving to a
MAXSAT solver. The approach is tested on the LOD-a-lot dataset, and compared
against a semi-automatic version of our algorithm. We show how the number of
removed triples is a trade-off against the efficiency of the algorithm.",170,2412.15829v1,cs.LO,"cs.LO,cs.SC,math.OC,68T27, 68T20, 68T09,F.3.0; I.2.1; I.2.4",materials science,2024-12-20,2024-12-23T21:06:46.334903
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",materials science,2024-12-20,2024-12-23T21:06:46.335900
Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis,"Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.",134,2412.15814v1,cs.CR,"cs.CR,cs.DC,cs.LO",materials science,2024-12-20,2024-12-23T21:06:46.336897
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",materials science,2024-12-20,2024-12-23T21:06:46.337894
Elementary theory of Magnetoferrons: bringing magnons and ferrons together in multiferroic systems,"The collective excitations of a multiferroic material are analyzed. We show
that these excitations also exhibit magnetoelectric behavior, leading to the
hybridization of magnons ,oscillations of the magnetization field, and ferrons,
which are oscillations of the electric dipolar density field. We term these
emergent entities 'magnetoferrons', study their main properties, and discuss
their potential applications. Additionally, we provide a phenomenological
framework for these systems, which will be invaluable for describing the
dynamics of the multiferromagnetic state.",129,2412.15796v1,cond-mat.mes-hall,cond-mat.mes-hall,materials science,2024-12-20,2024-12-23T21:06:46.337894
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",materials science,2024-12-20,2024-12-23T21:06:46.338892
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",materials science,2024-12-20,2024-12-23T21:06:46.339889
Coherent Interactions of Free Electrons and Matter: Toward Tunable Compact X-ray Sources,"Compact laboratory-scale X-ray sources still rely on the same fundamental
principles as in the first X-ray tubes developed more than a century ago. In
recent years, significant research and development have focused on large-scale
X-ray sources such as synchrotrons and free-electron lasers, leading to the
generation of high-brightness coherent X-rays. However, the large size and high
costs of such sources prevent their widespread use. The quest for a compact and
coherent Xray source has long been a critical objective in modern physics,
gaining further importance in recent years for industrial applications and
fundamental scientific research. Here, we review the physical mechanisms
governing compact coherent X-ray generation. Of current interest are coherent
periodic interactions of free electrons in crystalline materials, creating hard
X-rays via a mechanism known as parametric X-ray radiation (PXR). Over the past
decade, X-ray sources leveraging this mechanism have demonstrated
state-of-the-art tunability, directionality, and broad spatial coherence,
enabling X-ray phase-contrast imaging on a compact scale. The coming years are
expected to show substantial miniaturization of compact X-ray sources,
facilitated by progress in electron beam technologies. This review compares the
most promising mechanisms used for hard-X-ray generation, contrasting
parametric X-ray radiation with inverse Compton scattering and characteristic
radiation from a liquid-jet anode. We cover the most recent advancements,
including the development of new materials, innovative geometrical designs, and
specialized optimization techniques, aiming toward X-ray flux levels suitable
for medical imaging and X-ray spectroscopy in compact scales.",334,2412.15775v1,physics.app-ph,"physics.app-ph,physics.optics",materials science,2024-12-20,2024-12-23T21:06:46.339889
Stabilization of active tissue deformation by a dynamic morphogen gradient,"A key process during animal morphogenesis is oriented tissue deformation,
which is often driven by internally generated active stresses. Yet, such active
oriented materials are prone to well-known instabilities, raising the question
of how oriented tissue deformation can be robust during morphogenesis. In a
simple scenario, we recently showed that active oriented deformation can be
stabilized by the boundary-imposed gradient of a scalar field, which
represents, e.g., a morphogen gradient in a developing embryo. Here, we discuss
a more realistic scenario, where the morphogen is produced by a localized
source region, diffuses across the tissue, and degrades. Consistent with our
earlier results, we find that oriented tissue deformation is stable in the
gradient-extensile case, i.e. when active stresses act to extend the tissue
along the direction of the gradient, but it is unstable in the
gradient-contractile case. In addition, we now show that gradient-contractile
tissues can not be stabilized even by morphogen diffusion. Finally, we point
out the existence of an additional instability, which results from the
interplay of tissue shear and morphogen diffusion. Our theoretical results
explain the lack of gradient-contractile tissues in the biological literature,
suggesting that the active matter instability acts as an evolutionary selection
criterion.",271,2412.15774v1,cond-mat.soft,"cond-mat.soft,q-bio.TO",materials science,2024-12-20,2024-12-23T21:06:46.340886
Network-forming phase separation of oppositely charged polyelectrolytes forming coacervates in a solvent,"The formation of coacervates through phase separation of oppositely charged
polyelectrolytes (PEs) is critical for understanding biological condensates and
developing responsive materials. Traditionally, coacervates are viewed as
spherical droplets with growth dynamics resembling liquid-liquid phase
separation. However, our fluid particle dynamics simulations incorporating
hydrodynamic and electrostatic interactions challenge this perspective. Here,
we find that oppositely charged PEs form a percolated network even in
semi-dilute solutions, coarsening with a unique growth law, $\ell \propto
t^{1/2}$. This self-similarity, absent for neutral polymers in poor solvents,
arises because PEs in good solvents exhibit weaker, longer-range attractions
due to spatial charge inhomogeneity under global charge neutrality. This
results in a lower density of the PEs-rich phase and reduced interfacial
tension. Increased charge asymmetry further slows network coarsening.
Additionally, coacervate droplets initially display irregular shapes due to
weak interfacial tension, transitioning slowly to spherical forms. Our research
provides new insights into coacervate morphology and coarsening dynamics.",243,2412.15753v1,cond-mat.soft,"cond-mat.soft,physics.bio-ph,physics.chem-ph",materials science,2024-12-20,2024-12-23T21:06:46.341884
Photon proliferation from $N$-body dark matter annihilation,"We demonstrate a photon proliferation effect from $N$-body dark matter (DM)
annihilation in the early Universe, which can induce a drastic
photon-temperature shift after neutrino decoupling. For pseudoscalar DM mass
below the eV scale, we show that the photon proliferation effect becomes
significant as the mass approaches the ultralight end, due to the huge
enhancement from the background DM number density. This presents the leading
constraints on the DM-photon coupling, DM self-interaction, and DM-electron
coupling, which are stronger than the existing bounds up to several orders of
magnitude. The present research can be extended to other interactions and DM
candidates, and highlights the importance of multi-body processes in the early
Universe.",156,2412.15749v1,hep-ph,"hep-ph,astro-ph.CO",materials science,2024-12-20,2024-12-23T21:06:46.341884
Building Bridges: AI Custom Chatbots as Mediators between Mathematics and Physics,"This work explores the integration of AI custom chatbots in educational
settings, with a particular focus on their applicability in the context of
mathematics and physics. In view of the increasing deployment of AI tools such
as ChatGPT in educational contexts, the present study examines their potential
as personalized tutoring systems. The study assesses the impact of AI-generated
learning materials on the learning experiences and performance of sixth-grade
students, with a particular focus on proportional relationships in mathematical
and physical contexts. The randomized controlled study with N = 214 students
compared traditional textbook materials with explanations generated by a custom
chatbot. The results demonstrated that while AI-generated materials had an
indefinite impact on learning outcomes, they significantly enhanced
positive-activating emotions, situational interest, and self-efficacy, while
reducing intrinsic and extrinsic cognitive load. These findings underscore the
potential of AI to transform educational practices by fostering a superior
learning experience. However, further research is required to clarify its
impact on learning performance and long-term learning outcomes. The study
highlights the importance of careful integration and customization of AI tools
to maximize their benefits in physics education.",233,2412.15747v1,physics.ed-ph,physics.ed-ph,materials science,2024-12-20,2024-12-23T21:06:46.342881
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,materials science,2024-12-20,2024-12-23T21:06:46.343878
Distribution-Free Normal Modal Logics,"This article initiates the semantic study of distribution-free normal modal
logic systems, laying the semantic foundations and anticipating further
research in the area. The article explores roughly the same area, though taking
a different approach, with a recent article by Bezhanishvili, de Groot,
Dmitrieva and Morachini, who studied a distribution-free version of Dunn's
Positive Modal Logic (PML). Unlike PML, we consider logics that may drop
distribution and which are equipped with both an implication connective and
modal operators. We adopt a uniform relational semantics approach, relying on
recent results on representation and duality for normal lattice expansions. We
prove canonicity and completeness in the relational semantics of the minimal
distribution-free normal modal logic, assuming just the K-axiom, as well as of
its axiomatic extensions obtained by adding any of the D, T, B, S4 or S5
axioms. Adding distribution can be easily accommodated and, as a side result,
we also obtain a new semantic treatment of Intuitionistic Modal Logic.",224,2412.15736v1,cs.LO,"cs.LO,math.LO",materials science,2024-12-20,2024-12-23T21:06:46.343878
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials science,2024-12-20,2024-12-23T21:06:46.344876
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",semiconductor physics,2024-12-20,2024-12-23T21:06:48.187260
Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?,"The animation of realistic virtual avatars in crowd scenarios is an important
element of immersive virtual environments. However, achieving this realism
requires attention to multiple factors, such as their visual appearance and
motion cues. We investigated how body shape diversity influences the perception
of motion clones in virtual crowds. A physics-based model was used to simulate
virtual avatars in a small-scale crowd of size twelve. Participants viewed
side-by-side video clips of these virtual crowds: one featuring all unique
motions (Baseline) and the other containing motion clones (i.e., the same
motion used to animate two or more avatars in the crowd). We also varied the
levels of body shape and motion diversity. Our findings revealed that body
shape diversity did not influence participants' ratings of motion clone
detection, and motion variety had a greater impact on their perception of the
crowd. Further research is needed to investigate how other visual factors
interact with motion in order to enhance the perception of virtual crowd
realism.",200,2412.16151v1,cs.HC,"cs.HC,cs.GR",semiconductor physics,2024-12-20,2024-12-23T21:06:48.188258
The Classical Super-Phaserotation Infrared Triangle,"The universality of the logarithmic soft photon theorem in four dimensions
can be traced to an infinite-dimensional asymptotic symmetry which acts as a
local phase rotation on matter as we have shown in 2403.13053. Here we extend
our earlier results for the charges associated to these superphaserotations to
all orders in the coupling and prove that their conservation is exactly the
classical logarithmic soft photon theorem discovered by Saha, Sahoo and Sen in
1912.06413. We furthermore generalize the formulae for the associated
electromagnetic displacement memory and its tail from particles to scalar
matter fields. This completes the classical superphaserotation infrared
triangle.",142,2412.16149v1,hep-th,hep-th,semiconductor physics,2024-12-20,2024-12-23T21:06:48.188258
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",semiconductor physics,2024-12-20,2024-12-23T21:06:48.189255
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",semiconductor physics,2024-12-20,2024-12-23T21:06:48.190252
Borel singularities and Stokes constants of the topological string free energy on one-parameter Calabi-Yau threefolds,"We study the Borel plane of the topological string free energy on all
hypergeometric one-parameter Calabi-Yau models close to singular points in
moduli space, focusing on the location of Borel singularities and the value of
the associated Stokes constants. We find in particular that in models which
exhibit massless D-branes at a singular point, the central charge of the
D-brane close to the singular point coincides with the location of the leading
Borel singularity, and the generalized Donaldson-Thomas invariant associated to
the charge of the D-brane, in as far as its value is known, coincides with the
Stokes constant associated to the Borel singularity.",144,2412.16140v1,hep-th,"hep-th,math.AG",semiconductor physics,2024-12-20,2024-12-23T21:06:48.190252
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",semiconductor physics,2024-12-20,2024-12-23T21:06:48.191250
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.192248
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",semiconductor physics,2024-12-20,2024-12-23T21:06:48.193244
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",semiconductor physics,2024-12-20,2024-12-23T21:06:48.193244
Revisiting Global Income Convergence in the 21st Century,"This paper revisits the debate on income convergence between poor and rich
countries. I challenge the view that there is little to no catch-up, and that
changes in total factor productivity (TFP) drives cross-country income
differences. Since 2000, income levels in poor countries have converged with
rich countries at 0.8% annually, rising to 1.5% when excluding Sub-Saharan
Africa. A growth accounting exercise incorporating capital income share
heterogeneity shows that most convergence since 1980, and over half since 2000
outside Sub-Saharan Africa, results from convergence in physical and human
capital inputs rather than TFP.",129,2412.16127v1,econ.GN,"econ.GN,q-fin.EC",semiconductor physics,2024-12-20,2024-12-23T21:06:48.194244
Observational Properties of Harmonic EMIC waves: Statistical Study,"Electromagnetic ion cyclotron (EMIC) waves are discrete electromagnetic
emissions separated by multiple ion gyrofrequencies. Harmonic EMIC waves are
defined as waves with a strong electric or magnetic field (or both) at the
harmonics of the fundamental EMIC mode. In this paper, for the first time, we
present a statistical study on harmonic EMIC waves by the Van Allen Probes. The
EMIC waves are categorized into three types based on their harmonics: (1)
fundamental mode only (without higher harmonics), (2) electrostatic (ES)
harmonics, and (3) electromagnetic (EM) harmonics. Our statistical study shows
that ES and EM harmonic EMIC waves predominantly occur on the dayside, outside
the plasmasphere with $L >5$ and are associated with a low $f_{pe}/f_{ce}$, a
high proton $\beta_H$, and a strong fundamental EMIC mode. The results will
advance our understanding of harmonic EMIC waves and their generation
mechanisms.",217,2412.16124v1,physics.space-ph,physics.space-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.194244
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.195241
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",semiconductor physics,2024-12-20,2024-12-23T21:06:48.196238
Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli,"As humans perceive and actively engage with the world, we adjust our
decisions in response to shifting group dynamics and are influenced by social
interactions. This study aims to identify which aspects of interaction affect
cooperation-defection choices. Specifically, we investigate human cooperation
within the Prisoner's Dilemma game, using the Drift-Diffusion Model to describe
the decision-making process. We introduce a novel Bayesian model for the
evolution of the model's parameters based on the nature of interactions
experienced with other players. This approach enables us to predict the
evolution of the population's expected cooperation rate. We successfully
validate our model using an unseen test dataset and apply it to explore three
strategic scenarios: co-player manipulation, use of rewards and punishments,
and time pressure. These results support the potential of our model as a
foundational tool for developing and testing strategies aimed at enhancing
cooperation, ultimately contributing to societal welfare.",182,2412.16121v1,physics.soc-ph,physics.soc-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.196238
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",semiconductor physics,2024-12-20,2024-12-23T21:06:48.197236
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",semiconductor physics,2024-12-20,2024-12-23T21:06:48.198232
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",semiconductor physics,2024-12-20,2024-12-23T21:06:48.198232
Full S-matrices and Witten diagrams with (relative) L-infinity algebras,"The $L_\infty$-algebra approach to scattering amplitudes elegantly describes
the nontrivial part of the $S$-matrix but fails to take into account the
trivial part. We argue that the trivial contribution to the $S$-matrix should
be accounted for by another, complementary $L_\infty$-algebra, such that a
perturbative field theory is described by a cyclic relative $L_\infty$-algebra.
We further demonstrate that this construction reproduces Witten diagrams that
arise in AdS/CFT including, in particular, the trivial Witten diagrams
corresponding to CFT two-point functions. We also discuss Chern-Simons theory
and Yang-Mills theory on manifolds with boundaries using this approach.",161,2412.16106v1,hep-th,"hep-th,math-ph,math.MP,81T18 (Primary) 81T13, 81T35, 17B56, 17B81 (Secondary)",semiconductor physics,2024-12-20,2024-12-23T21:06:48.199230
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",semiconductor physics,2024-12-20,2024-12-23T21:06:48.199230
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",semiconductor physics,2024-12-20,2024-12-23T21:06:48.199230
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.200227
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",semiconductor physics,2024-12-20,2024-12-23T21:06:48.201225
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.201225
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",semiconductor physics,2024-12-20,2024-12-23T21:06:48.202222
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.203219
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.203219
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.204217
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",semiconductor physics,2024-12-20,2024-12-23T21:06:48.205214
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.205214
Eigenvalue Bounds for Multi-Particle Reduced Density Matrices of Coulombic Wavefunctions,"For bound states of atoms and molecules of $N$ electrons we consider the
corresponding $K$-particle reduced density matrices, $\Gamma^{(K)}$, for $1 \le
K \le N-1$. Previously, eigenvalue bounds were obtained in the case of $K=1$
and $K=N-1$ by A.V. Sobolev. The purpose of the current work is to obtain
bounds in the case of $2 \le K \le N-2$. For such $K$ we label the eigenvalues
of the positive, trace class operators $\Gamma^{(K)}$ by
$\lambda_n(\Gamma^{(K)})$ for $n=1,2,\dots$, and obtain the bounds
$\lambda_n(\Gamma^{(K)}) \le Cn^{-\alpha_K}$ for all $n$, where $\alpha_K = 1 +
7/(3L)$ and $L = \min\{K,N-K\}$.",240,2412.16073v1,math-ph,"math-ph,math.MP,35J10",semiconductor physics,2024-12-20,2024-12-23T21:06:48.206211
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.207208
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",semiconductor physics,2024-12-20,2024-12-23T21:06:48.207208
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",semiconductor physics,2024-12-20,2024-12-23T21:06:48.207208
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",semiconductor physics,2024-12-20,2024-12-23T21:06:48.208713
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",semiconductor physics,2024-12-20,2024-12-23T21:06:48.209712
Approximation of Schrödinger operators with point interactions on bounded domains,"We consider Schr\""odinger operators on a bounded domain $\Omega\subset
\mathbb{R}^3$, with homogeneous Robin or Dirichlet boundary conditions on
$\partial\Omega$ and a point (zero-range) interaction placed at an interior
point of $\Omega$. We show that, under suitable spectral assumptions, and by
means of an extension-restriction procedure which exploit the already known
result on the entire space, the singular interaction is approximated by
rescaled sequences of regular potentials. The result is missing in the
literature, and we also take the opportunity to point out some general issues
in the approximation of point interactions and the role of zero energy
resonances.",146,2412.16056v1,math-ph,"math-ph,math.MP",semiconductor physics,2024-12-20,2024-12-23T21:06:48.209712
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.210709
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",semiconductor physics,2024-12-20,2024-12-23T21:06:48.210709
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,semiconductor physics,2024-12-20,2024-12-23T21:06:48.211706
Millikelvin Nb nanoSQUID-embedded tuneable resonator fabricated with a neon focused-ion-beam,"SQUID-embedded superconducting resonators are of great interest due to their
potential for coupling highly scalable superconducting circuits with quantum
memories based on solid-state spin ensembles. Such an application requires a
high-$Q$, frequency-tuneable resonator which is both resilient to magnetic
field, and able to operate at millikelvin temperatures. These requirements
motivate the use of a higher $H_{c}$ metal such as niobium, however the
challenge then becomes to sufficiently reduce the operating temperature. We
address this by presenting a monolithic Nb nanoSQUID-embedded resonator, where
neon focused-ion-beam fabrication of the nanoSQUID results in a device
displaying frequency tuneability at $T = 16$ mK. In order to assess the
applicability of the device for coupling to small spin clusters, we
characterise the flux sensitivity as a function of microwave drive power and
externally applied magnetic field, and find that the noise is dominated by
dielectric noise in the resonator. Finally, we discuss improvements to the
device design which can dramatically improve the flux sensitivity, which
highlights the promise of Nb SQUID-embedded resonators for hybrid
superconductor-spin applications.",261,2412.16045v1,quant-ph,"quant-ph,cond-mat.supr-con",semiconductor physics,2024-12-20,2024-12-23T21:06:48.212704
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.213701
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",semiconductor physics,2024-12-20,2024-12-23T21:06:48.213701
Integrability versus chaos in the steady state of many-body open quantum systems,"The Lindblad description of an open quantum system gives rise to two types of
integrability, since the nonequilibrium steady state can be integrable
independently of the Liouvillian. Taking boundary-driven and dephasing spin
chains as a representative example, we discriminate Liouvillian and
steady-state chaos by combining level spacing statistics and an extension of
the eigenstate thermalization hypothesis to open quantum systems. Moreover, we
analyze the structure of the steady states by expanding it in the basis of
Pauli strings and comparing the weight of strings of different lengths. We show
that the natural expectation that integrable steady states are ""simple"" (i.e.,
built from few-body local operators) does not hold: the steady states of both
chaotic and integrable models have relevant contributions coming from Pauli
strings of all possible lengths, including long-range and many-body
interactions. Nevertheless, we show that one can effectively use the
operator-size distribution to distinguish chaotic and integrable steady states.",217,2412.16041v1,cond-mat.stat-mech,"cond-mat.stat-mech,cond-mat.mes-hall,nlin.CD,quant-ph",semiconductor physics,2024-12-20,2024-12-23T21:06:48.214699
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",semiconductor physics,2024-12-20,2024-12-23T21:06:48.215696
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",semiconductor physics,2024-12-20,2024-12-23T21:06:48.215696
Non-stationary Aharonov-Bohm effect,"The non-stationary Aharonov-Bohm effect (scattering of electron in the field
of a narrow solenoid with alternating current) is considered. Using the eikonal
approximation, the wave function of electron, the differential and total
scattering cross sections are found. Unlike the case of direct current, the
total cross section in the case of alternating current turns out to be finite.
An oscillating asymmetry in the differential scattering cross section is
discovered. The possibility of experimental observation of the effect is
discussed.",107,2412.16030v1,physics.atom-ph,physics.atom-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.215696
CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images,"3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.",246,2412.16028v1,cs.CV,cs.CV,semiconductor physics,2024-12-20,2024-12-23T21:06:48.216693
Entropy maximizers for kinetic wave equations set on tori,"We consider the kinetic wave equation, or phonon Boltzmann equation, set on
the torus (physical system set on the lattice). We describe entropy maximizers
for fixed mass and energy; our framework is very general, being valid in any
dimension, for any dispersion relation, and even including the quantum kinetic
wave equation. Of particular interest is the presence of condensation in
certain regimes which we characterize.",89,2412.16026v1,math.AP,"math.AP,math-ph,math.MP",semiconductor physics,2024-12-20,2024-12-23T21:06:48.216693
Knowledge-dependent optimal Gaussian strategies for phase estimation,"When estimating an unknown phase rotation of a continuous-variable system
with homodyne detection, the optimal probe state strongly depends on the value
of the estimated parameter. In this article, we identify the optimal pure
single-mode Gaussian probe states depending on the knowledge of the estimated
phase parameter before the measurement. We find that for a large prior
uncertainty, the optimal probe states are close to coherent states, a result in
line with findings from noisy parameter estimation. But with increasingly
precise estimates of the parameter it becomes beneficial to put more of the
available energy into the squeezing of the probe state. Surprisingly, there is
a clear jump, where the optimal probe state changes abruptly to a squeezed
vacuum state, which maximizes the Fisher information for this estimation task.
We use our results to study repeated measurements and compare different methods
to adapt the probe state based on the changing knowledge of the parameter
according to the previous findings.",184,2412.16023v1,quant-ph,quant-ph,semiconductor physics,2024-12-20,2024-12-23T21:06:48.217690
The Classical Super-Phaserotation Infrared Triangle,"The universality of the logarithmic soft photon theorem in four dimensions
can be traced to an infinite-dimensional asymptotic symmetry which acts as a
local phase rotation on matter as we have shown in 2403.13053. Here we extend
our earlier results for the charges associated to these superphaserotations to
all orders in the coupling and prove that their conservation is exactly the
classical logarithmic soft photon theorem discovered by Saha, Sahoo and Sen in
1912.06413. We furthermore generalize the formulae for the associated
electromagnetic displacement memory and its tail from particles to scalar
matter fields. This completes the classical superphaserotation infrared
triangle.",142,2412.16149v1,hep-th,hep-th,renewable energy,2024-12-20,2024-12-23T21:06:49.211241
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",renewable energy,2024-12-20,2024-12-23T21:06:49.212239
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",renewable energy,2024-12-20,2024-12-23T21:06:49.213237
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,renewable energy,2024-12-20,2024-12-23T21:06:49.213237
Borel singularities and Stokes constants of the topological string free energy on one-parameter Calabi-Yau threefolds,"We study the Borel plane of the topological string free energy on all
hypergeometric one-parameter Calabi-Yau models close to singular points in
moduli space, focusing on the location of Borel singularities and the value of
the associated Stokes constants. We find in particular that in models which
exhibit massless D-branes at a singular point, the central charge of the
D-brane close to the singular point coincides with the location of the leading
Borel singularity, and the generalized Donaldson-Thomas invariant associated to
the charge of the D-brane, in as far as its value is known, coincides with the
Stokes constant associated to the Borel singularity.",144,2412.16140v1,hep-th,"hep-th,math.AG",renewable energy,2024-12-20,2024-12-23T21:06:49.214234
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",renewable energy,2024-12-20,2024-12-23T21:06:49.214234
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",renewable energy,2024-12-20,2024-12-23T21:06:49.215232
Terbium under High Pressure: First-Principles Dynamical Mean-Field Theory Study,"Elemental rare-earth metals provide a playground for studying novel electron
correlation effects and complex magnetism. However, ab initio simulations of
these systems remain challenging. Here, we employ fully charge self-consistent
density functional theory and dynamical mean-field theory (DFT+DMFT) to
investigate terbium (Tb) metal under pressure. We show that Tb exhibits a
strong band renormalization due to correlation effects, with the calculated
electron density of states in good agreement with the experiments. At higher
pressures, the correlated electronic structures persist but with modulation in
the Hubbard gap, highlighting the tunability of effective Coulomb interactions
and kinetic energies. Our DFT+DMFT calculations further indicate a
ferromagnetic ground state of Tb at low pressure and low temperature, as well
as a transition from ferromagnetism to paramagnetism at elevated temperatures.
These ab initio results also align with the experiments. Our study paves the
way for exploring heavy lanthanides via advanced first-principles simulations.",212,2412.16125v1,cond-mat.str-el,cond-mat.str-el,renewable energy,2024-12-20,2024-12-23T21:06:49.216229
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,renewable energy,2024-12-20,2024-12-23T21:06:49.217226
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",renewable energy,2024-12-20,2024-12-23T21:06:49.218224
Full S-matrices and Witten diagrams with (relative) L-infinity algebras,"The $L_\infty$-algebra approach to scattering amplitudes elegantly describes
the nontrivial part of the $S$-matrix but fails to take into account the
trivial part. We argue that the trivial contribution to the $S$-matrix should
be accounted for by another, complementary $L_\infty$-algebra, such that a
perturbative field theory is described by a cyclic relative $L_\infty$-algebra.
We further demonstrate that this construction reproduces Witten diagrams that
arise in AdS/CFT including, in particular, the trivial Witten diagrams
corresponding to CFT two-point functions. We also discuss Chern-Simons theory
and Yang-Mills theory on manifolds with boundaries using this approach.",161,2412.16106v1,hep-th,"hep-th,math-ph,math.MP,81T18 (Primary) 81T13, 81T35, 17B56, 17B81 (Secondary)",renewable energy,2024-12-20,2024-12-23T21:06:49.218224
Quantifying the benefit of load uncertainty reduction for the design of district energy systems under grid constraints using the Value of Information,"Load uncertainty must be accounted for during design to ensure building
energy systems can meet energy demands during operation. Reducing building load
uncertainty allows for improved designs with less compromise to be identified,
reducing the cost of decarbonizing energy usage. However, the building
monitoring required to reduce load uncertainty is costly. This study quantifies
the economic benefit of practical building monitoring for supporting energy
system design decisions, to determine if its benefits outweigh its cost. Value
of Information analysis (VoI) is a numerical framework for quantifying the
benefit of uncertainty reduction to support decision making. An extension of
the framework, termed 'On-Policy' VoI, is proposed, which admits complex
decision making tasks where decision policies are required. This is applied to
a case study district energy system design problem, where a Linear Program
model is used to size solar-battery systems and grid connection capacity under
uncertain building loads, modelled using historic electricity metering data.
Load uncertainty is found to have a significant impact on both system operating
costs (\pm30%) and the optimal system design (\pm20%). However, using building
monitoring is found to reduce overall costs by less than 2% on average, less
than the cost of measurement, and is therefore not economically worthwhile.
This provides the first numerical evidence to support the sufficiency of using
standard building load profiles for energy system design. Further, reducing
only uncertainty in mean load is found to provide all available decision
support benefit, meaning using hourly measurement data provides no benefit for
energy retrofit design.",312,2412.16105v1,eess.SY,"eess.SY,cs.SY",renewable energy,2024-12-20,2024-12-23T21:06:49.220219
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",renewable energy,2024-12-20,2024-12-23T21:06:49.222213
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",renewable energy,2024-12-20,2024-12-23T21:06:49.223210
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,renewable energy,2024-12-20,2024-12-23T21:06:49.223210
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,renewable energy,2024-12-20,2024-12-23T21:06:49.225205
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",renewable energy,2024-12-20,2024-12-23T21:06:49.227200
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",renewable energy,2024-12-20,2024-12-23T21:06:49.228197
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",renewable energy,2024-12-20,2024-12-23T21:06:49.229194
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",renewable energy,2024-12-20,2024-12-23T21:06:49.229194
Approximation of Schrödinger operators with point interactions on bounded domains,"We consider Schr\""odinger operators on a bounded domain $\Omega\subset
\mathbb{R}^3$, with homogeneous Robin or Dirichlet boundary conditions on
$\partial\Omega$ and a point (zero-range) interaction placed at an interior
point of $\Omega$. We show that, under suitable spectral assumptions, and by
means of an extension-restriction procedure which exploit the already known
result on the entire space, the singular interaction is approximated by
rescaled sequences of regular potentials. The result is missing in the
literature, and we also take the opportunity to point out some general issues
in the approximation of point interactions and the role of zero energy
resonances.",146,2412.16056v1,math-ph,"math-ph,math.MP",renewable energy,2024-12-20,2024-12-23T21:06:49.230191
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",renewable energy,2024-12-20,2024-12-23T21:06:49.230191
Self-organized critical characteristics of TeV-photons from GRB 221009A,"The very high-energy afterglow in GRB 221009A, known as the `Brightest Of All
Time' (B.O.A.T.), has been thoroughly analyzed in previous studies. In this
paper, we conducted a statistical analysis of the waiting time behavior of 172
TeV photons from the B.O.A.T. observed by LHAASO-KM2A. The following results
were obtained: (I) The waiting time distribution (WTD) of these photons
deviates from the exponential distribution. (II) The behavior of these photons
exhibits characteristics resembling those of a self-organized critical system,
such as power-law distribution and scale-invariance features in the waiting
time distribution. The power-law distribution of waiting times is consistent
with the prediction of a non-stationary process. (III) The relationship between
the power-law slopes of the WTD and the scale-invariant characteristics of the
Tsallis q-Gaussian distribution deviates from existing theory. We suggest that
this deviation is due to the photons not being completely independent of each
other. In summary, the power-law and scale-free characteristics observed in
these photons imply a self-organized critical process in the generation of TeV
photons from GRB 221009A. Based on other relevant research, we propose that the
involvement of a partially magnetically dominated component and the continuous
energy injection from the central engine can lead to deviations in the
generation of TeV afterglow from the simple external shock-dominated process,
thereby exhibiting the self-organized critical characteristics mentioned above.",335,2412.16052v1,astro-ph.HE,astro-ph.HE,renewable energy,2024-12-20,2024-12-23T21:06:49.231189
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",renewable energy,2024-12-20,2024-12-23T21:06:49.232187
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,renewable energy,2024-12-20,2024-12-23T21:06:49.233183
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",renewable energy,2024-12-20,2024-12-23T21:06:49.233183
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",renewable energy,2024-12-20,2024-12-23T21:06:49.234181
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",renewable energy,2024-12-20,2024-12-23T21:06:49.235178
X-ray polarization of the magnetar 1E 1841-045 in outburst,"We report on IXPE and NuSTAR observations that began forty days following the
onset of the 2024 outburst of the magnetar 1E 1841-045, marking the first ever
IXPE observation of a magnetar in an enhanced state. Our spectropolarimetric
analysis indicates that a non-thermal double power-law (PL) spectral model can
fit the phase-averaged intensity data well, with the soft and hard components
dominating below and above around 5 keV, respectively. We find that the soft PL
exhibits a polarization degree (PD) of about 20% while the hard X-ray PL
displays a PD of about 50%; both components have a polarization angle (PA)
compatible with 0 degree. These results are supported through model-independent
polarization analysis which shows an increasing PD from about 15% to 70% in the
2-3 keV and 6-8 keV ranges, respectively, while the PA remains consistent with
0 degree. We find marginal evidence for variability in the polarization
properties with pulse phase, namely a higher PD at spin phases coinciding with
the peak in the hard X-ray pulse. We compare the hard X-ray PL to the
expectation from direct resonant inverse Compton scattering (RICS) and
secondary pair cascade synchrotron radiation from primary high-energy RICS
photons, finding that both can provide reasonable spectropolarimetric agreement
with the data, yet, the latter more naturally. Finally, we suggest that the
soft power law X-ray component may be emission emanating from a Comptonized
corona in the inner magnetosphere.",327,2412.16036v1,astro-ph.HE,astro-ph.HE,renewable energy,2024-12-20,2024-12-23T21:06:49.236175
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",renewable energy,2024-12-20,2024-12-23T21:06:49.236175
Integral representation for a relaxed optimal design problem for non-simple grade two materials,"A measure representation result for a functional modelling optimal design
problems for plastic deformations, under linear growth conditions, is obtained.
  Departing from an energy with a bulk term depending on the second gradient,
as well as a perimeter term, the functional in question corresponds to the
relaxation of this energy with respect to a pair $(\chi,u)$, where $\chi$ is
the characteristic function of a set of finite perimeter and $u$ is a function
of bounded hessian.",99,2412.16027v1,math.AP,"math.AP,math.OC,49J45, 49Q20, 26B25",renewable energy,2024-12-20,2024-12-23T21:06:49.237173
Entropy maximizers for kinetic wave equations set on tori,"We consider the kinetic wave equation, or phonon Boltzmann equation, set on
the torus (physical system set on the lattice). We describe entropy maximizers
for fixed mass and energy; our framework is very general, being valid in any
dimension, for any dispersion relation, and even including the quantum kinetic
wave equation. Of particular interest is the presence of condensation in
certain regimes which we characterize.",89,2412.16026v1,math.AP,"math.AP,math-ph,math.MP",renewable energy,2024-12-20,2024-12-23T21:06:49.237173
High-efficiency position resolved gamma ray detectors for 2D-measurements of the angular correlation of annihilation radiation,"The measurement of the 2D-Angular Correlation of Electron Positron
Annihilation Radiation (ACAR) provides unique information about the bulk
electronic structure of single crystals. We set up a new prototype for 2D-ACAR
measurements using two 24 x 24 (26.8 mm x 26.8 mm) pixelated LYSO scintillation
crystals in combination with a glass light guide and 8 x 8 (24 mm x 24 mm)
Multi Pixel Photon Counters (MPPCs). Compared to conventional Anger-cameras,
typically comprising large NaI(Tl) scintillators read out with photomultiplier
arrays a larger implementation of our prototype would drastically improve
resolution and count rate by taking advantage of the small pixel size of the
scintillator, its much higher attenuation coefficient for 511 keV
{\gamma}-quanta and faster digital readout. With our prototype we achieved a
detection efficiency of 45%, i.e. five times higher compared to NaI(Tl) used in
our Anger cameras, leading to a 25 (!) times higher coincidence count rate in
ACAR measurements. A spatial resolution of 1 mm was obtained, which is limited
by the pixel size of the scintillator. We demonstrate the high performance of
the setup by (i) imaging the local distribution of 22Na in a proton-irradiated
aluminum target and (ii) determining the Fermi energy of Cu from 2D-ACAR
spectra recorded for a polycrystalline copper sample.",311,2412.16024v1,physics.ins-det,physics.ins-det,renewable energy,2024-12-20,2024-12-23T21:06:49.238170
Knowledge-dependent optimal Gaussian strategies for phase estimation,"When estimating an unknown phase rotation of a continuous-variable system
with homodyne detection, the optimal probe state strongly depends on the value
of the estimated parameter. In this article, we identify the optimal pure
single-mode Gaussian probe states depending on the knowledge of the estimated
phase parameter before the measurement. We find that for a large prior
uncertainty, the optimal probe states are close to coherent states, a result in
line with findings from noisy parameter estimation. But with increasingly
precise estimates of the parameter it becomes beneficial to put more of the
available energy into the squeezing of the probe state. Surprisingly, there is
a clear jump, where the optimal probe state changes abruptly to a squeezed
vacuum state, which maximizes the Fisher information for this estimation task.
We use our results to study repeated measurements and compare different methods
to adapt the probe state based on the changing knowledge of the parameter
according to the previous findings.",184,2412.16023v1,quant-ph,quant-ph,renewable energy,2024-12-20,2024-12-23T21:06:49.238170
Dimension-8 operators in $W^+W^-$ production via gluon fusion,"We investigate the impact of dimension-8 operators on $W^+W^-$ production at
the LHC for the incoming gluon-gluon channel. To this end, we have identified
all dimension-8 CP-even operators contributing to the process in question, and
computed the corresponding tree-level helicity amplitudes for fully-leptonic
decays of the $W$ bosons. These are implemented in the program MCFM-RE, which
automatically incorporates the effect of a jet-veto to reduce the otherwise
overwhelming $t\bar t$ background. We find that, unless we break the hierarchy
of the effective field theory (EFT), the interference of the dimension-8
operators with the Standard Model is negligible across the considered
distributions. This justifies including the square of dimension-6 operators
when performing EFT fits with this channel. We then present new constraints on
CP-even and CP-odd dimension-6 operators within the EFT regime. Lastly, we
postulate a scenario in which the hierarchy of the EFT is broken, justified by
the strong constraints on dimension-6 operators from existing on-shell Higgs
data. In this scenario, we discuss the constraints that can be reasonably set
on CP-even dimension-8 operators with current and future data. We remark that
the effect of the jet-veto on the ability to constrain new physics in the
$W^+W^-$ channel is quite dramatic and must be properly taken into account.",309,2412.16020v1,hep-ph,"hep-ph,hep-ex",renewable energy,2024-12-20,2024-12-23T21:06:49.239167
Distributed Beam Alignment in sub-THz D2D Networks,"Devices in a device-to-device (D2D) network operating in sub-THz frequencies
require knowledge of the spatial channel that connects them to their peers.
Acquiring such high dimensional channel state information entails large
overhead, which drastically increases with the number of network devices. In
this paper, we propose an accelerated method to achieve network-wide beam
alignment in an efficient way. To this aim, we consider compressed sensing
estimation enabled by a novel design of pilot sequences. Our designed pilots
have constant envelope to alleviate hardware requirements at the transmitters,
while they exhibit a ""comb-like""' spectrum that flexibly allocates energy only
on certain frequencies. This design enables multiple devices to transmit thier
pilots concurrently while remaining orthogonal in frequency, achieving
simultaneous alignment of multiple devices. Furthermore, we present a
sequential partitioning strategy into transmitters and receivers that results
in logarithmic scaling of the overhead with the number of devices, as opposed
to the conventional linear scaling. Finally, we show via accurate modeling of
the indoor propagation environment and ray tracing simulations that the
resulting sub-THz channels after successful beamforming are approximately
frequency flat, therefore suitable for efficient single carrier transmission
without equalization. We compare our results against an ""802.11ad inspired""
baseline and show that our method is capable to greatly reduce the number of
pilots required to achieve network-wide alignment.",277,2412.16015v1,eess.SP,eess.SP,renewable energy,2024-12-20,2024-12-23T21:06:49.240165
Fuzzy-Space Engineering,"The techniques developed for matrix models and fuzzy geometry are powerful
tools for representing strings and membranes in quantum physics. We study the
representation of fuzzy surfaces using these techniques. This involves
constructing graphs and writing their coordinates and connectivity into
matrices. To construct arbitrary graphs and quickly change them, we use 3D
software. A script generates the three matrices from the graphs. These matrices
are then processed in Wolfram Mathematica to calculate the zero modes of the
Dirac operator. Our first result shows the quantization of a two-dimensional
Trefoil knot. Additional examples illustrate various properties and behaviors
of this process. This helps us to gain a deeper understanding of fuzzy spaces
and zero-mode surfaces. This work contributes to advancing the understanding of
visualization aspects in fuzzy geometry.",153,2412.16011v1,hep-th,hep-th,renewable energy,2024-12-20,2024-12-23T21:06:49.240165
Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep Learning,"Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a
cornerstone to assessing the authenticity of the received information and
guaranteeing robust service delivery in several application domains. The
solutions available today for spoofing detection either rely on additional
communication systems, receivers, and antennas, or require mobile deployments.
Detection systems working at the Physical (PHY) layer of the satellite
communication link also require time-consuming and energy-hungry training
processes on all satellites of the constellation, and rely on the availability
of spoofed data, which are often challenging to collect. Moreover, none of such
contributions investigate the feasibility of aerial spoofing attacks launched
via drones operating at various altitudes. In this paper, we propose a new
spoofing detection technique for LEO satellite constellation systems, applying
anomaly detection on the received PHY signal via autoencoders. We validate our
solution through an extensive measurement campaign involving the deployment of
an actual spoofer (Software-Defined Radio) installed on a drone and injecting
rogue IRIDIUM messages while flying at different altitudes with various
movement patterns. Our results demonstrate that the proposed technique can
reliably detect LEO spoofing attacks launched at different altitudes, while
state-of-the-art competing approaches simply fail. We also release the
collected data as open source, fostering further research on satellite
security.",276,2412.16008v1,cs.CR,cs.CR,renewable energy,2024-12-20,2024-12-23T21:06:49.241162
Collective single-photon emission and energy transfer in thin-layer dielectric and plasmonic systems,"We study the collective photon decay of multiple quantum emitters embedded in
a thin high-index dielectric layer such as hexagonal boron nitride (hBN), with
and without a metal substrate. We first explore the significant role that
guided modes including surface plasmon modes play in the collective decay of
identical singlephoton emitters (super- and subradiance). Surprisingly, on
distances relevant for collective emission, the guided or surface-plasmon modes
do not always enhance the collective emission. We identify configurations with
inhibition, and others with enhancement of the dipole interaction due to the
guided modes. We interpret our results in terms of local and cross densities of
optical states. In the same structure, we show a remarkably favorable
configuration for enhanced F\""orster resonance energy transfer between a donor
and acceptor in the dielectric layer on a metallic substrate. We compare our
results to theoretical limits for energy transfer efficiency.",195,2412.16000v1,physics.optics,"physics.optics,cond-mat.mes-hall,quant-ph",renewable energy,2024-12-20,2024-12-23T21:06:49.241162
"Ti and Spi, Carrollian extended boundaries at timelike and spatial infinity","The goal of this paper is to provide a definition for a notion of extended
boundary at time and space-like infinity which, following
Figueroa-O'Farril--Have--Prohazka--Salzer, we refer to as Ti and Spi. This
definition applies to asymptotically flat spacetime in the sense of
Ashtekar--Romano and we wish to demonstrate, by example, its pertinence in a
number of situations. The definition is invariant, is constructed solely from
the asymptotic data of the metric and is such that automorphisms of the
extended boundaries are canonically identified with asymptotic symmetries.
Furthermore, scattering data for massive fields are realised as functions on Ti
and a geometric identification of cuts of Ti with points of Minkowksi then
produces an integral formula of Kirchhoff type. Finally, Ti and Spi are both
naturally equipped with (strong) Carrollian geometries which, under mild
assumptions, enable to reduce the symmetry group down to the BMS group, or to
Poincar\'e in the flat case. In particular, Strominger's matching conditions
are naturally realised by restricting to Carrollian geometries compatible with
a discrete symmetry of Spi.",262,2412.15996v1,gr-qc,"gr-qc,hep-th,math-ph,math.MP",renewable energy,2024-12-20,2024-12-23T21:06:49.242162
From discrete to continuum in the helical XY-model: emergence of chirality transitions in the $S^1$ to $S^2$ limit,"We analyze the discrete-to-continuum limit of a frustrated
ferromagnetic/anti-ferromagnetic $\mathbb{S}^2$-valued spin system on the
lattice $\lambda_n\mathbb{Z}^2$ as $\lambda_n\to 0$. For $\mathbb{S}^2$ spin
systems close to the Landau-Lifschitz point (where the
helimagnetic/ferromagnetic transition occurs), it is well established that for
chirality transitions emerge with vanishing energy. Inspired by recent work on
the $N$-clock model, we consider a spin model where spins are constrained to
$k_n$ copies of $\mathbb{S}^1$ covering $\mathbb{S}^2$ as $n\to\infty$. We
identify a critical energy-scaling regime and a threshold for the divergence
rate of $k_n\to+\infty$, below which the $\Gamma$-limit of the discrete
energies capture chirality transitions while retaining an $\mathbb{S}^2$-valued
energy description in the continuum limit.",256,2412.15994v1,math.AP,math.AP,renewable energy,2024-12-20,2024-12-23T21:06:49.243161
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",renewable energy,2024-12-20,2024-12-23T21:06:49.244155
Active Flow Control for Bluff Body under High Reynolds Number Turbulent Flow Conditions Using Deep Reinforcement Learning,"This study employs Deep Reinforcement Learning (DRL) for active flow control
in a turbulent flow field of high Reynolds numbers at $Re=274000$. That is, an
agent is trained to obtain a control strategy that can reduce the drag of a
cylinder while also minimizing the oscillations of the lift. Probes are placed
only around the surface of the cylinder, and a Proximal Policy Optimization
(PPO) agent controls nine zero-net mass flux jets on the downstream side of the
cylinder. The trained PPO agent effectively reduces drag by $29\%$ and
decreases lift oscillations by $18\%$ of amplitude, with the control effect
demonstrating good repeatability. Control tests of this agent within the
Reynolds number range of $Re=260000$ to $288000$ show the agent's control
strategy possesses a certain degree of robustness, with very similar drag
reduction effects under different Reynolds numbers. Analysis using power
spectral energy reveals that the agent learns specific flow frequencies in the
flow field and effectively suppresses low-frequency, large-scale structures.
Graphically visualizing the policy, combined with pressure, vorticity, and
turbulent kinetic energy contours, reveals the mechanism by which jets achieve
drag reduction by influencing reattachment vortices. This study successfully
implements robust active flow control in realistically significant high
Reynolds number turbulent flows, minimizing time costs (using two-dimensional
geometrical models and turbulence models) and maximally considering the
feasibility of future experimental implementation.",315,2412.15975v1,physics.flu-dyn,physics.flu-dyn,renewable energy,2024-12-20,2024-12-23T21:06:49.245152
Adding interferometric lightning detection to the Pierre Auger Observatory,"The Pierre Auger Observatory has detected downward terrestrial gamma-ray
flashes (TGFs) with its Surface Detector. A key to understanding this
high-energy radiation in thunderstorms is to combine such measurements with
measurements of lightning processes in their earliest stages. With eleven
modified Auger Engineering Radio Array (AERA) stations we can build an
interferometric lightning detection array working in the bandwidth between 30 -
80 MHz inside the Surface Detector array to precisely measure lightning stepped
leaders in 3D. These measurements allow us to decipher the cause of TGFs and
clarify the reason for the observed high-energy particles in thunderstorms. We
will present the current status of the detection plans including the
configuration of the interferometric lightning detection array and the steps to
take as well as the reconstruction characteristics obtained with AERA.",166,2412.15972v1,astro-ph.IM,"astro-ph.IM,astro-ph.HE",renewable energy,2024-12-20,2024-12-23T21:06:49.245152
Optimization of Beyond Diagonal RIS: A Universal Framework Applicable to Arbitrary Architectures,"Reconfigurable intelligent surfaces (RISs) are envisioned as a promising
technology for future wireless communication systems due to their ability to
control the propagation environment in a hardware- and energy-efficient way.
Recently, the concept of RISs has been extended to beyond diagonal RISs
(BD-RISs), which unlock the full potential of RISs thanks to the presence of
tunable interconnections between RIS elements. While various algorithms have
been proposed for specific BD-RIS architectures, a universal optimization
framework applicable to arbitrary architectures is still lacking. In this
paper, we bridge this research gap by proposing an architecture-independent
framework for BD-RIS optimization, with the main focus on sum-rate maximization
and transmit power minimization in multiuser multi-input single-output
(MU-MISO) systems. Specifically, we first incorporate BD-RIS architectures into
the models by connecting the scattering matrix with the admittance matrix and
introducing appropriate constraints to the admittance matrix. The formulated
problems are then solved by our custom-designed partially proximal alternating
direction method of multipliers (pp-ADMM) algorithms. The pp-ADMM algorithms
are computationally efficient, with each subproblem either admitting a
closed-form solution or being easily solvable. We further explore the extension
of the proposed framework to general utility functions and multiuser
multi-input multi-output (MU-MIMO) systems. Simulation results demonstrate that
the proposed approaches achieve a better trade-off between performance and
computational efficiency compared to existing methods. We also compare the
performance of various BD-RIS architectures in MU-MISO systems using the
proposed approach, which has not been explored before due to the lack of an
architecture-independent framework.",366,2412.15965v1,eess.SP,"eess.SP,cs.IT,math.IT,math.OC",renewable energy,2024-12-20,2024-12-23T21:06:49.246149
What shall we learn from a future supernova?,"Core-collapse supernovae constitute a unique laboratory for particle physics
and astrophysics. They are powerful neutrino sources of all flavors, emitting
essentially all the gravitational binding energy through neutrinos, at the end
of their life. I will highlight how crucial is the observation of the next
core-collapse supernova and of the diffuse supernova neutrino background, whose
discovery might be imminent.",84,2412.15964v1,astro-ph.SR,"astro-ph.SR,astro-ph.HE,hep-ph",renewable energy,2024-12-20,2024-12-23T21:06:49.247147
Feynman Integral Reduction without Integration-By-Parts,"We present an interesting study of Feynman integral reduction that does not
employ integration-by-parts identities. Our approach proceeds by studying the
equivalence relations of integral contours in the Feynman parameterization. We
find that the integration contour can take a more general form than that given
by the Cheng-Wu theorem. We apply this idea to one-loop integrals, and derive
universal reduction formulas that can be used to efficiently reduce any
one-loop integral. We expect that this approach can be useful in the reduction
of multi-loop integrals as well.",117,2412.15962v1,hep-th,"hep-th,hep-ph",renewable energy,2024-12-20,2024-12-23T21:06:49.247147
Effective Metric Description of 2+1 Dimensional Quantum Black Holes,"We develop an effective metric description of 2+1 dimensional black holes
describing deviations from the classical Ba\~nados-Teitelboim-Zanelli (BTZ)
black hole. The latter is a classical 2+1 dimensional rotating black hole with
constant negative curvature. The effective metric is constrained by imposing
the black hole symmetries and asymptotic classical behavior. The deformed
metric is parametrized in terms of a physical quantity that we choose to be a
physical distance. The latter can be solved for in three main regions of
interest, the one around the horizon, origin, and spatial infinity. The
finiteness of physical quantities at the horizon, such as the Ricci and
Kretschmann scalars, leads to universal constraints on the physical parameters
of the metric around the horizon. This allows us to further derive the general
form of the corrected Hawking temperature in terms of the physical parameters
of the effective metric. Assuming that the approach can be generalized to the
interior of the black hole, we further develop an effective metric description
near the origin. To illustrate the approach, we show how to recast the
information encoded in a specific model of quantum BTZ known as quBTZ black
hole in terms of the effective metric coefficients.",256,2412.15960v1,gr-qc,"gr-qc,hep-ph,hep-th",renewable energy,2024-12-20,2024-12-23T21:06:49.248144
Heavy-quark mass effects in off-light-cone distributions,"We compute the one-loop correction to the forward matrix element of an
off-light-cone bi-local quark correlator characterised by a space-like
separation $z^2$ in the presence of heavy quarks with mass $m$. This
calculation allows us to extract the one-loop matching kernel, necessary to
connect quasi and pseudo-distributions to collinear parton distribution
functions (PDFs), accounting for heavy-quark mass effects. Our result is exact
in that it includes all powers of $z^2m^2$ at one loop in $\alpha_s$. In the
limit $z^2m^2\rightarrow 0$, it consistently reduces to the known massless
result. We also carry out an implementation of our expression, which allows us
to compute the charm pseudo-distribution of the proton given its PDF. We
finally comment on the quantitative impact of heavy-quark mass corrections.",197,2412.15958v1,hep-ph,"hep-ph,hep-lat,nucl-th",renewable energy,2024-12-20,2024-12-23T21:06:49.248144
Continuous evolution of the polarization properties in the transient X-ray pulsar RX J0440.9+4431/LS V +44 17,"We present a detailed time-resolved and phase-resolved polarimetric analysis
of the transient X-ray pulsar RX J0440.9+4431/LS V +44 17, using data from the
Imaging X-ray Polarimetry Explorer (IXPE) during the 2023 giant outburst. We
conducted a time-resolved analysis by dividing the data into several intervals
for each observation. This analysis reveals a continuous rotation of the
phase-averaged polarization angle (PA) across the observations performed during
the super-critical and sub-critical regimes. To investigate the origin of the
PA rotation, we performed a pulse phase-resolved polarimetric analysis over
four time intervals, each spanning approximately three days. Applying the
rotating vector model (RVM), the geometric parameters of the system were
determined for each interval. Despite the short time gap of just $\sim$ 20
days, we observed significant variation in the RVM parameters between the first
interval and the subsequent three, indicating the presence of an additional
polarized component alongside the RVM component. Using a two-polarized
component model with the assumption that this additional component remains
constant across pulse phases, we calculated the phase-averaged PA and polarized
flux of both the variable and constant components. The phase-averaged PA of
each component remained relatively stable over time, but the polarized flux of
the constant component decreased, while that of the variable component
increased. The observed rotation of the PA is attributed to the gradual shift
in the polarized flux ratio between the two components and is not directly
related to the different accretion regimes.",326,2412.15955v1,astro-ph.HE,astro-ph.HE,renewable energy,2024-12-20,2024-12-23T21:06:49.249141
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",quantum physics,2024-12-20,2024-12-23T21:06:49.773326
Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?,"The animation of realistic virtual avatars in crowd scenarios is an important
element of immersive virtual environments. However, achieving this realism
requires attention to multiple factors, such as their visual appearance and
motion cues. We investigated how body shape diversity influences the perception
of motion clones in virtual crowds. A physics-based model was used to simulate
virtual avatars in a small-scale crowd of size twelve. Participants viewed
side-by-side video clips of these virtual crowds: one featuring all unique
motions (Baseline) and the other containing motion clones (i.e., the same
motion used to animate two or more avatars in the crowd). We also varied the
levels of body shape and motion diversity. Our findings revealed that body
shape diversity did not influence participants' ratings of motion clone
detection, and motion variety had a greater impact on their perception of the
crowd. Further research is needed to investigate how other visual factors
interact with motion in order to enhance the perception of virtual crowd
realism.",200,2412.16151v1,cs.HC,"cs.HC,cs.GR",quantum physics,2024-12-20,2024-12-23T21:06:49.774323
The Classical Super-Phaserotation Infrared Triangle,"The universality of the logarithmic soft photon theorem in four dimensions
can be traced to an infinite-dimensional asymptotic symmetry which acts as a
local phase rotation on matter as we have shown in 2403.13053. Here we extend
our earlier results for the charges associated to these superphaserotations to
all orders in the coupling and prove that their conservation is exactly the
classical logarithmic soft photon theorem discovered by Saha, Sahoo and Sen in
1912.06413. We furthermore generalize the formulae for the associated
electromagnetic displacement memory and its tail from particles to scalar
matter fields. This completes the classical superphaserotation infrared
triangle.",142,2412.16149v1,hep-th,hep-th,quantum physics,2024-12-20,2024-12-23T21:06:49.774323
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",quantum physics,2024-12-20,2024-12-23T21:06:49.775321
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",quantum physics,2024-12-20,2024-12-23T21:06:49.776318
Borel singularities and Stokes constants of the topological string free energy on one-parameter Calabi-Yau threefolds,"We study the Borel plane of the topological string free energy on all
hypergeometric one-parameter Calabi-Yau models close to singular points in
moduli space, focusing on the location of Borel singularities and the value of
the associated Stokes constants. We find in particular that in models which
exhibit massless D-branes at a singular point, the central charge of the
D-brane close to the singular point coincides with the location of the leading
Borel singularity, and the generalized Donaldson-Thomas invariant associated to
the charge of the D-brane, in as far as its value is known, coincides with the
Stokes constant associated to the Borel singularity.",144,2412.16140v1,hep-th,"hep-th,math.AG",quantum physics,2024-12-20,2024-12-23T21:06:49.776318
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",quantum physics,2024-12-20,2024-12-23T21:06:53.982304
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",quantum physics,2024-12-20,2024-12-23T21:06:53.983301
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",quantum physics,2024-12-20,2024-12-23T21:06:53.984298
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",quantum physics,2024-12-20,2024-12-23T21:06:53.985295
Revisiting Global Income Convergence in the 21st Century,"This paper revisits the debate on income convergence between poor and rich
countries. I challenge the view that there is little to no catch-up, and that
changes in total factor productivity (TFP) drives cross-country income
differences. Since 2000, income levels in poor countries have converged with
rich countries at 0.8% annually, rising to 1.5% when excluding Sub-Saharan
Africa. A growth accounting exercise incorporating capital income share
heterogeneity shows that most convergence since 1980, and over half since 2000
outside Sub-Saharan Africa, results from convergence in physical and human
capital inputs rather than TFP.",129,2412.16127v1,econ.GN,"econ.GN,q-fin.EC",quantum physics,2024-12-20,2024-12-23T21:06:53.985295
Observational Properties of Harmonic EMIC waves: Statistical Study,"Electromagnetic ion cyclotron (EMIC) waves are discrete electromagnetic
emissions separated by multiple ion gyrofrequencies. Harmonic EMIC waves are
defined as waves with a strong electric or magnetic field (or both) at the
harmonics of the fundamental EMIC mode. In this paper, for the first time, we
present a statistical study on harmonic EMIC waves by the Van Allen Probes. The
EMIC waves are categorized into three types based on their harmonics: (1)
fundamental mode only (without higher harmonics), (2) electrostatic (ES)
harmonics, and (3) electromagnetic (EM) harmonics. Our statistical study shows
that ES and EM harmonic EMIC waves predominantly occur on the dayside, outside
the plasmasphere with $L >5$ and are associated with a low $f_{pe}/f_{ce}$, a
high proton $\beta_H$, and a strong fundamental EMIC mode. The results will
advance our understanding of harmonic EMIC waves and their generation
mechanisms.",217,2412.16124v1,physics.space-ph,physics.space-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.986293
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.986293
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",quantum physics,2024-12-20,2024-12-23T21:06:53.987290
Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli,"As humans perceive and actively engage with the world, we adjust our
decisions in response to shifting group dynamics and are influenced by social
interactions. This study aims to identify which aspects of interaction affect
cooperation-defection choices. Specifically, we investigate human cooperation
within the Prisoner's Dilemma game, using the Drift-Diffusion Model to describe
the decision-making process. We introduce a novel Bayesian model for the
evolution of the model's parameters based on the nature of interactions
experienced with other players. This approach enables us to predict the
evolution of the population's expected cooperation rate. We successfully
validate our model using an unseen test dataset and apply it to explore three
strategic scenarios: co-player manipulation, use of rewards and punishments,
and time pressure. These results support the potential of our model as a
foundational tool for developing and testing strategies aimed at enhancing
cooperation, ultimately contributing to societal welfare.",182,2412.16121v1,physics.soc-ph,physics.soc-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.988287
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",quantum physics,2024-12-20,2024-12-23T21:06:53.988287
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",quantum physics,2024-12-20,2024-12-23T21:06:53.989285
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",quantum physics,2024-12-20,2024-12-23T21:06:53.989285
Full S-matrices and Witten diagrams with (relative) L-infinity algebras,"The $L_\infty$-algebra approach to scattering amplitudes elegantly describes
the nontrivial part of the $S$-matrix but fails to take into account the
trivial part. We argue that the trivial contribution to the $S$-matrix should
be accounted for by another, complementary $L_\infty$-algebra, such that a
perturbative field theory is described by a cyclic relative $L_\infty$-algebra.
We further demonstrate that this construction reproduces Witten diagrams that
arise in AdS/CFT including, in particular, the trivial Witten diagrams
corresponding to CFT two-point functions. We also discuss Chern-Simons theory
and Yang-Mills theory on manifolds with boundaries using this approach.",161,2412.16106v1,hep-th,"hep-th,math-ph,math.MP,81T18 (Primary) 81T13, 81T35, 17B56, 17B81 (Secondary)",quantum physics,2024-12-20,2024-12-23T21:06:53.990282
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",quantum physics,2024-12-20,2024-12-23T21:06:53.990282
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",quantum physics,2024-12-20,2024-12-23T21:06:53.990282
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",quantum physics,2024-12-20,2024-12-23T21:06:53.991279
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",quantum physics,2024-12-20,2024-12-23T21:06:53.992278
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.992278
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",quantum physics,2024-12-20,2024-12-23T21:06:53.993274
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.994272
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.994272
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",quantum physics,2024-12-20,2024-12-23T21:06:53.995269
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",quantum physics,2024-12-20,2024-12-23T21:06:53.996266
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,quantum physics,2024-12-20,2024-12-23T21:06:53.996266
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,quantum physics,2024-12-20,2024-12-23T21:06:53.997263
Eigenvalue Bounds for Multi-Particle Reduced Density Matrices of Coulombic Wavefunctions,"For bound states of atoms and molecules of $N$ electrons we consider the
corresponding $K$-particle reduced density matrices, $\Gamma^{(K)}$, for $1 \le
K \le N-1$. Previously, eigenvalue bounds were obtained in the case of $K=1$
and $K=N-1$ by A.V. Sobolev. The purpose of the current work is to obtain
bounds in the case of $2 \le K \le N-2$. For such $K$ we label the eigenvalues
of the positive, trace class operators $\Gamma^{(K)}$ by
$\lambda_n(\Gamma^{(K)})$ for $n=1,2,\dots$, and obtain the bounds
$\lambda_n(\Gamma^{(K)}) \le Cn^{-\alpha_K}$ for all $n$, where $\alpha_K = 1 +
7/(3L)$ and $L = \min\{K,N-K\}$.",240,2412.16073v1,math-ph,"math-ph,math.MP,35J10",quantum physics,2024-12-20,2024-12-23T21:06:53.997263
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",quantum physics,2024-12-20,2024-12-23T21:06:53.998261
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",quantum physics,2024-12-20,2024-12-23T21:06:53.999258
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",quantum physics,2024-12-20,2024-12-23T21:06:53.999258
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",quantum physics,2024-12-20,2024-12-23T21:06:54.000255
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",quantum physics,2024-12-20,2024-12-23T21:06:54.001253
Approximation of Schrödinger operators with point interactions on bounded domains,"We consider Schr\""odinger operators on a bounded domain $\Omega\subset
\mathbb{R}^3$, with homogeneous Robin or Dirichlet boundary conditions on
$\partial\Omega$ and a point (zero-range) interaction placed at an interior
point of $\Omega$. We show that, under suitable spectral assumptions, and by
means of an extension-restriction procedure which exploit the already known
result on the entire space, the singular interaction is approximated by
rescaled sequences of regular potentials. The result is missing in the
literature, and we also take the opportunity to point out some general issues
in the approximation of point interactions and the role of zero energy
resonances.",146,2412.16056v1,math-ph,"math-ph,math.MP",quantum physics,2024-12-20,2024-12-23T21:06:54.001253
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",quantum physics,2024-12-20,2024-12-23T21:06:54.001253
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",quantum physics,2024-12-20,2024-12-23T21:06:54.002250
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,quantum physics,2024-12-20,2024-12-23T21:06:54.003248
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",quantum physics,2024-12-20,2024-12-23T21:06:54.004245
Millikelvin Nb nanoSQUID-embedded tuneable resonator fabricated with a neon focused-ion-beam,"SQUID-embedded superconducting resonators are of great interest due to their
potential for coupling highly scalable superconducting circuits with quantum
memories based on solid-state spin ensembles. Such an application requires a
high-$Q$, frequency-tuneable resonator which is both resilient to magnetic
field, and able to operate at millikelvin temperatures. These requirements
motivate the use of a higher $H_{c}$ metal such as niobium, however the
challenge then becomes to sufficiently reduce the operating temperature. We
address this by presenting a monolithic Nb nanoSQUID-embedded resonator, where
neon focused-ion-beam fabrication of the nanoSQUID results in a device
displaying frequency tuneability at $T = 16$ mK. In order to assess the
applicability of the device for coupling to small spin clusters, we
characterise the flux sensitivity as a function of microwave drive power and
externally applied magnetic field, and find that the noise is dominated by
dielectric noise in the resonator. Finally, we discuss improvements to the
device design which can dramatically improve the flux sensitivity, which
highlights the promise of Nb SQUID-embedded resonators for hybrid
superconductor-spin applications.",261,2412.16045v1,quant-ph,"quant-ph,cond-mat.supr-con",quantum physics,2024-12-20,2024-12-23T21:06:54.004245
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",quantum physics,2024-12-20,2024-12-23T21:06:54.005242
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",quantum physics,2024-12-20,2024-12-23T21:06:54.006239
Integrability versus chaos in the steady state of many-body open quantum systems,"The Lindblad description of an open quantum system gives rise to two types of
integrability, since the nonequilibrium steady state can be integrable
independently of the Liouvillian. Taking boundary-driven and dephasing spin
chains as a representative example, we discriminate Liouvillian and
steady-state chaos by combining level spacing statistics and an extension of
the eigenstate thermalization hypothesis to open quantum systems. Moreover, we
analyze the structure of the steady states by expanding it in the basis of
Pauli strings and comparing the weight of strings of different lengths. We show
that the natural expectation that integrable steady states are ""simple"" (i.e.,
built from few-body local operators) does not hold: the steady states of both
chaotic and integrable models have relevant contributions coming from Pauli
strings of all possible lengths, including long-range and many-body
interactions. Nevertheless, we show that one can effectively use the
operator-size distribution to distinguish chaotic and integrable steady states.",217,2412.16041v1,cond-mat.stat-mech,"cond-mat.stat-mech,cond-mat.mes-hall,nlin.CD,quant-ph",quantum physics,2024-12-20,2024-12-23T21:06:54.006239
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",quantum physics,2024-12-20,2024-12-23T21:06:54.007237
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",quantum physics,2024-12-20,2024-12-23T21:06:54.007237
Non-stationary Aharonov-Bohm effect,"The non-stationary Aharonov-Bohm effect (scattering of electron in the field
of a narrow solenoid with alternating current) is considered. Using the eikonal
approximation, the wave function of electron, the differential and total
scattering cross sections are found. Unlike the case of direct current, the
total cross section in the case of alternating current turns out to be finite.
An oscillating asymmetry in the differential scattering cross section is
discovered. The possibility of experimental observation of the effect is
discussed.",107,2412.16030v1,physics.atom-ph,physics.atom-ph,quantum physics,2024-12-20,2024-12-23T21:06:54.008235
CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images,"3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.",246,2412.16028v1,cs.CV,cs.CV,quantum physics,2024-12-20,2024-12-23T21:06:54.008235
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",particle physics,2024-12-20,2024-12-23T21:06:55.402685
Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?,"The animation of realistic virtual avatars in crowd scenarios is an important
element of immersive virtual environments. However, achieving this realism
requires attention to multiple factors, such as their visual appearance and
motion cues. We investigated how body shape diversity influences the perception
of motion clones in virtual crowds. A physics-based model was used to simulate
virtual avatars in a small-scale crowd of size twelve. Participants viewed
side-by-side video clips of these virtual crowds: one featuring all unique
motions (Baseline) and the other containing motion clones (i.e., the same
motion used to animate two or more avatars in the crowd). We also varied the
levels of body shape and motion diversity. Our findings revealed that body
shape diversity did not influence participants' ratings of motion clone
detection, and motion variety had a greater impact on their perception of the
crowd. Further research is needed to investigate how other visual factors
interact with motion in order to enhance the perception of virtual crowd
realism.",200,2412.16151v1,cs.HC,"cs.HC,cs.GR",particle physics,2024-12-20,2024-12-23T21:06:55.403682
The Classical Super-Phaserotation Infrared Triangle,"The universality of the logarithmic soft photon theorem in four dimensions
can be traced to an infinite-dimensional asymptotic symmetry which acts as a
local phase rotation on matter as we have shown in 2403.13053. Here we extend
our earlier results for the charges associated to these superphaserotations to
all orders in the coupling and prove that their conservation is exactly the
classical logarithmic soft photon theorem discovered by Saha, Sahoo and Sen in
1912.06413. We furthermore generalize the formulae for the associated
electromagnetic displacement memory and its tail from particles to scalar
matter fields. This completes the classical superphaserotation infrared
triangle.",142,2412.16149v1,hep-th,hep-th,particle physics,2024-12-20,2024-12-23T21:06:55.403682
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",particle physics,2024-12-20,2024-12-23T21:06:55.404680
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",particle physics,2024-12-20,2024-12-23T21:06:55.405677
Borel singularities and Stokes constants of the topological string free energy on one-parameter Calabi-Yau threefolds,"We study the Borel plane of the topological string free energy on all
hypergeometric one-parameter Calabi-Yau models close to singular points in
moduli space, focusing on the location of Borel singularities and the value of
the associated Stokes constants. We find in particular that in models which
exhibit massless D-branes at a singular point, the central charge of the
D-brane close to the singular point coincides with the location of the leading
Borel singularity, and the generalized Donaldson-Thomas invariant associated to
the charge of the D-brane, in as far as its value is known, coincides with the
Stokes constant associated to the Borel singularity.",144,2412.16140v1,hep-th,"hep-th,math.AG",particle physics,2024-12-20,2024-12-23T21:06:55.405677
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",particle physics,2024-12-20,2024-12-23T21:06:55.406674
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",particle physics,2024-12-20,2024-12-23T21:06:55.407672
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",particle physics,2024-12-20,2024-12-23T21:06:55.408669
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",particle physics,2024-12-20,2024-12-23T21:06:55.408669
Role of the ratio of tangential to normal stiffness coefficient on the behaviour of vibrofluidised particles,"The selection of parameters in the contact law for inter-particle
interactions affects the results of simulations of flowing granular materials.
The present study aims to understand the effect of the ratio of tangential to
normal spring stiffness coefficient ($\kappa$) on inter-particle contact
behaviour in terms of the rotational coefficient of restitution determined
using data obtained from multi-particle simulations. The effect of $\kappa$ on
the profiles of the micro- and macroscopic properties of particles in a
vibrofluidised bed is also investigated. The Discrete Element Method (DEM) is
used to simulate a vertically vibrated fluidised bed using the open-source
software LAMMPS. The inter-particle and wall-particle contact forces are
determined using the linear spring-dashpot (LSD) model. The distribution of the
mean co-ordination number, force during the contact, contact regimes, and
rotational coefficient of restitution are determined from the data obtained
from simulations. It was shown that $\kappa$ plays a significant role in the
distribution of inter-particle contacts between different regimes and, thereby,
the velocity distribution and profiles of statistically averaged properties of
the vibrofluidised particles. Our results show that for particles with surface
friction coefficient $\mu>0.1$, the commonly used value $\kappa=\frac{2}{7}$
results in quantitatively different results from those obtained using $0.67 \le
\kappa < 1$, a range consistent with the realistic values of Poisson ratios for
simple materials.",315,2412.16133v1,cond-mat.soft,cond-mat.soft,particle physics,2024-12-20,2024-12-23T21:06:55.409667
Revisiting Global Income Convergence in the 21st Century,"This paper revisits the debate on income convergence between poor and rich
countries. I challenge the view that there is little to no catch-up, and that
changes in total factor productivity (TFP) drives cross-country income
differences. Since 2000, income levels in poor countries have converged with
rich countries at 0.8% annually, rising to 1.5% when excluding Sub-Saharan
Africa. A growth accounting exercise incorporating capital income share
heterogeneity shows that most convergence since 1980, and over half since 2000
outside Sub-Saharan Africa, results from convergence in physical and human
capital inputs rather than TFP.",129,2412.16127v1,econ.GN,"econ.GN,q-fin.EC",particle physics,2024-12-20,2024-12-23T21:06:55.410664
Observational Properties of Harmonic EMIC waves: Statistical Study,"Electromagnetic ion cyclotron (EMIC) waves are discrete electromagnetic
emissions separated by multiple ion gyrofrequencies. Harmonic EMIC waves are
defined as waves with a strong electric or magnetic field (or both) at the
harmonics of the fundamental EMIC mode. In this paper, for the first time, we
present a statistical study on harmonic EMIC waves by the Van Allen Probes. The
EMIC waves are categorized into three types based on their harmonics: (1)
fundamental mode only (without higher harmonics), (2) electrostatic (ES)
harmonics, and (3) electromagnetic (EM) harmonics. Our statistical study shows
that ES and EM harmonic EMIC waves predominantly occur on the dayside, outside
the plasmasphere with $L >5$ and are associated with a low $f_{pe}/f_{ce}$, a
high proton $\beta_H$, and a strong fundamental EMIC mode. The results will
advance our understanding of harmonic EMIC waves and their generation
mechanisms.",217,2412.16124v1,physics.space-ph,physics.space-ph,particle physics,2024-12-20,2024-12-23T21:06:55.410664
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,particle physics,2024-12-20,2024-12-23T21:06:55.411662
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",particle physics,2024-12-20,2024-12-23T21:06:55.412659
Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli,"As humans perceive and actively engage with the world, we adjust our
decisions in response to shifting group dynamics and are influenced by social
interactions. This study aims to identify which aspects of interaction affect
cooperation-defection choices. Specifically, we investigate human cooperation
within the Prisoner's Dilemma game, using the Drift-Diffusion Model to describe
the decision-making process. We introduce a novel Bayesian model for the
evolution of the model's parameters based on the nature of interactions
experienced with other players. This approach enables us to predict the
evolution of the population's expected cooperation rate. We successfully
validate our model using an unseen test dataset and apply it to explore three
strategic scenarios: co-player manipulation, use of rewards and punishments,
and time pressure. These results support the potential of our model as a
foundational tool for developing and testing strategies aimed at enhancing
cooperation, ultimately contributing to societal welfare.",182,2412.16121v1,physics.soc-ph,physics.soc-ph,particle physics,2024-12-20,2024-12-23T21:06:55.412659
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",particle physics,2024-12-20,2024-12-23T21:06:55.413657
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",particle physics,2024-12-20,2024-12-23T21:06:55.414654
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",particle physics,2024-12-20,2024-12-23T21:06:55.414654
Full S-matrices and Witten diagrams with (relative) L-infinity algebras,"The $L_\infty$-algebra approach to scattering amplitudes elegantly describes
the nontrivial part of the $S$-matrix but fails to take into account the
trivial part. We argue that the trivial contribution to the $S$-matrix should
be accounted for by another, complementary $L_\infty$-algebra, such that a
perturbative field theory is described by a cyclic relative $L_\infty$-algebra.
We further demonstrate that this construction reproduces Witten diagrams that
arise in AdS/CFT including, in particular, the trivial Witten diagrams
corresponding to CFT two-point functions. We also discuss Chern-Simons theory
and Yang-Mills theory on manifolds with boundaries using this approach.",161,2412.16106v1,hep-th,"hep-th,math-ph,math.MP,81T18 (Primary) 81T13, 81T35, 17B56, 17B81 (Secondary)",particle physics,2024-12-20,2024-12-23T21:06:55.415651
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",particle physics,2024-12-20,2024-12-23T21:06:55.415651
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",particle physics,2024-12-20,2024-12-23T21:06:55.416649
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",particle physics,2024-12-20,2024-12-23T21:06:55.416649
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",particle physics,2024-12-20,2024-12-23T21:06:55.417646
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,particle physics,2024-12-20,2024-12-23T21:06:55.418643
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",particle physics,2024-12-20,2024-12-23T21:06:55.418643
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,particle physics,2024-12-20,2024-12-23T21:06:55.419641
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,particle physics,2024-12-20,2024-12-23T21:06:55.420638
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",particle physics,2024-12-20,2024-12-23T21:06:55.420638
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",particle physics,2024-12-20,2024-12-23T21:06:55.421635
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,particle physics,2024-12-20,2024-12-23T21:06:55.421635
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,particle physics,2024-12-20,2024-12-23T21:06:55.422633
Eigenvalue Bounds for Multi-Particle Reduced Density Matrices of Coulombic Wavefunctions,"For bound states of atoms and molecules of $N$ electrons we consider the
corresponding $K$-particle reduced density matrices, $\Gamma^{(K)}$, for $1 \le
K \le N-1$. Previously, eigenvalue bounds were obtained in the case of $K=1$
and $K=N-1$ by A.V. Sobolev. The purpose of the current work is to obtain
bounds in the case of $2 \le K \le N-2$. For such $K$ we label the eigenvalues
of the positive, trace class operators $\Gamma^{(K)}$ by
$\lambda_n(\Gamma^{(K)})$ for $n=1,2,\dots$, and obtain the bounds
$\lambda_n(\Gamma^{(K)}) \le Cn^{-\alpha_K}$ for all $n$, where $\alpha_K = 1 +
7/(3L)$ and $L = \min\{K,N-K\}$.",240,2412.16073v1,math-ph,"math-ph,math.MP,35J10",particle physics,2024-12-20,2024-12-23T21:06:55.423631
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",particle physics,2024-12-20,2024-12-23T21:06:55.424627
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",particle physics,2024-12-20,2024-12-23T21:06:55.425625
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",particle physics,2024-12-20,2024-12-23T21:06:55.425625
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",particle physics,2024-12-20,2024-12-23T21:06:55.427125
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",particle physics,2024-12-20,2024-12-23T21:06:55.427125
Approximation of Schrödinger operators with point interactions on bounded domains,"We consider Schr\""odinger operators on a bounded domain $\Omega\subset
\mathbb{R}^3$, with homogeneous Robin or Dirichlet boundary conditions on
$\partial\Omega$ and a point (zero-range) interaction placed at an interior
point of $\Omega$. We show that, under suitable spectral assumptions, and by
means of an extension-restriction procedure which exploit the already known
result on the entire space, the singular interaction is approximated by
rescaled sequences of regular potentials. The result is missing in the
literature, and we also take the opportunity to point out some general issues
in the approximation of point interactions and the role of zero energy
resonances.",146,2412.16056v1,math-ph,"math-ph,math.MP",particle physics,2024-12-20,2024-12-23T21:06:55.428123
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",particle physics,2024-12-20,2024-12-23T21:06:55.428123
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",particle physics,2024-12-20,2024-12-23T21:06:55.429120
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,particle physics,2024-12-20,2024-12-23T21:06:55.429120
Millikelvin Nb nanoSQUID-embedded tuneable resonator fabricated with a neon focused-ion-beam,"SQUID-embedded superconducting resonators are of great interest due to their
potential for coupling highly scalable superconducting circuits with quantum
memories based on solid-state spin ensembles. Such an application requires a
high-$Q$, frequency-tuneable resonator which is both resilient to magnetic
field, and able to operate at millikelvin temperatures. These requirements
motivate the use of a higher $H_{c}$ metal such as niobium, however the
challenge then becomes to sufficiently reduce the operating temperature. We
address this by presenting a monolithic Nb nanoSQUID-embedded resonator, where
neon focused-ion-beam fabrication of the nanoSQUID results in a device
displaying frequency tuneability at $T = 16$ mK. In order to assess the
applicability of the device for coupling to small spin clusters, we
characterise the flux sensitivity as a function of microwave drive power and
externally applied magnetic field, and find that the noise is dominated by
dielectric noise in the resonator. Finally, we discuss improvements to the
device design which can dramatically improve the flux sensitivity, which
highlights the promise of Nb SQUID-embedded resonators for hybrid
superconductor-spin applications.",261,2412.16045v1,quant-ph,"quant-ph,cond-mat.supr-con",particle physics,2024-12-20,2024-12-23T21:06:55.430118
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",particle physics,2024-12-20,2024-12-23T21:06:55.431115
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",particle physics,2024-12-20,2024-12-23T21:06:55.432112
Integrability versus chaos in the steady state of many-body open quantum systems,"The Lindblad description of an open quantum system gives rise to two types of
integrability, since the nonequilibrium steady state can be integrable
independently of the Liouvillian. Taking boundary-driven and dephasing spin
chains as a representative example, we discriminate Liouvillian and
steady-state chaos by combining level spacing statistics and an extension of
the eigenstate thermalization hypothesis to open quantum systems. Moreover, we
analyze the structure of the steady states by expanding it in the basis of
Pauli strings and comparing the weight of strings of different lengths. We show
that the natural expectation that integrable steady states are ""simple"" (i.e.,
built from few-body local operators) does not hold: the steady states of both
chaotic and integrable models have relevant contributions coming from Pauli
strings of all possible lengths, including long-range and many-body
interactions. Nevertheless, we show that one can effectively use the
operator-size distribution to distinguish chaotic and integrable steady states.",217,2412.16041v1,cond-mat.stat-mech,"cond-mat.stat-mech,cond-mat.mes-hall,nlin.CD,quant-ph",particle physics,2024-12-20,2024-12-23T21:06:55.432112
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",particle physics,2024-12-20,2024-12-23T21:06:55.433110
A moment approach for the convergence of spatial branching processes to the Continuum Random Tree,"We consider a general class of branching processes in discrete time, where
particles have types belonging to a Polish space and reproduce independently
according to their type. If the process is critical and the mean distribution
of types converges for large times, we prove that the tree structure of the
process converges to the Brownian Continuum Random Tree, under a moment
assumption. We provide a general approach to prove similar invariance
principles for branching processes, which relies on deducing the convergence of
the genealogy from computing its moments. These are obtained using a new
many-to-few formula, which provides an expression for the moments of order $k$
of a branching process in terms of a Markov chain indexed by a uniform tree
with $k$ leaves.",151,2412.16035v1,math.PR,math.PR,particle physics,2024-12-20,2024-12-23T21:06:55.433110
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",particle physics,2024-12-20,2024-12-23T21:06:55.434107
Non-stationary Aharonov-Bohm effect,"The non-stationary Aharonov-Bohm effect (scattering of electron in the field
of a narrow solenoid with alternating current) is considered. Using the eikonal
approximation, the wave function of electron, the differential and total
scattering cross sections are found. Unlike the case of direct current, the
total cross section in the case of alternating current turns out to be finite.
An oscillating asymmetry in the differential scattering cross section is
discovered. The possibility of experimental observation of the effect is
discussed.",107,2412.16030v1,physics.atom-ph,physics.atom-ph,particle physics,2024-12-20,2024-12-23T21:06:55.434107
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",condensed matter physics,2024-12-20,2024-12-23T21:06:56.196649
Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?,"The animation of realistic virtual avatars in crowd scenarios is an important
element of immersive virtual environments. However, achieving this realism
requires attention to multiple factors, such as their visual appearance and
motion cues. We investigated how body shape diversity influences the perception
of motion clones in virtual crowds. A physics-based model was used to simulate
virtual avatars in a small-scale crowd of size twelve. Participants viewed
side-by-side video clips of these virtual crowds: one featuring all unique
motions (Baseline) and the other containing motion clones (i.e., the same
motion used to animate two or more avatars in the crowd). We also varied the
levels of body shape and motion diversity. Our findings revealed that body
shape diversity did not influence participants' ratings of motion clone
detection, and motion variety had a greater impact on their perception of the
crowd. Further research is needed to investigate how other visual factors
interact with motion in order to enhance the perception of virtual crowd
realism.",200,2412.16151v1,cs.HC,"cs.HC,cs.GR",condensed matter physics,2024-12-20,2024-12-23T21:06:56.196649
The Classical Super-Phaserotation Infrared Triangle,"The universality of the logarithmic soft photon theorem in four dimensions
can be traced to an infinite-dimensional asymptotic symmetry which acts as a
local phase rotation on matter as we have shown in 2403.13053. Here we extend
our earlier results for the charges associated to these superphaserotations to
all orders in the coupling and prove that their conservation is exactly the
classical logarithmic soft photon theorem discovered by Saha, Sahoo and Sen in
1912.06413. We furthermore generalize the formulae for the associated
electromagnetic displacement memory and its tail from particles to scalar
matter fields. This completes the classical superphaserotation infrared
triangle.",142,2412.16149v1,hep-th,hep-th,condensed matter physics,2024-12-20,2024-12-23T21:06:56.197647
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",condensed matter physics,2024-12-20,2024-12-23T21:06:56.198644
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",condensed matter physics,2024-12-20,2024-12-23T21:06:56.199642
Borel singularities and Stokes constants of the topological string free energy on one-parameter Calabi-Yau threefolds,"We study the Borel plane of the topological string free energy on all
hypergeometric one-parameter Calabi-Yau models close to singular points in
moduli space, focusing on the location of Borel singularities and the value of
the associated Stokes constants. We find in particular that in models which
exhibit massless D-branes at a singular point, the central charge of the
D-brane close to the singular point coincides with the location of the leading
Borel singularity, and the generalized Donaldson-Thomas invariant associated to
the charge of the D-brane, in as far as its value is known, coincides with the
Stokes constant associated to the Borel singularity.",144,2412.16140v1,hep-th,"hep-th,math.AG",condensed matter physics,2024-12-20,2024-12-23T21:06:56.199642
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",condensed matter physics,2024-12-20,2024-12-23T21:06:56.200638
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.201636
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",condensed matter physics,2024-12-20,2024-12-23T21:06:56.202633
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",condensed matter physics,2024-12-20,2024-12-23T21:06:56.202633
Role of the ratio of tangential to normal stiffness coefficient on the behaviour of vibrofluidised particles,"The selection of parameters in the contact law for inter-particle
interactions affects the results of simulations of flowing granular materials.
The present study aims to understand the effect of the ratio of tangential to
normal spring stiffness coefficient ($\kappa$) on inter-particle contact
behaviour in terms of the rotational coefficient of restitution determined
using data obtained from multi-particle simulations. The effect of $\kappa$ on
the profiles of the micro- and macroscopic properties of particles in a
vibrofluidised bed is also investigated. The Discrete Element Method (DEM) is
used to simulate a vertically vibrated fluidised bed using the open-source
software LAMMPS. The inter-particle and wall-particle contact forces are
determined using the linear spring-dashpot (LSD) model. The distribution of the
mean co-ordination number, force during the contact, contact regimes, and
rotational coefficient of restitution are determined from the data obtained
from simulations. It was shown that $\kappa$ plays a significant role in the
distribution of inter-particle contacts between different regimes and, thereby,
the velocity distribution and profiles of statistically averaged properties of
the vibrofluidised particles. Our results show that for particles with surface
friction coefficient $\mu>0.1$, the commonly used value $\kappa=\frac{2}{7}$
results in quantitatively different results from those obtained using $0.67 \le
\kappa < 1$, a range consistent with the realistic values of Poisson ratios for
simple materials.",315,2412.16133v1,cond-mat.soft,cond-mat.soft,condensed matter physics,2024-12-20,2024-12-23T21:06:56.203630
Revisiting Global Income Convergence in the 21st Century,"This paper revisits the debate on income convergence between poor and rich
countries. I challenge the view that there is little to no catch-up, and that
changes in total factor productivity (TFP) drives cross-country income
differences. Since 2000, income levels in poor countries have converged with
rich countries at 0.8% annually, rising to 1.5% when excluding Sub-Saharan
Africa. A growth accounting exercise incorporating capital income share
heterogeneity shows that most convergence since 1980, and over half since 2000
outside Sub-Saharan Africa, results from convergence in physical and human
capital inputs rather than TFP.",129,2412.16127v1,econ.GN,"econ.GN,q-fin.EC",condensed matter physics,2024-12-20,2024-12-23T21:06:56.204628
Observational Properties of Harmonic EMIC waves: Statistical Study,"Electromagnetic ion cyclotron (EMIC) waves are discrete electromagnetic
emissions separated by multiple ion gyrofrequencies. Harmonic EMIC waves are
defined as waves with a strong electric or magnetic field (or both) at the
harmonics of the fundamental EMIC mode. In this paper, for the first time, we
present a statistical study on harmonic EMIC waves by the Van Allen Probes. The
EMIC waves are categorized into three types based on their harmonics: (1)
fundamental mode only (without higher harmonics), (2) electrostatic (ES)
harmonics, and (3) electromagnetic (EM) harmonics. Our statistical study shows
that ES and EM harmonic EMIC waves predominantly occur on the dayside, outside
the plasmasphere with $L >5$ and are associated with a low $f_{pe}/f_{ce}$, a
high proton $\beta_H$, and a strong fundamental EMIC mode. The results will
advance our understanding of harmonic EMIC waves and their generation
mechanisms.",217,2412.16124v1,physics.space-ph,physics.space-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.204628
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.205625
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",condensed matter physics,2024-12-20,2024-12-23T21:06:56.206622
Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli,"As humans perceive and actively engage with the world, we adjust our
decisions in response to shifting group dynamics and are influenced by social
interactions. This study aims to identify which aspects of interaction affect
cooperation-defection choices. Specifically, we investigate human cooperation
within the Prisoner's Dilemma game, using the Drift-Diffusion Model to describe
the decision-making process. We introduce a novel Bayesian model for the
evolution of the model's parameters based on the nature of interactions
experienced with other players. This approach enables us to predict the
evolution of the population's expected cooperation rate. We successfully
validate our model using an unseen test dataset and apply it to explore three
strategic scenarios: co-player manipulation, use of rewards and punishments,
and time pressure. These results support the potential of our model as a
foundational tool for developing and testing strategies aimed at enhancing
cooperation, ultimately contributing to societal welfare.",182,2412.16121v1,physics.soc-ph,physics.soc-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.206622
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",condensed matter physics,2024-12-20,2024-12-23T21:06:56.207620
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",condensed matter physics,2024-12-20,2024-12-23T21:06:56.208617
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",condensed matter physics,2024-12-20,2024-12-23T21:06:56.209616
Full S-matrices and Witten diagrams with (relative) L-infinity algebras,"The $L_\infty$-algebra approach to scattering amplitudes elegantly describes
the nontrivial part of the $S$-matrix but fails to take into account the
trivial part. We argue that the trivial contribution to the $S$-matrix should
be accounted for by another, complementary $L_\infty$-algebra, such that a
perturbative field theory is described by a cyclic relative $L_\infty$-algebra.
We further demonstrate that this construction reproduces Witten diagrams that
arise in AdS/CFT including, in particular, the trivial Witten diagrams
corresponding to CFT two-point functions. We also discuss Chern-Simons theory
and Yang-Mills theory on manifolds with boundaries using this approach.",161,2412.16106v1,hep-th,"hep-th,math-ph,math.MP,81T18 (Primary) 81T13, 81T35, 17B56, 17B81 (Secondary)",condensed matter physics,2024-12-20,2024-12-23T21:06:56.209616
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",condensed matter physics,2024-12-20,2024-12-23T21:06:56.210612
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",condensed matter physics,2024-12-20,2024-12-23T21:06:56.210612
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.211609
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",condensed matter physics,2024-12-20,2024-12-23T21:06:56.212606
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.213605
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",condensed matter physics,2024-12-20,2024-12-23T21:06:56.213605
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.214603
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.215600
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.216597
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",condensed matter physics,2024-12-20,2024-12-23T21:06:56.216597
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.217594
Eigenvalue Bounds for Multi-Particle Reduced Density Matrices of Coulombic Wavefunctions,"For bound states of atoms and molecules of $N$ electrons we consider the
corresponding $K$-particle reduced density matrices, $\Gamma^{(K)}$, for $1 \le
K \le N-1$. Previously, eigenvalue bounds were obtained in the case of $K=1$
and $K=N-1$ by A.V. Sobolev. The purpose of the current work is to obtain
bounds in the case of $2 \le K \le N-2$. For such $K$ we label the eigenvalues
of the positive, trace class operators $\Gamma^{(K)}$ by
$\lambda_n(\Gamma^{(K)})$ for $n=1,2,\dots$, and obtain the bounds
$\lambda_n(\Gamma^{(K)}) \le Cn^{-\alpha_K}$ for all $n$, where $\alpha_K = 1 +
7/(3L)$ and $L = \min\{K,N-K\}$.",240,2412.16073v1,math-ph,"math-ph,math.MP,35J10",condensed matter physics,2024-12-20,2024-12-23T21:06:56.218591
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.218591
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",condensed matter physics,2024-12-20,2024-12-23T21:06:56.219589
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",condensed matter physics,2024-12-20,2024-12-23T21:06:56.220586
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",condensed matter physics,2024-12-20,2024-12-23T21:06:56.220586
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",condensed matter physics,2024-12-20,2024-12-23T21:06:56.221584
Approximation of Schrödinger operators with point interactions on bounded domains,"We consider Schr\""odinger operators on a bounded domain $\Omega\subset
\mathbb{R}^3$, with homogeneous Robin or Dirichlet boundary conditions on
$\partial\Omega$ and a point (zero-range) interaction placed at an interior
point of $\Omega$. We show that, under suitable spectral assumptions, and by
means of an extension-restriction procedure which exploit the already known
result on the entire space, the singular interaction is approximated by
rescaled sequences of regular potentials. The result is missing in the
literature, and we also take the opportunity to point out some general issues
in the approximation of point interactions and the role of zero energy
resonances.",146,2412.16056v1,math-ph,"math-ph,math.MP",condensed matter physics,2024-12-20,2024-12-23T21:06:56.221584
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.222581
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",condensed matter physics,2024-12-20,2024-12-23T21:06:56.222581
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,condensed matter physics,2024-12-20,2024-12-23T21:06:56.223578
Millikelvin Nb nanoSQUID-embedded tuneable resonator fabricated with a neon focused-ion-beam,"SQUID-embedded superconducting resonators are of great interest due to their
potential for coupling highly scalable superconducting circuits with quantum
memories based on solid-state spin ensembles. Such an application requires a
high-$Q$, frequency-tuneable resonator which is both resilient to magnetic
field, and able to operate at millikelvin temperatures. These requirements
motivate the use of a higher $H_{c}$ metal such as niobium, however the
challenge then becomes to sufficiently reduce the operating temperature. We
address this by presenting a monolithic Nb nanoSQUID-embedded resonator, where
neon focused-ion-beam fabrication of the nanoSQUID results in a device
displaying frequency tuneability at $T = 16$ mK. In order to assess the
applicability of the device for coupling to small spin clusters, we
characterise the flux sensitivity as a function of microwave drive power and
externally applied magnetic field, and find that the noise is dominated by
dielectric noise in the resonator. Finally, we discuss improvements to the
device design which can dramatically improve the flux sensitivity, which
highlights the promise of Nb SQUID-embedded resonators for hybrid
superconductor-spin applications.",261,2412.16045v1,quant-ph,"quant-ph,cond-mat.supr-con",condensed matter physics,2024-12-20,2024-12-23T21:06:56.223578
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.224576
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",condensed matter physics,2024-12-20,2024-12-23T21:06:56.225574
Integrability versus chaos in the steady state of many-body open quantum systems,"The Lindblad description of an open quantum system gives rise to two types of
integrability, since the nonequilibrium steady state can be integrable
independently of the Liouvillian. Taking boundary-driven and dephasing spin
chains as a representative example, we discriminate Liouvillian and
steady-state chaos by combining level spacing statistics and an extension of
the eigenstate thermalization hypothesis to open quantum systems. Moreover, we
analyze the structure of the steady states by expanding it in the basis of
Pauli strings and comparing the weight of strings of different lengths. We show
that the natural expectation that integrable steady states are ""simple"" (i.e.,
built from few-body local operators) does not hold: the steady states of both
chaotic and integrable models have relevant contributions coming from Pauli
strings of all possible lengths, including long-range and many-body
interactions. Nevertheless, we show that one can effectively use the
operator-size distribution to distinguish chaotic and integrable steady states.",217,2412.16041v1,cond-mat.stat-mech,"cond-mat.stat-mech,cond-mat.mes-hall,nlin.CD,quant-ph",condensed matter physics,2024-12-20,2024-12-23T21:06:56.226572
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",condensed matter physics,2024-12-20,2024-12-23T21:06:56.227568
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",condensed matter physics,2024-12-20,2024-12-23T21:06:56.227568
Non-stationary Aharonov-Bohm effect,"The non-stationary Aharonov-Bohm effect (scattering of electron in the field
of a narrow solenoid with alternating current) is considered. Using the eikonal
approximation, the wave function of electron, the differential and total
scattering cross sections are found. Unlike the case of direct current, the
total cross section in the case of alternating current turns out to be finite.
An oscillating asymmetry in the differential scattering cross section is
discovered. The possibility of experimental observation of the effect is
discussed.",107,2412.16030v1,physics.atom-ph,physics.atom-ph,condensed matter physics,2024-12-20,2024-12-23T21:06:56.227568
CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images,"3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.",246,2412.16028v1,cs.CV,cs.CV,condensed matter physics,2024-12-20,2024-12-23T21:06:56.229069
Entropy maximizers for kinetic wave equations set on tori,"We consider the kinetic wave equation, or phonon Boltzmann equation, set on
the torus (physical system set on the lattice). We describe entropy maximizers
for fixed mass and energy; our framework is very general, being valid in any
dimension, for any dispersion relation, and even including the quantum kinetic
wave equation. Of particular interest is the presence of condensation in
certain regimes which we characterize.",89,2412.16026v1,math.AP,"math.AP,math-ph,math.MP",condensed matter physics,2024-12-20,2024-12-23T21:06:56.229069
Role of the ratio of tangential to normal stiffness coefficient on the behaviour of vibrofluidised particles,"The selection of parameters in the contact law for inter-particle
interactions affects the results of simulations of flowing granular materials.
The present study aims to understand the effect of the ratio of tangential to
normal spring stiffness coefficient ($\kappa$) on inter-particle contact
behaviour in terms of the rotational coefficient of restitution determined
using data obtained from multi-particle simulations. The effect of $\kappa$ on
the profiles of the micro- and macroscopic properties of particles in a
vibrofluidised bed is also investigated. The Discrete Element Method (DEM) is
used to simulate a vertically vibrated fluidised bed using the open-source
software LAMMPS. The inter-particle and wall-particle contact forces are
determined using the linear spring-dashpot (LSD) model. The distribution of the
mean co-ordination number, force during the contact, contact regimes, and
rotational coefficient of restitution are determined from the data obtained
from simulations. It was shown that $\kappa$ plays a significant role in the
distribution of inter-particle contacts between different regimes and, thereby,
the velocity distribution and profiles of statistically averaged properties of
the vibrofluidised particles. Our results show that for particles with surface
friction coefficient $\mu>0.1$, the commonly used value $\kappa=\frac{2}{7}$
results in quantitatively different results from those obtained using $0.67 \le
\kappa < 1$, a range consistent with the realistic values of Poisson ratios for
simple materials.",315,2412.16133v1,cond-mat.soft,cond-mat.soft,materials chemistry,2024-12-20,2024-12-23T21:06:56.987621
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials chemistry,2024-12-20,2024-12-23T21:06:56.989616
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",materials chemistry,2024-12-20,2024-12-23T21:06:56.989616
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",materials chemistry,2024-12-20,2024-12-23T21:06:56.991610
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:56.991610
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",materials chemistry,2024-12-20,2024-12-23T21:06:56.992609
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",materials chemistry,2024-12-20,2024-12-23T21:06:56.993606
Integral representation for a relaxed optimal design problem for non-simple grade two materials,"A measure representation result for a functional modelling optimal design
problems for plastic deformations, under linear growth conditions, is obtained.
  Departing from an energy with a bulk term depending on the second gradient,
as well as a perimeter term, the functional in question corresponds to the
relaxation of this energy with respect to a pair $(\chi,u)$, where $\chi$ is
the characteristic function of a set of finite perimeter and $u$ is a function
of bounded hessian.",99,2412.16027v1,math.AP,"math.AP,math.OC,49J45, 49Q20, 26B25",materials chemistry,2024-12-20,2024-12-23T21:06:56.994603
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",materials chemistry,2024-12-20,2024-12-23T21:06:56.994603
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",materials chemistry,2024-12-20,2024-12-23T21:06:56.995601
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:56.995601
Identifying and quantifying Su-Schrieffer-Heeger-like interactions with RIXS,"Su-Schrieffer-Heeger (SSH)-like electron-phonon (e-ph) interactions can drive
the formation of light (bi)polarons and several novel states of matter. It is,
therefore, prudent to develop experimental protocols for identifying such
couplings in real materials and quantifying their strength. Here, we
investigate how resonant inelastic x-ray scattering (RIXS) probes e-ph
interactions in the one-dimensional half-filled Hubbard-SSH model with onsite
phonons. Using the density matrix renormalization group method, we compute the
full RIXS response and find that the lattice excitations generated during the
scattering process inevitably couple to the system's charge and magnetic
sectors, resulting in combined multi-particle excitations that cannot be easily
disentangled from one another. While this aspect complicates the interpretation
of RIXS experiments, we outline how it can be leveraged to identify and
quantify SSH-like interactions in quantum materials.",220,2412.15981v1,cond-mat.str-el,cond-mat.str-el,materials chemistry,2024-12-20,2024-12-23T21:06:56.996598
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials chemistry,2024-12-20,2024-12-23T21:06:56.997595
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",materials chemistry,2024-12-20,2024-12-23T21:06:56.997595
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",materials chemistry,2024-12-20,2024-12-23T21:06:56.998592
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:56.998592
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:56.999590
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials chemistry,2024-12-20,2024-12-23T21:06:57.000587
IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing,"In inverse rendering, accurately modeling visibility and indirect radiance
for incident light is essential for capturing secondary effects. Due to the
absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have
either adopted a simplified rendering equation or used learnable parameters to
approximate incident light, resulting in inaccurate material and lighting
estimations. To this end, we introduce inter-reflective Gaussian splatting
(IRGS) for inverse rendering. To capture inter-reflection, we apply the full
rendering equation without simplification and compute incident radiance on the
fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we
present an efficient optimization scheme to handle the computational demands of
Monte Carlo sampling for rendering equation evaluation. Furthermore, we
introduce a novel strategy for querying the indirect radiance of incident light
when relighting the optimized scenes. Extensive experiments on multiple
standard benchmarks validate the effectiveness of IRGS, demonstrating its
capability to accurately model complex inter-reflection effects.",207,2412.15867v1,cs.CV,cs.CV,materials chemistry,2024-12-20,2024-12-23T21:06:57.000587
Revealing spin-flip two-level systems using ultra-thin film superconducting resonators,"Material disorders are one of the major sources of noise and loss in
solid-state quantum devices, whose behaviors are often modeled as two-level
systems (TLSs) formed by charge tunneling between neighboring sites. However,
the role of their spins in tunneling and its impact on device performance
remain highly unexplored. In this work, employing ultra-thin TiN
superconducting resonators, we reveal anomalous TLS behaviors by demonstrating
an unexpected increase in resonant frequency at low magnetic fields.
Furthermore, a spin-flip TLS model is proposed, in which an effective
spin-orbit coupling is generated by inhomogeneous local magnetic fields from
defect spins. This mechanism mixes charge tunnelings and spin flips,
quantitatively reproducing the observed frequency-field relationship and its
temperature dependence. This work deepens the understanding of spin-dependent
TLS behaviors, offering the possibility of magnetically engineering noise and
loss in solid-state quantum devices.",201,2412.15856v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",materials chemistry,2024-12-20,2024-12-23T21:06:57.001584
Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis,"Complying with traffic rules is challenging for automated vehicles, as
numerous rules need to be considered simultaneously. If a planned trajectory
violates traffic rules, it is common to replan a new trajectory from scratch.
We instead propose a trajectory repair technique to save computation time. By
coupling satisfiability modulo theories with set-based reachability analysis,
we determine if and in what manner the initial trajectory can be repaired.
Experiments in high-fidelity simulators and in the real world demonstrate the
benefits of our proposed approach in various scenarios. Even in complex
environments with intricate rules, we efficiently and reliably repair
rule-violating trajectories, enabling automated vehicles to swiftly resume
legally safe operation in real-time.",146,2412.15837v1,cs.RO,"cs.RO,cs.AI",materials chemistry,2024-12-20,2024-12-23T21:06:57.001584
Efficient Hamiltonian Simulation: A Utility Scale Perspective for Covalent Inhibitor Reactivity Prediction,"Quantum computing applications in the noisy intermediate-scale quantum (NISQ)
era demand algorithms capable of generating shallower circuits that are
feasible to run on today's quantum systems. This is a challenge, particularly
for quantum chemistry applications, considering the inherent complexity of
molecular systems. In this paper, we demonstrate advancements that expand the
size of chemistry problems that can be run on today's quantum systems by
applying hardware-efficient approaches, such as Quantum-Centric Data-Driven
Research and Development (QDDRD), optimized algorithms with reduced circuit
depth, and execute the experiments with middleware-supported quantum error
mitigation. We report up to a 29-fold reduction in circuit depth for covalent
drug molecules, enabling Hamiltonian dynamics for reactivity predictions,
assuming all-to-all connectivity of quantum hardware. When employed on IBMQ's
Heron architecture, we see up to a 16-fold reduction. The overarching impact of
this work is that it highlights promising methods that allow researchers to
explore the dynamics of commercially relevant chemistry on real quantum
hardware via Hamiltonian simulation.",223,2412.15804v1,quant-ph,quant-ph,materials chemistry,2024-12-20,2024-12-23T21:06:57.002582
Elementary theory of Magnetoferrons: bringing magnons and ferrons together in multiferroic systems,"The collective excitations of a multiferroic material are analyzed. We show
that these excitations also exhibit magnetoelectric behavior, leading to the
hybridization of magnons ,oscillations of the magnetization field, and ferrons,
which are oscillations of the electric dipolar density field. We term these
emergent entities 'magnetoferrons', study their main properties, and discuss
their potential applications. Additionally, we provide a phenomenological
framework for these systems, which will be invaluable for describing the
dynamics of the multiferromagnetic state.",129,2412.15796v1,cond-mat.mes-hall,cond-mat.mes-hall,materials chemistry,2024-12-20,2024-12-23T21:06:57.002582
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",materials chemistry,2024-12-20,2024-12-23T21:06:57.003579
Coherent Interactions of Free Electrons and Matter: Toward Tunable Compact X-ray Sources,"Compact laboratory-scale X-ray sources still rely on the same fundamental
principles as in the first X-ray tubes developed more than a century ago. In
recent years, significant research and development have focused on large-scale
X-ray sources such as synchrotrons and free-electron lasers, leading to the
generation of high-brightness coherent X-rays. However, the large size and high
costs of such sources prevent their widespread use. The quest for a compact and
coherent Xray source has long been a critical objective in modern physics,
gaining further importance in recent years for industrial applications and
fundamental scientific research. Here, we review the physical mechanisms
governing compact coherent X-ray generation. Of current interest are coherent
periodic interactions of free electrons in crystalline materials, creating hard
X-rays via a mechanism known as parametric X-ray radiation (PXR). Over the past
decade, X-ray sources leveraging this mechanism have demonstrated
state-of-the-art tunability, directionality, and broad spatial coherence,
enabling X-ray phase-contrast imaging on a compact scale. The coming years are
expected to show substantial miniaturization of compact X-ray sources,
facilitated by progress in electron beam technologies. This review compares the
most promising mechanisms used for hard-X-ray generation, contrasting
parametric X-ray radiation with inverse Compton scattering and characteristic
radiation from a liquid-jet anode. We cover the most recent advancements,
including the development of new materials, innovative geometrical designs, and
specialized optimization techniques, aiming toward X-ray flux levels suitable
for medical imaging and X-ray spectroscopy in compact scales.",334,2412.15775v1,physics.app-ph,"physics.app-ph,physics.optics",materials chemistry,2024-12-20,2024-12-23T21:06:57.004576
Stabilization of active tissue deformation by a dynamic morphogen gradient,"A key process during animal morphogenesis is oriented tissue deformation,
which is often driven by internally generated active stresses. Yet, such active
oriented materials are prone to well-known instabilities, raising the question
of how oriented tissue deformation can be robust during morphogenesis. In a
simple scenario, we recently showed that active oriented deformation can be
stabilized by the boundary-imposed gradient of a scalar field, which
represents, e.g., a morphogen gradient in a developing embryo. Here, we discuss
a more realistic scenario, where the morphogen is produced by a localized
source region, diffuses across the tissue, and degrades. Consistent with our
earlier results, we find that oriented tissue deformation is stable in the
gradient-extensile case, i.e. when active stresses act to extend the tissue
along the direction of the gradient, but it is unstable in the
gradient-contractile case. In addition, we now show that gradient-contractile
tissues can not be stabilized even by morphogen diffusion. Finally, we point
out the existence of an additional instability, which results from the
interplay of tissue shear and morphogen diffusion. Our theoretical results
explain the lack of gradient-contractile tissues in the biological literature,
suggesting that the active matter instability acts as an evolutionary selection
criterion.",271,2412.15774v1,cond-mat.soft,"cond-mat.soft,q-bio.TO",materials chemistry,2024-12-20,2024-12-23T21:06:57.005573
Network-forming phase separation of oppositely charged polyelectrolytes forming coacervates in a solvent,"The formation of coacervates through phase separation of oppositely charged
polyelectrolytes (PEs) is critical for understanding biological condensates and
developing responsive materials. Traditionally, coacervates are viewed as
spherical droplets with growth dynamics resembling liquid-liquid phase
separation. However, our fluid particle dynamics simulations incorporating
hydrodynamic and electrostatic interactions challenge this perspective. Here,
we find that oppositely charged PEs form a percolated network even in
semi-dilute solutions, coarsening with a unique growth law, $\ell \propto
t^{1/2}$. This self-similarity, absent for neutral polymers in poor solvents,
arises because PEs in good solvents exhibit weaker, longer-range attractions
due to spatial charge inhomogeneity under global charge neutrality. This
results in a lower density of the PEs-rich phase and reduced interfacial
tension. Increased charge asymmetry further slows network coarsening.
Additionally, coacervate droplets initially display irregular shapes due to
weak interfacial tension, transitioning slowly to spherical forms. Our research
provides new insights into coacervate morphology and coarsening dynamics.",243,2412.15753v1,cond-mat.soft,"cond-mat.soft,physics.bio-ph,physics.chem-ph",materials chemistry,2024-12-20,2024-12-23T21:06:57.006570
Photon proliferation from $N$-body dark matter annihilation,"We demonstrate a photon proliferation effect from $N$-body dark matter (DM)
annihilation in the early Universe, which can induce a drastic
photon-temperature shift after neutrino decoupling. For pseudoscalar DM mass
below the eV scale, we show that the photon proliferation effect becomes
significant as the mass approaches the ultralight end, due to the huge
enhancement from the background DM number density. This presents the leading
constraints on the DM-photon coupling, DM self-interaction, and DM-electron
coupling, which are stronger than the existing bounds up to several orders of
magnitude. The present research can be extended to other interactions and DM
candidates, and highlights the importance of multi-body processes in the early
Universe.",156,2412.15749v1,hep-ph,"hep-ph,astro-ph.CO",materials chemistry,2024-12-20,2024-12-23T21:06:57.006570
Building Bridges: AI Custom Chatbots as Mediators between Mathematics and Physics,"This work explores the integration of AI custom chatbots in educational
settings, with a particular focus on their applicability in the context of
mathematics and physics. In view of the increasing deployment of AI tools such
as ChatGPT in educational contexts, the present study examines their potential
as personalized tutoring systems. The study assesses the impact of AI-generated
learning materials on the learning experiences and performance of sixth-grade
students, with a particular focus on proportional relationships in mathematical
and physical contexts. The randomized controlled study with N = 214 students
compared traditional textbook materials with explanations generated by a custom
chatbot. The results demonstrated that while AI-generated materials had an
indefinite impact on learning outcomes, they significantly enhanced
positive-activating emotions, situational interest, and self-efficacy, while
reducing intrinsic and extrinsic cognitive load. These findings underscore the
potential of AI to transform educational practices by fostering a superior
learning experience. However, further research is required to clarify its
impact on learning performance and long-term learning outcomes. The study
highlights the importance of careful integration and customization of AI tools
to maximize their benefits in physics education.",233,2412.15747v1,physics.ed-ph,physics.ed-ph,materials chemistry,2024-12-20,2024-12-23T21:06:57.007568
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:57.007568
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:57.008565
Strain tuning of the nonlinear anomalous Hall effect in MoS2 monolayer,"Due to the time reversal symmetry, the linear anomalous Hall effect (AHE)
usually vanishes in MoS2 monolayer. In contrast, the nonlinear AHE plays an
essential role in such system when the uniaxial strain breaks the C3v symmetry
and eventually results in the nonzero Berry curvature dipole (BCD). We find
that not only the magnitude of the AHE but also the nonlinear Hall angle can be
tuned by the strain. Especially the nonlinear Hall angle exhibits a deep
relationship which is analogy to the birefraction phenomenon in optics. It
actually results from the pseudotensor nature of the BCD moment. Besides the
ordinary positive and negative crystals in optics, there are two more
birefraction-like cases corresponding to an imaginary refraction index ratio in
monolayer MoS2. Our findings shed lights on the strain controlled electronic
devices based on the two-dimensional (2D) materials with BCD.",195,2412.15659v1,cond-mat.mes-hall,cond-mat.mes-hall,materials chemistry,2024-12-20,2024-12-23T21:06:57.008565
Unified real-space construction scheme for flat bands based on symmetric compact localized states,"Flat band (FB) systems provide ideal playgrounds for studying correlation
physics, whereas multi-orbital characteristics in real materials are
distinguished from most simple FB models. Here, we propose a systematic and
versatile framework for FB constructions in tight-binding (TB) models based on
symmetric compact localized states (CLSs), integrating lattice and orbital
degrees of freedom. We first demonstrate that any CLS can be symmetrized into a
representation of the point group, which remains valid for high orbitals with
finite spin-orbit coupling (SOC). Second, we determine the candidate CLS sites
according to lattice symmetry, and simplify the hopping as a linear mapping
between two Hilbert spaces: one of CLS sites and another of their adjacent
sites. The existence of FBs depends on a non-empty kernel of the mapping.
Finally, we distinguish eigenstates in the kernel to qualify as a CLS. To
illustrate the versatility of our framework, we construct three representative
FB models: one in two dimensions (2D) and the rest in three dimensions (3D).
All of them lack special lattice structures and incorporate high orbitals.
Notably, the 3D FBs can exhibit not only band touchings at points but also
along lines, a feature of significant physical interest. For a comprehensive
understanding, we derive a concise criterion for determining band touchings,
which provides a natural explanation for the occurrence of both gapped and
gapless FBs. By unifying symmetry principles in real space, our work offers a
systematic approach to constructing FBs across diverse lattice systems. This
framework opens new avenues for understanding and engineering FB systems, with
potential implications for correlated quantum phenomena and exotic phases of
matter.",346,2412.15653v1,cond-mat.mes-hall,cond-mat.mes-hall,materials chemistry,2024-12-20,2024-12-23T21:06:57.010560
Two-Dimensional Graphene: Theoretical Study of Multi-photon Non-linear Absorption Coefficient of a Strong Electromagnetic Wave by Using Quantum Kinetic Equation,"Based on the quantum kinetic equation for electrons, we theoretically study
the quantum multi-photon non-linear absorption of a strong electromagnetic wave
(EMW) in two-dimensional graphene. Two cases of the electron scattering
mechanism are considered: Electron-optical phonon scattering and
electron-acoustic phonon scattering. The general multi-photon absorption
coefficient is presented as a function of the temperature, the external
magnetic field, the photon energy and the amplitude of external EMW. These
analytical expressions for multi-photon non-linear absorption coefficient
(MNAC) are numerically calculated and the results are discussed in both the
absence and presence of a magnetic field perpendicular to the graphene sheet.
The results show that there is no absorption peak in the absence of the
magnetic field, which contrasts with previous results in 2D systems such as
quantum wells or superlattices. However, when there is a strong magnetic field
along the direction perpendicular to the 2D graphene, absorption spectral lines
appear consistent with the magneto-phonon resonance conditions. Our
calculations show that the MPA's effect is stronger than mono-photon
absorption. Besides, the quantum multi-photon non-linear absorption phenomenon
has been studied from low to high temperatures. This transcends the limits of
the classical BKE which is studied in the high-temperature domain. The
computational results show that the dependence of MNAC on the above quantities
is consistent with the previous theoretical investigation. Another novel
feature of this work is that the general analytic expression for MNAC shows the
Half Width at Half Maximum dependence on the magnetic field which is in good
agreement with the previous experimental observations. Thus, our estimation
might give a critical prediction for future experimental observations in 2D
graphene.",347,2412.15638v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:57.011557
Theoretical study of Magnetoresistance Oscillations in Semi-parabolic Plus Semi-inverse Squared Quantum Wells in the Presence of Intense Electromagnetic Waves,"Magnetoresistance oscillations in semiconductor quantum wells, with the
semi-parabolic plus semi-inverse squared potential, under the influence of
intense electromagnetic waves (IEMW), is studied theoretically. Analytical
expression for the longitudinal magnetoresistance (LMR) is derived from the
quantum kinetic equation for electrons, using the Fr\""ohlich Hamiltonian of the
electron-acoustic phonon system. Numerical calculation results show the complex
dependence of LMR on the parameters of the external field (electric, magnetic
field and temperature) as well as the structure parameters of the confinement
potential. In the absence of IMEW, Shubnikov-de Haas (SdH) oscillations appear
with amplitudes that decrease with temperature in agreement with previous
theoretical and experimental results. In the presence of IEMW, the SdH
oscillations appear in beats with amplitudes that increase with the intensity
of the IEMW. SdH oscillations under the influence of electromagnetic waves are
called microwave-induced magnetoresistance oscillations. The maximum and
minimum peaks appear at the positions where the IEMW frequencies are integer
and half-integer values of the cyclotron frequency, respectively. In addition,
the structural parameters of the quantum well such as the confinement frequency
and the geometrical parameters have a significant influence on the LMR as well
as the SdH oscillations. When the confinement frequency is small, the
two-dimensional electronic system in the quantum well behaves as a bulk
semiconductor, resulting in the absence of SdH oscillations. In addition, the
LMR increases with the geometrical parameter $\beta_z$ of the quantum well. The
obtained results provide a solid theoretical foundation for the possibility of
controlling SdH oscillations by IEMW as well as the structural properties of
materials in future experimental observations.",390,2412.15630v1,cond-mat.mes-hall,cond-mat.mes-hall,materials chemistry,2024-12-20,2024-12-23T21:06:57.012554
Room-temperature nonlinear transport and microwave rectification in antiferromagnetic MnBi$_2$Te$_4$ films,"The discovery of the nonlinear Hall effect provides an avenue for studying
the interplay among symmetry, topology, and phase transitions, with potential
applications in signal doubling and high-frequency rectification. However,
practical applications require devices fabricated on large area thin film as
well as room-temperature operation. Here, we demonstrate robust
room-temperature nonlinear transverse response and microwave rectification in
MnBi$_2$Te$_4$ films grown by molecular beam epitaxy. We observe multiple
sign-reversals in the nonlinear response by tuning the chemical potential.
Through theoretical analysis, we identify skew scattering and side jump,
arising from extrinsic spin-orbit scattering, as the main mechanisms underlying
the observed nonlinear signals. Furthermore, we demonstrate radio frequency
(RF) rectification in the range of 1-8 gigahertz at 300 K. These findings not
only enhance our understanding of the relationship between nonlinear response
and magnetism, but also expand the potential applications as energy harvesters
and detectors in high-frequency scenarios.",210,2412.15591v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials chemistry,2024-12-20,2024-12-23T21:06:57.012554
Cellular Dynamics of Herium-rich Detonation on sub-Chandrasekhar Mass White Dwarf,"Most previous efforts for hydrodynamic studies on detonation in the context
of Type Ia supernovae did not take into account the scale of the cellular
structure for a criterion in initiation, propagation, quenching, and the
resolution requirement of detonation, whereas it is quite common to consider
cell sizes in the discussion on terrestrial detonation in chemically reactive
systems. In our recent study, the terrestrial cell-based theories, which
incorporates the cell-size data acquired in 2D simulations of helium detonation
in the double-detonation model, were demonstrated to be a powerful diagnostics
in reproducing the thresholds in the initiation and quenching provided by
previous studies. In the present study, 2D simulation results of the cellular
detonation in the base of white-dwarf (WD) envelope are described in detail, in
terms of the dynamic wave morphology and chemical abundance structure. The
cellular structure is observed at a range of upstream density and envelope
composition explored in the present work. C/O contamination by the WD core
material reduces the cell width rapidly, as accelerated by the {\alpha}-capture
reaction. It is also indicated that nickel production could be significantly
delayed for the C/O-rich composition. The small cell width makes it extremely
demanding to resolve the detonation structure in full-star simulations of SNe
Ia; this could raise a concern on the robustness of the outcomes of some
numerical simulations in terms of the success and failure of detonation. This
issue may be overcome by sub-grid modeling that incorporates the cellular
dynamics acquired in resolved simulations.",328,2412.15580v1,astro-ph.HE,astro-ph.HE,materials chemistry,2024-12-20,2024-12-23T21:06:57.013552
Thin films as practical quantum materials: a status quo and beyond,"Quantum materials have been in the limelight for several years now. These
materials exhibit intriguing quantum phenomena, which when harnessed properly,
promise extraordinary advancements across various scientific and technological
domains. To fully exploit their potential, it is imperative to synthesize such
quantum materials in thin film form so that they are compatible with
well-established device fabrication techniques. In this perspective, an
overview of the current status and future directions of thin film quantum
material synthesis is provided. The criteria for quantum materials are
discussed, as well as the many benefits of preparing them as thin films.
Prominent deposition techniques such as molecular beam epitaxy and chemical
vapor deposition are reviewed along with potential contenders. Despite
challenges, progress in thin film quantum material technology holds the
potential to realize practical devices with unprecedented functionalities.",158,2412.15565v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials chemistry,2024-12-20,2024-12-23T21:06:57.014549
Influence of Phase Segregation on the Hysteresis of Perovskite Solar Cells,"Organic-inorganic hybrid perovskite solar cells (PSC) have demonstrated
impressive performance improvement. Among the various characteristics, the
time-dependent current-voltage (J-V) hysteresis allows a direct exploration of
various critical phenomena that affect the stability of PSCs. The hysteresis is
associated with various spatial heterogeneity-related phenomena, including
lifetime, bandgap, and phase segregation. We investigate these phenomena
through numerical simulations and quantify how the spatial non-uniformity in
the perovskite active layer impacts the hysteresis. Further, we correlate the
time dependent device degradation with the hysteresis trends in terms of ion
density and effective carrier lifetime.",149,2412.15558v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,materials chemistry,2024-12-20,2024-12-23T21:06:57.014549
"Bi, Cr and Ag dopants in PbTe and SnTe: impact of the host band symmetry on doping properties by ab initio calculations","Doping properties of Bi, Cr and Ag dopants in thermoelectric and topological
materials PbTe and SnTe are analyzed based on density functional theory
calculations in the local density approximations and the large supercell
method. In agreement with experiment, in both PbTe and SnTe, Bi is a donor and
Ag is an acceptor with a vanishing magnetic moment. In contrast, Cr is a
resonant donor in PbTe, and an resonant acceptor in SnTe. We also consider the
electronic structure of cation vacancies in PbTe and SnTe, since these abundant
native defects induce $p$-type conductivity in both hosts. The quantitatively
different impact of these dopants/defects on the host band structure of PbTe
and SnTe (level energies, band splittings, band inversion, and a different
level of hybridization between dopant and host states) is explained based on
the group-theoretical arguments.",201,2412.15512v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.other",materials chemistry,2024-12-20,2024-12-23T21:06:57.015547
Scattering-Based Structural Inversion of Soft Materials via Kolmogorov-Arnold Networks,"Small-angle scattering (SAS) techniques are indispensable tools for probing
the structure of soft materials. However, traditional analytical models often
face limitations in structural inversion for complex systems, primarily due to
the absence of closed-form expressions of scattering functions. To address
these challenges, we present a machine learning framework based on the
Kolmogorov-Arnold Network (KAN) for directly extracting real-space structural
information from scattering spectra in reciprocal space. This
model-independent, data-driven approach provides a versatile solution for
analyzing intricate configurations in soft matter. By applying the KAN to
lyotropic lamellar phases and colloidal suspensions -- two representative soft
matter systems -- we demonstrate its ability to accurately and efficiently
resolve structural collectivity and complexity. Our findings highlight the
transformative potential of machine learning in enhancing the quantitative
analysis of soft materials, paving the way for robust structural inversion
across diverse systems.",186,2412.15474v1,cond-mat.soft,"cond-mat.soft,cond-mat.mtrl-sci",materials chemistry,2024-12-20,2024-12-23T21:06:57.015547
TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through Large/Vision Language Models,"TalkWithMachines aims to enhance human-robot interaction by contributing to
interpretable industrial robotic systems, especially for safety-critical
applications. The presented paper investigates recent advancements in Large
Language Models (LLMs) and Vision Language Models (VLMs), in combination with
robotic perception and control. This integration allows robots to understand
and execute commands given in natural language and to perceive their
environment through visual and/or descriptive inputs. Moreover, translating the
LLM's internal states and reasoning into text that humans can easily understand
ensures that operators gain a clearer insight into the robot's current state
and intentions, which is essential for effective and safe operation. Our paper
outlines four LLM-assisted simulated robotic control workflows, which explore
(i) low-level control, (ii) the generation of language-based feedback that
describes the robot's internal states, (iii) the use of visual information as
additional input, and (iv) the use of robot structure information for
generating task plans and feedback, taking the robot's physical capabilities
and limitations into account. The proposed concepts are presented in a set of
experiments, along with a brief discussion. Project description, videos, and
supplementary materials will be available on the project website:
https://talk-machines.github.io.",266,2412.15462v1,cs.RO,"cs.RO,cs.AI,cs.CL,cs.HC,cs.LG",materials chemistry,2024-12-19,2024-12-23T21:06:57.016545
Looking for optimal materials for whispering gallery modes applications at the 2 $μ$m window,"The diverse applications of whispering gallery modes in spherical
microresonators are strongly related to the sphere size and material
composition. Their design should therefore be optimized to ensure that
parameters such as the quality factor and the free spectral range are
maximized. Because of the imminent capacity crisis of the optical communication
systems operating at the 1550 nm wavelength regime, it is time to explore
optical communications at the 2 $\mu\mbox{m}$ wavelength window. In this work,
we analytically investigate key resonator parameters - quality factor and free
spectral range - as a function of wavelength, aiming to establish a methodology
to help identify optimal materials for whispering gallery mode sensors, with
special attention at the 2 $\mu\mbox{m}$ wavelength window. Specifically, we
examine three materials: fused silica, AsSe chalcogenide glass and calcium
fluoride, and we perform a comparison between them in order to identify the
region in the parameter space of resonant wavelengths and sphere radius,
$(\lambda_R,R$), where the WGM resonators are optimal at wavelengths $1.8
\mu\mbox{m}<\lambda<2.1 \mu\mbox{m}$.",257,2412.15456v1,physics.optics,physics.optics,materials chemistry,2024-12-19,2024-12-23T21:06:57.016545
Learning charges and long-range interactions from energies and forces,"Accurate modeling of long-range forces is critical in atomistic simulations,
as they play a central role in determining the properties of materials and
chemical systems. However, standard machine learning interatomic potentials
(MLIPs) often rely on short-range approximations, limiting their applicability
to systems with significant electrostatics and dispersion forces. We recently
introduced the Latent Ewald Summation (LES) method, which captures long-range
electrostatics without explicitly learning atomic charges or charge
equilibration. Extending LES, we incorporate the ability to learn physical
partial charges, encode charge states, and the option to impose charge
neutrality constraints. We benchmark LES on diverse and challenging systems,
including charged molecules, ionic liquid, electrolyte solution, polar
dipeptides, surface adsorption, electrolyte/solid interfaces, and solid-solid
interfaces. Our results show that LES can effectively infer physical partial
charges, dipole and quadrupole moments, as well as achieve better accuracy
compared to methods that explicitly learn charges. LES thus provides an
efficient, interpretable, and generalizable MLIP framework for simulating
complex systems with intricate charge transfer and long-range",245,2412.15455v1,physics.comp-ph,"physics.comp-ph,cond-mat.mtrl-sci,cs.LG",materials chemistry,2024-12-19,2024-12-23T21:06:57.017542
Mechanistic Insights into the Oxygen Evolution Reaction on Nickel-Doped Barium Titanate via Machine Learning-Accelerated Simulations,"Electrocatalytic water splitting, which produces hydrogen and oxygen through
water electrolysis, is a promising method for generating renewable, carbon-free
alternative fuels. However, its widespread adoption is hindered by the high
costs of Pt cathodes and IrO$_{x}$/RuO$_{x}$ anode catalysts. In the search for
cost-effective alternatives, barium titanate (BaTiO$_{3}$) has emerged as a
compelling candidate. This inexpensive, non-toxic perovskite oxide can be
synthesized from earth-abundant precursors and has shown potential for
catalyzing the oxygen evolution reaction (OER) in recent studies. In this work,
we explore the OER activity of pristine and Ni-doped BaTiO$_{3}$ at explicit
water interfaces using metadynamics (MetaD) simulations. To enable efficient
and practical MetaD for OER, we developed a machine learning interatomic
potential based on artificial neural networks (ANN), achieving large-scale and
long-time simulations with near-DFT accuracy. Our simulations reveal that
Ni-doping enhances the catalytic activity of BaTiO$_{3}$ for OER, consistent
with experimental observations, while providing mechanistic insights into this
enhancement.",268,2412.15452v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn",materials chemistry,2024-12-19,2024-12-23T21:06:57.018539
Tailoring MBE Growth of c-Mn3Sn Directly on MgO (111): From Islands to Film,"We present our study of (0001) oriented Mn$_3$Sn (c-Mn$_3$Sn) thin films
synthesized directly on an MgO (111) substrate via molecular beam epitaxy. We
identify a growth window where Mn$_3$Sn growth can be controlled through slight
adjustments of the Mn flux, achieving either $\mu$m$^2$-sized high
crystalline-quality islands or an almost completely continuous film.
High-resolution X-ray diffraction results indicate that both films are highly
(0001) oriented. The atomic resolution images show clear film-substrate
interfaces displaying an epitaxial relationship. Scanning precession electron
diffraction measurements reveal that the island featured sample has highly
crystallized Mn3Sn. The sample featuring a high continuity exhibits defects in
some areas but retains the dominant Mn$_3$Sn structure. This work demonstrates
a potential method for synthesizing high crystalline-quality Mn3Sn films with
substantial coverage, facilitating the study of Mn3Sn films without the
influence of an additional buffer layer and promoting their application in
integrated spintronics.",237,2412.15442v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.app-ph",materials chemistry,2024-12-19,2024-12-23T21:06:57.018539
"Unveiling the Cosmic Chemistry II: ""direct"" $T_e$--based metallicity of galaxies at 3 $< z <$ 10 with JWST/NIRSpec","We report the detection of the [O III] auroral line in 42 galaxies within the
redshift range of $3 < z < 10$. These galaxies were selected from publicly
available JWST data releases, including the JADES and PRIMALsurveys, and
observed using both the low-resolution PRISM/CLEAR configuration and
medium-resolution gratings. The measured electron temperatures in the
high-ionization regions of these galaxies range from $T_e$([O III]) = 12,000 to
24,000 K, consistent with temperatures observed in local metal-poor galaxies
and previous JWST studies. In 10 galaxies, we also detect the [O II] auroral
line, allowing us to determine electron temperatures in the low-ionization
regions, which range between $T_e$([O II]) = 10,830 and 20,000 K. The
direct-$T_e$-based metallicities of our sample span from 12 + log(O/H) = 7.2 to
8.4, indicating these high-redshift galaxies are relatively metal-poor. By
combining our sample with 25 galaxies from the literature, we expand the
dataset to a total of 67 galaxies within $3 < z < 10$, effectively more than
doubling the previous sample size for direct-$T_e$ based metallicity studies.
This larger dataset allow us to derive empirical metallicity calibration
relations based exclusively on high-redshift galaxies, using six key line
ratios: R3, R2, R23, Ne3O2, O32, and O3N2. Notably, we derive a novel
metallicity calibration relation for the first time using high-redshift
$T_e$-based metallicities: $\hat{R}$ = 0.18log $R2$ + 0.98log $R3$. This new
calibration significantly reduces the scatter in high-redshift galaxies
compared to the $\hat{R}$ relation previously calibrated for low-redshift
galaxies.",437,2412.15435v1,astro-ph.GA,astro-ph.GA,materials chemistry,2024-12-19,2024-12-23T21:06:57.019537
Superconductivity in Epitaxial SiGe for Cryogenic Electronics,"Introducing superconductivity into group IV elements by doping has long
promised a pathway to introduce quantum functionalities into well-established
semiconductor technologies. The non-equilibrium hyperdoping of group III atoms
into Si or Ge has successfully shown superconductivity can be achieved,
however, the origin of superconductivity has been obscured by structural
disorder and dopant clustering. Here, we report the epitaxial growth of
hyperdoped Ga:Ge films by molecular beam epitaxy with extreme hole
concentrations (n$_{h}$ = 4.15 $\times$ 10$^{21}$ cm$^{-3}$, ~17.9\% Ga
substitution) that yield superconductivity with a critical temperature of
T$_{C}$ = 3.5 K, and an out-of-plane critical field of 1 T at 270 mK.
Synchrotron-based X-ray absorption and scattering methods reveal that Ga
dopants are substitutionally incorporated within the Ge lattice, introducing a
tetragonal distortion to the crystal unit cell. Our findings, corroborated by
first-principles calculations, suggest that the structural order of Ga dopants
creates a flat band for the emergence of superconductivity in Ge, establishing
hyperdoped Ga:Ge as a low-disorder, epitaxial superconductor-semiconductor
platform.",293,2412.15421v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",materials chemistry,2024-12-19,2024-12-23T21:06:57.020534
Selective tracking of charge carrier dynamics in CuInS2 quantum dots,"CuInS2 quantum dots have been studied in a broad range of applications, but
despite this, the fine details of their charge carrier dynamics remain a
subject of intense debate. Two of the most relevant points of discussion are
the hole dynamics and the influence of Cu:In synthesis stoichiometry on them.
It has been proposed that Cu-deficiency leads to the formation of Cu2+,
affecting the localization of holes into Cu defects. Importantly, it is
precisely these confined hole states which are used to explain the interesting
photoluminescence properties of CuInS2 quantum dots. We use static X-ray
spectroscopy to reveal no evidence for a measurable amount of native Cu2+
states in Cu-deficient samples. Instead, the improved properties of these
samples are explained by an increase of crystallinity, reducing the
concentration of mid gap states. Furthermore, to understand the charge carrier
dynamics, herein we employ ultrafast optical transient absorption, and
fluorescence up-conversion spectroscopies in combination with ultrafast X-ray
absorption spectroscopy using a hard X-ray free electron laser. We demonstrate
that in non-passivated samples, holes are transferred from Cu atoms in
sub-picosecond timescales. We assign this transfer to occur towards the
thiol-based ligands. Finally, we observe that Cu-deficient samples are more
robust against the photothermal heating effects of using higher laser fluences.
This is not the case for the stoichiometric sample, where heating effects on
the structure are directly observed.",315,2412.15418v1,physics.chem-ph,physics.chem-ph,materials chemistry,2024-12-19,2024-12-23T21:06:57.021531
A Decision Transformer Approach to Grain Boundary Network Optimization,"As microstructure property models improve, additional information from
crystallographic degrees of freedom and grain boundary networks (GBNs) can be
included in microstructure design problems. However, the high dimensional
nature of including this information precludes the use of many common
optimization approaches and requires less efficient methods to generate quality
designs. Previous work demonstrated that human-in-the-loop optimization,
instantiated as a video game, achieved high-quality, efficient solutions to
these design problems. However, such data is expensive to obtain. In the
present work, we show how a Decision Transformer machine learning (ML) model
can be used to learn from the optimization trajectories generated by human
players, and subsequently solve materials design problems. We compare the ML
optimization trajectories against players and a common global optimization
algorithm: simulated annealing (SA). We find that the ML model exhibits a
validation accuracy of 84% against player decisions, and achieves solutions of
comparable quality to SA (92%), but does so using three orders of magnitude
fewer iterations. We find that the ML model generalizes in important and
surprising ways, including the ability to train using a simple constitutive
structure-property model and then solve microstructure design problems for a
different, higher-fidelity, constitutive structure-property model without any
retraining. These results demonstrate the potential of Decision Transformer
models for the solution of materials design problems.",299,2412.15393v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",materials chemistry,2024-12-19,2024-12-23T21:06:57.022529
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",astronomical sciences,2024-12-20,2024-12-23T21:06:57.752653
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,astronomical sciences,2024-12-20,2024-12-23T21:06:57.753651
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",astronomical sciences,2024-12-20,2024-12-23T21:06:57.754648
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",astronomical sciences,2024-12-20,2024-12-23T21:06:57.755645
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",astronomical sciences,2024-12-20,2024-12-23T21:06:57.755645
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.756642
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",astronomical sciences,2024-12-20,2024-12-23T21:06:57.757640
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",astronomical sciences,2024-12-20,2024-12-23T21:06:57.757640
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",astronomical sciences,2024-12-20,2024-12-23T21:06:57.758636
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,astronomical sciences,2024-12-20,2024-12-23T21:06:57.759636
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,astronomical sciences,2024-12-20,2024-12-23T21:06:57.760631
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",astronomical sciences,2024-12-20,2024-12-23T21:06:57.761629
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,astronomical sciences,2024-12-20,2024-12-23T21:06:57.761629
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",astronomical sciences,2024-12-20,2024-12-23T21:06:57.762626
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.762626
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,astronomical sciences,2024-12-20,2024-12-23T21:06:57.763624
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",astronomical sciences,2024-12-20,2024-12-23T21:06:57.764620
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",astronomical sciences,2024-12-20,2024-12-23T21:06:57.764620
First Constraint on the Diffuse Supernova Neutrino Background through the CE$ν$NS process from the LZ experiment,"We report the limits on the diffuse supernova neutrino background (DSNB) flux
and the fundamental DSNB parameters measured from the first science run of the
LUX-ZEPLIN (LZ) experiment, a dual-phase xenon detector located at the Sanford
Underground Research Facility in Lead, South Dakota, USA. This is the first
time the DSNB limit is measured through the process of the coherent elastic
neutrino-nucleus scattering (CE$\nu$NS). Using an exposure of 60~live days and
a fiducial mass of 5.5~t, the upper limit on the DSNB $\nu_x$ (each of
$\nu_\mu$, $\nu_\tau$, $\bar\nu_\mu$, $\bar\nu_\tau$) flux is
$686-826$~cm$^{-2}$s$^{-1}$ at the 90\% confidence level for neutrino energies
E$>$19.3~MeV, assuming the flux for each $\nu_x$ flavor is the same. The
interval accounts for the uncertainty in existing DSNB models. The present
result is comparable to the existing best limit and further improvements are
expected after collecting data from an estimated 1,000-day exposure in the
future.",281,2412.15886v1,hep-ex,hep-ex,astronomical sciences,2024-12-20,2024-12-23T21:06:57.765618
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.766615
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.767612
On the Power of Strategic Corpus Enrichment in Content Creation Games,"Search and recommendation ecosystems exhibit competition among content
creators. This competition has been tackled in a variety of game-theoretic
frameworks. Content creators generate documents with the aim of being
recommended by a content ranker for various information needs. In order for the
ecosystem, modeled as a content ranking game, to be effective and maximize user
welfare, it should guarantee stability, where stability is associated with the
existence of pure Nash equilibrium in the corresponding game. Moreover, if the
contents' ranking algorithm possesses a game in which any best-response
learning dynamics of the content creators converge to equilibrium of high
welfare, the system is considered highly attractive. However, as classical
content ranking algorithms, employed by search and recommendation systems, rank
documents by their distance to information needs, it has been shown that they
fail to provide such stability properties. As a result, novel content ranking
algorithms have been devised. In this work, we offer an alternative approach:
corpus enrichment with a small set of fixed dummy documents. It turns out that,
with the right design, such enrichment can lead to pure Nash equilibrium and
even to the convergence of any best-response dynamics to a high welfare result,
where we still employ the classical/current content ranking approach. We show
two such corpus enrichment techniques with tight bounds on the number of
documents needed to obtain the desired results. Interestingly, our study is a
novel extension of Borel's Colonel Blotto game.",287,2412.15878v1,cs.GT,cs.GT,astronomical sciences,2024-12-20,2024-12-23T21:06:57.767612
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",astronomical sciences,2024-12-20,2024-12-23T21:06:57.768610
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,astronomical sciences,2024-12-20,2024-12-23T21:06:57.769607
Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science,"Understanding how transportation networks work is important for improving
connectivity, efficiency, and safety. In Brazil, where road transport is a
significant portion of freight and passenger movement, network science can
provide valuable insights into the structural properties of the infrastructure,
thus helping decision makers responsible for proposing improvements to the
system. This paper models the federal road network as weighted networks, with
the intent to unveil its topological characteristics and identify key locations
(cities) that play important roles for the country through 75,000 kilometres of
roads. We start with a simple network to examine basic connectivity and
topology, where weights are the distance of the road segment. We then
incorporate other weights representing number of incidents, population, and
number of cities in-between each segment. We then focus on community detection
as a way to identify clusters of cities that form cohesive groups within a
network. Our findings aim to bring clarity to the overall structure of federal
roads in Brazil, thus providing actionable insights for improving
infrastructure planning and prioritising resources to enhance network
resilience.",208,2412.15865v1,physics.soc-ph,"physics.soc-ph,cs.CY",astronomical sciences,2024-12-20,2024-12-23T21:06:57.770604
Enriching Social Science Research via Survey Item Linking,"Questions within surveys, called survey items, are used in the social
sciences to study latent concepts, such as the factors influencing life
satisfaction. Instead of using explicit citations, researchers paraphrase the
content of the survey items they use in-text. However, this makes it
challenging to find survey items of interest when comparing related work.
Automatically parsing and linking these implicit mentions to survey items in a
knowledge base can provide more fine-grained references. We model this task,
called Survey Item Linking (SIL), in two stages: mention detection and entity
disambiguation. Due to an imprecise definition of the task, existing datasets
used for evaluating the performance for SIL are too small and of low-quality.
We argue that latent concepts and survey item mentions should be
differentiated. To this end, we create a high-quality and richly annotated
dataset consisting of 20,454 English and German sentences. By benchmarking deep
learning systems for each of the two stages independently and sequentially, we
demonstrate that the task is feasible, but observe that errors propagate from
the first stage, leading to a lower overall task performance. Moreover,
mentions that require the context of multiple sentences are more challenging to
identify for models in the first stage. Modeling the entire context of a
document and combining the two stages into an end-to-end system could mitigate
these problems in future work, and errors could additionally be reduced by
collecting more diverse data and by improving the quality of the knowledge
base. The data and code are available at https://github.com/e-tornike/SIL .",338,2412.15831v1,cs.DL,"cs.DL,cs.CL",astronomical sciences,2024-12-20,2024-12-23T21:06:57.771602
SUBMASSIVE: Resolving Subclass Cycles in Very Large Knowledge Graphs,"Large knowledge graphs capture information of a large number of entities and
their relations. Among the many relations they capture, class subsumption
assertions are usually present and expressed using the \texttt{rdfs:subClassOf}
construct. From our examination, publicly available knowledge graphs contain
many potentially erroneous cyclic subclass relations, a problem that can be
exacerbated when different knowledge graphs are integrated as Linked Open Data.
In this paper, we present an automatic approach for resolving such cycles at
scale using automated reasoning by encoding the problem of cycle-resolving to a
MAXSAT solver. The approach is tested on the LOD-a-lot dataset, and compared
against a semi-automatic version of our algorithm. We show how the number of
removed triples is a trade-off against the efficiency of the algorithm.",170,2412.15829v1,cs.LO,"cs.LO,cs.SC,math.OC,68T27, 68T20, 68T09,F.3.0; I.2.1; I.2.4",astronomical sciences,2024-12-20,2024-12-23T21:06:57.771602
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",astronomical sciences,2024-12-20,2024-12-23T21:06:57.772599
Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis,"Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.",134,2412.15814v1,cs.CR,"cs.CR,cs.DC,cs.LO",astronomical sciences,2024-12-20,2024-12-23T21:06:57.773596
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",astronomical sciences,2024-12-20,2024-12-23T21:06:57.774594
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",astronomical sciences,2024-12-20,2024-12-23T21:06:57.775592
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",astronomical sciences,2024-12-20,2024-12-23T21:06:57.776590
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,astronomical sciences,2024-12-20,2024-12-23T21:06:57.777586
Distribution-Free Normal Modal Logics,"This article initiates the semantic study of distribution-free normal modal
logic systems, laying the semantic foundations and anticipating further
research in the area. The article explores roughly the same area, though taking
a different approach, with a recent article by Bezhanishvili, de Groot,
Dmitrieva and Morachini, who studied a distribution-free version of Dunn's
Positive Modal Logic (PML). Unlike PML, we consider logics that may drop
distribution and which are equipped with both an implication connective and
modal operators. We adopt a uniform relational semantics approach, relying on
recent results on representation and duality for normal lattice expansions. We
prove canonicity and completeness in the relational semantics of the minimal
distribution-free normal modal logic, assuming just the K-axiom, as well as of
its axiomatic extensions obtained by adding any of the D, T, B, S4 or S5
axioms. Adding distribution can be easily accommodated and, as a side result,
we also obtain a new semantic treatment of Intuitionistic Modal Logic.",224,2412.15736v1,cs.LO,"cs.LO,math.LO",astronomical sciences,2024-12-20,2024-12-23T21:06:57.778583
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.778583
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.779581
Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion,"This paper addresses the question of whether or not uncoupled online learning
algorithms converge to the Nash equilibrium in pricing competition or whether
they can learn to collude. Algorithmic collusion has been debated among
competition regulators, and it is a highly relevant phenomenon for buyers and
sellers on online retail platforms. We analyze formally if mean-based
algorithms, a class of bandit algorithms relevant to algorithmic pricing,
converge to the Nash equilibrium in repeated Bertrand oligopolies. Bandit
algorithms only learn the profit of the agent for the price set in each step.
In addition, we provide results of extensive experiments with different types
of multi-armed bandit algorithms used for algorithmic pricing. In a
mathematical proof, we show that mean-based algorithms converge to correlated
rational strategy profiles, which coincide with the Nash equilibrium in
versions of the Bertrand competition. Learning algorithms do not converge to a
Nash equilibrium in general, and the fact that Bertrand pricing games are
learnable with bandit algorithms is remarkable. Our numerical results suggest
that wide-spread bandit algorithms that are not mean-based also converge to
equilibrium and that algorithmic collusion only arises with symmetric
implementations of UCB or Q-learning, but not if different algorithms are used
by sellers. In addition, the level of supra-competitive prices decreases with
increasing numbers of sellers. Supra-competitive prices decrease consumer
welfare. If algorithms lead to algorithmic collusion, this is important for
consumers, sellers, and regulators to understand. We show that for the
important class of multi-armed bandit algorithms such fears are overrated
unless all sellers agree on a symmetric implementation of certain collusive
algorithms.",330,2412.15707v1,cs.GT,cs.GT,astronomical sciences,2024-12-20,2024-12-23T21:06:57.779581
High-efficiency fast pinching radiation of electron beams in nonuniform plasma,"The continuous development of bright x/gamma-ray sources has opened up new
frontiers of science and advanced applications. Currently, there is still a
lack of efficient approaches to produce gamma-rays with photon energies up to
GeV and with high peak brilliance comparable to modern free-electron lasers.
Here we report a novel mechanism called beam fast pinching radiation burst to
generate such gamma-ray sources. It is achieved by injecting a GeV electron
beam into a submillimeter plasma with an upramp density profile, enabling
violent beam pinching to occur rapidly. During this process, a burst of
collimated gamma-rays is efficiently produced with photon energy up to GeV,
energy conversion efficiency exceeding $30\%$, and peak brilliance exceeding
$10^{28}$ photons s$^{-1}$ mm$^{-2}$ mrad$^{-2}$ per $0.1\%$ bandwidth. All of
these are several orders of magnitude higher than existing gamma-ray sources.
This opens a novel avenue for the development of extremely bright gamma-ray
sources for both fundamental research and cutting-edge applications.",238,2412.15706v1,physics.plasm-ph,"physics.plasm-ph,physics.acc-ph",astronomical sciences,2024-12-20,2024-12-23T21:06:57.780578
High-Dimensional Bayesian Optimisation with Large-Scale Constraints via Latent Space Gaussian Processes,"Design optimisation offers the potential to develop lightweight aircraft
structures with reduced environmental impact. Due to the high number of design
variables and constraints, these challenges are typically addressed using
gradient-based optimisation methods to maintain efficiency. However, this
approach often results in a local solution, overlooking the global design
space. Moreover, gradients are frequently unavailable. Bayesian Optimisation
presents a promising alternative, enabling sample-efficient global optimisation
through probabilistic surrogate models that do not depend on gradients.
Although Bayesian Optimisation has shown its effectiveness for problems with a
small number of design variables, it struggles to scale to high-dimensional
problems, particularly when incorporating large-scale constraints. This
challenge is especially pronounced in aeroelastic tailoring, where directional
stiffness properties are integrated into the structural design to manage
aeroelastic deformations and enhance both aerodynamic and structural
performance. Ensuring the safe operation of the system requires simultaneously
addressing constraints from various analysis disciplines, making global design
space exploration even more complex. This study seeks to address this issue by
employing high-dimensional Bayesian Optimisation combined with a dimensionality
reduction technique to tackle the optimisation challenges in aeroelastic
tailoring. The proposed approach is validated through experiments on a
well-known benchmark case with black-box constraints, as well as its
application to the aeroelastic tailoring problem, demonstrating the feasibility
of Bayesian Optimisation for high-dimensional problems with large-scale
constraints.",301,2412.15679v1,cs.CE,cs.CE,astronomical sciences,2024-12-20,2024-12-23T21:06:57.781575
Two-Dimensional Graphene: Theoretical Study of Multi-photon Non-linear Absorption Coefficient of a Strong Electromagnetic Wave by Using Quantum Kinetic Equation,"Based on the quantum kinetic equation for electrons, we theoretically study
the quantum multi-photon non-linear absorption of a strong electromagnetic wave
(EMW) in two-dimensional graphene. Two cases of the electron scattering
mechanism are considered: Electron-optical phonon scattering and
electron-acoustic phonon scattering. The general multi-photon absorption
coefficient is presented as a function of the temperature, the external
magnetic field, the photon energy and the amplitude of external EMW. These
analytical expressions for multi-photon non-linear absorption coefficient
(MNAC) are numerically calculated and the results are discussed in both the
absence and presence of a magnetic field perpendicular to the graphene sheet.
The results show that there is no absorption peak in the absence of the
magnetic field, which contrasts with previous results in 2D systems such as
quantum wells or superlattices. However, when there is a strong magnetic field
along the direction perpendicular to the 2D graphene, absorption spectral lines
appear consistent with the magneto-phonon resonance conditions. Our
calculations show that the MPA's effect is stronger than mono-photon
absorption. Besides, the quantum multi-photon non-linear absorption phenomenon
has been studied from low to high temperatures. This transcends the limits of
the classical BKE which is studied in the high-temperature domain. The
computational results show that the dependence of MNAC on the above quantities
is consistent with the previous theoretical investigation. Another novel
feature of this work is that the general analytic expression for MNAC shows the
Half Width at Half Maximum dependence on the magnetic field which is in good
agreement with the previous experimental observations. Thus, our estimation
might give a critical prediction for future experimental observations in 2D
graphene.",347,2412.15638v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",astronomical sciences,2024-12-20,2024-12-23T21:06:57.782573
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",astronomical sciences,2024-12-20,2024-12-23T21:06:57.783570
Room-temperature nonlinear transport and microwave rectification in antiferromagnetic MnBi$_2$Te$_4$ films,"The discovery of the nonlinear Hall effect provides an avenue for studying
the interplay among symmetry, topology, and phase transitions, with potential
applications in signal doubling and high-frequency rectification. However,
practical applications require devices fabricated on large area thin film as
well as room-temperature operation. Here, we demonstrate robust
room-temperature nonlinear transverse response and microwave rectification in
MnBi$_2$Te$_4$ films grown by molecular beam epitaxy. We observe multiple
sign-reversals in the nonlinear response by tuning the chemical potential.
Through theoretical analysis, we identify skew scattering and side jump,
arising from extrinsic spin-orbit scattering, as the main mechanisms underlying
the observed nonlinear signals. Furthermore, we demonstrate radio frequency
(RF) rectification in the range of 1-8 gigahertz at 300 K. These findings not
only enhance our understanding of the relationship between nonlinear response
and magnetism, but also expand the potential applications as energy harvesters
and detectors in high-frequency scenarios.",210,2412.15591v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,astronomical sciences,2024-12-20,2024-12-23T21:06:57.783570
J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM,"Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made
available the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image
archive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive
serves as a valuable resource for researchers and scholars interested in
deep-sea imagery. The dataset comprises images and videos of deep-sea
phenomena, predominantly of marine organisms, but also of the seafloor and
physical processes. In this study, we propose J-EDI QA, a benchmark for
understanding images of deep-sea organisms using a multimodal large language
model (LLM). The benchmark is comprised of 100 images, accompanied by questions
and answers with four options by JAMSTEC researchers for each image. The QA
pairs are provided in Japanese, and the benchmark assesses the ability to
understand deep-sea species in Japanese. In the evaluation presented in this
paper, OpenAI o1 achieved a 50% correct response rate. This result indicates
that even with the capabilities of state-of-the-art models as of December 2024,
deep-sea species comprehension is not yet at an expert level. Further advances
in deep-sea species-specific LLMs are therefore required.",277,2412.15574v1,cs.CV,cs.CV,astronomical sciences,2024-12-20,2024-12-23T21:06:57.784567
Thin films as practical quantum materials: a status quo and beyond,"Quantum materials have been in the limelight for several years now. These
materials exhibit intriguing quantum phenomena, which when harnessed properly,
promise extraordinary advancements across various scientific and technological
domains. To fully exploit their potential, it is imperative to synthesize such
quantum materials in thin film form so that they are compatible with
well-established device fabrication techniques. In this perspective, an
overview of the current status and future directions of thin film quantum
material synthesis is provided. The criteria for quantum materials are
discussed, as well as the many benefits of preparing them as thin films.
Prominent deposition techniques such as molecular beam epitaxy and chemical
vapor deposition are reviewed along with potential contenders. Despite
challenges, progress in thin film quantum material technology holds the
potential to realize practical devices with unprecedented functionalities.",158,2412.15565v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,astronomical sciences,2024-12-20,2024-12-23T21:06:57.785565
Spatial Clustering of Citizen Science Data Improves Downstream Species Distribution Models,"Citizen science biodiversity data present great opportunities for ecology and
conservation across vast spatial and temporal scales. However, the
opportunistic nature of these data lacks the sampling structure required by
modeling methodologies that address a pervasive challenge in ecological data
collection: imperfect detection, i.e., the likelihood of under-observing
species on field surveys. Occupancy modeling is an example of an approach that
accounts for imperfect detection by explicitly modeling the observation process
separately from the biological process of habitat selection. This produces
species distribution models that speak to the pattern of the species on a
landscape after accounting for imperfect detection in the data, rather than the
pattern of species observations corrupted by errors. To achieve this benefit,
occupancy models require multiple surveys of a site across which the site's
status (i.e., occupied or not) is assumed constant. Since citizen science data
are not collected under the required repeated-visit protocol, observations may
be grouped into sites post hoc. Existing approaches for constructing sites
discard some observations and/or consider only geographic distance and not
environmental similarity. In this study, we compare ten approaches for site
construction in terms of their impact on downstream species distribution models
for 31 bird species in Oregon, using observations recorded in the eBird
database. We find that occupancy models built on sites constructed by spatial
clustering algorithms perform better than existing alternatives.",280,2412.15559v1,cs.LG,cs.LG,astronomical sciences,2024-12-20,2024-12-23T21:06:57.785565
Influence of Phase Segregation on the Hysteresis of Perovskite Solar Cells,"Organic-inorganic hybrid perovskite solar cells (PSC) have demonstrated
impressive performance improvement. Among the various characteristics, the
time-dependent current-voltage (J-V) hysteresis allows a direct exploration of
various critical phenomena that affect the stability of PSCs. The hysteresis is
associated with various spatial heterogeneity-related phenomena, including
lifetime, bandgap, and phase segregation. We investigate these phenomena
through numerical simulations and quantify how the spatial non-uniformity in
the perovskite active layer impacts the hysteresis. Further, we correlate the
time dependent device degradation with the hysteresis trends in terms of ion
density and effective carrier lifetime.",149,2412.15558v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,astronomical sciences,2024-12-20,2024-12-23T21:06:57.786562
From Galaxy Zoo DECaLS to BASS/MzLS: detailed galaxy morphology classification with unsupervised domain adaption,"The DESI Legacy Imaging Surveys (DESI-LIS) comprise three distinct surveys:
the Dark Energy Camera Legacy Survey (DECaLS), the Beijing-Arizona Sky Survey
(BASS), and the Mayall z-band Legacy Survey (MzLS). The citizen science project
Galaxy Zoo DECaLS 5 (GZD-5) has provided extensive and detailed morphology
labels for a sample of 253,287 galaxies within the DECaLS survey. This dataset
has been foundational for numerous deep learning-based galaxy morphology
classification studies. However, due to differences in signal-to-noise ratios
and resolutions between the DECaLS images and those from BASS and MzLS
(collectively referred to as BMz), a neural network trained on DECaLS images
cannot be directly applied to BMz images due to distributional mismatch. In
this study, we explore an unsupervised domain adaptation (UDA) method that
fine-tunes a source domain model trained on DECaLS images with GZD-5 labels to
BMz images, aiming to reduce bias in galaxy morphology classification within
the BMz survey. Our source domain model, used as a starting point for UDA,
achieves performance on the DECaLS galaxies' validation set comparable to the
results of related works. For BMz galaxies, the fine-tuned target domain model
significantly improves performance compared to the direct application of the
source domain model, reaching a level comparable to that of the source domain.
We also release a catalogue of detailed morphology classifications for 248,088
galaxies within the BMz survey, accompanied by usage recommendations.",329,2412.15533v1,astro-ph.GA,"astro-ph.GA,astro-ph.IM,cs.CV",astronomical sciences,2024-12-20,2024-12-23T21:06:57.787559
"Bi, Cr and Ag dopants in PbTe and SnTe: impact of the host band symmetry on doping properties by ab initio calculations","Doping properties of Bi, Cr and Ag dopants in thermoelectric and topological
materials PbTe and SnTe are analyzed based on density functional theory
calculations in the local density approximations and the large supercell
method. In agreement with experiment, in both PbTe and SnTe, Bi is a donor and
Ag is an acceptor with a vanishing magnetic moment. In contrast, Cr is a
resonant donor in PbTe, and an resonant acceptor in SnTe. We also consider the
electronic structure of cation vacancies in PbTe and SnTe, since these abundant
native defects induce $p$-type conductivity in both hosts. The quantitatively
different impact of these dopants/defects on the host band structure of PbTe
and SnTe (level energies, band splittings, band inversion, and a different
level of hybridization between dopant and host states) is explained based on
the group-theoretical arguments.",201,2412.15512v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.other",astronomical sciences,2024-12-20,2024-12-23T21:06:57.787559
Stylish and Functional: Guided Interpolation Subject to Physical Constraints,"Generative AI is revolutionizing engineering design practices by enabling
rapid prototyping and manipulation of designs. One example of design
manipulation involves taking two reference design images and using them as
prompts to generate a design image that combines aspects of both. Real
engineering designs have physical constraints and functional requirements in
addition to aesthetic design considerations. Internet-scale foundation models
commonly used for image generation, however, are unable to take these physical
constraints and functional requirements into consideration as part of the
generation process. We consider the problem of generating a design inspired by
two input designs, and propose a zero-shot framework toward enforcing physical,
functional requirements over the generation process by leveraging a pretrained
diffusion model as the backbone. As a case study, we consider the example of
rotational symmetry in generation of wheel designs. Automotive wheels are
required to be rotationally symmetric for physical stability. We formulate the
requirement of rotational symmetry by the use of a symmetrizer, and we use this
symmetrizer to guide the diffusion process towards symmetric wheel generations.
Our experimental results find that the proposed approach makes generated
interpolations with higher realism than methods in related work, as evaluated
by Fr\'echet inception distance (FID). We also find that our approach generates
designs that more closely satisfy physical and functional requirements than
generating without the symmetry guidance.",269,2412.15507v1,cs.LG,"cs.LG,cs.CV",astronomical sciences,2024-12-20,2024-12-23T21:06:57.788557
Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework,"Natural language processing (NLP) has seen remarkable advancements with the
development of large language models (LLMs). Despite these advancements, LLMs
often produce socially biased outputs. Recent studies have mainly addressed
this problem by prompting LLMs to behave ethically, but this approach results
in unacceptable performance degradation. In this paper, we propose a
multi-objective approach within a multi-agent framework (MOMA) to mitigate
social bias in LLMs without significantly compromising their performance. The
key idea of MOMA involves deploying multiple agents to perform causal
interventions on bias-related contents of the input questions, breaking the
shortcut connection between these contents and the corresponding answers.
Unlike traditional debiasing techniques leading to performance degradation,
MOMA substantially reduces bias while maintaining accuracy in downstream tasks.
Our experiments conducted on two datasets and two models demonstrate that MOMA
reduces bias scores by up to 87.7%, with only a marginal performance
degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly
enhances the multi-objective metric icat in the StereoSet dataset by up to
58.1%. Code will be made available at https://github.com/Cortantse/MOMA.",258,2412.15504v1,cs.CL,cs.CL,astronomical sciences,2024-12-20,2024-12-23T21:06:57.789554
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",molecular biology,2024-12-20,2024-12-23T21:06:58.559584
Cloud-scale elemental abundance variations and the CO-to-dust-mass conversion factor in M31,"From a spectroscopic survey of candidate H II regions in the Andromeda galaxy
(M31) with MMT/Hectospec, we have identified 294 H II regions using emission
line ratios and calculated elemental abundances from strong-line diagnostics
(values ranging from sub-solar to super-solar) producing both Oxygen and
Nitrogen radial abundance gradients. The Oxygen gradient is relatively flat,
while the Nitrogen gradient is significantly steeper, indicating a higher N/O
ratio in M31's inner regions, consistent with recent simulations of galaxy
chemical evolution. No strong evidence was found of systematic galaxy-scale
trends beyond the radial gradient. After subtracting the radial gradient from
abundance values, we find an apparently stochastic and statistically
significant scatter of standard deviation 0.06 dex, which exceeds measurement
uncertainties. One explanation includes a possible collision with M32 200 - 800
Myrs ago. Using the two-point correlation function of the Oxygen abundance, we
find that, similar to other spiral galaxies, M31 is well-mixed on sub-kpc
scales but less so on larger (kpc) scales, which could be a result of an
exponential decrease in mixing speed with spatial scale, and the aforementioned
recent merger. Finally, the MMT spectroscopy is complemented by a dust
continuum and CO survey of individual Giant Molecular Clouds, conducted with
the Submillimeter Array. By combining the MMT and SMA observations, we obtain a
unique direct test of the Oxygen abundance dependence of the
$\alpha^{\prime}(^{12}\mathrm{CO})$ factor which is crucial to convert CO
emission to dust mass. Our results suggest that within our sample there is no
trend of the $\alpha^{\prime}(^{12}\mathrm{CO})$ with Oxygen abundance.",378,2412.16069v1,astro-ph.GA,astro-ph.GA,molecular biology,2024-12-20,2024-12-23T21:06:58.560580
Time-reversible implementation of MASH for efficient nonadiabatic molecular dynamics,"In this work, we describe various improved implementations of the mapping
approach to surface hopping (MASH) for simulating nonadiabatic dynamics. These
include time-reversible and piecewise-continuous integrators, which is only
formally possible because of the deterministic nature of the underlying MASH
equations of motion. The new algorithms allow for the use of either
wave-function overlaps or nonadiabatic coupling vectors to propagate the spin,
which encodes the electronic state. For a given time-step, $\Delta t$, it is
demonstrated that the global error for these methods is $\mathcal{O}(\Delta
t^2)$ compared to the $\mathcal{O}(\Delta t)$ error of standard
implementations. This allows larger time-steps to be used for a desired error
tolerance, or conversely, more accurate observables given a fixed value of
$\Delta t$. The newly developed integrators thus provide further advantages for
the MASH method, demonstrating that it can be implemented more efficiently than
other surface-hopping approaches, which cannot construct time-reversible
integrators due to their stochastic nature.",251,2412.15976v1,physics.chem-ph,physics.chem-ph,molecular biology,2024-12-20,2024-12-23T21:06:58.561577
Topological strongly correlated phases in orthorhombic diamond lattice compounds,"We explore the Mott transition in orthorhombic diamond lattices relevant to
(ET)Ag$_4$(CN)$_5$ molecular compounds. The non-interacting phases include
nodal line, Dirac and/or Weyl semimetals depending on the strength of
spin-orbit coupling and the degree of dimerization of the lattice. Based on an
extension of slave-rotor mean-field theory which accounts for magnetic order,
we find a transition from a semimetal to a paramagnetic Mott insulator at a
critical $U_c$ which becomes N\'eel ordered at a larger Coulomb repulsion,
$U_{cm}>U_{c}$. The resulting intermediate Mott phase is a $U(1)$ quantum spin
liquid (QSL) consisting on spinon preserving the nodal structure of the nearby
semimetallic phases. An analysis of the Green's function in this Mott phase
shows how the zeros follow the spinon band dispersions carrying the topology
while the poles describe the Hubbard bands. Our results are relevant to recent
observations in (ET)Ag$_4$(CN)$_5$ molecular compounds in which the ambient
pressure N\'eel ordered Mott insulator is gradually suppressed until
semimetallic behavior arises at larger pressures.",279,2412.15812v1,cond-mat.str-el,cond-mat.str-el,molecular biology,2024-12-20,2024-12-23T21:06:58.562574
Efficient Hamiltonian Simulation: A Utility Scale Perspective for Covalent Inhibitor Reactivity Prediction,"Quantum computing applications in the noisy intermediate-scale quantum (NISQ)
era demand algorithms capable of generating shallower circuits that are
feasible to run on today's quantum systems. This is a challenge, particularly
for quantum chemistry applications, considering the inherent complexity of
molecular systems. In this paper, we demonstrate advancements that expand the
size of chemistry problems that can be run on today's quantum systems by
applying hardware-efficient approaches, such as Quantum-Centric Data-Driven
Research and Development (QDDRD), optimized algorithms with reduced circuit
depth, and execute the experiments with middleware-supported quantum error
mitigation. We report up to a 29-fold reduction in circuit depth for covalent
drug molecules, enabling Hamiltonian dynamics for reactivity predictions,
assuming all-to-all connectivity of quantum hardware. When employed on IBMQ's
Heron architecture, we see up to a 16-fold reduction. The overarching impact of
this work is that it highlights promising methods that allow researchers to
explore the dynamics of commercially relevant chemistry on real quantum
hardware via Hamiltonian simulation.",223,2412.15804v1,quant-ph,quant-ph,molecular biology,2024-12-20,2024-12-23T21:06:58.562574
Electromagnetic particle-in-cell modeling of an electron cyclotron resonance plasma discharge in hydrogen,"A low pressure discharge sustained in molecular hydrogen with help of the
electron cyclotron resonance heating at a frequency of 2.45 GHz is simulated
using a fully electromagnetic implicit charge- and energy-conserving
particle-in-cell/Monte Carlo code. The simulations show a number of kinetic
effects, and the results are in good agreement with various experimentally
measured data such as electron density, electron temperature and degree of
dissociation. The electron energy distribution shows a tri-Maxwellian form due
to a number of different electron heating mechanisms, agreeing with the
experimental data in the measured electron energy interval. The simulation
results are also verified against a drift-diffusion model and proximity is
observed between the computational results for the plasma density at the
location of experimental measurement. However, the fluid approximation fails to
accurately predict radical density and electron temperature because of the
assumption of a single electron temperature. Special attention is paid to the
characteristics of hydrogen radicals, whose production is strongly
underestimated by the fluid model, whereas it is well predicted by the model
considered here. The energy distribution of such radicals demonstrates the
presence of a relatively large number of energetic hydrogen atoms produced by
the dissociation of molecular hydrogen. The new insights are of significance
for practical applications of hydrogen plasmas.",254,2412.15802v1,physics.plasm-ph,physics.plasm-ph,molecular biology,2024-12-20,2024-12-23T21:06:58.563571
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",molecular biology,2024-12-20,2024-12-23T21:06:58.563571
Room-temperature nonlinear transport and microwave rectification in antiferromagnetic MnBi$_2$Te$_4$ films,"The discovery of the nonlinear Hall effect provides an avenue for studying
the interplay among symmetry, topology, and phase transitions, with potential
applications in signal doubling and high-frequency rectification. However,
practical applications require devices fabricated on large area thin film as
well as room-temperature operation. Here, we demonstrate robust
room-temperature nonlinear transverse response and microwave rectification in
MnBi$_2$Te$_4$ films grown by molecular beam epitaxy. We observe multiple
sign-reversals in the nonlinear response by tuning the chemical potential.
Through theoretical analysis, we identify skew scattering and side jump,
arising from extrinsic spin-orbit scattering, as the main mechanisms underlying
the observed nonlinear signals. Furthermore, we demonstrate radio frequency
(RF) rectification in the range of 1-8 gigahertz at 300 K. These findings not
only enhance our understanding of the relationship between nonlinear response
and magnetism, but also expand the potential applications as energy harvesters
and detectors in high-frequency scenarios.",210,2412.15591v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,molecular biology,2024-12-20,2024-12-23T21:06:58.564568
Thin films as practical quantum materials: a status quo and beyond,"Quantum materials have been in the limelight for several years now. These
materials exhibit intriguing quantum phenomena, which when harnessed properly,
promise extraordinary advancements across various scientific and technological
domains. To fully exploit their potential, it is imperative to synthesize such
quantum materials in thin film form so that they are compatible with
well-established device fabrication techniques. In this perspective, an
overview of the current status and future directions of thin film quantum
material synthesis is provided. The criteria for quantum materials are
discussed, as well as the many benefits of preparing them as thin films.
Prominent deposition techniques such as molecular beam epitaxy and chemical
vapor deposition are reviewed along with potential contenders. Despite
challenges, progress in thin film quantum material technology holds the
potential to realize practical devices with unprecedented functionalities.",158,2412.15565v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,molecular biology,2024-12-20,2024-12-23T21:06:58.564568
Tailoring MBE Growth of c-Mn3Sn Directly on MgO (111): From Islands to Film,"We present our study of (0001) oriented Mn$_3$Sn (c-Mn$_3$Sn) thin films
synthesized directly on an MgO (111) substrate via molecular beam epitaxy. We
identify a growth window where Mn$_3$Sn growth can be controlled through slight
adjustments of the Mn flux, achieving either $\mu$m$^2$-sized high
crystalline-quality islands or an almost completely continuous film.
High-resolution X-ray diffraction results indicate that both films are highly
(0001) oriented. The atomic resolution images show clear film-substrate
interfaces displaying an epitaxial relationship. Scanning precession electron
diffraction measurements reveal that the island featured sample has highly
crystallized Mn3Sn. The sample featuring a high continuity exhibits defects in
some areas but retains the dominant Mn$_3$Sn structure. This work demonstrates
a potential method for synthesizing high crystalline-quality Mn3Sn films with
substantial coverage, facilitating the study of Mn3Sn films without the
influence of an additional buffer layer and promoting their application in
integrated spintronics.",237,2412.15442v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.app-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.565566
Superconductivity in Epitaxial SiGe for Cryogenic Electronics,"Introducing superconductivity into group IV elements by doping has long
promised a pathway to introduce quantum functionalities into well-established
semiconductor technologies. The non-equilibrium hyperdoping of group III atoms
into Si or Ge has successfully shown superconductivity can be achieved,
however, the origin of superconductivity has been obscured by structural
disorder and dopant clustering. Here, we report the epitaxial growth of
hyperdoped Ga:Ge films by molecular beam epitaxy with extreme hole
concentrations (n$_{h}$ = 4.15 $\times$ 10$^{21}$ cm$^{-3}$, ~17.9\% Ga
substitution) that yield superconductivity with a critical temperature of
T$_{C}$ = 3.5 K, and an out-of-plane critical field of 1 T at 270 mK.
Synchrotron-based X-ray absorption and scattering methods reveal that Ga
dopants are substitutionally incorporated within the Ge lattice, introducing a
tetragonal distortion to the crystal unit cell. Our findings, corroborated by
first-principles calculations, suggest that the structural order of Ga dopants
creates a flat band for the emergence of superconductivity in Ge, establishing
hyperdoped Ga:Ge as a low-disorder, epitaxial superconductor-semiconductor
platform.",293,2412.15421v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",molecular biology,2024-12-19,2024-12-23T21:06:58.566563
Machine learning assisted canonical sampling (MLACS),"The acceleration of material property calculations while maintaining ab
initio accuracy (1 meV/atom) is one of the major challenges in computational
physics. In this paper, we introduce a Python package enhancing the computation
of (finite temperature) material properties at the ab initio level using
machine learning interatomic potentials (MLIP). The Machine-Learning Assisted
Canonical Sampling (MLACS) method, grounded in a self-consistent variational
approach, iteratively trains a MLIP using an active learning strategy in order
to significantly reduce the computational cost of ab initio simulations.
  MLACS offers a modular and user-friendly interface that seamlessly integrates
Density Functional Theory (DFT) codes, MLIP potentials, and molecular dynamics
packages, enabling a wide range of applications, while maintaining a near-DFT
accuracy. These include sampling the canonical ensemble of a system, performing
free energy calculations, transition path sampling, and geometry optimization,
all by utilizing surrogate MLIP potentials, in place of ab initio calculations.
  This paper provides a comprehensive overview of the theoretical foundations
and implementation of the MLACS method. We also demonstrate its accuracy and
efficiency through various examples, showcasing the capabilities of the MLACS
package.",251,2412.15370v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.567560
Oscillations of a Water Droplet onto a Horizontally Vibrating Substrate,"Deformed droplets are ubiquitous in various industrial applications, such as
inkjet printing, lab-on-a-chip devices, and spray cooling, and can
fundamentally affect the involved applications both favorably and unfavorably.
Here, we employ many-body dissipative particle dynamics to investigate the
oscillations of water droplets on a harmonically and horizontally vibrating,
solid substrate. Three distinct scenarios of oscillations as a response to the
substrate vibrations have been identified. The first scenario reflects a common
situation where the droplet can follow the substrate vibrations. In the other
two scenarios, favored in the case of hydrophilic substrates, droplet
oscillations generate high shear rates that ultimately lead to droplet breakup.
Leveraging our simulation model, the properties of the droplet and the
mechanisms related to the oscillations are analyzed with a molecular-level
resolution, while results are also put in the perspective of experiment. Our
study suggests that the three scenarios can be distinguished by the
contact-surface velocity of the oscillating droplet, with threshold velocities
influenced by the substrate's wettability. Moreover, the mean magnitude of the
particle velocity at the contact surface plays a key role in determining the
three oscillation phases, suggesting that the capillary number of the
oscillating droplet governs the phase behavior. Thus, our approach aims to
optimize droplet oscillations and deformations on solid substrates, which have
direct implications for technological applications.",311,2412.15125v1,physics.flu-dyn,"physics.flu-dyn,cond-mat.soft",molecular biology,2024-12-19,2024-12-23T21:06:58.567560
A Full Transformer-based Framework for Automatic Pain Estimation using Videos,"The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.",108,2412.15095v1,cs.CV,"cs.CV,cs.AI,cs.LG",molecular biology,2024-12-19,2024-12-23T21:06:58.568558
Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation,"We consider the conditional generation of 3D drug-like molecules with
\textit{explicit control} over molecular properties such as drug-like
properties (e.g., Quantitative Estimate of Druglikeness or Synthetic
Accessibility score) and effectively binding to specific protein sites. To
tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and
factorize the latent space of our generative model into two disentangled
aspects: molecular properties and the remaining structural context of 3D
molecules. Our model ensures explicit control over these molecular attributes
while maintaining equivariance of coordinate representation and invariance of
data likelihood. Furthermore, we introduce a novel alignment-based coordinate
loss to adapt equivariant networks for auto-regressive de-novo 3D molecule
generation from scratch. Extensive experiments validate our model's
effectiveness on property-guided and context-guided molecule generation, both
for de-novo 3D molecule design and structure-based drug discovery against
protein targets.",208,2412.15086v1,cs.LG,"cs.LG,cs.AI",molecular biology,2024-12-19,2024-12-23T21:06:58.568558
Velocity Jumps for Molecular Dynamics,"We introduce the Velocity Jumps approach, denoted as JUMP, a new class of
Molecular dynamics integrators, replacing the Langevin dynamics by a hybrid
model combining a classical Langevin diffusion and a piecewise deterministic
Markov process, where the expensive computation of long-range pairwise
interactions is replaced by a resampling of the velocities at random times.
This framework allows for an acceleration in the simulation speed while
preserving sampling and dynamical properties such as the diffusion constant. It
can also be integrated in classical multi-timestep methods, pushing further the
computational speedup, while avoiding some of the resonance issues of the
latter thanks to the random nature of jumps. The JUMP, JUMP-RESPA and
JUMP-RESPA1 integrators have been implemented in the GPU-accelerated version of
the Tinker-HP package and are shown to provide significantly enhanced
performances compared to their BAOAB, BAOAB-RESPA and BAOAB-RESPA1 counterparts
respectively.",203,2412.15073v1,physics.comp-ph,"physics.comp-ph,physics.chem-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.569556
Enhancing dynamic range through quantum deamplification,"Balancing high sensitivity with a broad dynamic range (DR) is a fundamental
challenge in measurement science, as improving one often compromises the other.
While traditional quantum metrology has prioritized enhancing local
sensitivity, a large DR is crucial for applications such as atomic clocks,
where extended phase interrogation times contribute to wider phase range. In
this Letter, we introduce a novel quantum deamplification mechanism that
extends DR at a minimal cost of sensitivity. Our approach uses two sequential
spin-squeezing operations to generate and detect an entangled probe state,
respectively. We demonstrate that the optimal quantum interferometer limit can
be approached through two-axis counter-twisting dynamics. Further expansion of
DR is possible by using sequential quantum deamplification interspersed with
phase encoding processes. Additionally, we show that robustness against
detection noise can be enhanced by a hybrid sensing scheme that combines
quantum deamplification with quantum amplification. Our protocol is within the
reach of state-of-the-art atomic-molecular-optical platforms, offering a
scalable, noise-resilient pathway for entanglement-enhanced metrology.",225,2412.15061v1,quant-ph,"quant-ph,cond-mat.quant-gas",molecular biology,2024-12-19,2024-12-23T21:06:58.570552
Non-Markovian Effects in Quantum Rate Calculations of Hydrogen Diffusion with Electronic Friction,"We address the challenge of incorporating non-Markovian electronic friction
effects in quantum-mechanical approximations of dynamical observables. A
generalized Langevin equation (GLE) is formulated for ring-polymer molecular
dynamics (RPMD) rate calculations, which combines electronic friction with a
description of nuclear quantum effects (NQEs) for adsorbates on metal surfaces.
An efficient propagation algorithm is introduced that captures both the spatial
dependence of friction strength and non-Markovian frictional memory. This
framework is applied to a model of hydrogen diffusing on Cu(111) derived from
ab initio density functional theory (DFT) calculations, revealing significant
alterations in rate constants and tunnelling crossover temperatures due to
non-Markovian effects. Our findings explain why previous classical molecular
dynamics simulations with Markovian friction showed unexpectedly good agreement
with experiment, highlighting the critical role of non-Markovian effects in
first-principles atomistic simulations.",192,2412.15014v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.chem-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.570552
Observation of liquid-solid transition of nanoconfined water at ambient temperature,"Nanoconfined water plays an indispensable role in various phenomena in
biology, chemistry, and engineering. It exhibits many abnormal properties
compared to bulk water, especially under strong confinement. However, the
origin of those anomalies is still elusive due to the lack of structural
information on hydrogen-bonding networks. Considering the inhomogeneity of the
nanocavity and the tiny amount of water molecules, conventional optical
spectroscopies and nuclear magnetic resonance (NMR) fail to realize the
structure analysis of nanoconfined water. Here, we addressed this issue by
combining scanning probe microscopy (SPM) with advanced quantum sensing(QS)
based on an atomic-size quantum sensor like nitrogen-vacancy (NV) center in
diamond, which can apply the nanoscale-NMR for characterizing both the dynamics
and structure of confined water at ambient conditions. We built a
two-dimensional (2D) nanoconfined water system with a hexagonal-boron nitride
(hBN) flake and a hydrophilic diamond surface. By using the SPM tip to measure
the confinement size precisely, we observed a critical confinement size of ~2
nm, below which the water diffusion was significantly suppressed and the
hydrogen-bonding network of water showed an ordered structure. Meanwhile,
molecular dynamics (MD) simulation revealed a solid-like water contact layer on
the diamond surface under strong confinement, which also reproduced the
measured nanoscale-NMR spectra and confirmed the liquid-solid phase transition
observed in the experiments. Notably, with this new SPM-QS platform, our
results showed a promising way to elucidate the abnormal properties of
nanoconfined water in future applications.",350,2412.15001v1,cond-mat.mes-hall,cond-mat.mes-hall,molecular biology,2024-12-19,2024-12-23T21:06:58.571550
The liquid-liquid phase transition of hydrogen and its critical point: Analysis from ab initio simulation and a machine-learned potential,"We simulate high-pressure hydrogen in its liquid phase close to molecular
dissociation using a machine-learned interatomic potential. The model is
trained with density functional theory (DFT) forces and energies, with the
Perdew-Burke-Ernzerhof (PBE) exchange-correlation functional. We show that an
accurate NequIP model, an E(3)-equivariant neural network potential, accurately
reproduces the phase transition present in PBE. Moreover, the computational
efficiency of this model allows for substantially longer molecular dynamics
trajectories, enabling us to perform a finite-size scaling (FSS) analysis to
distinguish between a crossover and a true first-order phase transition. We
locate the critical point of this transition, the liquid-liquid phase
transition (LLPT), at 1200-1300 K and 155-160 GPa, a temperature lower than
most previous estimates and close to the melting transition.",194,2412.14953v1,cond-mat.stat-mech,cond-mat.stat-mech,molecular biology,2024-12-19,2024-12-23T21:06:58.572547
Diffusion priors for Bayesian 3D reconstruction from incomplete measurements,"Many inverse problems are ill-posed and need to be complemented by prior
information that restricts the class of admissible models. Bayesian approaches
encode this information as prior distributions that impose generic properties
on the model such as sparsity, non-negativity or smoothness. However, in case
of complex structured models such as images, graphs or three-dimensional (3D)
objects,generic prior distributions tend to favor models that differ largely
from those observed in the real world. Here we explore the use of diffusion
models as priors that are combined with experimental data within a Bayesian
framework. We use 3D point clouds to represent 3D objects such as household
items or biomolecular complexes formed from proteins and nucleic acids. We
train diffusion models that generate coarse-grained 3D structures at a medium
resolution and integrate these with incomplete and noisy experimental data. To
demonstrate the power of our approach, we focus on the reconstruction of
biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which
is an important inverse problem in structural biology. We find that posterior
sampling with diffusion model priors allows for 3D reconstruction from very
sparse, low-resolution and partial observations.",244,2412.14897v1,cs.LG,cs.LG,molecular biology,2024-12-19,2024-12-23T21:06:58.572547
Individual assembly of two-species Rydberg molecules using optical tweezers,"We present a new approach to investigating Rydberg molecules by demonstrating
the formation and characterization of individual Rb$^{*}$Cs Rydberg molecules
using optical tweezers. By employing single-atom detection of Rb and Cs, we
observe molecule formation via correlated loss of both species and study the
formation dynamics with single-particle resolution. We control the interatomic
distances by manipulating the relative wavefunction of atom pairs using the
tweezer intensity, optimizing the coupling to molecular states and exploring
the effect of the tweezer on these states. Additionally, we demonstrate
molecule association with atoms trapped in separate tweezers, paving the way
for state-selective assembly of polyatomic molecules. The observed binding
energies, molecular alignment, and bond lengths are in good agreement with
theory. Our approach is broadly applicable to Rydberg tweezer platforms,
expanding the range of available molecular systems and enabling the integration
of Rydberg molecules into existing quantum science platforms.",195,2412.14888v1,physics.atom-ph,"physics.atom-ph,quant-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.573544
Rydberg states and new resonant states of the imidogen molecule NH: pathways for nitrogen release,"Neutral resonant states of molecules play a very important role in the
dissociation dynamics and other electronic processes that occur via
intermediate capture into these states. With the goal of identifying resonant
states, and their corresponding widths, of the imidogen molecule NH as a
function of internuclear distance, we have performed detailed R-matrix
calculations on the e + NH+ system. In a previous work, we had identified bound
states of NH and Feshbach resonances in the e + NH+ system at a single
geometry, namely the NH+ equilibrium Re = 2.0205 a0 . Here we present a much
more detailed work by repeating the calculation on over 60 internuclear
distances to obtain the corresponding potential energy curves. The bound states
for nine symmetries have been detailed many of which, particularly the singlet
states, were never studied before. Several resonant states of different
symmetries, which were unknown until now, have been systematically identified
and their widths calculated in the present work, which proved much more
challenging due to presence of many avoided crossings. It is hoped that the
bound and the new resonant states obtained here will open up other molecular
dynamics studies, since for several dissociative processes, although
experimental data existed for more than a decade, these are still
uncorroborated due to absence of molecular data, and hence subsequent
theoretical calculations.",288,2412.14830v1,physics.atom-ph,"physics.atom-ph,physics.plasm-ph,quant-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.574542
Polarized Dust Emission in Arp220: Magnetic Fields in the Core of an Ultraluminous Infrared Galaxy,"Arp 220 is the prototypical Ultraluminous Infrared Galaxy (ULIRG), and one of
the brightest objects in the extragalactic far-infrared sky. It is the result
of a merger between two gas rich spiral galaxies which has triggered
starbursting activity in the merger nuclear regions. Observations with the
Submillimeter Array centred at a frequency of 345 GHz and with a synthesised
beamsize of 0.77 x 0.45 arcseconds were used to search for polarized dust
emission from the nuclear regions of Arp 220. Polarized dust emission was
clearly detected at 6 sigma significance associated with the brighter, western
nucleus, with a peak polarization fraction of 2.7 +/- 0.35 per cent somewhat
offset from the western nucleus. A suggestive 2.6 sigma signal is seen from the
fainter eastern nucleus. The dust emission polarization is oriented roughly
perpendicular to the molecular disk in the western nucleus suggesting that the
magnetic field responsible is orientated broadly in the plane of the disk, but
may be being reordered by the interaction between the two nuclei. Unlike more
evolved interacting systems, we see no indication that the magnetic field is
being reordered by the outflow from the western nucleus. These observations are
the first detection of dust polarization, and thus of magnetic fields, in the
core of a ULIRG.",279,2412.14770v1,astro-ph.GA,astro-ph.GA,molecular biology,2024-12-19,2024-12-23T21:06:58.574542
Milestones at the Origin of Life,"Living organisms have some common structures, chemical reactions and
molecular structures. The organisms consist of cells with cell division, they
have homochirality of protein and carbohydrate units, and metabolism, and
genetics, and they are mortal. The molecular structures and chemical reactions
underlying these features are common from the simplest bacteria to human
beings. The origin of life is evolutionary with the emergence of a network of
spontaneous biochemical reactions, and the evolution has taken place over a
very long time. The evolution contains, however some ""landmarks"" and
bottlenecks, which in a revolutionary manner directed the evolution, and the
article tries to establish the order of these events. The article advocates
that a possible order in the emergence of life is that the first milestone in
prebiotic evolution is at the emergence of homochirality in proteins. The
homochirality of peptides is, however, with instability and racemization which
causes aging of the peptides and mortality. The metabolism and genetics are
established through homochiral enzymes in the Earth's crust for $\approx$ 4 Gyr
ago. Finally, the cells with cell division are established in the Hot Springs
environment at the interface between the crust and the Hadean Ocean.",249,2412.14754v1,q-bio.PE,"q-bio.PE,astro-ph.EP",molecular biology,2024-12-19,2024-12-23T21:06:58.575541
Towards a mathematical framework for modelling cell fate dynamics,"An adult human body is made up of some 30 to 40 trillion cells, all of which
stem from a single fertilized egg cell. The process by which the right cells
appear to arrive in their right numbers at the right time at the right place --
development -- is only understood in the roughest of outlines. This process
does not happen in isolation: the egg, the embryo, the developing foetus, and
the adult organism all interact intricately with their changing environments.
Conceptual and, increasingly, mathematical approaches to modelling development
have centred around Waddington's concept of an epigenetic landscape. This
perspective enables us to talk about the molecular and cellular factors that
contribute to cells reaching their terminally differentiated state: their fate.
The landscape metaphor is however only a simplification of the complex process
of development; it for instance does not consider environmental influences, a
context which we argue needs to be explicitly taken into account and from the
outset. When delving into the literature, it also quickly becomes clear that
there is a lack of consistency and agreement on even fundamental concepts; for
example, the precise meaning of what we refer to when talking about a `cell
type' or `cell state.' Here we engage with previous theoretical and
mathematical approaches to modelling cell fate -- focused on trees, networks,
and landscape descriptions -- and argue that they require a level of
simplification that can be problematic. We introduce random dynamical systems
as one natural alternative. These provide a flexible conceptual and
mathematical framework that is free of extraneous assumptions. We develop some
of the basic concepts and discuss them in relation to now `classical'
depictions of cell fate dynamics, in particular Waddington's landscape.",347,2412.14726v1,q-bio.SC,"q-bio.SC,cond-mat.stat-mech,math.DS,q-bio.CB",molecular biology,2024-12-19,2024-12-23T21:06:58.576537
Computing Gram Matrix for SMILES Strings using RDKFingerprint and Sinkhorn-Knopp Algorithm,"In molecular structure data, SMILES (Simplified Molecular Input Line Entry
System) strings are used to analyze molecular structure design. Numerical
feature representation of SMILES strings is a challenging task. This work
proposes a kernel-based approach for encoding and analyzing molecular
structures from SMILES strings. The proposed approach involves computing a
kernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal
component analysis (PCA) for dimensionality reduction. The resulting
low-dimensional embeddings are then used for classification and regression
analysis. The kernel matrix is computed by converting the SMILES strings into
molecular structures using the Morgan Fingerprint, which computes a fingerprint
for each molecule. The distance matrix is computed using the pairwise kernels
function. The Sinkhorn-Knopp algorithm is used to compute the final kernel
matrix that satisfies the constraints of a probability distribution. This is
achieved by iteratively adjusting the kernel matrix until the marginal
distributions of the rows and columns match the desired marginal distributions.
We provided a comprehensive empirical analysis of the proposed kernel method to
evaluate its goodness with greater depth. The suggested method is assessed for
drug subcategory prediction (classification task) and solubility AlogPS
``Aqueous solubility and Octanol/Water partition coefficient"" (regression task)
using the benchmark SMILES string dataset. The outcomes show the proposed
method outperforms several baseline methods in terms of supervised analysis and
has potential uses in molecular design and drug discovery. Overall, the
suggested method is a promising avenue for kernel methods-based molecular
structure analysis and design.",319,2412.14717v1,cs.LG,cs.LG,molecular biology,2024-12-19,2024-12-23T21:06:58.577534
The JWST/NIRSpec view of the nuclear region in the prototypical merging galaxy NGC 6240,"Merger events are thought to be an important phase in the assembly of massive
galaxies. At the same time, Active Galactic Nuclei (AGN) play a fundamental
role in the evolution of their star formation histories. Both phenomena can be
observed at work in NGC 6240, a local prototypical merger, classified as an
UltraLuminous InfraRed Galaxy (ULIRG) thanks to its elevated infrared
luminosity. Interestingly, NGC 6240 hosts two AGN separated by 1.5''(~ 735 pc),
detected in both X-ray and radio band. Taking advantage of the unprecedented
sensitivity and wavelength coverage provided by the Integral Field Unit (IFU)
of the NIRSpec instrument onboard JWST, we observed the nuclear region of NGC
6240 in a FoV of 3.7'' x 3.7''(1.9 x 1.9 kpc^2), to investigate gas kinematics
and InterStellar Medium (ISM) properties with a high spatial resolution of ~
0.1'' (or ~ 50 pc). We separated the different gas kinematic components through
multi-Gaussian fitting and studied the excitation properties of the ISM from
the NIR diagnostic diagram based on the H_2 1-0 S(1)/BrGamma and [Fe
II]1.257micron/PaBeta lines ratios. We isolated the ionization cones of the two
nuclei, and detected coronal lines emission from both of them. Using H_2 line
ratios, we found that the molecular hydrogen gas is excited mostly by thermal
processes. We computed a hot molecular gas mass of 1.3 x 10^5 M_sun and an
ionized gas mass in the range of 10^5 - 10^7 M_sun. We studied with
unprecedented spatial resolution and sensitivity the kinematics of the
molecular and ionized gas phases. We revealed the complex structure of the
molecular gas and found a blueshifted outflow near the Southern nucleus,
together with filaments connecting a highly redshifted H_2 cloud with the two
nuclei. We speculate on the possible nature of this H_2 cloud and propose two
possible scenarios: either outflowing gas, or a tidal cloud falling onto the
nuclei.",468,2412.14685v1,astro-ph.GA,astro-ph.GA,molecular biology,2024-12-19,2024-12-23T21:06:58.579528
Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural Network Force Field Performance with Only Dozens of Additional Parameters,"At the heart of neural network force fields (NNFFs) is the architecture of
neural networks, where the capacity to model complex interactions is typically
enhanced through widening or deepening multilayer perceptrons (MLPs) or by
increasing layers of graph neural networks (GNNs). These enhancements, while
improving the model's performance, often come at the cost of a substantial
increase in the number of parameters. By applying the Trainable Adaptive
Activation Function Structure (TAAFS), we introduce a method that selects
distinct mathematical formulations for non-linear activations, thereby
increasing the precision of NNFFs with an insignificant addition to the
parameter count. In this study, we integrate TAAFS into a variety of neural
network models, resulting in observed accuracy improvements, and further
validate these enhancements through molecular dynamics (MD) simulations using
DeepMD.",178,2412.14655v1,cs.LG,cs.LG,molecular biology,2024-12-19,2024-12-23T21:06:58.579528
Dual Photonics Probing of Nano- to Submicron-Scale Structural Alterations in Human Brain Tissues or Cells and Chromatin or DNA with the Progression of Alzheimers Disease,"Understanding alterations in structural disorders in tissue or cells or
building blocks, such as DNA or chromatin in the human brain, at the nano to
submicron level provides us with efficient biomarkers for Alzheimers detection.
Here, we report a dual photonics technique to detect nano- to submicron-scale
alterations in brain tissues or cells and DNA or chromatin due to the early to
late progression of Alzheimers disease in humans. Using a recently developed
mesoscopic light transport technique, fine-focused nano-sensitive partial wave
spectroscopy (PWS), we measure the degree of structural disorder in tissues.
Furthermore, the chemical-specific inverse participation ratio technique (IPR)
was used to measure the DNA or chromatin structural alterations. The results of
the PWS and IPR experiments showed a significant increase in the degree of
structural disorder at the nano to submicron scale at different stages of AD
relative to their controls for both the tissue or cell and DNA cellular levels.
The increase in the structural disorder in cells or tissues and DNA or
chromatin in the nuclei can be attributed to higher mass density fluctuations
in the tissue and DNA or chromatin damage in the nuclei caused by the
rearrangements of macromolecules due to the deposition of the amyloid beta
protein and damage in DNA or chromatin with the progress of AD.",279,2412.14651v1,physics.med-ph,"physics.med-ph,physics.bio-ph,physics.optics",molecular biology,2024-12-19,2024-12-23T21:06:58.580526
Quantum expectation value estimation by doubling the number of qubits,"Expectation value estimation is ubiquitous in quantum algorithms. The
expectation value of a Hamiltonian, which is essential in various practical
applications, is often estimated by measuring a large number of Pauli strings
on quantum computers and performing classical post-processing. In the case of
$n$-qubit molecular Hamiltonians in quantum chemistry calculations, it is
necessary to evaluate $O(n^4)$ Pauli strings, requiring a large number of
measurements for accurate estimation. To reduce the measurement cost, we assess
an existing idea that uses two copies of an $n$-qubit quantum state of interest
and coherently measures them in the Bell basis, which enables the simultaneous
estimation of the absolute values of expectation values of all the $n$-qubit
Pauli strings. We numerically investigate the efficiency of energy estimation
for molecular Hamiltonians of up to 12 qubits. The results show that, when the
target precision is no smaller than tens of milli-Hartree, this method requires
fewer measurements than conventional sampling methods. This suggests that the
method may be useful for many applications that rely on expectation value
estimation of Hamiltonians and other observables as well when moderate
precision is sufficient.",242,2412.14466v1,quant-ph,quant-ph,molecular biology,2024-12-19,2024-12-23T21:06:58.581523
Time-Reversal Symmetry-Protected Coherent Control of Ultracold Molecular Collisions,"Coherent control of atomic and molecular scattering relies on the preparation
of colliding particles in superpositions of internal states, establishing
interfering pathways that can be used to tune the outcome of a scattering
process. However, incoherent addition of different partial wave contributions
to the integral cross sections (partial wave scrambling), commonly encountered
in systems with complex collisional dynamics, poses a significant challenge,
often limiting control. This work demonstrates that time-reversal symmetry can
overcome these limitations by constraining the relative phases of S-matrix
elements, thereby protecting coherent control against partial wave scrambling,
even for collisions mediated by highly anisotropic interactions. Using the
example of ultracold O$_2$-O$_2$ scattering, we show that coherent control is
robust against short-range dynamical complexity. Furthermore, the time-reversal
symmetry also protects the control against a distribution of collisional
energy. These findings show that ultracold scattering into the final states
that are time-reversal-invariant, such as the J = 0, M = 0 rotational state,
can always be optimally controlled by using time-reversal-invariant initial
superpositions. Beyond the ultracold regime, we observe significant differences
in the controllability of crossed-molecular beam vs. trap experiments with the
former being easier to control, emphasizing the cooperative role of
time-reversal and permutation symmetries in maintaining control at any
temperature. These results open new avenues for the coherent control of complex
inelastic collisions and chemical reactions both in and outside of the
ultracold regime.",319,2412.14425v1,physics.atom-ph,"physics.atom-ph,physics.chem-ph,quant-ph",molecular biology,2024-12-19,2024-12-23T21:06:58.581523
Gaussian-convolution-invariant shell approximation to spherically-symmetric functions,"We develop a class of functions Omega_N(x; mu, nu) in N-dimensional space
concentrated around a spherical shell of the radius mu and such that, being
convoluted with an isotropic Gaussian function, these functions do not change
their expression but only a value of its 'width' parameter, nu. Isotropic
Gaussian functions are a particular case of Omega_N(x; mu, nu) corresponding to
mu = 0. Due to their features, these functions are an efficient tool to build
approximations to smooth and continuous spherically-symmetric functions
including oscillating ones. Atomic images in limited-resolution maps of the
electron density, electrostatic scattering potential and other scalar fields
studied in physics, chemistry, biology, and other natural sciences are examples
of such functions. We give simple analytic expressions of Omega_N(x; mu, nu)
for N = 1, 2, 3 and analyze properties of these functions. Representation of
oscillating functions by a sum of Omega_N(x; mu, nu) allows calculating
distorted maps for the same cost as the respective theoretical fields. We give
practical examples of such representation for the interference functions of the
uniform unit spheres for N = 1, 2, 3 that define the resolution of the
respective images. Using the chain rule and analytic expressions of the
Omega_N(x; mu, nu) derivatives makes simple refinement of parameters of the
models which describe these fields.",303,2412.14350v1,math.NA,"math.NA,cs.CE,cs.NA,math-ph,math.MP,q-bio.BM",molecular biology,2024-12-18,2024-12-23T21:06:58.582520
Revisiting the Nowosiółka skull with RMaCzek,"One of the first fully quantitative distance matrix visualization methods was
proposed by Jan Czekanowski at the beginning of the previous century. Recently,
a software package, RMaCzek, was made available that allows for producing such
diagrams in R. Here we reanalyze the original data that Czekanowski used for
introducing his method, and in the accompanying code show how the user can
specify their own custom distance functions in the package.",91,2412.14343v1,stat.AP,"stat.AP,q-bio.PE,62H99, 62-04, 92B10",molecular biology,2024-12-18,2024-12-23T21:06:58.582520
GREGoR: Accelerating Genomics for Rare Diseases,"Rare diseases are collectively common, affecting approximately one in twenty
individuals worldwide. In recent years, rapid progress has been made in rare
disease diagnostics due to advances in DNA sequencing, development of new
computational and experimental approaches to prioritize genes and genetic
variants, and increased global exchange of clinical and genetic data. However,
more than half of individuals suspected to have a rare disease lack a genetic
diagnosis. The Genomics Research to Elucidate the Genetics of Rare Diseases
(GREGoR) Consortium was initiated to study thousands of challenging rare
disease cases and families and apply, standardize, and evaluate emerging
genomics technologies and analytics to accelerate their adoption in clinical
practice. Further, all data generated, currently representing ~7500 individuals
from ~3000 families, is rapidly made available to researchers worldwide via the
Genomic Data Science Analysis, Visualization, and Informatics Lab-space (AnVIL)
to catalyze global efforts to develop approaches for genetic diagnoses in rare
diseases (https://gregorconsortium.org/data). The majority of these families
have undergone prior clinical genetic testing but remained unsolved, with most
being exome-negative. Here, we describe the collaborative research framework,
datasets, and discoveries comprising GREGoR that will provide foundational
resources and substrates for the future of rare disease genomics.",269,2412.14338v1,q-bio.OT,q-bio.OT,molecular biology,2024-12-18,2024-12-23T21:06:58.583518
Henry constant of helium in liquid lead-lithium alloys,"The solubility of helium in liquid metals is a knowledge of fundamental
importance in the design of the future nuclear fusion reactors, since the
formation of helium bubbles inside the breeding blankets of the reactors can be
a threat to the durability of the devices and, more importantly, to the
efficiency of tritium recovery. In the present work we report a detailed set of
calculations of the solubility of helium in lead and lead-lithium alloys. A
series of molecular dynamics simulations have been combined with a classical
perturbative procedure able to compute the free energy of insertion of a helium
atom inside a liquid metal bath, directly related to the solubility of helium.
As the most important case, the concentration of the eutectic solution has been
explored in full detail. We have found that solubility of helium in pure
lithium is lower than in pure lead, predicting a value at the eutectic state
(16% Li-84% Pb at 508 K) of about $5 \times 10^{-16}$ Pa$^{-1}$. The observed
trend indicates that solubilties rise with increasing temperatures.",235,2412.14152v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,molecular biology,2024-12-18,2024-12-23T21:06:58.584515
Re-visiting the intracellular pathway of transferrin on board of a mathematical simulation,"Modeling and simulation are transforming all fields of biology. Tools like
AlphaFold have revolutionized structural biology, while molecular dynamics
simulations provide invaluable insights into the behavior of macromolecules in
solution or on membranes. In contrast, we lack effective tools to represent the
dynamic behavior of the endomembrane system. Static diagrams that connect
organelles with arrows are used to depict transport across space and time but
fail to specify the underlying mechanisms. This static representation obscures
the dynamism of intracellular traffic, freezing it in an immobilized framework.
The intracellular transport of transferrin, a key process for cellular iron
delivery, is among the best-characterized trafficking pathways. In this
commentary, we revisit this process using a mathematical simulation of the
endomembrane system. Our model reproduces many experimental observations and
highlights the strong contrast between dynamic simulations and static
illustrations. This work underscores the urgent need for a consensus-based
minimal functional working model for the endomembrane system and emphasizes the
importance of generating more quantitative experimental data -- including
precise measurements of organelle size, volume, and transport kinetics --
practices that were more common among cell biologists in past decades.",247,2412.14131v1,q-bio.SC,q-bio.SC,molecular biology,2024-12-18,2024-12-23T21:06:58.585513
Machine Learning Co-pilot for Screening of Organic Molecular Additives for Perovskite Solar Cells,"Machine learning (ML) has been extensively employed in planar perovskite
photovoltaics to screen effective organic molecular additives, while
encountering predictive biases for novel materials due to small datasets and
reliance on predefined descriptors. Present work thus proposes an effective
approach, Co-Pilot for Perovskite Additive Screener (Co-PAS), an ML-driven
framework designed to accelerate additive screening for perovskite solar cells
(PSCs). Co-PAS overcomes predictive biases by integrating the Molecular
Scaffold Classifier (MSC) for scaffold-based pre-screening and utilizing
Junction Tree Variational Autoencoder (JTVAE) latent vectors to enhance
molecular structure representation, thereby enhancing the accuracy of power
conversion efficiency (PCE) predictions. Leveraging Co-PAS, we integrate domain
knowledge to screen an extensive dataset of 250,000 molecules from PubChem,
prioritizing candidates based on predicted PCE values and key molecular
properties such as donor number, dipole moment, and hydrogen bond acceptor
count. This workflow leads to the identification of several promising
passivating molecules, including the novel Boc-L-threonine N-hydroxysuccinimide
ester (BTN), which, to our knowledge, has not been explored as an additive in
PSCs and achieves a device PCE of 25.20%. Our results underscore the potential
of Co-PAS in advancing additive discovery for high-performance PSCs.",320,2412.14109v1,cs.LG,"cs.LG,cond-mat.mtrl-sci,physics.app-ph",molecular biology,2024-12-18,2024-12-23T21:06:58.586510
Dynamical Screening of Local Spin Moments at Metal-Molecule Interfaces,"Transition-metal phthalocyanine molecules have attracted considerable
interest in the context of spintronics device development due to their
amenability to diverse bonding regimes and their intrinsic magnetism. The
latter is highly influenced by the quantum fluctuations that arise at the
inevitable metal-molecule interface in a device architecture. In this study, we
have systematically investigated the dynamical screening effects in
phthalocyanine molecules hosting a series of transition-metal ions (Ti, V, Cr,
Mn, Fe, Co, and Ni) in contact with the Cu(111) surface. Using comprehensive
density functional theory plus Anderson's Impurity Model calculations, we show
that the orbital-dependent hybridization and electron correlation together
result in strong charge and spin fluctuations. While the instantaneous spin
moments of the transition-metal ions are near atomic-like, we find that
screening gives rise to considerable lowering or even quenching of these. Our
results highlight the importance of quantum fluctuations in metal-contacted
molecular devices, which may influence the results obtained from theoretical or
experimental probes, depending on their possibly material-dependent
characteristic sampling time-scales.",228,2412.14078v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci,physics.chem-ph,quant-ph",molecular biology,2024-12-18,2024-12-23T21:06:58.586510
Using Partial Structure R1 to Do Molecular Replacement Calculations,"The concept of partial structure R1 (pR1) is a generalization of the concept
of single atom R1 (sR1) (Zhang & Donahue, 2024). The hypothesis is that the
deepest hole of a pR1 map determines the orientation and location of a missing
fragment. In current implementation, the calculation is divided into two steps.
The first step is to detect possible orientations of all missing fragments by
the holes of a pR1 map of a free-standing fragment in a 3-dimensional
orientation space. The second step is to determine the orientation and location
of a missing fragment. To this end, if done strictly, all the candidate
orientations are tried. With each candidate orientation, the best choice of
location of the missing fragment is determined by the deepest hole of a pR1 map
in a 3-dimensional location space. This best choice is combined with the trial
orientation to form one candidate orientation-location. After trying all
candidate orientations, a list of candidate orientation-locations are formed,
from which, the one with the lowest R1 determines the orientation and location
of a missing fragment. Then a newer pR1 is defined by including the atoms of
this newly determined fragment into the known atoms. This newer pR1 is used to
determine the next missing fragment in the same way. To shorten the calculation
time, the possible locations of all missing fragments can be predicted by the
holes of a pR1 map of a completely disoriented model of a fragment. All these
ideas of using pR1 to do molecular replacement calculations have been
demonstrated by four example data sets.",322,2412.14034v1,physics.data-an,physics.data-an,molecular biology,2024-12-18,2024-12-23T21:06:58.587507
Machine learning in wastewater treatment: insights from modelling a pilot denitrification reactor,"Wastewater treatment plants are increasingly recognized as promising
candidates for machine learning applications, due to their societal importance
and high availability of data. However, their varied designs, operational
conditions, and influent characteristics hinder straightforward automation. In
this study, we use data from a pilot reactor at the Veas treatment facility in
Norway to explore how machine learning can be used to optimize biological
nitrate ($\mathrm{NO_3^-}$) reduction to molecular nitrogen ($\mathrm{N_2}$) in
the biogeochemical process known as \textit{denitrification}. Rather than
focusing solely on predictive accuracy, our approach prioritizes understanding
the foundational requirements for effective data-driven modelling of wastewater
treatment. Specifically, we aim to identify which process parameters are most
critical, the necessary data quantity and quality, how to structure data
effectively, and what properties are required by the models. We find that
nonlinear models perform best on the training and validation data sets,
indicating nonlinear relationships to be learned, but linear models transfer
better to the unseen test data, which comes later in time. The variable
measuring the water temperature has a particularly detrimental effect on the
models, owing to a significant change in distributions between training and
test data. We therefore conclude that multiple years of data is necessary to
learn robust machine learning models. By addressing foundational elements,
particularly in the context of the climatic variability faced by northern
regions, this work lays the groundwork for a more structured and tailored
approach to machine learning for wastewater treatment. We share publicly both
the data and code used to produce the results in the paper.",334,2412.14030v1,cs.LG,cs.LG,molecular biology,2024-12-18,2024-12-23T21:06:58.588504
Cold source of atomic hydrogen for loading large magnetic traps,"We present a design and performance tests of an intense source of cold
hydrogen atoms for loading large magnetic traps. Our source is based on a
cryogenic dissociator of molecular hydrogen at 0.6 K followed by a series of
thermal accommodators at 0.5, 0.2 and 0.13 K with inner surfaces covered by a
superfluid helium film. All components are thermally anchored to corresponding
stages of a dilution refrigerator. The source provides a continuous flux of
7$\cdot$$10^{13}$ H atoms/s in a temperature range of 130-200 mK. We have
successfully used the source for loading a large Ioffe-Pritchard magnetic trap
recently built in our laboratory [arXiv:2108.09123 or Rev. Sci. Instr. 93 (2),
023201 (2022)]. Calorimetric measurements of the atomic recombination heat
allow reliable determination of the atomic flux and H gas density in the trap.
We have tested the performance of the source and loading of H atoms into the
trap at various configurations of the trapping field, reducing the magnetic
barrier height to 75% and 50% of the nominal value of 0.8 T (0.54 K) as well as
at the open configuration of the trap at its lower end, when the atoms are in
contact with the trapping cell walls covered by a superfluid helium film. In
the latter case, raising the trapping cell temperature to 200-250 mK, the
low-field seeking atoms at densities exceeding 10$^{11}$ cm$^{-3}$ can be
stored for the time over 1000 s, sufficiently long for experiments on precision
spectroscopy of cold H gas.",359,2412.13981v2,physics.atom-ph,physics.atom-ph,molecular biology,2024-12-18,2024-12-23T21:06:58.589502
Ion Sieving in Two-Dimensional Membranes from First Principles,"A first-principles approach for calculating ion separation in solution
through two-dimensional (2D) membranes is proposed and applied. Ionic energy
profiles across the membrane are obtained first, where solvation effects are
simulated explicitly with machine-learning molecular dynamics, electrostatic
corrections are applied to remove finite-size capacitive effects, and a
mean-field treatment of the charging of the electrochemical double layer is
used. Entropic contributions are assessed analytically and validated against
thermodynamic integration. Ionic separations are then inferred through a
microkinetic model of the filtration process, accounting for steady-state
charge separation effects across the membrane. The approach is applied to
Li$^{+}$, Na$^{+}$, K$^{+}$ sieving through a crown-ether functionalized
graphene membrane, with a case study of the mechanisms for a highly selective
and efficient extraction of lithium from aqueous solutions.",198,2412.13899v1,cond-mat.soft,"cond-mat.soft,cond-mat.mtrl-sci",molecular biology,2024-12-18,2024-12-23T21:06:58.590499
Fool's gold: ligand-receptor interactions and the origins of life,"The origins of life is a question that continues to intrigue scientists
across disciplines. One theory - the iron-sulphur theory - suggests that
reactions essential to the synthesis of biological materials got their
catalytic 'spark' from mineral surfaces such as iron pyrite, commonly known as
fool's gold. Additionally, the binding affinity of the ligands synthesised in
this 'surface metabolism' acted as an early version of natural selection: more
strongly-binding ligands were accumulated into further autocatalytic reactions
and the aggregation of complex biological materials. Ligand-receptor binding is
thus fundamental to the origins of life. In this paper, we use the iron-sulphur
theory as a lens through which to review ligand-receptor interactions as they
are more commonly understood today. In particular we focus on the electron
tunnelling theory of receptor activation that has emerged from research into
quantum biology. We revisit criticism against this theory, particularly the
lack of evidence for electron transfer in receptors, to see what insights might
be offered by ligand-receptor interactions mediated by iron pyrite at the
origins of life. What emerges from this comparison is the central importance of
redox activity in receptors, in particular with respect to the recurring
presence of the disulphide bond. While the paper is a speculative exercise, we
conclude that conductivity in biomolecules, particularly the selective
conductivity conferred by appropriate ligand-receptor binding, is a powerful
tool for understanding diverse phenomena such as pharmacological potency and
viral infection. As such it deserves further investigation.",315,2412.13836v1,physics.bio-ph,"physics.bio-ph,quant-ph",molecular biology,2024-12-18,2024-12-23T21:06:58.591497
Hunting pre-stellar cores with APEX: IRAS16293E (Oph464),"Pre-stellar cores are the first steps in the process of star and planet
formation. However, the dynamical and chemical evolution of pre-stellar cores
is still not well understood. We aim at estimating the central density of the
pre-stellar core IRAS16293E and at carrying out an inventory of molecular
species towards the density peak of the core. We observed high-$J$ rotational
transitions of N$_2$H$^+$ and N$_2$D$^+$, and several other molecular lines
towards the dust emission peak using the Atacama Pathfinder EXperiment (APEX)
telescope, and derived the density and temperature profiles of the core using
far-infrared surface brightness maps from $Herschel$. The N$_2$H$^+$ and
N$_2$D$^+$ lines were analysed by non-LTE radiative transfer modelling. Our
best-fit core model consists in a static inner region, embedded in an infalling
envelope with an inner radius of approximately 3000 au (21"" at 141 pc). The
observed high-J lines of N$_2$H$^+$ and N$_2$D$^+$ (with critical densities
greater than 10$^6$ cm$^{-3}$) turn out to be very sensitive to depletion; the
present single-dish observations are best explained with no depletion of
N$_2$H$^+$ and N$_2$D$^+$ in the inner core. The N$_2$D$^+$/N$_2$H$^+$ ratio
that best reproduces our observations is 0.44, one of the largest observed to
date in pre-stellar cores. Additionally, half of the molecules that we observed
are deuterated isotopologues, confirming the high-level of deuteration towards
this source. Non-LTE radiative transfer modelling of N$_2$H$^+$ and N$_2$D$^+$
lines proved to be an excellent diagnostic of the chemical structure and
dynamics of a pre-stellar core. Probing the physical conditions immediately
before the protostellar collapse is a necessary reference for theoretical
studies and simulations with the aim of understanding the earliest stages of
star and planet formation and the time scale of this process.",497,2412.13760v1,astro-ph.GA,astro-ph.GA,molecular biology,2024-12-18,2024-12-23T21:06:58.593492
"Segregation, ordering, and precipitation in refractory alloys","Tungsten-based low-activation high-entropy alloys are possible candidates for
next-generation fusion reactors due to their exceptional tolerance to
irradiation, thermal loads, and stress. We develop an accurate and efficient
machine-learned interatomic potential for the W-Ta-Cr-V system and use it in
hybrid Monte Carlo molecular dynamics simulations of ordering and segregation
to all common types of defects in WTaCrV. The predictions are compared to atom
probe tomography analysis of segregation and precipitation in WTaCrV thin
films. By also considering two other alloys, WTaV and MoNbTaVW, we are able to
draw general conclusions about preferred segregation in refractory alloys and
the reasons behind it, guiding future alloy design and elucidating experimental
observations. We show that the experimentally observed CrV precipitates in
WTaCrV form semicoherent bcc-to-bcc interfaces with the surrounding matrix, as
coherent precipitates are not thermodynamically stable due to excessive lattice
mismatch. The predictions from simulations align well with our atom probe
tomography analysis as well as previous experimental observations.",233,2412.13750v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,molecular biology,2024-12-18,2024-12-23T21:06:58.593492
Data-driven Discovery of Biophysical T Cell Receptor Co-specificity Rules,"The biophysical interactions between the T cell receptor (TCR) and its
ligands determine the specificity of the cellular immune response. However, the
immense diversity of receptors and ligands has made it challenging to discover
generalizable rules across the distinct binding affinity landscapes created by
different ligands. Here, we present an optimization framework for discovering
biophysical rules that predict whether TCRs share specificity to a ligand.
Applying this framework to TCRs associated with a collection of SARS-CoV-2
peptides we establish how co-specificity depends on the type and position of
amino-acid differences between receptors. We also demonstrate that the inferred
rules generalize to ligands not seen during training. Our analysis reveals that
matching of steric properties between substituted amino acids is important for
receptor co-specificity, in contrast with the hydrophobic properties that more
prominently determine evolutionary substitutability. We furthermore find that
positions not in direct contact with the peptide still significantly impact
specificity. These findings highlight the potential for data-driven approaches
to uncover the molecular mechanisms underpinning the specificity of adaptive
immune responses.",229,2412.13722v1,q-bio.BM,"q-bio.BM,cs.LG,physics.bio-ph",molecular biology,2024-12-18,2024-12-23T21:06:58.594489
Soft Modes as a Predictive Framework for Low Dimensional Biological Systems across Scales,"All biological systems are subject to perturbations: due to thermal
fluctuations, external environments, or mutations. Yet, while biological
systems are composed of thousands of interacting components, recent
high-throughput experiments show that their response to perturbations is
surprisingly low-dimensional: confined to only a few stereotyped changes out of
the many possible. Here, we explore a unifying dynamical systems framework -
soft modes - to explain and analyze low-dimensionality in biology, from
molecules to eco-systems. We argue that this one framework of soft modes makes
non-trivial predictions that generalize classic ideas from developmental
biology to disparate systems, namely: phenocopying, dual buffering, and global
epistasis. While some of these predictions have been borne out in experiments,
we discuss how soft modes allow for a surprisingly far-reaching and unifying
framework in which to analyze data from protein biophysics to microbial
ecology.",194,2412.13637v1,q-bio.QM,q-bio.QM,molecular biology,2024-12-18,2024-12-23T21:06:58.595488
Length-dependent residence time of contacts in simple polymeric models,"Starting from the reported experimental evidence that the residence time of
contacts between the ends of biopolymers is length dependent, we investigate
the kinetics of contact breaking in simple polymer models from a theoretical
point of view. We solved Kramers equation first for an ideal chain and then for
a polymer with attracting ends, and compared the predictions with the results
of molecular dynamics simulations. We found that the mean residence time always
shows a power--law dependence on the length of the polymer with exponent $-1$,
although is significantly smaller when obtained from the analysis of a single
trajectory than when calculated from independent initial conformations. Only
when the interaction is strong (>>kT) and the interaction range is small (of
the order of the distance between consecutive monomers) does the residence time
converge to that of the Arrhenius equation, independent of the length. We are
able to provide expressions of the mean residence time for cases when the exact
definition of contact is not available a priori, expressions that can be useful
in typical cases of microscopy experiments.",215,2412.13563v1,cond-mat.soft,"cond-mat.soft,q-bio.BM",molecular biology,2024-12-18,2024-12-23T21:06:58.596484
Open-Source Protein Language Models for Function Prediction and Protein Design,"Protein language models (PLMs) have shown promise in improving the
understanding of protein sequences, contributing to advances in areas such as
function prediction and protein engineering. However, training these models
from scratch requires significant computational resources, limiting their
accessibility. To address this, we integrate a PLM into DeepChem, an
open-source framework for computational biology and chemistry, to provide a
more accessible platform for protein-related tasks.
  We evaluate the performance of the integrated model on various protein
prediction tasks, showing that it achieves reasonable results across
benchmarks. Additionally, we present an exploration of generating
plastic-degrading enzyme candidates using the model's embeddings and latent
space manipulation techniques. While the results suggest that further
refinement is needed, this approach provides a foundation for future work in
enzyme design. This study aims to facilitate the use of PLMs in research fields
like synthetic biology and environmental sustainability, even for those with
limited computational resources.",194,2412.13519v1,cs.LG,"cs.LG,q-bio.BM",molecular biology,2024-12-18,2024-12-23T21:06:58.596484
Statistical mechanics of directed networks,"Directed networks are essential for representing complex systems, capturing
the asymmetry of interactions in fields such as neuroscience, transportation,
and social networks. Directionality reveals how influence, information, or
resources flow within a network, fundamentally shaping the behavior of
dynamical processes and distinguishing directed networks from their undirected
counterparts. Robust null models are crucial for identifying meaningful
patterns in these representations, yet designing models that preserve key
features remains a significant challenge. One such critical feature is
reciprocity, which reflects the balance of bidirectional interactions in
directed networks and provides insights into the underlying structural and
dynamical principles that shape their connectivity. This paper introduces a
statistical mechanics framework for directed networks, modeling them as
ensembles of interacting fermions. By controlling reciprocity and other network
properties, our formalism offers a principled approach to analyzing directed
network structures and dynamics, introducing a new perspectives, and models and
analytical tools for empirical studies.",188,2412.15923v1,cond-mat.dis-nn,cond-mat.dis-nn,neuroscience,2024-12-20,2024-12-23T21:06:59.352543
Does the brain behave like a (complex) network? I. Dynamics,"Graph theory is now becoming a standard tool in system-level neuroscience.
However, endowing observed brain anatomy and dynamics with a complex network
structure does not entail that the brain actually works as a network. Asking
whether the brain behaves as a network means asking whether network properties
count. From the viewpoint of neurophysiology and, possibly, of brain physics,
the most substantial issues a network structure may be instrumental in
addressing relate to the influence of network properties on brain dynamics and
to whether these properties ultimately explain some aspects of brain function.
Here, we address the dynamical implications of complex network, examining which
aspects and scales of brain activity may be understood to genuinely behave as a
network. To do so, we first define the meaning of networkness, and analyse some
of its implications. We then examine ways in which brain anatomy and dynamics
can be endowed with a network structure and discuss possible ways in which
network structure may be shown to represent a genuine organisational principle
of brain activity, rather than just a convenient description of its anatomy and
dynamics.",213,2412.15711v1,q-bio.NC,"q-bio.NC,cond-mat.dis-nn,nlin.AO",neuroscience,2024-12-20,2024-12-23T21:06:59.352543
Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture,"Large language models (LLMs) demonstrate an impressive ability to utilise
information within the context of their input sequences to appropriately
respond to data unseen by the LLM during its training procedure. This ability
is known as in-context learning (ICL). Humans and non-human animals demonstrate
similar abilities, however their neural architectures differ substantially from
LLMs. Despite this, a critical component within LLMs, the attention mechanism,
resembles modern associative memory models, widely used in and influenced by
the computational neuroscience community to model biological memory systems.
Using this connection, we introduce an associative memory model capable of
performing ICL. We use this as inspiration for a novel residual stream
architecture which allows information to directly flow between attention heads.
We test this architecture during training within a two-layer Transformer and
show its ICL abilities manifest more quickly than without this modification. We
then apply our architecture in small language models with 8 million parameters,
focusing on attention head values, with results also indicating improved ICL
performance at this larger and more naturalistic scale.",218,2412.15113v1,cs.NE,"cs.NE,cs.AI,cs.CL,92B20, 68T01, 68T37, 68T50,I.2; I.5; I.7; J.2; J.3",neuroscience,2024-12-19,2024-12-23T21:06:59.353540
Deep reinforcement learning with time-scale invariant memory,"The ability to estimate temporal relationships is critical for both animals
and artificial agents. Cognitive science and neuroscience provide remarkable
insights into behavioral and neural aspects of temporal credit assignment. In
particular, scale invariance of learning dynamics, observed in behavior and
supported by neural data, is one of the key principles that governs animal
perception: proportional rescaling of temporal relationships does not alter the
overall learning efficiency. Here we integrate a computational neuroscience
model of scale invariant memory into deep reinforcement learning (RL) agents.
We first provide a theoretical analysis and then demonstrate through
experiments that such agents can learn robustly across a wide range of temporal
scales, unlike agents built with commonly used recurrent memory architectures
such as LSTM. This result illustrates that incorporating computational
principles from neuroscience and cognitive science into deep neural networks
can enhance adaptability to complex temporal dynamics, mirroring some of the
core properties of human learning.",181,2412.15292v1,cs.AI,"cs.AI,cs.LG",neuroscience,2024-12-19,2024-12-23T21:06:59.354537
Multi-Modal Latent Variables for Cross-Individual Primary Visual Cortex Modeling and Analysis,"Elucidating the functional mechanisms of the primary visual cortex (V1)
remains a fundamental challenge in systems neuroscience. Current computational
models face two critical limitations, namely the challenge of cross-modal
integration between partial neural recordings and complex visual stimuli, and
the inherent variability in neural characteristics across individuals,
including differences in neuron populations and firing patterns. To address
these challenges, we present a multi-modal identifiable variational autoencoder
(miVAE) that employs a two-level disentanglement strategy to map neural
activity and visual stimuli into a unified latent space. This framework enables
robust identification of cross-modal correlations through refined latent space
modeling. We complement this with a novel score-based attribution analysis that
traces latent variables back to their origins in the source data space.
Evaluation on a large-scale mouse V1 dataset demonstrates that our method
achieves state-of-the-art performance in cross-individual latent representation
and alignment, without requiring subject-specific fine-tuning, and exhibits
improved performance with increasing data size. Significantly, our attribution
algorithm successfully identifies distinct neuronal subpopulations
characterized by unique temporal patterns and stimulus discrimination
properties, while simultaneously revealing stimulus regions that show specific
sensitivity to edge features and luminance variations. This scalable framework
offers promising applications not only for advancing V1 research but also for
broader investigations in neuroscience.",288,2412.14536v1,q-bio.NC,q-bio.NC,neuroscience,2024-12-19,2024-12-23T21:06:59.355535
CAE-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection,"Electroencephalogram (EEG) signals are critical for detecting abnormal brain
activity, but their high dimensionality and complexity pose significant
challenges for effective analysis. In this paper, we propose CAE-T, a novel
framework that combines a channelwise CNN-based autoencoder with a single-head
transformer classifier for efficient EEG abnormality detection. The channelwise
autoencoder compresses raw EEG signals while preserving channel independence,
reducing computational costs and retaining biologically meaningful features.
The compressed representations are then fed into the transformer-based
classifier, which efficiently models long-term dependencies to distinguish
between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus,
the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2%
specificity at the per-case level, outperforming baseline models such as
EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs
and 2.9M parameters, making it significantly more efficient than
transformer-based alternatives. The framework retains interpretability through
its channelwise design, demonstrating great potential for future applications
in neuroscience research and clinical practice. The source code is available at
https://github.com/YossiZhao/CAE-T.",280,2412.14522v1,cs.LG,"cs.LG,cs.AI,cs.NE,eess.SP,I.2",neuroscience,2024-12-19,2024-12-23T21:06:59.355535
Diverging network architecture of the $\textit{C. elegans}$ connectome and signaling network,"The connectome describes the complete set of synaptic contacts through which
neurons communicate. While the architecture of the $\textit{C. elegans}$
connectome has been extensively characterized, much less is known about the
organization of causal signaling networks arising from functional interactions
between neurons. Understanding how effective communication pathways relate to
or diverge from the underlying structure is a central question in neuroscience.
Here, we analyze the modular architecture of the $\textit{C. elegans}$ signal
propagation network, measured via calcium imaging and optogenetics, and compare
it to the underlying anatomical wiring measured by electron microscopy.
Compared to the connectome, we find that signaling modules are not aligned with
the modular boundaries of the anatomical network, highlighting an instance
where function deviates from structure. An exception to this is the pharynx
which is delineated into a separate community in both anatomy and signaling. We
analyze the cellular compositions of the signaling architecture and find that
its modules are enriched for specific cell types and functions, suggesting that
the network modules are neurobiologically relevant. Lastly, we identify a ""rich
club"" of hub neurons in the signaling network. The membership of the signaling
rich club differs from the rich club detected in the anatomical network,
challenging the view that structural hubs occupy positions of influence in
functional (signaling) networks. Our results provide new insight into the
interplay between brain structure, in the form of a complete synaptic-level
connectome, and brain function, in the form of a system-wide causal signal
propagation atlas.",320,2412.14498v1,q-bio.NC,q-bio.NC,neuroscience,2024-12-19,2024-12-23T21:06:59.356532
Starting a Synthetic Biological Intelligence Lab from Scratch,"With the recent advancements in artificial intelligence, researchers and
industries are deploying gigantic models trained on billions of samples. While
training these models consumes a huge amount of energy, human brains produce
similar outputs (along with other capabilities) with massively lower data and
energy requirements. For this reason, more researchers are increasingly
considering alternatives. One of these alternatives is known as synthetic
biological intelligence, which involves training \textit{in vitro} neurons for
goal-directed tasks. This multidisciplinary field requires knowledge of tissue
engineering, bio-materials, digital signal processing, computer programming,
neuroscience, and even artificial intelligence. The multidisciplinary
requirements make starting synthetic biological intelligence research highly
non-trivial and time-consuming. Generally, most labs either specialize in the
biological aspects or the computational ones. Here, we propose how a lab
focusing on computational aspects, including machine learning and device
interfacing, can start working on synthetic biological intelligence, including
organoid intelligence. We will also discuss computational aspects, which can be
helpful for labs that focus on biological research. To facilitate synthetic
biological intelligence research, we will describe such a general process step
by step, including risks and precautions that could lead to substantial delay
or additional cost.",250,2412.14112v1,q-bio.NC,q-bio.NC,neuroscience,2024-12-18,2024-12-23T21:06:59.357529
Functional connectomes of neural networks,"The human brain is a complex system, and understanding its mechanisms has
been a long-standing challenge in neuroscience. The study of the functional
connectome, which maps the functional connections between different brain
regions, has provided valuable insights through various advanced analysis
techniques developed over the years. Similarly, neural networks, inspired by
the brain's architecture, have achieved notable success in diverse applications
but are often noted for their lack of interpretability. In this paper, we
propose a novel approach that bridges neural networks and human brain functions
by leveraging brain-inspired techniques. Our approach, grounded in the insights
from the functional connectome, offers scalable ways to characterize topology
of large neural networks using stable statistical and machine learning
techniques. Our empirical analysis demonstrates its capability to enhance the
interpretability of neural networks, providing a deeper understanding of their
underlying mechanisms.",169,2412.15279v1,cs.NE,"cs.NE,cs.AI,cs.LG,q-bio.NC",neuroscience,2024-12-18,2024-12-23T21:06:59.357529
Shared Attention-based Autoencoder with Hierarchical Fusion-based Graph Convolution Network for sEEG SOZ Identification,"Diagnosing seizure onset zone (SOZ) is a challenge in neurosurgery, where
stereoelectroencephalography (sEEG) serves as a critical technique. In sEEG SOZ
identification, the existing studies focus solely on the intra-patient
representation of epileptic information, overlooking the general features of
epilepsy across patients and feature interdependencies between feature elements
in each contact site. In order to address the aforementioned challenges, we
propose the shared attention-based autoencoder (sATAE). sATAE is trained by
sEEG data across all patients, with attention blocks introduced to enhance the
representation of interdependencies between feature elements. Considering the
spatial diversity of sEEG across patients, we introduce graph-based method for
identification SOZ of each patient. However, the current graph-based methods
for sEEG SOZ identification rely exclusively on static graphs to model
epileptic networks. Inspired by the finding of neuroscience that epileptic
network is intricately characterized by the interplay of sophisticated
equilibrium between fluctuating and stable states, we design the hierarchical
fusion-based graph convolution network (HFGCN) to identify the SOZ. HFGCN
integrates the dynamic and static characteristics of epileptic networks through
hierarchical weighting across different hierarchies, facilitating a more
comprehensive learning of epileptic features and enriching node information for
sEEG SOZ identification. Combining sATAE and HFGCN, we perform comprehensive
experiments with sATAE-HFGCN on the self-build sEEG dataset, which includes
sEEG data from 17 patients with temporal lobe epilepsy. The results show that
our method, sATAE-HFGCN, achieves superior performance for identifying the SOZ
of each patient, effectively addressing the aforementioned challenges,
providing an efficient solution for sEEG-based SOZ identification.",392,2412.12651v1,cs.LG,"cs.LG,cs.AI,eess.SP,q-bio.NC",neuroscience,2024-12-17,2024-12-23T21:06:59.359525
Modality-Driven Design for Multi-Step Dexterous Manipulation: Insights from Neuroscience,"Multi-step dexterous manipulation is a fundamental skill in household
scenarios, yet remains an underexplored area in robotics. This paper proposes a
modular approach, where each step of the manipulation process is addressed with
dedicated policies based on effective modality input, rather than relying on a
single end-to-end model. To demonstrate this, a dexterous robotic hand performs
a manipulation task involving picking up and rotating a box. Guided by insights
from neuroscience, the task is decomposed into three sub-skills, 1)reaching,
2)grasping and lifting, and 3)in-hand rotation, based on the dominant sensory
modalities employed in the human brain. Each sub-skill is addressed using
distinct methods from a practical perspective: a classical controller, a
Vision-Language-Action model, and a reinforcement learning policy with force
feedback, respectively. We tested the pipeline on a real robot to demonstrate
the feasibility of our approach. The key contribution of this study lies in
presenting a neuroscience-inspired, modality-driven methodology for multi-step
dexterous manipulation.",217,2412.11337v1,cs.RO,"cs.RO,cs.AI,cs.CV",neuroscience,2024-12-15,2024-12-23T21:06:59.359525
Distribution free uncertainty quantification in neuroscience-inspired deep operators,"Energy-efficient deep learning algorithms are essential for a sustainable
future and feasible edge computing setups. Spiking neural networks (SNNs),
inspired from neuroscience, are a positive step in the direction of achieving
the required energy efficiency. However, in a bid to lower the energy
requirements, accuracy is marginally sacrificed. Hence, predictions of such
deep learning algorithms require an uncertainty measure that can inform users
regarding the bounds of a certain output. In this paper, we introduce the
Conformalized Randomized Prior Operator (CRP-O) framework that leverages
Randomized Prior (RP) networks and Split Conformal Prediction (SCP) to quantify
uncertainty in both conventional and spiking neural operators. To further
enable zero-shot super-resolution in UQ, we propose an extension incorporating
Gaussian Process Regression. This enhanced super-resolution-enabled CRP-O
framework is integrated with the recently developed Variable Spiking Wavelet
Neural Operator (VSWNO). To test the performance of the obtained calibrated
uncertainty bounds, we discuss four different examples covering both
one-dimensional and two-dimensional partial differential equations. Results
demonstrate that the uncertainty bounds produced by the conformalized RP-VSWNO
significantly enhance UQ estimates compared to vanilla RP-VSWNO, Quantile WNO
(Q-WNO), and Conformalized Quantile WNO (CQ-WNO). These findings underscore the
potential of the proposed approach for practical applications.",308,2412.09369v1,stat.ML,"stat.ML,cs.LG",neuroscience,2024-12-12,2024-12-23T21:06:59.360522
A Spectral Framework for Tracking Communities in Evolving Networks,"Discovering and tracking communities in time-varying networks is an important
task in network science, motivated by applications in fields ranging from
neuroscience to sociology. In this work, we characterize the celebrated family
of spectral methods for static clustering in terms of the low-rank
approximation of high-dimensional node embeddings. From this perspective, it
becomes natural to view the evolving community detection problem as one of
subspace tracking on the Grassmann manifold. While the resulting optimization
problem is nonconvex, we adopt a block majorize-minimize Riemannian
optimization scheme to learn the Grassmann geodesic which best fits the data.
Our framework generalizes any static spectral community detection approach and
leads to algorithms achieving favorable performance on synthetic and real
temporal networks, including those that are weighted, signed, directed,
mixed-membership, multiview, hierarchical, cocommunity-structured, bipartite,
or some combination thereof. We demonstrate how to specifically cast a wide
variety of methods into our framework, and demonstrate greatly improved dynamic
community detection results in all cases.",215,2412.07378v1,cs.SI,"cs.SI,cs.LG,stat.ML",neuroscience,2024-12-10,2024-12-23T21:06:59.361519
QuantFormer: Learning to Quantize for Neural Activity Forecasting in Mouse Visual Cortex,"Understanding complex animal behaviors hinges on deciphering the neural
activity patterns within brain circuits, making the ability to forecast neural
activity crucial for developing predictive models of brain dynamics. This
capability holds immense value for neuroscience, particularly in applications
such as real-time optogenetic interventions. While traditional encoding and
decoding methods have been used to map external variables to neural activity
and vice versa, they focus on interpreting past data. In contrast, neural
forecasting aims to predict future neural activity, presenting a unique and
challenging task due to the spatiotemporal sparsity and complex dependencies of
neural signals. Existing transformer-based forecasting methods, while effective
in many domains, struggle to capture the distinctiveness of neural signals
characterized by spatiotemporal sparsity and intricate dependencies. To address
this challenge, we here introduce QuantFormer, a transformer-based model
specifically designed for forecasting neural activity from two-photon calcium
imaging data. Unlike conventional regression-based approaches,
QuantFormerreframes the forecasting task as a classification problem via
dynamic signal quantization, enabling more effective learning of sparse neural
activation patterns. Additionally, QuantFormer tackles the challenge of
analyzing multivariate signals from an arbitrary number of neurons by
incorporating neuron-specific tokens, allowing scalability across diverse
neuronal populations. Trained with unsupervised quantization on the Allen
dataset, QuantFormer sets a new benchmark in forecasting mouse visual cortex
activity. It demonstrates robust performance and generalization across various
stimuli and individuals, paving the way for a foundational model in neural
signal prediction.",334,2412.07264v1,q-bio.NC,"q-bio.NC,cs.CV,eess.IV,eess.SP",neuroscience,2024-12-10,2024-12-23T21:06:59.362516
Exploring Coding Spot: Understanding Parametric Contributions to LLM Coding Performance,"Large Language Models (LLMs) have demonstrated notable proficiency in both
code generation and comprehension across multiple programming languages.
However, the mechanisms underlying this proficiency remain underexplored,
particularly with respect to whether distinct programming languages are
processed independently or within a shared parametric region. Drawing an
analogy to the specialized regions of the brain responsible for distinct
cognitive functions, we introduce the concept of Coding Spot, a specialized
parametric region within LLMs that facilitates coding capabilities. Our
findings identify this Coding Spot and show that targeted modifications to this
subset significantly affect performance on coding tasks, while largely
preserving non-coding functionalities. This compartmentalization mirrors the
functional specialization observed in cognitive neuroscience, where specific
brain regions are dedicated to distinct tasks, suggesting that LLMs may
similarly employ specialized parameter regions for different knowledge domains.",161,2412.07113v1,cs.CL,cs.CL,neuroscience,2024-12-10,2024-12-23T21:06:59.362516
Primary visual cortex contributes to color constancy by predicting rather than discounting the illuminant: evidence from a computational study,"Color constancy (CC) is an important ability of the human visual system to
stably perceive the colors of objects despite considerable changes in the color
of the light illuminating them. While increasing evidence from the field of
neuroscience supports that multiple levels of the visual system contribute to
the realization of CC, how the primary visual cortex (V1) plays role in CC is
not fully resolved. In specific, double-opponent (DO) neurons in V1 have been
thought to contribute to realizing a degree of CC, but the computational
mechanism is not clear. We build an electrophysiologically based V1 neural
model to learn the color of the light source from a natural image dataset with
the ground truth illuminants as the labels. Based on the qualitative and
quantitative analysis of the responsive properties of the learned model
neurons, we found that both the spatial structures and color weights of the
receptive fields of the learned model neurons are quite similar to those of the
simple and DO neurons recorded in V1. Computationally, DO cells perform more
robustly than the simple cells in V1 for illuminant prediction. Therefore, this
work provides computational evidence supporting that V1 DO neurons serve to
realize color constancy by encoding the illuminant,which is contradictory to
the common hypothesis that V1 contributes to CC by discounting the illuminant
using its DO cells. This evidence is expected to not only help resolve the
visual mechanisms of CC, but also provide inspiration to develop more effective
computer vision models.",307,2412.07102v1,q-bio.NC,"q-bio.NC,cs.CV",neuroscience,2024-12-10,2024-12-23T21:06:59.363513
Convolution goes higher-order: a biologically inspired mechanism empowers image classification,"We propose a novel approach to image classification inspired by complex
nonlinear biological visual processing, whereby classical convolutional neural
networks (CNNs) are equipped with learnable higher-order convolutions. Our
model incorporates a Volterra-like expansion of the convolution operator,
capturing multiplicative interactions akin to those observed in early and
advanced stages of biological visual processing. We evaluated this approach on
synthetic datasets by measuring sensitivity to testing higher-order
correlations and performance in standard benchmarks (MNIST, FashionMNIST,
CIFAR10, CIFAR100 and Imagenette). Our architecture outperforms traditional CNN
baselines, and achieves optimal performance with expansions up to 3rd/4th
order, aligning remarkably well with the distribution of pixel intensities in
natural images. Through systematic perturbation analysis, we validate this
alignment by isolating the contributions of specific image statistics to model
performance, demonstrating how different orders of convolution process distinct
aspects of visual information. Furthermore, Representational Similarity
Analysis reveals distinct geometries across network layers, indicating
qualitatively different modes of visual information processing. Our work
bridges neuroscience and deep learning, offering a path towards more effective,
biologically inspired computer vision models. It provides insights into visual
information processing and lays the groundwork for neural networks that better
capture complex visual patterns, particularly in resource-constrained
scenarios.",287,2412.06740v1,cs.CV,"cs.CV,cs.LG,q-bio.NC",neuroscience,2024-12-09,2024-12-23T21:06:59.364511
Diff5T: Benchmarking Human Brain Diffusion MRI with an Extensive 5.0 Tesla K-Space and Spatial Dataset,"Diffusion magnetic resonance imaging (dMRI) provides critical insights into
the microstructural and connectional organization of the human brain. However,
the availability of high-field, open-access datasets that include raw k-space
data for advanced research remains limited. To address this gap, we introduce
Diff5T, a first comprehensive 5.0 Tesla diffusion MRI dataset focusing on the
human brain. This dataset includes raw k-space data and reconstructed diffusion
images, acquired using a variety of imaging protocols. Diff5T is designed to
support the development and benchmarking of innovative methods in artifact
correction, image reconstruction, image preprocessing, diffusion modelling and
tractography. The dataset features a wide range of diffusion parameters,
including multiple b-values and gradient directions, allowing extensive
research applications in studying human brain microstructure and connectivity.
With its emphasis on open accessibility and detailed benchmarks, Diff5T serves
as a valuable resource for advancing human brain mapping research using
diffusion MRI, fostering reproducibility, and enabling collaboration across the
neuroscience and medical imaging communities.",228,2412.06666v1,eess.IV,"eess.IV,cs.CV,physics.med-ph",neuroscience,2024-12-09,2024-12-23T21:06:59.365508
Flexible and Scalable Deep Dendritic Spiking Neural Networks with Multiple Nonlinear Branching,"Recent advances in spiking neural networks (SNNs) have a predominant focus on
network architectures, while relatively little attention has been paid to the
underlying neuron model. The point neuron models, a cornerstone of deep SNNs,
pose a bottleneck on the network-level expressivity since they depict somatic
dynamics only. In contrast, the multi-compartment models in neuroscience offer
remarkable expressivity by introducing dendritic morphology and dynamics, but
remain underexplored in deep learning due to their unaffordable computational
cost and inflexibility. To combine the advantages of both sides for a flexible,
efficient yet more powerful model, we propose the dendritic spiking neuron
(DendSN) incorporating multiple dendritic branches with nonlinear dynamics.
Compared to the point spiking neurons, DendSN exhibits significantly higher
expressivity. DendSN's flexibility enables its seamless integration into
diverse deep SNN architectures. To accelerate dendritic SNNs (DendSNNs), we
parallelize dendritic state updates across time steps, and develop Triton
kernels for GPU-level acceleration. As a result, we can construct large-scale
DendSNNs with depth comparable to their point SNN counterparts. Next, we
comprehensively evaluate DendSNNs' performance on various demanding tasks. By
modulating dendritic branch strengths using a context signal, catastrophic
forgetting of DendSNNs is substantially mitigated. Moreover, DendSNNs
demonstrate enhanced robustness against noise and adversarial attacks compared
to point SNNs, and excel in few-shot learning settings. Our work firstly
demonstrates the possibility of training bio-plausible dendritic SNNs with
depths and scales comparable to traditional point SNNs, and reveals superior
expressivity and robustness of reduced dendritic neuron models in deep
learning, thereby offering a fresh perspective on advancing neural network
design.",399,2412.06355v1,cs.NE,cs.NE,neuroscience,2024-12-09,2024-12-23T21:06:59.366505
Statistical Mechanics of Support Vector Regression,"A key problem in deep learning and computational neuroscience is relating the
geometrical properties of neural representations to task performance. Here, we
consider this problem for continuous decoding tasks where neural variability
may affect task precision. Using methods from statistical mechanics, we study
the average-case learning curves for $\varepsilon$-insensitive Support Vector
Regression ($\varepsilon$-SVR) and discuss its capacity as a measure of linear
decodability. Our analysis reveals a phase transition in the training error at
a critical load, capturing the interplay between the tolerance parameter
$\varepsilon$ and neural variability. We uncover a double-descent phenomenon in
the generalization error, showing that $\varepsilon$ acts as a regularizer,
both suppressing and shifting these peaks. Theoretical predictions are
validated both on toy models and deep neural networks, extending the theory of
Support Vector Machines to continuous tasks with inherent neural variability.",196,2412.05439v1,cond-mat.dis-nn,"cond-mat.dis-nn,q-bio.NC,stat.ML",neuroscience,2024-12-06,2024-12-23T21:06:59.366505
Learning Networks from Wide-Sense Stationary Stochastic Processes,"Complex networked systems driven by latent inputs are common in fields like
neuroscience, finance, and engineering. A key inference problem here is to
learn edge connectivity from node outputs (potentials). We focus on systems
governed by steady-state linear conservation laws: $X_t = {L^{\ast}}Y_{t}$,
where $X_t, Y_t \in \mathbb{R}^p$ denote inputs and potentials, respectively,
and the sparsity pattern of the $p \times p$ Laplacian $L^{\ast}$ encodes the
edge structure. Assuming $X_t$ to be a wide-sense stationary stochastic process
with a known spectral density matrix, we learn the support of $L^{\ast}$ from
temporally correlated samples of $Y_t$ via an $\ell_1$-regularized Whittle's
maximum likelihood estimator (MLE). The regularization is particularly useful
for learning large-scale networks in the high-dimensional setting where the
network size $p$ significantly exceeds the number of samples $n$.
  We show that the MLE problem is strictly convex, admitting a unique solution.
Under a novel mutual incoherence condition and certain sufficient conditions on
$(n, p, d)$, we show that the ML estimate recovers the sparsity pattern of
$L^\ast$ with high probability, where $d$ is the maximum degree of the graph
underlying $L^{\ast}$. We provide recovery guarantees for $L^\ast$ in
element-wise maximum, Frobenius, and operator norms. Finally, we complement our
theoretical results with several simulation studies on synthetic and benchmark
datasets, including engineered systems (power and water networks), and
real-world datasets from neural systems (such as the human brain).",406,2412.03768v1,stat.ML,"stat.ML,cs.LG,eess.SP",neuroscience,2024-12-04,2024-12-23T21:06:59.367503
Artificial Intelligence without Restriction Surpassing Human Intelligence with Probability One: Theoretical Insight into Secrets of the Brain with AI Twins of the Brain,"Artificial Intelligence (AI) has apparently become one of the most important
techniques discovered by humans in history while the human brain is widely
recognized as one of the most complex systems in the universe. One fundamental
critical question which would affect human sustainability remains open: Will
artificial intelligence (AI) evolve to surpass human intelligence in the
future? This paper shows that in theory new AI twins with fresh cellular level
of AI techniques for neuroscience could approximate the brain and its
functioning systems (e.g. perception and cognition functions) with any expected
small error and AI without restrictions could surpass human intelligence with
probability one in the end. This paper indirectly proves the validity of the
conjecture made by Frank Rosenblatt 70 years ago about the potential
capabilities of AI, especially in the realm of artificial neural networks.
Intelligence is just one of fortuitous but sophisticated creations of the
nature which has not been fully discovered. Like mathematics and physics, with
no restrictions artificial intelligence would lead to a new subject with its
self-contained systems and principles. We anticipate that this paper opens new
doors for 1) AI twins and other AI techniques to be used in cellular level of
efficient neuroscience dynamic analysis, functioning analysis of the brain and
brain illness solutions; 2) new worldwide collaborative scheme for
interdisciplinary teams concurrently working on and modelling different types
of neurons and synapses and different level of functioning subsystems of the
brain with AI techniques; 3) development of low energy of AI techniques with
the aid of fundamental neuroscience properties; and 4) new controllable,
explainable and safe AI techniques with reasoning capabilities of discovering
principles in nature.",321,2412.06820v1,cs.AI,cs.AI,neuroscience,2024-12-04,2024-12-23T21:06:59.368500
Demonstrating the Advantages of Analog Wafer-Scale Neuromorphic Hardware,"As numerical simulations grow in size and complexity, they become
increasingly resource-intensive in terms of time and energy. While specialized
hardware accelerators often provide order-of-magnitude gains and are state of
the art in other scientific fields, their availability and applicability in
computational neuroscience is still limited. In this field, neuromorphic
accelerators, particularly mixed-signal architectures like the BrainScaleS
systems, offer the most significant performance benefits. These systems
maintain a constant, accelerated emulation speed independent of network model
and size. This is especially beneficial when traditional simulators reach their
limits, such as when modeling complex neuron dynamics, incorporating plasticity
mechanisms, or running long or repetitive experiments. However, the analog
nature of these systems introduces new challenges. In this paper we demonstrate
the capabilities and advantages of the BrainScaleS-1 system and how it can be
used in combination with conventional software simulations. We report the
emulation time and energy consumption for two biologically inspired networks
adapted to the neuromorphic hardware substrate: a balanced random network based
on Brunel and the cortical microcircuit from Potjans and Diesmann.",233,2412.02619v1,cs.NE,cs.NE,neuroscience,2024-12-03,2024-12-23T21:06:59.369498
Multi-timescale synaptic plasticity on analog neuromorphic hardware,"As numerical simulations grow in complexity, their demands on computing time
and energy increase. Hardware accelerators offer significant efficiency gains
in many computationally intensive scientific fields, but their use in
computational neuroscience remains limited. Neuromorphic substrates, such as
the BrainScaleS architectures, offer significant advantages, especially for
studying complex plasticity rules that require extended simulation runtimes.
This work presents the implementation of a calcium-based plasticity rule that
integrates calcium dynamics based on the synaptic tagging-and-capture
hypothesis on the BrainScaleS-2 system. The implementation of the plasticity
rule for a single synapse involves incorporating the calcium dynamics and the
plasticity rule equations. The calcium dynamics are mapped to the analog
circuits of BrainScaleS-2, while the plasticity rule equations are numerically
solved on its embedded digital processors. The main hardware constraints
include the speed of the processors and the use of integer arithmetic. By
adjusting the timestep of the numerical solver and introducing stochastic
rounding, we demonstrate that BrainScaleS-2 accurately emulates a single
synapse following a calcium-based plasticity rule across four stimulation
protocols and validate our implementation against a software reference model.",246,2412.02515v1,q-bio.QM,"q-bio.QM,q-bio.NC",neuroscience,2024-12-03,2024-12-23T21:06:59.369498
The Landscape of Causal Discovery Data: Grounding Causal Discovery in Real-World Applications,"Causal discovery aims to automatically uncover causal relationships from
data, a capability with significant potential across many scientific
disciplines. However, its real-world applications remain limited. Current
methods often rely on unrealistic assumptions and are evaluated only on simple
synthetic toy datasets, often with inadequate evaluation metrics. In this
paper, we substantiate these claims by performing a systematic review of the
recent causal discovery literature. We present applications in biology,
neuroscience, and Earth sciences - fields where causal discovery holds promise
for addressing key challenges. We highlight available simulated and real-world
datasets from these domains and discuss common assumption violations that have
spurred the development of new methods. Our goal is to encourage the community
to adopt better evaluation practices by utilizing realistic datasets and more
adequate metrics.",159,2412.01953v1,cs.LG,"cs.LG,stat.ME",neuroscience,2024-12-02,2024-12-23T21:06:59.370495
Handwriting-based Automated Assessment and Grading of Degree of Handedness: A Pilot Study,"Hand preference and degree of handedness (DoH) are two different aspects of
human behavior which are often confused to be one. DoH is a person's inherent
capability of the brain; affected by nature and nurture. In this study, we used
dominant and non-dominant handwriting traits to assess DoH for the first time,
on 43 subjects of three categories- Unidextrous, Partially Unidextrous, and
Ambidextrous. Features extracted from the segmented handwriting signals called
strokes were used for DoH quantification. Davies Bouldin Index, Multilayer
perceptron, and Convolutional Neural Network (CNN) were used for automated
grading of DoH. The outcomes of these methods were compared with the widely
used DoH assessment questionnaires from Edinburgh Inventory (EI). The CNN based
automated grading outperformed other computational methods with an average
classification accuracy of 95.06% under stratified 10-fold cross-validation.
The leave-one-subject-out strategy on this CNN resulted in a test individual's
DoH score which was converted into a 4-point score. Around 90% of the obtained
scores from all the implemented computational methods were found to be in
accordance with the EI scores under 95% confidence interval. Automated grading
of degree of handedness using handwriting signals can provide more resolution
to the Edinburgh Inventory scores. This could be used in multiple applications
concerned with neuroscience, rehabilitation, physiology, psychometry,
behavioral sciences, and forensics.",311,2412.01587v1,cs.AI,"cs.AI,cs.HC",neuroscience,2024-12-02,2024-12-23T21:06:59.371492
Federated Block-Term Tensor Regression for decentralised data analysis in healthcare,"Block-Term Tensor Regression (BTTR) has proven to be a powerful tool for
modeling complex, high-dimensional data by leveraging multilinear
relationships, making it particularly well-suited for applications in
healthcare and neuroscience. However, traditional implementations of BTTR rely
on centralized datasets, which pose significant privacy risks and hinder
collaboration across institutions. To address these challenges, we introduce
Federated Block-Term Tensor Regression (FBTTR), an extension of BTTR designed
for federated learning scenarios. FBTTR enables decentralized data analysis,
allowing institutions to collaboratively build predictive models while
preserving data privacy and complying with regulations.
  FBTTR represents a major step forward in applying tensor regression to
federated learning environments. Its performance is evaluated in two case
studies: finger movement decoding from Electrocorticography (ECoG) signals and
heart disease prediction. In the first case study, using the BCI Competition IV
dataset, FBTTR outperforms non-multilinear models, demonstrating superior
accuracy in decoding finger movements. For the dataset, for subject 3, the
thumb obtained a performance of 0.76 $\pm$ .05 compared to 0.71 $\pm$ 0.05 for
centralised BTTR. In the second case study, FBTTR is applied to predict heart
disease using real-world clinical datasets, outperforming both standard
federated learning approaches and centralized BTTR models. In the
Fed-Heart-Disease Dataset, an AUC-ROC was obtained of 0.872 $\pm$ 0.02 and an
accuracy of 0.772 $\pm$ 0.02 compared to 0.812 $\pm$ 0.003 and 0.753 $\pm$
0.007 for the centralized model.",375,2412.06815v1,cs.LG,cs.LG,neuroscience,2024-12-02,2024-12-23T21:06:59.372489
Shadow of the (Hierarchical) Tree: Reconciling Symbolic and Predictive Components of the Neural Code for Syntax,"Natural language syntax can serve as a major test for how to integrate two
infamously distinct frameworks: symbolic representations and connectionist
neural networks. Building on a recent neurocomputational architecture for
syntax (ROSE), I discuss the prospects of reconciling the neural code for
hierarchical 'vertical' syntax with linear and predictive 'horizontal'
processes via a hybrid neurosymbolic model. I argue that the former can be
accounted for via the higher levels of ROSE in terms of vertical phrase
structure representations, while the latter can explain horizontal forms of
linguistic information via the tuning of the lower levels to statistical and
perceptual inferences. One prediction of this is that artificial language
models will contribute to the cognitive neuroscience of horizontal
morphosyntax, but much less so to hierarchically compositional structures. I
claim that this perspective helps resolve many current tensions in the
literature. Options for integrating these two neural codes are discussed, with
particular emphasis on how predictive coding mechanisms can serve as interfaces
between symbolic oscillatory phase codes and population codes for the
statistics of linearized aspects of syntax. Lastly, I provide a neurosymbolic
mathematical model for how to inject symbolic representations into a neural
regime encoding lexico-semantic statistical features.",258,2412.01276v1,cs.CL,cs.CL,neuroscience,2024-12-02,2024-12-23T21:06:59.372489
Simplified derivations for high-dimensional convex learning problems,"Statistical physics provides tools for analyzing high-dimensional problems in
machine learning and theoretical neuroscience. These calculations, particularly
those using the replica method, often involve lengthy derivations that can
obscure physical interpretation. We give concise, non-replica derivations of
several key results and highlight their underlying similarities. Specifically,
we introduce a cavity approach to analyzing high-dimensional learning problems
and apply it to three cases: perceptron classification of points, perceptron
classification of manifolds, and kernel ridge regression. These problems share
a common structure -- a bipartite system of interacting feature and datum
variables -- enabling a unified analysis. For perceptron-capacity problems, we
identify a symmetry that allows derivation of correct capacities through a
na\""ive method. These results match those obtained through the replica method.",168,2412.01110v2,cond-mat.dis-nn,"cond-mat.dis-nn,cs.NE,q-bio.NC",neuroscience,2024-12-02,2024-12-23T21:06:59.373487
Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations,"We propose a probabilistic framework for developing computational models of
biological neural systems. In this framework, physiological recordings are
viewed as discrete-time partial observations of an underlying continuous-time
stochastic dynamical system which implements computations through its state
evolution. To model this dynamical system, we employ a system of coupled
stochastic differential equations with differentiable drift and diffusion
functions and use variational inference to infer its states and parameters.
This formulation enables seamless integration of existing mathematical models
in the literature, neural networks, or a hybrid of both to learn and compare
different models. We demonstrate this in our framework by developing a
generative model that combines coupled oscillators with neural networks to
capture latent population dynamics from single-cell recordings. Evaluation
across three neuroscience datasets spanning different species, brain regions,
and behavioral tasks show that these hybrid models achieve competitive
performance in predicting stimulus-evoked neural and behavioral responses
compared to sophisticated black-box approaches while requiring an order of
magnitude fewer parameters, providing uncertainty estimates, and offering a
natural language for interpretation.",218,2412.12112v1,q-bio.NC,"q-bio.NC,cs.LG",neuroscience,2024-12-01,2024-12-23T21:06:59.374484
Why does time feel the way it does? Towards a principled account of temporal experience,"Time flows, or at least the time of our experience does. Can we provide an
objective account of why experience, confined to the short window of the
conscious present, encompasses a succession of moments that slip away from now
to then--an account of why time feels flowing? Integrated Information Theory
(IIT) aims to account for both the presence and quality of consciousness in
objective, physical terms. Given a substrate's architecture and current state,
the formalism of IIT allows one to unfold the cause-effect power of the
substrate, yielding a cause-effect structure. According to IIT, this accounts
in full for the presence and quality of experience, without any additional
ingredients. In previous work, we showed how unfolding the cause-effect
structure of non-directed grids, like those found in many posterior cortical
areas, can account for the way space feels--namely, extended. Here we show that
unfolding the cause-effect structure of directed grids can account for how time
feels--namely, flowing. First, we argue that the conscious present is
experienced as flowing because it is composed of phenomenal distinctions
(moments) that are directed, and these distinctions are related in a way that
satisfies directed inclusion, connection, and fusion. We then show that
directed grids, which we conjecture constitute the substrate of temporal
experience, yield a cause-effect structure that accounts for these and other
properties of temporal experience. In this account, the experienced present
does not correspond to a process unrolling in ""clock time,"" but to a
cause-effect structure specified by a system in its current state: time is a
structure, not a process. We conclude by outlining similarities and differences
between the experience of time and space, and some implications for the
neuroscience, psychophysics, and philosophy of time.",372,2412.13198v1,q-bio.NC,q-bio.NC,neuroscience,2024-11-30,2024-12-23T21:06:59.375482
Spatial Clustering of Molecular Localizations with Graph Neural Networks,"Single-molecule localization microscopy generates point clouds corresponding
to fluorophore localizations. Spatial cluster identification and analysis of
these point clouds are crucial for extracting insights about molecular
organization. However, this task becomes challenging in the presence of
localization noise, high point density, or complex biological structures. Here,
we introduce MIRO (Multimodal Integration through Relational Optimization), an
algorithm that uses recurrent graph neural networks to transform the point
clouds in order to improve clustering efficiency when applying conventional
clustering techniques. We show that MIRO supports simultaneous processing of
clusters of different shapes and at multiple scales, demonstrating improved
performance across varied datasets. Our comprehensive evaluation demonstrates
MIRO's transformative potential for single-molecule localization applications,
showcasing its capability to revolutionize cluster analysis and provide
accurate, reliable details of molecular architecture. In addition, MIRO's
robust clustering capabilities hold promise for applications in various fields
such as neuroscience, for the analysis of neural connectivity patterns, and
environmental science, for studying spatial distributions of ecological data.",212,2412.00173v1,cs.LG,"cs.LG,physics.bio-ph,physics.data-an,q-bio.QM",neuroscience,2024-11-29,2024-12-23T21:06:59.376479
A potassium ion channel simulated with a universal neural network potential,"Potassium ion channels are critical components of biology. They conduct
potassium ions across the cell membrane with remarkable speed and selectivity.
Understanding how they do this is crucially important for applications in
neuroscience, medicine, and materials science. However, many fundamental
questions about the mechanism they use remain unresolved, partly because it is
extremely difficult to computationally model due to the scale and complexity of
the necessary simulations. Here, the selectivity filter (SF) of the KcsA
potassium ion channel is simulated using Orb-D3, a recently released universal
neural network potential. A previously unreported hydrogen bond between water
in the SF and the T75 hydroxyl side group at the entrance to the SF is
observed. This hydrogen bond appears to stabilize water in the SF, enabling a
soft knock-on transport mechanism where water is co-transported through the SF
with a reasonable conductivity (80 $\pm$ 20 pS). Carbonyl backbone flipping is
also observed at new sites in the SF. This work demonstrates the potential of
universal neural network potentials to provide insights into previously
intractable questions about complex systems far outside their training data
distribution.",231,2411.18931v1,q-bio.BM,"q-bio.BM,cond-mat.soft",neuroscience,2024-11-28,2024-12-23T21:06:59.377476
NeuroAI for AI Safety,"As AI systems become increasingly powerful, the need for safe AI has become
more pressing. Humans are an attractive model for AI safety: as the only known
agents capable of general intelligence, they perform robustly even under
conditions that deviate significantly from prior experiences, explore the world
safely, understand pragmatics, and can cooperate to meet their intrinsic goals.
Intelligence, when coupled with cooperation and safety mechanisms, can drive
sustained progress and well-being. These properties are a function of the
architecture of the brain and the learning algorithms it implements.
Neuroscience may thus hold important keys to technical AI safety that are
currently underexplored and underutilized. In this roadmap, we highlight and
critically evaluate several paths toward AI safety inspired by neuroscience:
emulating the brain's representations, information processing, and
architecture; building robust sensory and motor systems from imitating brain
data and bodies; fine-tuning AI systems on brain data; advancing
interpretability using neuroscience methods; and scaling up
cognitively-inspired architectures. We make several concrete recommendations
for how neuroscience can positively impact AI safety.",220,2411.18526v1,cs.AI,"cs.AI,cs.LG",neuroscience,2024-11-27,2024-12-23T21:06:59.377476
Storing overlapping associative memories on latent manifolds in low-rank spiking networks,"Associative memory architectures such as the Hopfield network have long been
important conceptual and theoretical models for neuroscience and artificial
intelligence. However, translating these abstract models into spiking neural
networks has been surprisingly difficult. Indeed, much previous work has been
restricted to storing a small number of primarily non-overlapping memories in
large networks, thereby limiting their scalability. Here, we revisit the
associative memory problem in light of recent advances in understanding
spike-based computation. Using a recently-established geometric framework, we
show that the spiking activity for a large class of all-inhibitory networks is
situated on a low-dimensional, convex, and piecewise-linear manifold, with
dynamics that move along the manifold. We then map the associative memory
problem onto these dynamics, and demonstrate how the vertices of a hypercubic
manifold can be used to store stable, overlapping activity patterns with a
direct correspondence to the original Hopfield model. We propose several
learning rules, and demonstrate a linear scaling of the storage capacity with
the number of neurons, as well as robust pattern completion abilities. Overall,
this work serves as a case study to demonstrate the effectiveness of using a
geometrical perspective to design dynamics on neural manifolds, with
implications for neuroscience and machine learning.",261,2411.17485v1,q-bio.NC,"q-bio.NC,cs.LG,cs.NE",neuroscience,2024-11-26,2024-12-23T21:06:59.378473
"Energy Consumption Optimization, Response Time Differences and Indicators in Cortical Working Memory Revealed by Nonequilibrium","The neocortex, a complex system driving multi-region interactions, remains a
core puzzle in neuroscience. Despite quantitative insights across brain scales,
understanding the mechanisms underlying neural activities is challenging.
Advances from Hopfield networks to large-scale cortical models have deepened
neural network theory, yet these models often fall short of capturing global
brain functions. In large-scale cortical networks, an intriguing hierarchy of
timescales reflects diverse information processing speeds across spatial
regions. As a non-equilibrium system, the brain incurs significant energy
costs, with long-distance connectivity suggesting an evolutionary spatial
organization. To explore these complexities, we introduce a nonequilibrium
landscape flux approach to analyze cortical networks. This allows us to
quantify potential landscapes and principal transition paths, uncovering
dynamical characteristics across timescales. We examine whether temporal
hierarchies correlate with stimuli distribution and how hierarchical networks
exhibit differential responses. Furthermore, our analysis quantifies the
thermodynamic cost of sustaining cognition, highlighting a link to network
connectivity. These findings provide insights into energy consumption during
cognitive processes and emphasize the spatial benefits for working memory
tasks. Experimental validation is challenging due to evolutionary variability,
making our theoretical approach valuable for quantifying complex dynamics. By
assessing time irreversibility and critical slowdown, we gain predictive
insights into network bifurcations and state transitions, offering practical
tools for identifying cortical state changes. These results advance our
understanding of cortical dynamics.",305,2411.17206v1,q-bio.NC,q-bio.NC,neuroscience,2024-11-26,2024-12-23T21:06:59.379471
Energy landscape analysis based on the Ising model: Tutorial review,"We review a class of energy landscape analysis method that uses the Ising
model and takes multivariate time series data as input. The method allows one
to capture dynamics of the data as trajectories of a ball from one basin to a
different basin to yet another, constrained on the energy landscape specified
by the estimated Ising model. While this energy landscape analysis has mostly
been applied to functional magnetic resonance imaging (fMRI) data from the
brain for historical reasons, there are emerging applications outside fMRI data
and neuroscience. To inform such applications in various research fields, this
review paper provides a detailed tutorial on each step of the analysis,
terminologies, concepts underlying the method, and validation, as well as
recent developments of extended and related methods.",154,2411.16979v1,cond-mat.dis-nn,cond-mat.dis-nn,neuroscience,2024-11-25,2024-12-23T21:06:59.379471
Stability of Brain Functional Network During Working Memory Using Structural Balance Theory,"Working memory plays a crucial role in various aspects of human life.
Therefore, it has been an area of interest in different research studies,
especially neuroscience. The neuroscientists investigating working memory have
primarily emphasized the brain's functional modularity. At the same time, a
holistic perspective is still required to investigate the brain as an
integrated and unified system. We hypothesized that the brain should shift
towards a more stable state during working memory than the resting state.
Therefore, based on the Structural Balance Theory (SBT), we aimed to address
this process. To achieve this, we examined triadic associations in signed fMRI
networks in healthy individuals using the N-back as the working memory task. We
demonstrated that the number of balanced triads increased during the working
memory task compared to the resting state, while the opposite is true for
imbalanced triads. The increase of balanced triads forced the network to a more
stable state with a lower balance energy level. The increase of balanced triads
was crucially related to changes in anti-synchrony to synchronous activities
between the Temporal Cortex, the Prefrontal Cortex, and the Parietal Cortex,
which are known to be involved in various aspects of working memory, during the
working memory process. We hope these findings pave the way to a better
understanding the working memory process.",279,2411.16558v1,q-bio.NC,q-bio.NC,neuroscience,2024-11-25,2024-12-23T21:06:59.380468
The brain versus AI: World-model-based versatile circuit computation underlying diverse functions in the neocortex and cerebellum,"AI's significant recent advances using general-purpose circuit computations
offer a potential window into how the neocortex and cerebellum of the brain are
able to achieve a diverse range of functions across sensory, cognitive, and
motor domains, despite their uniform circuit structures. However, comparing the
brain and AI is challenging unless clear similarities exist, and past reviews
have been limited to comparison of brain-inspired vision AI and the visual
neocortex. Here, to enable comparisons across diverse functional domains, we
subdivide circuit computation into three elements -- circuit structure,
input/outputs, and the learning algorithm -- and evaluate the similarities for
each element. With this novel approach, we identify wide-ranging similarities
and convergent evolution in the brain and AI, providing new insights into key
concepts in neuroscience. Furthermore, inspired by processing mechanisms of AI,
we propose a new theory that integrates established neuroscience theories,
particularly the theories of internal models and the mirror neuron system. Both
the neocortex and cerebellum predict future world events from past information
and learn from prediction errors, thereby acquiring models of the world. These
models enable three core processes: (1) Prediction -- generating future
information, (2) Understanding -- interpreting the external world via
compressed and abstracted sensory information, and (3) Generation --
repurposing the future-information generation mechanism to produce other types
of outputs. The universal application of these processes underlies the ability
of the neocortex and cerebellum to accomplish diverse functions with uniform
circuits. Our systematic approach, insights, and theory promise groundbreaking
advances in understanding the brain.",336,2411.16075v2,q-bio.NC,"q-bio.NC,cs.AI",neuroscience,2024-11-25,2024-12-23T21:06:59.381465
Minimizing information loss reduces spiking neuronal networks to differential equations,"Spiking neuronal networks (SNNs) are widely used in computational
neuroscience, from biologically realistic modeling of local cortical networks
to phenomenological modeling of the whole brain. Despite their prevalence, a
systematic mathematical theory for finite-sized SNNs remains elusive, even for
idealized homogeneous networks. The primary challenges are twofold: 1) the
rich, parameter-sensitive SNN dynamics, and 2) the singularity and
irreversibility of spikes. These challenges pose significant difficulties when
relating SNNs to systems of differential equations, leading previous studies to
impose additional assumptions or to focus on individual dynamic regimes. In
this study, we introduce a Markov approximation of homogeneous SNN dynamics to
minimize information loss when translating SNNs into ordinary differential
equations. Our only assumption for the Markov approximation is the fast
self-decorrelation of synaptic conductances. The system of ordinary
differential equations derived from the Markov model effectively captures
high-frequency partial synchrony and the metastability of finite-neuron
networks produced by interacting excitatory and inhibitory populations. Besides
accurately predicting dynamical statistics, such as firing rates, our theory
also quantitatively captures the geometry of attractors and bifurcation
structures of SNNs. Thus, our work provides a comprehensive mathematical
framework that can systematically map parameters of single-neuron physiology,
network coupling, and external stimuli to homogeneous SNN dynamics.",293,2411.14801v1,q-bio.NC,"q-bio.NC,math.DS",neuroscience,2024-11-22,2024-12-23T21:06:59.382463
Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology,"Musical mode is one of the most critical element that establishes the
framework of pitch organization and determines the harmonic relationships.
Previous works often use the simplistic and rigid alignment method, and
overlook the diversity of modes. However, in contrast to AI models, humans
possess cognitive mechanisms for perceiving the various modes and keys. In this
paper, we propose a spiking neural network inspired by brain mechanisms and
psychological theories to represent musical modes and keys, ultimately
generating musical pieces that incorporate tonality features. Specifically, the
contributions are detailed as follows: 1) The model is designed with multiple
collaborated subsystems inspired by the structures and functions of
corresponding brain regions; 2)We incorporate mechanisms for neural circuit
evolutionary learning that enable the network to learn and generate
mode-related features in music, reflecting the cognitive processes involved in
human music perception. 3)The results demonstrate that the proposed model shows
a connection framework closely similar to the Krumhansl-Schmuckler model, which
is one of the most significant key perception models in the music psychology
domain. 4) Experiments show that the model can generate music pieces with
characteristics of the given modes and keys. Additionally, the quantitative
assessments of generated pieces reveals that the generating music pieces have
both tonality characteristics and the melodic adaptability needed to generate
diverse and musical content. By combining insights from neuroscience,
psychology, and music theory with advanced neural network architectures, our
research aims to create a system that not only learns and generates music but
also bridges the gap between human cognition and artificial intelligence.",308,2411.14773v1,cs.SD,"cs.SD,cs.AI,eess.AS,q-bio.NC",neuroscience,2024-11-22,2024-12-23T21:06:59.383460
Evaluating Representational Similarity Measures from the Lens of Functional Correspondence,"Neuroscience and artificial intelligence (AI) both face the challenge of
interpreting high-dimensional neural data, where the comparative analysis of
such data is crucial for revealing shared mechanisms and differences between
these complex systems. Despite the widespread use of representational
comparisons and the abundance classes of comparison methods, a critical
question remains: which metrics are most suitable for these comparisons? While
some studies evaluate metrics based on their ability to differentiate models of
different origins or constructions (e.g., various architectures), another
approach is to assess how well they distinguish models that exhibit distinct
behaviors. To investigate this, we examine the degree of alignment between
various representational similarity measures and behavioral outcomes, employing
group statistics and a comprehensive suite of behavioral metrics for
comparison. In our evaluation of eight commonly used representational
similarity metrics in the visual domain -- spanning alignment-based, Canonical
Correlation Analysis (CCA)-based, inner product kernel-based, and
nearest-neighbor methods -- we found that metrics like linear Centered Kernel
Alignment (CKA) and Procrustes distance, which emphasize the overall geometric
structure or shape of representations, excelled in differentiating trained from
untrained models and aligning with behavioral measures, whereas metrics such as
linear predictivity, commonly used in neuroscience, demonstrated only moderate
alignment with behavior. These insights are crucial for selecting metrics that
emphasize behaviorally meaningful comparisons in NeuroAI research.",286,2411.14633v1,q-bio.NC,"q-bio.NC,cs.AI,cs.CV",neuroscience,2024-11-21,2024-12-23T21:06:59.384457
CODE-CL: COnceptor-Based Gradient Projection for DEep Continual Learning,"Continual learning, or the ability to progressively integrate new concepts,
is fundamental to intelligent beings, enabling adaptability in dynamic
environments. In contrast, artificial deep neural networks face the challenge
of catastrophic forgetting when learning new tasks sequentially. To alleviate
the problem of forgetting, recent approaches aim to preserve essential weight
subspaces for previous tasks by limiting updates to orthogonal subspaces via
gradient projection. While effective, this approach can lead to suboptimal
performance, particularly when tasks are highly correlated. In this work, we
introduce COnceptor-based gradient projection for DEep Continual Learning
(CODE-CL), a novel method that leverages conceptor matrix representations, a
computational model inspired by neuroscience, to more flexibly handle highly
correlated tasks. CODE-CL encodes directional importance within the input space
of past tasks, allowing new knowledge integration in directions modulated by
$1-S$, where $S$ represents the direction's relevance for prior tasks.
Additionally, we analyze task overlap using conceptor-based representations to
identify highly correlated tasks, facilitating efficient forward knowledge
transfer through scaled projection within their intersecting subspace. This
strategy enhances flexibility, allowing learning in correlated tasks without
significantly disrupting previous knowledge. Extensive experiments on continual
learning image classification benchmarks validate CODE-CL's efficacy,
showcasing superior performance with minimal forgetting, outperforming most
state-of-the-art methods.",282,2411.15235v1,cs.LG,"cs.LG,cs.AI,cs.CV,cs.NE",neuroscience,2024-11-21,2024-12-23T21:06:59.384457
Roadmap on Advances in Visual and Physiological Optics,"The field of visual and physiological optics is undergoing continuous
significant advancements, driven by a deeper understanding of the human visual
system and the development of cutting-edge optical technologies. This Roadmap,
authored by leading experts, delves into critical areas such as corneal
biomechanical properties, keratoconus, and advancements in corneal imaging and
elastography. It explores the intricate structure-function relationship within
the eye lens, offering new perspectives through lens models and ray tracing
techniques. The document also covers advancements in retinal imaging,
highlighting the current state and future directions, and the role of adaptive
optics in evaluating retinal structure and function in both healthy and
diseased eyes. Furthermore, it addresses the modelling of ocular surfaces using
different mathematical functions and examines the factors affecting peripheral
image quality in the human eye, emphasizing the importance of these aspects in
visual performance. Additional topics include schematic and functional models
of the human eye, the impact of optical and chromatic aberrations, and the
design of contact, and intraocular lenses. Finally, the Roadmap addresses the
intersection of neurosciences with vision health, presenting a comprehensive
overview of current research and future trends aimed at improving visual health
and optical performance. Ultimately, this Roadmap aims to serve as a valuable
resource for ophthalmologists, optometrists, vision scientists, and engineers
dedicated to advancing the field of visual and physiological optics.",294,2411.14606v1,physics.optics,physics.optics,neuroscience,2024-11-21,2024-12-23T21:06:59.385455
Adaptive Intelligence: leveraging insights from adaptive behavior in animals to build flexible AI systems,"Biological intelligence is inherently adaptive -- animals continually adjust
their actions based on environmental feedback. However, creating adaptive
artificial intelligence (AI) remains a major challenge. The next frontier is to
go beyond traditional AI to develop ""adaptive intelligence,"" defined here as
harnessing insights from biological intelligence to build agents that can learn
online, generalize, and rapidly adapt to changes in their environment. Recent
advances in neuroscience offer inspiration through studies that increasingly
focus on how animals naturally learn and adapt their world models. In this
Perspective, I will review the behavioral and neural foundations of adaptive
biological intelligence, the parallel progress in AI, and explore
brain-inspired approaches for building more adaptive algorithms.",135,2411.15234v2,q-bio.NC,"q-bio.NC,cs.AI",neuroscience,2024-11-21,2024-12-23T21:06:59.386452
OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs,"Scientific progress depends on researchers' ability to synthesize the growing
body of literature. Can large language models (LMs) assist scientists in this
task? We introduce OpenScholar, a specialized retrieval-augmented LM that
answers scientific queries by identifying relevant passages from 45 million
open-access papers and synthesizing citation-backed responses. To evaluate
OpenScholar, we develop ScholarQABench, the first large-scale multi-domain
benchmark for literature search, comprising 2,967 expert-written queries and
208 long-form answers across computer science, physics, neuroscience, and
biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and
PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o
hallucinates citations 78 to 90% of the time, OpenScholar achieves citation
accuracy on par with human experts. OpenScholar's datastore, retriever, and
self-feedback inference loop also improves off-the-shelf LMs: for instance,
OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,
experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over
expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's
32%. We open-source all of our code, models, datastore, data and a public demo.",324,2411.14199v1,cs.CL,"cs.CL,cs.AI,cs.DL,cs.IR,cs.LG",neuroscience,2024-11-21,2024-12-23T21:06:59.386452
Spiking neural networks: Towards bio-inspired multimodal perception in robotics,"Spiking neural networks (SNNs) have captured apparent interest over the
recent years, stemming from neuroscience and reaching the field of artificial
intelligence. However, due to their nature SNNs remain far behind in achieving
the exceptional performance of deep neural networks (DNNs). As a result, many
scholars are exploring ways to enhance SNNs by using learning techniques from
DNNs. While this approach has been proven to achieve considerable improvements
in SNN performance, we propose another perspective: enhancing the biological
plausibility of the models to leverage the advantages of SNNs fully. Our
approach aims to propose a brain-like combination of audio-visual signal
processing for recognition tasks, intended to succeed in more bio-plausible
human-robot interaction applications.",155,2411.14147v1,eess.IV,eess.IV,neuroscience,2024-11-21,2024-12-23T21:06:59.387449
EEG Signal Denoising Using pix2pix GAN: Enhancing Neurological Data Analysis,"Electroencephalography (EEG) is essential in neuroscience and clinical
practice, yet it suffers from physiological artifacts, particularly
electromyography (EMG), which distort signals. We propose a deep learning model
using pix2pixGAN to remove such noise and generate reliable EEG signals.
Leveraging the EEGdenoiseNet dataset, we created synthetic datasets with
controlled EMG noise levels for model training and testing across a
signal-to-noise ratio (SNR) from -7 to 2. Our evaluation metrics included RRMSE
and Pearson's CC, assessing both time and frequency domains, and compared our
model with others. The pix2pixGAN model excelled, especially under high noise
conditions, showing significant improvements in lower RRMSE and higher CC
values. This demonstrates the model's superior accuracy and stability in
purifying EEG signals, offering a robust solution for EEG analysis challenges
and advancing clinical and neuroscience applications.",202,2411.13288v1,eess.SP,"eess.SP,I.4.9",neuroscience,2024-11-20,2024-12-23T21:06:59.387449
Neuro-3D: Towards 3D Visual Decoding from EEG Signals,"Human's perception of the visual world is shaped by the stereo processing of
3D information. Understanding how the brain perceives and processes 3D visual
stimuli in the real world has been a longstanding endeavor in neuroscience.
Towards this goal, we introduce a new neuroscience task: decoding 3D visual
perception from EEG signals, a neuroimaging technique that enables real-time
monitoring of neural dynamics enriched with complex visual cues. To provide the
essential benchmark, we first present EEG-3D, a pioneering dataset featuring
multimodal analysis data and extensive EEG recordings from 12 subjects viewing
72 categories of 3D objects rendered in both videos and images. Furthermore, we
propose Neuro-3D, a 3D visual decoding framework based on EEG signals. This
framework adaptively integrates EEG features derived from static and dynamic
stimuli to learn complementary and robust neural representations, which are
subsequently utilized to recover both the shape and color of 3D objects through
the proposed diffusion-based colored point cloud decoder. To the best of our
knowledge, we are the first to explore EEG-based 3D visual decoding.
Experiments indicate that Neuro-3D not only reconstructs colored 3D objects
with high fidelity, but also learns effective neural representations that
enable insightful brain region analysis. The dataset and associated code will
be made publicly available.",270,2411.12248v2,cs.CV,cs.CV,neuroscience,2024-11-19,2024-12-23T21:06:59.388447
Unbiased Approximations for Stationary Distributions of McKean-Vlasov SDEs,"We consider the development of unbiased estimators, to approximate the
stationary distribution of Mckean-Vlasov stochastic differential equations
(MVSDEs). These are an important class of processes, which frequently appear in
applications such as mathematical finance, biology and opinion dynamics.
Typically the stationary distribution is unknown and indeed one cannot simulate
such processes exactly. As a result one commonly requires a time-discretization
scheme which results in a discretization bias and a bias from not being able to
simulate the associated stationary distribution. To overcome this bias, we
present a new unbiased estimator taking motivation from the literature on
unbiased Monte Carlo. We prove the unbiasedness of our estimator, under
assumptions. In order to prove this we require developing ergodicity results of
various discrete time processes, through an appropriate discretization scheme,
towards the invariant measure. Numerous numerical experiments are provided, on
a range of MVSDEs, to demonstrate the effectiveness of our unbiased estimator.
Such examples include the Currie-Weiss model, a 3D neuroscience model and a
parameter estimation problem.",232,2411.11270v1,stat.ME,"stat.ME,cs.NA,math.NA,math.PR,stat.CO",neuroscience,2024-11-18,2024-12-23T21:06:59.389445
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",genomics,2024-12-20,2024-12-23T21:07:00.173431
AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals,"Generative AI has the potential to transform knowledge work, but further
research is needed to understand how knowledge workers envision using and
interacting with generative AI. We investigate the development of generative AI
tools to support domain experts in knowledge work, examining task delegation
and the design of human-AI interactions. Our research focused on designing a
generative AI assistant to aid genetic professionals in analyzing whole genome
sequences (WGS) and other clinical data for rare disease diagnosis. Through
interviews with 17 genetics professionals, we identified current challenges in
WGS analysis. We then conducted co-design sessions with six genetics
professionals to determine tasks that could be supported by an AI assistant and
considerations for designing interactions with the AI assistant. From our
findings, we identified sensemaking as both a current challenge in WGS analysis
and a process that could be supported by AI. We contribute an understanding of
how domain experts envision interacting with generative AI in their knowledge
work, a detailed empirical study of WGS analysis, and three design
considerations for using generative AI to support domain experts in sensemaking
during knowledge work.
  CCS CONCEPTS: Human-centered computing, Human-computer interaction, Empirical
studies in HCI
  Additional Keywords and Phrases: whole genome sequencing, generative AI,
large language models, knowledge work, sensemaking, co-design, rare disease
  Contact Author: Angela Mastrianni (This work was done during the author's
internship at Microsoft Research)
  Ashley Mae Conard and Amanda K. Hall contributed equally",300,2412.15444v1,cs.HC,"cs.HC,cs.AI",genomics,2024-12-19,2024-12-23T21:07:00.174429
GREGoR: Accelerating Genomics for Rare Diseases,"Rare diseases are collectively common, affecting approximately one in twenty
individuals worldwide. In recent years, rapid progress has been made in rare
disease diagnostics due to advances in DNA sequencing, development of new
computational and experimental approaches to prioritize genes and genetic
variants, and increased global exchange of clinical and genetic data. However,
more than half of individuals suspected to have a rare disease lack a genetic
diagnosis. The Genomics Research to Elucidate the Genetics of Rare Diseases
(GREGoR) Consortium was initiated to study thousands of challenging rare
disease cases and families and apply, standardize, and evaluate emerging
genomics technologies and analytics to accelerate their adoption in clinical
practice. Further, all data generated, currently representing ~7500 individuals
from ~3000 families, is rapidly made available to researchers worldwide via the
Genomic Data Science Analysis, Visualization, and Informatics Lab-space (AnVIL)
to catalyze global efforts to develop approaches for genetic diagnoses in rare
diseases (https://gregorconsortium.org/data). The majority of these families
have undergone prior clinical genetic testing but remained unsolved, with most
being exome-negative. Here, we describe the collaborative research framework,
datasets, and discoveries comprising GREGoR that will provide foundational
resources and substrates for the future of rare disease genomics.",269,2412.14338v1,q-bio.OT,q-bio.OT,genomics,2024-12-18,2024-12-23T21:07:00.175426
Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with MxDNA,"Foundation models have made significant strides in understanding the genomic
language of DNA sequences. However, previous models typically adopt the
tokenization methods designed for natural language, which are unsuitable for
DNA sequences due to their unique characteristics. In addition, the optimal
approach to tokenize DNA remains largely under-explored, and may not be
intuitively understood by humans even if discovered. To address these
challenges, we introduce MxDNA, a novel framework where the model autonomously
learns an effective DNA tokenization strategy through gradient decent. MxDNA
employs a sparse Mixture of Convolution Experts coupled with a deformable
convolution to model the tokenization process, with the discontinuous,
overlapping, and ambiguous nature of meaningful genomic segments explicitly
considered. On Nucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA
demonstrates superior performance to existing methods with less pretraining
data and time, highlighting its effectiveness. Finally, we show that MxDNA
learns unique tokenization strategy distinct to those of previous methods and
captures genomic functionalities at a token level during self-supervised
pretraining. Our MxDNA aims to provide a new perspective on DNA tokenization,
potentially offering broad applications in various domains and yielding
profound insights.",249,2412.13716v1,q-bio.GN,"q-bio.GN,cs.LG",genomics,2024-12-18,2024-12-23T21:07:00.176423
Single-cell spatial (scs) omics: Recent developments in data analysis,"Over the past few years, technological advances have allowed for measurement
of omics data at the cell level, creating a new type of data generally referred
to as single-cell (sc) omics. On the other hand, the so-called spatial omics
are a family of techniques that generate biological information in a spatial
domain, for instance, in the volume of a tissue. In this survey, we are mostly
interested in the intersection between sc and spatial (scs) omics and in the
challenges and opportunities that this new type of data pose for downstream
data analysis methodologies. Our goal is to cover all major omics modalities,
including transcriptomics, genomics, epigenomics, proteomics and metabolomics.",151,2412.13591v1,stat.CO,"stat.CO,q-bio.GN",genomics,2024-12-18,2024-12-23T21:07:00.176423
UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation Benchmarks with Unified Entrez Gene Identifiers,"Gene studies are crucial for fields such as protein structure prediction,
drug discovery, and cancer genomics, yet they face challenges in fully
utilizing the vast and diverse information available. Gene studies require
clean, factual datasets to ensure reliable results. Ontology graphs, neatly
organized domain terminology graphs, provide ideal sources for domain facts.
However, available gene ontology annotations are currently distributed across
various databases without unified identifiers for genes and gene products. To
address these challenges, we introduce Unified Entrez Gene Identifier Dataset
and Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale
public Gene Ontology Annotations (GOA) from various databases using unique gene
identifiers. UniEntrezDB includes a pre-training dataset and four downstream
tasks designed to comprehensively evaluate gene embedding performance from
gene, protein, and cell levels, ultimately enhancing the reliability and
applicability of LLMs in gene research and other professional settings.",205,2412.12688v1,cs.DB,cs.DB,genomics,2024-12-17,2024-12-23T21:07:00.177420
Artificial Intelligence for Central Dogma-Centric Multi-Omics: Challenges and Breakthroughs,"With the rapid development of high-throughput sequencing platforms, an
increasing number of omics technologies, such as genomics, metabolomics, and
transcriptomics, are being applied to disease genetics research. However,
biological data often exhibit high dimensionality and significant noise, making
it challenging to effectively distinguish disease subtypes using a single-omics
approach. To address these challenges and better capture the interactions among
DNA, RNA, and proteins described by the central dogma, numerous studies have
leveraged artificial intelligence to develop multi-omics models for disease
research. These AI-driven models have improved the accuracy of disease
prediction and facilitated the identification of genetic loci associated with
diseases, thus advancing precision medicine. This paper reviews the
mathematical definitions of multi-omics, strategies for integrating multi-omics
data, applications of artificial intelligence and deep learning in multi-omics,
the establishment of foundational models, and breakthroughs in multi-omics
technologies, drawing insights from over 130 related articles. It aims to
provide practical guidance for computational biologists to better understand
and effectively utilize AI-based multi-omics machine learning algorithms in the
context of central dogma.",237,2412.12668v1,q-bio.GN,q-bio.GN,genomics,2024-12-17,2024-12-23T21:07:00.178417
DLSOM: A Deep learning-based strategy for liver cancer subtyping,"Liver cancer is a leading cause of cancer-related mortality worldwide, with
its high genetic heterogeneity complicating diagnosis and treatment. This study
introduces DLSOM, a deep learning framework utilizing stacked autoencoders to
analyze the complete somatic mutation landscape of 1,139 liver cancer samples,
covering 20,356 protein-coding genes. By transforming high-dimensional mutation
data into three low-dimensional features, DLSOM enables robust clustering and
identifies five distinct liver cancer subtypes with unique mutational,
functional, and biological profiles. Subtypes SC1 and SC2 exhibit higher
mutational loads, while SC3 has the lowest, reflecting mutational
heterogeneity. Novel and COSMIC-associated mutational signatures reveal
subtype-specific molecular mechanisms, including links to hypermutation and
chemotherapy resistance. Functional analyses further highlight the biological
relevance of each subtype. This comprehensive framework advances precision
medicine in liver cancer by enabling the development of subtype-specific
diagnostics, biomarkers, and therapies, showcasing the potential of deep
learning in addressing cancer complexity.",222,2412.12214v1,q-bio.GN,"q-bio.GN,cs.LG",genomics,2024-12-15,2024-12-23T21:07:00.179415
BarcodeMamba: State Space Models for Biodiversity Analysis,"DNA barcodes are crucial in biodiversity analysis for building automatic
identification systems that recognize known species and discover unseen
species. Unlike human genome modeling, barcode-based invertebrate
identification poses challenges in the vast diversity of species and taxonomic
complexity. Among Transformer-based foundation models, BarcodeBERT excelled in
species-level identification of invertebrates, highlighting the effectiveness
of self-supervised pretraining on barcode-specific datasets. Recently,
structured state space models (SSMs) have emerged, with a time complexity that
scales sub-quadratically with the context length. SSMs provide an efficient
parameterization of sequence modeling relative to attention-based
architectures. Given the success of Mamba and Mamba-2 in natural language, we
designed BarcodeMamba, a performant and efficient foundation model for DNA
barcodes in biodiversity analysis. We conducted a comprehensive ablation study
on the impacts of self-supervised training and tokenization methods, and
compared both versions of Mamba layers in terms of expressiveness and their
capacity to identify ""unseen"" species held back from training. Our study shows
that BarcodeMamba has better performance than BarcodeBERT even when using only
8.3% as many parameters, and improves accuracy to 99.2% on species-level
accuracy in linear probing without fine-tuning for ""seen"" species. In our
scaling study, BarcodeMamba with 63.6% of BarcodeBERT's parameters achieved
70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing for unseen
species. The code repository to reproduce our experiments is available at
https://github.com/bioscan-ml/BarcodeMamba.",349,2412.11084v1,cs.LG,"cs.LG,q-bio.GN,q-bio.QM",genomics,2024-12-15,2024-12-23T21:07:00.179415
SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation,"Dynamic scenes contain intricate spatio-temporal information, crucial for
mobile robots, UAVs, and autonomous driving systems to make informed decisions.
Parsing these scenes into semantic triplets <Subject-Predicate-Object> for
accurate Scene Graph Generation (SGG) is highly challenging due to the
fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities
of Large Language Models (LLMs), we propose SceneLLM, a novel framework that
leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework
introduces a Video-to-Language (V2L) mapping module that transforms video
frames into linguistic signals (scene tokens), making the input more
comprehensible for LLMs. To better encode spatial information, we devise a
Spatial Information Aggregation (SIA) scheme, inspired by the structure of
Chinese characters, which encodes spatial data into tokens. Using Optimal
Transport (OT), we generate an implicit language signal from the frame-level
token sequence that captures the video's spatio-temporal information. To
further improve the LLM's ability to process this implicit linguistic input, we
apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a
transformer-based SGG predictor to decode the LLM's reasoning and predict
semantic triplets. Our method achieves state-of-the-art results on the Action
Genome (AG) benchmark, and extensive experiments show the effectiveness of
SceneLLM in understanding and generating accurate dynamic scene graphs.",319,2412.11026v1,cs.CV,"cs.CV,cs.AI",genomics,2024-12-15,2024-12-23T21:07:00.180412
APAR: Modeling Irregular Target Functions in Tabular Regression via Arithmetic-Aware Pre-Training and Adaptive-Regularized Fine-Tuning,"Tabular data are fundamental in common machine learning applications, ranging
from finance to genomics and healthcare. This paper focuses on tabular
regression tasks, a field where deep learning (DL) methods are not consistently
superior to machine learning (ML) models due to the challenges posed by
irregular target functions inherent in tabular data, causing sensitive label
changes with minor variations from features. To address these issues, we
propose a novel Arithmetic-Aware Pre-training and Adaptive-Regularized
Fine-tuning framework (APAR), which enables the model to fit irregular target
function in tabular data while reducing the negative impact of overfitting. In
the pre-training phase, APAR introduces an arithmetic-aware pretext objective
to capture intricate sample-wise relationships from the perspective of
continuous labels. In the fine-tuning phase, a consistency-based adaptive
regularization technique is proposed to self-learn appropriate data
augmentation. Extensive experiments across 10 datasets demonstrated that APAR
outperforms existing GBDT-, supervised NN-, and pretrain-finetune NN-based
methods in RMSE (+9.43% $\sim$ 20.37%), and empirically validated the effects
of pre-training tasks, including the study of arithmetic operations. Our code
and data are publicly available at https://github.com/johnnyhwu/APAR.",286,2412.10941v1,cs.LG,"cs.LG,cs.AI",genomics,2024-12-14,2024-12-23T21:07:00.181409
VEPerform: a web resource for evaluating the performance of variant effect predictors,"Computational variant effect predictors (VEPs) are providing increasingly
strong evidence to classify the pathogenicity of missense variants. Precision
vs. recall analysis is useful in evaluating VEP performance, especially when
adjusted for imbalanced test sets. Here, we describe VEPerform, a web-based
tool for evaluating the performance of VEPs at the gene level using balanced
precision vs. recall curve (BPRC) analysis.",87,2412.10262v1,q-bio.GN,q-bio.GN,genomics,2024-12-13,2024-12-23T21:07:00.182406
Simple Guidance Mechanisms for Discrete Diffusion Models,"Diffusion models for continuous data gained widespread adoption owing to
their high quality generation and control mechanisms. However, controllable
diffusion on discrete data faces challenges given that continuous guidance
methods do not directly apply to discrete diffusion. Here, we provide a
straightforward derivation of classifier-free and classifier-based guidance for
discrete diffusion, as well as a new class of diffusion models that leverage
uniform noise and that are more guidable because they can continuously edit
their outputs. We improve the quality of these models with a novel
continuous-time variational lower bound that yields state-of-the-art
performance, especially in settings involving guidance or fast generation.
Empirically, we demonstrate that our guidance mechanisms combined with uniform
noise diffusion improve controllable generation relative to autoregressive and
diffusion baselines on several discrete data domains, including genomic
sequences, small molecule design, and discretized image generation.",182,2412.10193v1,cs.LG,cs.LG,genomics,2024-12-13,2024-12-23T21:07:00.182406
"A robust, scalable K-statistic for quantifying immune cell clustering in spatial proteomics data","Spatial summary statistics based on point process theory are widely used to
quantify the spatial organization of cell populations in single-cell spatial
proteomics data. Among these, Ripley's $K$ is a popular metric for assessing
whether cells are spatially clustered or are randomly dispersed. However, the
key assumption of spatial homogeneity is frequently violated in spatial
proteomics data, leading to overestimates of cell clustering and
colocalization. To address this, we propose a novel $K$-based method, termed
\textit{KAMP} (\textbf{K} adjustment by \textbf{A}nalytical \textbf{M}oments of
the \textbf{P}ermutation distribution), for quantifying the spatial
organization of cells in spatial proteomics samples. \textit{KAMP} leverages
background cells in each sample along with a new closed-form representation of
the first and second moments of the permutation distribution of Ripley's $K$ to
estimate an empirical null model. Our method is robust to inhomogeneity,
computationally efficient even in large datasets, and provides approximate
$p$-values for testing spatial clustering and colocalization. Methodological
developments are motivated by a spatial proteomics study of 103 women with
ovarian cancer, where our analysis using \textit{KAMP} shows a positive
association between immune cell clustering and overall patient survival.
Notably, we also find evidence that using $K$ without correcting for sample
inhomogeneity may bias hazard ratio estimates in downstream analyses.
\textit{KAMP} completes this analysis in just 5 minutes, compared to 538
minutes for the only competing method that adequately addresses inhomogeneity.",370,2412.08498v2,stat.ME,"stat.ME,q-bio.GN",genomics,2024-12-11,2024-12-23T21:07:00.183404
Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models,"Existing Large Vision-Language Models (LVLMs) excel at matching concepts
across multi-modal inputs but struggle with compositional concepts and
high-level relationships between entities. This paper introduces Progressive
multi-granular Vision-Language alignments (PromViL), a novel framework to
enhance LVLMs' ability in performing grounded compositional visual reasoning
tasks. Our approach constructs a hierarchical structure of multi-modal
alignments, ranging from simple to complex concepts. By progressively aligning
textual descriptions with corresponding visual regions, our model learns to
leverage contextual information from lower levels to inform higher-level
reasoning. To facilitate this learning process, we introduce a data generation
process that creates a novel dataset derived from Visual Genome, providing a
wide range of nested compositional vision-language pairs. Experimental results
demonstrate that our PromViL framework significantly outperforms baselines on
various visual grounding and compositional question answering tasks. The code
is available at: https://github.com/lqh52/PromViL.",214,2412.08125v2,cs.CV,"cs.CV,cs.CL,cs.LG",genomics,2024-12-11,2024-12-23T21:07:00.184401
Can linguists better understand DNA?,"Multilingual transfer ability, which reflects how well models fine-tuned on
one source language can be applied to other languages, has been well studied in
multilingual pre-trained models. However, the existence of such capability
transfer between natural language and gene sequences/languages remains
underexplored.This study addresses this gap by drawing inspiration from the
sentence-pair classification task used for evaluating sentence similarity in
natural language. We constructed two analogous tasks: DNA-pair
classification(DNA sequence similarity) and DNA-protein-pair
classification(gene coding determination). These tasks were designed to
validate the transferability of capabilities from natural language to gene
sequences. Even a small-scale pre-trained model like GPT-2-small, which was
pre-trained on English, achieved an accuracy of 78% on the DNA-pair
classification task after being fine-tuned on English sentence-pair
classification data(XTREME PAWS-X). While training a BERT model on multilingual
text, the precision reached 82%.On the more complex DNA-protein-pair
classification task, however, the model's output was barely distinguishable
from random output.Experiments suggest that there may be a capability transfer
from natural language to genetic language, but further task testing is needed
to confirm this.",262,2412.07678v1,cs.CL,"cs.CL,q-bio.GN,92-10,J.3",genomics,2024-12-10,2024-12-23T21:07:00.184401
Systematically Examining Reproducibility: A Case Study for High Throughput Sequencing using the PRIMAD Model and BioCompute Object,"The reproducibility of computational pipelines is an expectation in
biomedical science, particularly in critical domains like human health. In this
context, reporting next generation genome sequencing methods used in precision
medicine spurred the development of the IEEE 2791-2020 standard for
Bioinformatics Analyses Generated by High Throughput Sequencing (HTS), known as
the BioCompute Object (BCO). Championed by the USA's Food and Drug
Administration, the BCO is a pragmatic framework for documenting pipelines;
however, it has not been systematically assessed for its reproducibility
claims.
  This study uses the PRIMAD model, a conceptual framework for describing
computational experiments for reproducibility purposes, to systematically
review the BCO for depth and coverage. A meticulous mapping of BCO and PRIMAD
elements onto a published BCO use case reveals potential omissions and
necessary extensions within both frameworks. This underscores the significance
of systematically validating claims of reproducibility for published digital
objects, thereby enhancing the reliability of scientific research in bioscience
and related disciplines.
  This study, along with its artifacts, is reported as a RO-Crate, providing a
structured reporting approach, which is available at
https://doi.org/10.5281/zenodo.14317922.",272,2412.07502v1,cs.CE,cs.CE,genomics,2024-12-10,2024-12-23T21:07:00.185399
A multimodal ensemble approach for clear cell renal cell carcinoma treatment outcome prediction,"Purpose: A reliable cancer prognosis model for clear cell renal cell
carcinoma (ccRCC) can enhance personalized treatment. We developed a
multi-modal ensemble model (MMEM) that integrates pretreatment clinical data,
multi-omics data, and histopathology whole slide image (WSI) data to predict
overall survival (OS) and disease-free survival (DFS) for ccRCC patients.
Methods: We analyzed 226 patients from The Cancer Genome Atlas Kidney Renal
Clear Cell Carcinoma (TCGA-KIRC) dataset, which includes OS, DFS follow-up
data, and five data modalities: clinical data, WSIs, and three multi-omics
datasets (mRNA, miRNA, and DNA methylation). Separate survival models were
built for OS and DFS. Cox-proportional hazards (CPH) model with forward feature
selection is used for clinical and multi-omics data. Features from WSIs were
extracted using ResNet and three general-purpose foundation models. A deep
learning-based CPH model predicted survival using encoded WSI features. Risk
scores from all models were combined based on training performance. Results:
Performance was assessed using concordance index (C-index) and AUROC. The
clinical feature-based CPH model received the highest weight for both OS and
DFS tasks. Among WSI-based models, the general-purpose foundation model (UNI)
achieved the best performance. The final MMEM model surpassed single-modality
models, achieving C-indices of 0.820 (OS) and 0.833 (DFS), and AUROC values of
0.831 (3-year patient death) and 0.862 (cancer recurrence). Using predicted
risk medians to stratify high- and low-risk groups, log-rank tests showed
improved performance in both OS and DFS compared to single-modality models.
Conclusion: MMEM is the first multi-modal model for ccRCC patients, integrating
five data modalities. It outperformed single-modality models in prognostic
ability and has the potential to assist in ccRCC patient management if
independently validated.",464,2412.07136v1,cs.CV,"cs.CV,q-bio.QM",genomics,2024-12-10,2024-12-23T21:07:00.186396
A Misclassification Network-Based Method for Comparative Genomic Analysis,"Classifying genome sequences based on metadata has been an active area of
research in comparative genomics for decades with many important applications
across the life sciences. Established methods for classifying genomes can be
broadly grouped into sequence alignment-based and alignment-free models.
Conventional alignment-based models rely on genome similarity measures
calculated based on local sequence alignments or consistent ordering among
sequences. However, such methods are computationally expensive when dealing
with large ensembles of even moderately sized genomes. In contrast,
alignment-free (AF) approaches measure genome similarity based on summary
statistics in an unsupervised setting and are efficient enough to analyze large
datasets. However, both alignment-based and AF methods typically assume fixed
scoring rubrics that lack the flexibility to assign varying importance to
different parts of the sequences based on prior knowledge. In this study, we
integrate AI and network science approaches to develop a comparative genomic
analysis framework that addresses these limitations. Our approach, termed the
Genome Misclassification Network Analysis (GMNA), simultaneously leverages
misclassified instances, a learned scoring rubric, and label information to
classify genomes based on associated metadata and better understand potential
drivers of misclassification. We evaluate the utility of the GMNA using Naive
Bayes and convolutional neural network models, supplemented by additional
experiments with transformer-based models, to construct SARS-CoV-2 sampling
location classifiers using over 500,000 viral genome sequences and study the
resulting network of misclassifications. We demonstrate the global health
potential of the GMNA by leveraging the SARS-CoV-2 genome misclassification
networks to investigate the role human mobility played in structuring
geographic clustering of SARS-CoV-2.",356,2412.07051v1,q-bio.GN,"q-bio.GN,cs.LG",genomics,2024-12-09,2024-12-23T21:07:00.187393
Advancing clinical trial outcomes using deep learning and predictive modelling: bridging precision medicine and patient-centered care,"The integration of artificial intelligence [AI] into clinical trials has
revolutionized the process of drug development and personalized medicine. Among
these advancements, deep learning and predictive modelling have emerged as
transformative tools for optimizing clinical trial design, patient recruitment,
and real-time monitoring. This study explores the application of deep learning
techniques, such as convolutional neural networks [CNNs] and transformerbased
models, to stratify patients, forecast adverse events, and personalize
treatment plans. Furthermore, predictive modelling approaches, including
survival analysis and time-series forecasting, are employed to predict trial
outcomes, enhancing efficiency and reducing trial failure rates. To address
challenges in analysing unstructured clinical data, such as patient notes and
trial protocols, natural language processing [NLP] techniques are utilized for
extracting actionable insights. A custom dataset comprising structured patient
demographics, genomic data, and unstructured text is curated for training and
validating these models. Key metrics, including precision, recall, and F1
scores, are used to evaluate model performance, while trade-offs between
accuracy and computational efficiency are examined to identify the optimal
model for clinical deployment. This research underscores the potential of
AI-driven methods to streamline clinical trial workflows, improve
patient-centric outcomes, and reduce costs associated with trial
inefficiencies. The findings provide a robust framework for integrating
predictive analytics into precision medicine, paving the way for more adaptive
and efficient clinical trials. By bridging the gap between technological
innovation and real-world applications, this study contributes to advancing the
role of AI in healthcare, particularly in fostering personalized care and
improving overall trial success rates.",351,2412.07050v1,cs.LG,cs.LG,genomics,2024-12-09,2024-12-23T21:07:00.188390
ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models,"With the rise of multimodal applications, instruction data has become
critical for training multimodal language models capable of understanding
complex image-based queries. Existing practices rely on powerful but costly
large language models (LLMs) or multimodal language models (MLMs) to produce
instruction data. These are often prone to hallucinations, licensing issues and
the generation process is often hard to scale and interpret. In this work, we
present a programmatic approach that employs scene graphs as symbolic
representations of images and human-written programs to systematically
synthesize vision-centric instruction data. Our approach ensures the
interpretability and controllability of the data generation process and scales
efficiently while maintaining factual accuracy. By implementing a suite of 24
single-image, 14 multi-image instruction generators, and a scene graph
generation pipeline, we build a scalable, cost-effective system: ProVision
which produces diverse question-answer pairs concerning objects, attributes,
relations, depth, etc., for any given image. Applied to Visual Genome and
DataComp datasets, we generate over 10 million instruction data points,
ProVision-10M, and leverage them in both pretraining and instruction tuning
stages of MLMs. When adopted in the instruction tuning stage, our single-image
instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D
split of CVBench, along with a 3% increase in performance on QBench2,
RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%
improvement on Mantis-Eval. Incorporation of our data in both pre-training and
fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%
across 11 benchmarks.",361,2412.07012v2,cs.CV,"cs.CV,cs.AI",genomics,2024-12-09,2024-12-23T21:07:00.189388
The transition to speciation in the finite genome Derrida-Higgs model: a heuristic solution,"The process of speciation, where an ancestral species divides in two or more
new species, involves several geographic, environmental and genetic components
that interact in a complex way. Understanding all these elements at once is
challenging and simple models can help unveiling the role of each factor
separately. The Derrida-Higgs model describes the evolution of a sexually
reproducing population subjected to mutations in a well mixed population.
Individuals are characterized by a string with entries $\pm1$ representing a
haploid genome with biallelic genes. If mating is restricted by genetic
similarity, so that only individuals that are sufficiently similar can mate,
sympatric speciation, i.e. the emergence of species without geographic
isolation, can occur. Only four parameters rule the dynamics: population size
$N$, mutation rate $\mu$, minimum similarity for mating $q_{min}$ and genome
size $B$. In the limit $B\rightarrow\infty$, speciation occurs if the simple
condition $q_{min}>(1+4\mu N)^{-1}$ is satisfied. However, this condition fails
for finite genomes, and speciation does not occur if the genome size is too
small. This indicates the existence of a critical genome size for speciation.
In this work, we develop an analytical theory of the distribution of
similarities between individuals, a quantity that defines how tight or spread
out is the genetic content of the population. This theory is carried out in the
absence of mating restrictions, where evolution equations for the mean and
variance of the similarity distribution can be derived. We then propose a
heuristic description of the speciation transition which allows us to
numerically calculate the critical genome size for speciation as a function of
the other model parameters. The result is in good agreement with the
simulations of the model and may guide further investigations on theoretical
conditions for species formation.",395,2412.06565v1,q-bio.PE,"q-bio.PE,cond-mat.stat-mech,92-10",genomics,2024-12-09,2024-12-23T21:07:00.190385
DNA Fragments in Crude Oil Reveals Earth's Hidden History,"This groundbreaking research extracted DNA from petroleum using nanoparticle
affinity bead technology, yielding 3,159,020 petroleum DNA (pDNA) sequences,
primarily environmental DNA. While most original in situ DNA (oriDNA) was lost,
ancient DNA (aDNA) from petroleum offers an important source of ecological and
evolutionary information, surpassing traditional fossils. This study reveals
that oil, mainly sourced from algae and lower aquatic plants, now serves as a
new type of fossil, providing detailed insights into Earth's hidden history,
including unclassified species and ancient events, revolutionizing petroleum
geology and paleontology.",126,2412.06550v1,q-bio.GN,q-bio.GN,genomics,2024-12-09,2024-12-23T21:07:00.190385
Ancient DNA from 120-Million-Year-Old Lycoptera Fossils Reveals Evolutionary Insights,"High quality ancient DNA (aDNA) is essential for molecular paleontology. Due
to DNA degradation and contamination by environmental DNA (eDNA), current
research is limited to fossils less than 1 million years old. The study
successfully extracted DNA from Lycoptera davidi fossils from the Early
Cretaceous period, dating 120 million years ago. Using high-throughput
sequencing, 1,258,901 DNA sequences were obtained. We established a rigorous
protocol known as the mega screen method. Using this method, we identified 243
original in situ DNA (oriDNA) sequences, likely from the Lycoptera genome.
These sequences have an average length of over 100 base pairs and show no signs
of deamination. Additionally, 10 transposase coding sequences were discovered,
shedding light on a unique self-renewal mechanism in the genome. This study
provides valuable DNA data for understanding ancient fish evolution and
advances paleontological research.",192,2412.06521v1,q-bio.GN,q-bio.GN,genomics,2024-12-09,2024-12-23T21:07:00.191382
Emerging Challenges in Molecular Paleontology: Misapplication of Environmental DNA Fragments and Misconception of Deamination as a Key Criterion for In Situ DNA Identification,"This article critically examines the methodologies applied in ancient DNA
(aDNA) research, particularly those developed by Dr. P\""a\""abo's team, which
have significantly influenced the field. The focus is on the challenges of
distinguishing original in situ DNA (oriDNA) from environmental DNA (eDNA)
contamination in fossil samples. Recent analyses indicate that even with
rigorous extraction and sequencing protocols, a considerable amount of eDNA
remains present, often misinterpreted as oriDNA. This misidentification risks
the accuracy of species ascription and evolutionary interpretations derived
from fossil analyses. The paper explores fossil preservation's physical and
chemical dynamics, which allow eDNA from similar and disparate species to
infiltrate bone matrices. We propose enhancements to methodological frameworks,
such as broader BLAST database usage and stringent E-value criteria, to improve
species-specific aDNA identification. Additionally, the article critiques the
reliance on deamination patterns as a definitive marker for aDNA, suggesting a
reevaluation of this criterion due to its inconsistency and the potential for
misleading sequencing results. Ultimately, our findings advocate for a more
cautious and refined approach to aDNA research, ensuring more reliable and
verifiable scientific outcomes",256,2412.06378v1,q-bio.GN,q-bio.GN,genomics,2024-12-09,2024-12-23T21:07:00.191382
ProtGO: A Transformer based Fusion Model for accurately predicting Gene Ontology (GO) Terms from full scale Protein Sequences,"Recent developments in next generation sequencing technology have led to the
creation of extensive, open-source protein databases consisting of hundreds of
millions of sequences. To render these sequences applicable in biomedical
applications, they must be meticulously annotated by wet lab testing or
extracting them from existing literature. Over the last few years, researchers
have developed numerous automatic annotation systems, particularly deep
learning models based on machine learning and artificial intelligence, to
address this issue. In this work, we propose a transformer-based fusion model
capable of predicting Gene Ontology (GO) terms from full-scale protein
sequences, achieving state-of-the-art accuracy compared to other contemporary
machine learning annotation systems. The approach performs particularly well on
clustered split datasets, which comprise training and testing samples
originating from distinct distributions that are structurally diverse. This
demonstrates that the model is able to understand both short and long term
dependencies within the enzyme's structure and can precisely identify the
motifs associated with the various GO terms. Furthermore, the technique is
lightweight and less computationally expensive compared to the benchmark
methods, while at the same time not unaffected by sequence length, rendering it
appropriate for diverse applications with varying sequence lengths.",245,2412.05776v1,cs.LG,"cs.LG,q-bio.GN",genomics,2024-12-08,2024-12-23T21:07:00.192381
DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA,"Recent advances in self-supervised models for natural language, vision, and
protein sequences have inspired the development of large genomic DNA language
models (DNALMs). These models aim to learn generalizable representations of
diverse DNA elements, potentially enabling various genomic prediction,
interpretation and design tasks. Despite their potential, existing benchmarks
do not adequately assess the capabilities of DNALMs on key downstream
applications involving an important class of non-coding DNA elements critical
for regulating gene activity. In this study, we introduce DART-Eval, a suite of
representative benchmarks specifically focused on regulatory DNA to evaluate
model performance across zero-shot, probed, and fine-tuned scenarios against
contemporary ab initio models as baselines. Our benchmarks target biologically
meaningful downstream tasks such as functional sequence feature discovery,
predicting cell-type specific regulatory activity, and counterfactual
prediction of the impacts of genetic variants. We find that current DNALMs
exhibit inconsistent performance and do not offer compelling gains over
alternative baseline models for most tasks, while requiring significantly more
computational resources. We discuss potentially promising modeling, data
curation, and evaluation strategies for the next generation of DNALMs. Our code
is available at https://github.com/kundajelab/DART-Eval.",263,2412.05430v1,cs.LG,"cs.LG,q-bio.GN",genomics,2024-12-06,2024-12-23T21:07:00.193378
An RNA condensate model for the origin of life,"The RNA World hypothesis predicts that self-replicating RNAs evolved before
DNA genomes and coded proteins. Despite widespread support for the RNA World,
self-replicating RNAs have yet to be identified in a natural context, leaving a
key 'missing link' for this explanation of the origin of life. Inspired by
recent work showing that condensates of charged polymers can create
electrochemical gradients capable of catalyzing hydrolysis, we consider a
catalytic RNA condensate as a candidate for the self-replicating RNA. We
develop a theoretical framework where an RNA condensate formed by the
spontaneous demixing of disordered RNA sequences undergoes self-replicative
amplification. Our theory addresses two central problems in the origins of
life: (i) the origin of compartmentalization and (ii) the error threshold for
the accuracy of templated replication. We show that many of the needed
properties of this self-replicating RNA condensate have been realized
experimentally in recent studies and can be formalized within a standard
polymer physics framework. Specifically, we propose that short, low-complexity
RNA polymers formed catalytic condensates capable of templated RNA
polymerization. Because the condensate properties depend on the RNA sequences,
RNAs that formed condensates with improved polymerization and demixing capacity
would be amplified, leading to a 'condensate chain reaction' and evolution by
natural selection. We believe this prediction could be tested with current
experimental and theoretical tools. Furthermore, we note that the extant
nucleolus appears to satisfy many of the requirements of an evolutionary relic
for the model we propose. More generally, we suggest that future work on the
origin of life would benefit from condensate-centric biophysical models of RNA
evolution.",357,2412.05396v1,q-bio.BM,q-bio.BM,genomics,2024-12-06,2024-12-23T21:07:00.194375
Approaches to studying virus pangenome variation graphs,"Pangenome variation graphs (PVGs) allow for the representation of genetic
diversity in a more nuanced way than traditional reference-based approaches.
Here we focus on how PVGs are a powerful tool for studying genetic variation in
viruses, offering insights into the complexities of viral quasispecies,
mutation rates, and population dynamics. PVGs allow for the representation of
genetic diversity in a more nuanced way than traditional reference-based
approaches. PVGs originated in human genomics and hold great promise for viral
genomics. Previous work has been constrained by small sample sizes and
gene-centric methods, PVGs enable a more comprehensive approach to studying
viral diversity. Large viral genome collections should be used to make PVGs,
which offer significant advantages: we outline accessible tools to achieve
this. This spans PVG construction, PVG file formats, PVG manipulation and
analysis, PVG visualisation, measuring PVG openness, and mapping reads to PVGs.
Additionally, the development of PVG-specific formats for mutation
representation and personalised PVGs that reflect specific research questions
will further enhance PVG applications. Although challenges remain, particularly
in managing nested variants and optimising error detection, PVGs offer a
promising direction for viral population genomics. These advances will enable
more accurate and comprehensive detection of viral mutations, contributing to a
deeper understanding of viral evolution and genotype-phenotype associations.",286,2412.05096v1,q-bio.GN,q-bio.GN,genomics,2024-12-06,2024-12-23T21:07:00.195372
Does your model understand genes? A benchmark of gene properties for biological and text models,"The application of deep learning methods, particularly foundation models, in
biological research has surged in recent years. These models can be text-based
or trained on underlying biological data, especially omics data of various
types. However, comparing the performance of these models consistently has
proven to be a challenge due to differences in training data and downstream
tasks. To tackle this problem, we developed an architecture-agnostic
benchmarking approach that, instead of evaluating the models directly,
leverages entity representation vectors from each model and trains simple
predictive models for each benchmarking task. This ensures that all types of
models are evaluated using the same input and output types. Here we focus on
gene properties collected from professionally curated bioinformatics databases.
These gene properties are categorized into five major groups: genomic
properties, regulatory functions, localization, biological processes, and
protein properties. Overall, we define hundreds of tasks based on these
databases, which include binary, multi-label, and multi-class classification
tasks. We apply these benchmark tasks to evaluate expression-based models,
large language models, protein language models, DNA-based models, and
traditional baselines. Our findings suggest that text-based models and protein
language models generally outperform expression-based models in genomic
properties and regulatory functions tasks, whereas expression-based models
demonstrate superior performance in localization tasks. These results should
aid in the development of more informed artificial intelligence strategies for
biological understanding and therapeutic discovery. To ensure the
reproducibility and transparency of our findings, we have made the source code
and benchmark data publicly accessible for further investigation and expansion
at github.com/BiomedSciAI/gene-benchmark.",342,2412.04075v1,cs.AI,cs.AI,genomics,2024-12-05,2024-12-23T21:07:00.196369
Multi-scale phylodynamic modelling of rapid punctuated pathogen evolution,"Computational multi-scale pandemic modelling remains a major and timely
challenge. Here we identify specific requirements for a new class of pandemic
models operating across three scales: (1) rapid pathogen evolution, punctuated
by emergence of new variants, (2) human interactions within a heterogeneous
population, and (3) public health responses which constrain individual actions
to control the disease transmission. We then present a pandemic modelling
framework satisfying these requirements and capable of simulating multi-scale
dynamic feedback loops. The developed framework comprises a stochastic
agent-based model of pandemic spread, coupled with a phylodynamic model of the
within-host pathogen evolution. It is validated with a case study, modelling a
rapid punctuated evolution of SARS-CoV-2, based on global and contemporary
genomic surveillance data, during the COVID-19 transmission within a large
heterogeneous population. We demonstrate that the model captures the essential
features of the COVID-19 pandemic and the novel coronavirus evolution, while
retaining computational tractability and scalability.",226,2412.03896v2,q-bio.PE,q-bio.PE,genomics,2024-12-05,2024-12-23T21:07:00.197366
End to End Collaborative Synthetic Data Generation,"The success of AI is based on the availability of data to train models. While
in some cases a single data custodian may have sufficient data to enable AI,
often multiple custodians need to collaborate to reach a cumulative size
required for meaningful AI research. The latter is, for example, often the case
for rare diseases, with each clinical site having data for only a small number
of patients. Recent algorithms for federated synthetic data generation are an
important step towards collaborative, privacy-preserving data sharing. Existing
techniques, however, focus exclusively on synthesizer training, assuming that
the training data is already preprocessed and that the desired synthetic data
can be delivered in one shot, without any hyperparameter tuning. In this paper,
we propose an end-to-end collaborative framework for publishing of synthetic
data that accounts for privacy-preserving preprocessing as well as evaluation.
We instantiate this framework with Secure Multiparty Computation (MPC)
protocols and evaluate it in a use case for privacy-preserving publishing of
synthetic genomic data for leukemia.",216,2412.03766v1,cs.CR,"cs.CR,cs.LG",genomics,2024-12-04,2024-12-23T21:07:00.197366
Deep Learning in Single-Cell and Spatial Transcriptomics Data Analysis: Advances and Challenges from a Data Science Perspective,"The development of single-cell and spatial transcriptomics has revolutionized
our capacity to investigate cellular properties, functions, and interactions in
both cellular and spatial contexts. However, the analysis of single-cell and
spatial omics data remains challenging. First, single-cell sequencing data are
high-dimensional and sparse, often contaminated by noise and uncertainty,
obscuring the underlying biological signals. Second, these data often encompass
multiple modalities, including gene expression, epigenetic modifications, and
spatial locations. Integrating these diverse data modalities is crucial for
enhancing prediction accuracy and biological interpretability. Third, while the
scale of single-cell sequencing has expanded to millions of cells, high-quality
annotated datasets are still limited. Fourth, the complex correlations of
biological tissues make it difficult to accurately reconstruct cellular states
and spatial contexts. Traditional feature engineering-based analysis methods
struggle to deal with the various challenges presented by intricate biological
networks. Deep learning has emerged as a powerful tool capable of handling
high-dimensional complex data and automatically identifying meaningful
patterns, offering significant promise in addressing these challenges. This
review systematically analyzes these challenges and discusses related deep
learning approaches. Moreover, we have curated 21 datasets from 9 benchmarks,
encompassing 58 computational methods, and evaluated their performance on the
respective modeling tasks. Finally, we highlight three areas for future
development from a technical, dataset, and application perspective. This work
will serve as a valuable resource for understanding how deep learning can be
effectively utilized in single-cell and spatial transcriptomics analyses, while
inspiring novel approaches to address emerging challenges.",322,2412.03614v2,q-bio.GN,"q-bio.GN,cs.LG",genomics,2024-12-04,2024-12-23T21:07:00.198364
Generating Synthetic Genotypes using Diffusion Models,"In this paper, we introduce the first diffusion model designed to generate
complete synthetic human genotypes, which, by standard protocols, one can
straightforwardly expand into full-length, DNA-level genomes. The synthetic
genotypes mimic real human genotypes without just reproducing known genotypes,
in terms of approved metrics. When training biomedically relevant classifiers
with synthetic genotypes, accuracy is near-identical to the accuracy achieved
when training classifiers with real data. We further demonstrate that
augmenting small amounts of real with synthetically generated genotypes
drastically improves performance rates. This addresses a significant challenge
in translational human genetics: real human genotypes, although emerging in
large volumes from genome wide association studies, are sensitive private data,
which limits their public availability. Therefore, the integration of
additional, insensitive data when striving for rapid sharing of biomedical
knowledge of public interest appears imperative.",192,2412.03278v1,cs.CE,cs.CE,genomics,2024-12-04,2024-12-23T21:07:00.199361
Hybrid deep learning-based strategy for the hepatocellular carcinoma cancer grade classification of H&E stained liver histopathology images,"Hepatocellular carcinoma (HCC) is a common type of liver cancer whose
early-stage diagnosis is a common challenge, mainly due to the manual
assessment of hematoxylin and eosin-stained whole slide images, which is a
time-consuming process and may lead to variability in decision-making. For
accurate detection of HCC, we propose a hybrid deep learning-based architecture
that uses transfer learning to extract the features from pre-trained
convolutional neural network (CNN) models and a classifier made up of a
sequence of fully connected layers. This study uses a publicly available The
Cancer Genome Atlas Hepatocellular Carcinoma (TCGA-LIHC)database (n=491) for
model development and database of Kasturba Gandhi Medical College (KMC), India
for validation. The pre-processing step involves patch extraction, colour
normalization, and augmentation that results in 3920 patches for the TCGA
dataset. The developed hybrid deep neural network consisting of a CNN-based
pre-trained feature extractor and a customized artificial neural network-based
classifier is trained using five-fold cross-validation. For this study, eight
different state-of-the-art models are trained and tested as feature extractors
for the proposed hybrid model. The proposed hybrid model with ResNet50-based
feature extractor provided the sensitivity, specificity, F1-score, accuracy,
and AUC of 100.00%, 100.00%, 100.00%, 100.00%, and 1.00, respectively on the
TCGA database. On the KMC database, EfficientNetb3 resulted in the optimal
choice of the feature extractor giving sensitivity, specificity, F1-score,
accuracy, and AUC of 96.97, 98.85, 96.71, 96.71, and 0.99, respectively. The
proposed hybrid models showed improvement in accuracy of 2% and 4% over the
pre-trained models in TCGA-LIHC and KMC databases.",422,2412.03084v1,eess.IV,"eess.IV,cs.CV,cs.LG,q-bio.QM",genomics,2024-12-04,2024-12-23T21:07:00.200359
Timestamp calibration for time-series single cell RNA-seq expression data,"Timestamp automatic annotation (TAA) is a crucial procedure for analyzing
time-series ScRNA-seq data, as they unveil dynamic biological developments and
cell regeneration process. However, current TAA methods heavily rely on manual
timestamps, often overlooking their reliability. This oversight can
significantly degrade the performance of timestamp automatic annotation due to
noisy timestamps. Nevertheless, the current approach for addressing this issue
tends to select less critical cleaned samples for timestamp calibration. To
tackle this challenge, we have developed a novel timestamp calibration model
called ScPace for handling noisy labeled time-series ScRNA-seq data. This
approach incorporates a latent variable indicator within a base classifier
instead of probability sampling to detect noisy samples effectively. To
validate our proposed method, we conducted experiments on both simulated and
real time-series ScRNA-seq datasets. Cross-validation experiments with
different artificial mislabeling rates demonstrate that ScPace outperforms
previous approaches. Furthermore, after calibrating the timestamps of the
original time-series ScRNA-seq data using our method, we performed supervised
pseudotime analysis, revealing that ScPace enhances its performance
significantly. These findings suggest that ScPace is an effective tool for
timestamp calibration by enabling reclassification and deletion of detected
noisy labeled samples while maintaining robustness across diverse ranges of
time-series ScRNA-seq datasets. The source code is available at
https://github.com/OPUS-Lightphenexx/ScPace.",334,2412.03027v1,q-bio.GN,q-bio.GN,genomics,2024-12-04,2024-12-23T21:07:00.200359
gghic: A Versatile R Package for Exploring and Visualizing 3D Genome Organization,"Motivation: The three-dimensional (3D) organization of the genome plays a
critical role in regulating gene expression and maintaining cellular
homeostasis. Disruptions in this spatial organization can result in abnormal
chromatin interactions, contributing to the development of various diseases
including cancer. Advances in chromosome conformation capture technologies,
such as Hi-C, have enabled researchers to study genome architecture at high
resolution. However, the efficient visualization and interpretation of these
complex datasets remain a major challenge, particularly when integrating
genomic annotations and inter-chromosomal interactions.
  Results: We present gghic, an R package that extends the ggplot2 framework to
enable intuitive and customizable visualization of genomic interaction data.
gghic introduces novel layers for generating triangular heatmaps of chromatin
interactions and annotating them with features such as chromatin loops,
topologically associated domains (TADs), gene/transcript models, and data
tracks (e.g., ChIP-seq signals). The package supports data from multiple
chromosomes, facilitating the exploration of inter-chromosomal interactions.
Built to integrate seamlessly with the R/Bioconductor ecosystem, gghic is
compatible with widely used genomic data formats, including HiCExperiment and
GInteractions objects. We demonstrate the utility of gghic by replicating a
published figure showing a translocation event in T-cell acute lymphoblastic
leukemia (T-ALL), highlighting its ability to integrate genomic annotations and
generate publication-quality figures.
  Availability and implementation: The R package can be accessed at
https://github.com/jasonwong-lab/gghic and is distributed under the GNU General
Public License version 3.0.",368,2412.03005v1,q-bio.GN,q-bio.GN,genomics,2024-12-04,2024-12-23T21:07:00.201356
Single-Cell Omics Arena: A Benchmark Study for Large Language Models on Cell Type Annotation Using Single-Cell Data,"Over the past decade, the revolution in single-cell sequencing has enabled
the simultaneous molecular profiling of various modalities across thousands of
individual cells, allowing scientists to investigate the diverse functions of
complex tissues and uncover underlying disease mechanisms. Among all the
analytical steps, assigning individual cells to specific types is fundamental
for understanding cellular heterogeneity. However, this process is usually
labor-intensive and requires extensive expert knowledge. Recent advances in
large language models (LLMs) have demonstrated their ability to efficiently
process and synthesize vast corpora of text to automatically extract essential
biological knowledge, such as marker genes, potentially promoting more
efficient and automated cell type annotations. To thoroughly evaluate the
capability of modern instruction-tuned LLMs in automating the cell type
identification process, we introduce SOAR, a comprehensive benchmarking study
of LLMs for cell type annotation tasks in single-cell genomics. Specifically,
we assess the performance of 8 instruction-tuned LLMs across 11 datasets,
spanning multiple cell types and species. Our study explores the potential of
LLMs to accurately classify and annotate cell types in single-cell RNA
sequencing (scRNA-seq) data, while extending their application to multiomics
data through cross-modality translation. Additionally, we evaluate the
effectiveness of chain-of-thought (CoT) prompting techniques in generating
detailed biological insights during the annotation process. The results
demonstrate that LLMs can provide robust interpretations of single-cell data
without requiring additional fine-tuning, advancing the automation of cell type
annotation in genomics research.",325,2412.02915v1,cs.CL,"cs.CL,q-bio.GN",genomics,2024-12-03,2024-12-23T21:07:00.202353
iSEEtree: interactive explorer for hierarchical data,"$\textbf{Motivation:}$ Hierarchical data structures are prevalent across
several fields of research, as they represent an organised and efficient
approach to study complex interconnected systems. Their significance is
particularly evident in microbiome analysis, where microbial communities are
classified at various taxonomic levels along the phylogenetic tree. In light of
this trend, the R/Bioconductor community has established a reproducible
analytical framework for hierarchical data, which relies on the highly generic
and optimised TreeSummarizedExperiment data container. However, using this
framework requires basic proficiency in programming.
  $\textbf{Results:}$ To reduce the entry requirements, we developed iSEEtree,
an R shiny app which provides a visual interface for the analysis and
exploration of TreeSummarizedExperiment objects, thereby expanding the
interactive graphics capabilities of related work to hierarchical structures.
This way, users can interactively explore several aspects of their data without
the need for extensive knowledge of R programming. We describe how iSEEtree
enables the exploration of hierarchical multi-table data and demonstrate its
functionality with applications to microbiome analysis.
  $\textbf{Availability and Implementation:}$ iSEEtree was implemented in the R
programming language and is available on Bioconductor at
https://bioconductor.org/packages/iSEEtree under an Artistic 2.0 license.
  $\textbf{Contact:}$ giulio.benedetti@utu.fi or leo.lahti@utu.fi.",316,2412.02882v2,cs.MS,"cs.MS,cs.GR,q-bio.GN,G.4, H.5.2",genomics,2024-12-03,2024-12-23T21:07:00.203350
Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action Tracklet Generation,"Understanding video content is pivotal for advancing real-world applications
like activity recognition, autonomous systems, and human-computer interaction.
While scene graphs are adept at capturing spatial relationships between objects
in individual frames, extending these representations to capture dynamic
interactions across video sequences remains a significant challenge. To address
this, we present TCDSG, Temporally Consistent Dynamic Scene Graphs, an
innovative end-to-end framework that detects, tracks, and links subject-object
relationships across time, generating action tracklets, temporally consistent
sequences of entities and their interactions. Our approach leverages a novel
bipartite matching mechanism, enhanced by adaptive decoder queries and feedback
loops, ensuring temporal coherence and robust tracking over extended sequences.
This method not only establishes a new benchmark by achieving over 60%
improvement in temporal recall@k on the Action Genome, OpenPVSG, and MEVA
datasets but also pioneers the augmentation of MEVA with persistent object ID
annotations for comprehensive tracklet generation. By seamlessly integrating
spatial and temporal dynamics, our work sets a new standard in multi-frame
video analysis, opening new avenues for high-impact applications in
surveillance, autonomous navigation, and beyond.",244,2412.02808v1,cs.CV,"cs.CV,cs.LG",genomics,2024-12-03,2024-12-23T21:07:00.204348
"Microbial Mat Metagenomes from Waikite Valley, Aotearoa New Zealand","The rise of complex multicellular ecosystems Neoproterozoic time was preceded
by a microbial Proterozoic biosphere, where productivity may have been largely
restricted to microbial mats made up of bacteria including oxygenic
photosynthetic Cyanobacteria, anoxygenic phototrophs, and heterotrophs. In
modern environments, analogous microbial mats can be found in restricted
environments such as carbonate tidal flats and terrestrial hot springs. Here,
we report metagenomic sequence data from an analog in the hot springs of
Waikite Valley, Aotearoa New Zealand, where carbon-rich, slightly-alkaline
geothermal waters support diverse phototrophic microbial mats.
  The Waikite Valley hot spring in the Taupo Volcanic Zone of Aotearoa New
Zealand was sampled in duplicate at 8 points along a temperature gradient
transect of the outflow, from ~62 C (near the source) to ~37 C (~100 meters
downstream). ~686 Gb of shotgun metagenomic sequence data was generated by
Illumina Novaseq. Each sample was assembled using SPAdes, followed by binning
of metagenome-assembled genomes (MAGs) by MetaBAT. These data are useful for
the genomic analysis of novel phototrophic bacteria, as well as for ecological
comparisons between thermophilic communities with varying temperatures but
otherwise similar conditions.",289,2412.01649v1,q-bio.GN,"q-bio.GN,q-bio.PE",genomics,2024-12-02,2024-12-23T21:07:00.204348
pasta: Pattern Analysis for Spatial Omics Data,"Spatial omics assays allow for the molecular characterisation of cells in
their spatial context. Notably, the two main technological streams,
imaging-based and high-throughput sequencing-based, can give rise to very
different data modalities. The characteristics of the two data types are well
known in adjacent fields such as spatial statistics as point patterns and
lattice data, and there is a wide range of tools available. This paper
discusses the application of spatial statistics to spatially-resolved omics
data and in particular, discusses various advantages, challenges, and nuances.
This work is accompanied by a vignette, pasta, that showcases the usefulness of
spatial statistics in biology using several R packages.",142,2412.01561v1,q-bio.QM,"q-bio.QM,q-bio.GN",genomics,2024-12-02,2024-12-23T21:07:00.205345
The influence of chromosomal inversions on genetic variation and clinal patterns in genomic data of Drosophila melanogaster,"Chromosomal inversions are structural mutations resulting in the reversal of
the gene order along the corresponding genomic region. Due to their influence
on recombination patterns, they can have a major influence on genetic variation
and the evolutionary process. Accordingly, inversions can act as supergenes
that keep together co-adapted gene complexes that form the genetic basis of
many complex phenotypes in diverse organisms. In this book chapter, I will
present an analysis pipeline to investigate the influence of two common
cosmopolitan inversion, In(2L)t and In(3R)Payne, on genome-wide genetic
variation and differentiation in world-wide populations of the vinegar fly
Drosophila melanogaster. We will use single-individual and pooled resequencing
data in combination with population genomics analysis tools to explore the
impact of these two inversions on genetic variation, population structure, and
clinal variation in natural populations.",191,2412.01352v2,q-bio.PE,"q-bio.PE,q-bio.GN",genomics,2024-12-02,2024-12-23T21:07:00.205345
EsurvFusion: An evidential multimodal survival fusion model based on Gaussian random fuzzy numbers,"Multimodal survival analysis aims to combine heterogeneous data sources
(e.g., clinical, imaging, text, genomics) to improve the prediction quality of
survival outcomes. However, this task is particularly challenging due to high
heterogeneity and noise across data sources, which vary in structure,
distribution, and context. Additionally, the ground truth is often censored
(uncertain) due to incomplete follow-up data. In this paper, we propose a novel
evidential multimodal survival fusion model, EsurvFusion, designed to combine
multimodal data at the decision level through an evidence-based decision fusion
layer that jointly addresses both data and model uncertainty while
incorporating modality-level reliability. Specifically, EsurvFusion first
models unimodal data with newly introduced Gaussian random fuzzy numbers,
producing unimodal survival predictions along with corresponding aleatoric and
epistemic uncertainties. It then estimates modality-level reliability through a
reliability discounting layer to correct the misleading impact of noisy data
modalities. Finally, a multimodal evidence-based fusion layer is introduced to
combine the discounted predictions to form a unified, interpretable multimodal
survival analysis model, revealing each modality's influence based on the
learned reliability coefficients. This is the first work that studies
multimodal survival analysis with both uncertainty and reliability. Extensive
experiments on four multimodal survival datasets demonstrate the effectiveness
of our model in handling high heterogeneity data, establishing new
state-of-the-art on several benchmarks.",322,2412.01215v1,cs.LG,cs.LG,genomics,2024-12-02,2024-12-23T21:07:00.206342
SUICA: Learning Super-high Dimensional Sparse Implicit Neural Representations for Spatial Transcriptomics,"Spatial Transcriptomics (ST) is a method that captures spatial gene
expression profiles within histological sections. The discrete spatial
distribution and the super-high dimensional sequencing results make ST data
challenging to be modeled effectively. In this paper, we manage to model ST in
a continuous and compact manner by the proposed tool, SUICA, empowered by the
great approximation capability of Implicit Neural Representations (INRs) that
can improve both the spatial resolution and the gene expression. Concretely
within the proposed SUICA, we incorporate a graph-augmented Autoencoder to
effectively model the context information of the unstructured spots and provide
informative embeddings that are structure-aware for spatial mapping. We also
tackle the extremely skewed distribution in a regression-by-classification
fashion and enforce classification-based loss functions for the optimization of
SUICA. By extensive experiments of a wide range of common ST platforms, SUICA
outperforms both conventional INR variants and SOTA methods for ST
super-resolution regarding numerical fidelity, statistical correlation, and
bio-conservation. The prediction by SUICA also showcases amplified gene
signatures that enriches the bio-conservation of the raw data and benefits
subsequent analysis. The code is available at https://github.com/Szym29/SUICA.",264,2412.01124v1,cs.LG,"cs.LG,q-bio.GN",genomics,2024-12-02,2024-12-23T21:07:00.207340
Computational Methods for Breast Cancer Molecular Profiling through Routine Histopathology: A Review,"Precision medicine has become a central focus in breast cancer management,
advancing beyond conventional methods to deliver more precise and
individualized therapies. Traditionally, histopathology images have been used
primarily for diagnostic purposes; however, they are now recognized for their
potential in molecular profiling, which provides deeper insights into cancer
prognosis and treatment response. Recent advancements in artificial
intelligence (AI) have enabled digital pathology to analyze histopathologic
images for both targeted molecular and broader omic biomarkers, marking a
pivotal step in personalized cancer care. These technologies offer the
capability to extract various biomarkers such as genomic, transcriptomic,
proteomic, and metabolomic markers directly from the routine hematoxylin and
eosin (H&E) stained images, which can support treatment decisions without the
need for costly molecular assays. In this work, we provide a comprehensive
review of AI-driven techniques for biomarker detection, with a focus on diverse
omic biomarkers that allow novel biomarker discovery. Additionally, we analyze
the major challenges faced in this field for robust algorithm development.
These challenges highlight areas where further research is essential to bridge
the gap between AI research and clinical application.",250,2412.10392v1,q-bio.QM,"q-bio.QM,cs.CV,cs.LG",genomics,2024-12-01,2024-12-23T21:07:00.208337
Towards Unified Molecule-Enhanced Pathology Image Representation Learning via Integrating Spatial Transcriptomics,"Recent advancements in multimodal pre-training models have significantly
advanced computational pathology. However, current approaches predominantly
rely on visual-language models, which may impose limitations from a molecular
perspective and lead to performance bottlenecks. Here, we introduce a Unified
Molecule-enhanced Pathology Image REpresentationn Learning framework (UMPIRE).
UMPIRE aims to leverage complementary information from gene expression profiles
to guide the multimodal pre-training, enhancing the molecular awareness of
pathology image representation learning. We demonstrate that this molecular
perspective provides a robust, task-agnostic training signal for learning
pathology image embeddings. Due to the scarcity of paired data, approximately 4
million entries of spatial transcriptomics gene expression were collected to
train the gene encoder. By leveraging powerful pre-trained encoders, UMPIRE
aligns the encoders across over 697K pathology image-gene expression pairs. The
performance of UMPIRE is demonstrated across various molecular-related
downstream tasks, including gene expression prediction, spot classification,
and mutation state prediction in whole slide images. Our findings highlight the
effectiveness of multimodal data integration and open new avenues for exploring
computational pathology enhanced by molecular perspectives. The code and
pre-trained weights are available at https://github.com/Hanminghao/UMPIRE.",262,2412.00651v1,cs.CV,"cs.CV,q-bio.GN",genomics,2024-12-01,2024-12-23T21:07:00.208337
MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption,"The integration of fully homomorphic encryption (FHE) in federated learning
(FL) has led to significant advances in data privacy. However, during the
aggregation phase, it often results in performance degradation of the
aggregated model, hindering the development of robust representational
generalization. In this work, we propose a novel multimodal quantum federated
learning framework that utilizes quantum computing to counteract the
performance drop resulting from FHE. For the first time in FL, our framework
combines a multimodal quantum mixture of experts (MQMoE) model with FHE,
incorporating multimodal datasets for enriched representation and task-specific
learning. Our MQMoE framework enhances performance on multimodal datasets and
combined genomics and brain MRI scans, especially for underrepresented
categories. Our results also demonstrate that the quantum-enhanced approach
mitigates the performance degradation associated with FHE and improves
classification accuracy across diverse datasets, validating the potential of
quantum interventions in enhancing privacy in FL.",211,2412.01858v3,quant-ph,"quant-ph,cs.CR,cs.DC,cs.ET,cs.LG",genomics,2024-11-30,2024-12-23T21:07:00.209335
LLaMA-Gene: A General-purpose Gene Task Large Language Model Based on Instruction Fine-tuning,"Building a general-purpose task model similar to ChatGPT has been an
important research direction for gene large language models. Instruction
fine-tuning is a key component in building ChatGPT, but existing instructions
are primarily based on natural language. Natural language and gene sequences
have significant differences in tokenization and encoding. Therefore,
constructing a multilingual model that can handle both natural language and
gene sequences is crucial for solving this problem.In this paper, we expand the
capabilities of the LLaMA large language model to include gene language. This
involves expanding the vocabulary using the Byte Pair Encoding (BPE) method,
specifically tailored for DNA and protein sequences, and conducting further
pre-training on these sequences. We then convert various downstream gene task
data into a unified format for instruction fine-tuning and further fine-tune
the model on this data.Our study demonstrates that a mixed model of gene and
natural language, fine-tuned with instructions, achieves results comparable to
the current state-of-the-art (SOTA) in tasks such as gene classification and
gene sequence interaction. This provides a promising direction for building a
unified large language model for gene tasks.",234,2412.00471v1,q-bio.GN,"q-bio.GN,92-10,J.3",genomics,2024-11-30,2024-12-23T21:07:00.210333
A Doubly Robust Method to Counteract Outcome-Dependent Selection Bias in Multi-Cohort EHR Studies,"Selection bias can hinder accurate estimation of association parameters in
binary disease risk models using non-probability samples like electronic health
records (EHRs). The issue is compounded when participants are recruited from
multiple clinics or centers with varying selection mechanisms that may depend
on the disease or outcome of interest. Traditional inverse-probability-weighted
(IPW) methods, based on constructed parametric selection models, often struggle
with misspecifications when selection mechanisms vary across cohorts. This
paper introduces a new Joint Augmented Inverse Probability Weighted (JAIPW)
method, which integrates individual-level data from multiple cohorts collected
under potentially outcome-dependent selection mechanisms, with data from an
external probability sample. JAIPW offers double robustness by incorporating a
flexible auxiliary score model to address potential misspecifications in the
selection models. We outline the asymptotic properties of the JAIPW estimator,
and our simulations reveal that JAIPW achieves up to five times lower relative
bias and three times lower root mean square error (RMSE) compared to the best
performing joint IPW methods under scenarios with misspecified selection
models. Applying JAIPW to the Michigan Genomics Initiative (MGI), a
multi-clinic EHR-linked biobank, combined with external national probability
samples, resulted in cancer-sex association estimates more closely aligned with
national estimates. We also analyzed the association between cancer and
polygenic risk scores (PRS) in MGI to illustrate a situation where the exposure
is not available in the external probability sample.",312,2412.00228v1,stat.ME,stat.ME,genomics,2024-11-29,2024-12-23T21:07:00.211330
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",synthetic biology,2024-12-20,2024-12-23T21:07:01.209788
Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation,"The increasing demand for privacy-preserving data analytics in finance
necessitates solutions for synthetic data generation that rigorously uphold
privacy standards. We introduce DP-Fed-FinDiff framework, a novel integration
of Differential Privacy, Federated Learning and Denoising Diffusion
Probabilistic Models designed to generate high-fidelity synthetic tabular data.
This framework ensures compliance with stringent privacy regulations while
maintaining data utility. We demonstrate the effectiveness of DP-Fed-FinDiff on
multiple real-world financial datasets, achieving significant improvements in
privacy guarantees without compromising data quality. Our empirical evaluations
reveal the optimal trade-offs between privacy budgets, client configurations,
and federated optimization strategies. The results affirm the potential of
DP-Fed-FinDiff to enable secure data sharing and robust analytics in highly
regulated domains, paving the way for further advances in federated learning
and privacy-preserving data synthesis.",186,2412.16083v1,cs.LG,"cs.LG,q-fin.ST",synthetic biology,2024-12-20,2024-12-23T21:07:01.210785
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",synthetic biology,2024-12-20,2024-12-23T21:07:01.211782
CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images,"3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.",246,2412.16028v1,cs.CV,cs.CV,synthetic biology,2024-12-20,2024-12-23T21:07:01.211782
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",synthetic biology,2024-12-20,2024-12-23T21:07:01.212779
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,synthetic biology,2024-12-20,2024-12-23T21:07:01.213776
Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation,"The rise of the generative models quality during the past years enabled the
generation of edited variations of images at an important scale. To counter the
harmful effects of such technology, the Image Difference Captioning (IDC) task
aims to describe the differences between two images. While this task is
successfully handled for simple 3D rendered images, it struggles on real-world
images. The reason is twofold: the training data-scarcity, and the difficulty
to capture fine-grained differences between complex images. To address those
issues, we propose in this paper a simple yet effective framework to both adapt
existing image captioning models to the IDC task and augment IDC datasets. We
introduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational
cost, and show it outperforms two-streams approaches by a significant margin on
real-world IDC datasets. We also propose to use synthetic augmentation to
improve the performance of IDC models in an agnostic fashion. We show that our
synthetic augmentation strategy provides high quality data, leading to a
challenging new dataset well-suited for IDC named Syned1.",244,2412.15939v1,cs.CV,"cs.CV,cs.AI",synthetic biology,2024-12-20,2024-12-23T21:07:01.213776
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",synthetic biology,2024-12-20,2024-12-23T21:07:01.214773
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",synthetic biology,2024-12-20,2024-12-23T21:07:01.215771
WigglyEyes: Inferring Eye Movements from Keypress Data,"We present a model for inferring where users look during interaction based on
keypress data only. Given a key log, it outputs a scanpath that tells,
moment-by-moment, how the user had moved eyes while entering those keys. The
model can be used as a proxy for human data in cases where collecting real eye
tracking data is expensive or impossible. Our technical insight is three-fold:
first, we present an inference architecture that considers the individual
characteristics of the user, inferred as a low-dimensional parameter vector;
second, we present a novel loss function for synchronizing inferred eye
movements with the keypresses; third, we train the model using a hybrid
approach with both human data and synthetically generated data. The approach
can be applied in interactive systems where predictive models of user behavior
are available. We report results from evaluation in the challenging case of
touchscreen typing, where the model accurately inferred real eye movements.",198,2412.15669v1,cs.HC,cs.HC,synthetic biology,2024-12-20,2024-12-23T21:07:01.216768
Synthetic Tabular Data Generation for Imbalanced Classification: The Surprising Effectiveness of an Overlap Class,"Handling imbalance in class distribution when building a classifier over
tabular data has been a problem of long-standing interest. One popular approach
is augmenting the training dataset with synthetically generated data. While
classical augmentation techniques were limited to linear interpolation of
existing minority class examples, recently higher capacity deep generative
models are providing greater promise.
  However, handling of imbalance in class distribution when building a deep
generative model is also a challenging problem, that has not been studied as
extensively as imbalanced classifier model training. We show that
state-of-the-art deep generative models yield significantly lower-quality
minority examples than majority examples. %In this paper, we start with the
observation that imbalanced data training of generative models trained
imbalanced dataset which under-represent the minority class. We propose a novel
technique of converting the binary class labels to ternary class labels by
introducing a class for the region where minority and majority distributions
overlap. We show that just this pre-processing of the training set,
significantly improves the quality of data generated spanning several
state-of-the-art diffusion and GAN-based models. While training the classifier
using synthetic data, we remove the overlap class from the training data and
justify the reasons behind the enhanced accuracy. We perform extensive
experiments on four real-life datasets, five different classifiers, and five
generative models demonstrating that our method enhances not only the
synthesizer performance of state-of-the-art models but also the classifier
performance.",315,2412.15657v1,cs.LG,cs.LG,synthetic biology,2024-12-20,2024-12-23T21:07:01.217766
Can Input Attributions Interpret the Inductive Reasoning Process Elicited in In-Context Learning?,"Elucidating the rationale behind neural models' outputs has been challenging
in the machine learning field, which is indeed applicable in this age of large
language models (LLMs) and in-context learning (ICL). When it comes to
estimating input attributions (IA), ICL poses a new issue of interpreting which
example in the prompt, consisting of a set of examples, contributed to
identifying the task/rule to be solved. To this end, in this paper, we
introduce synthetic diagnostic tasks inspired by the poverty of the stimulus
design in inductive reasoning; here, most in-context examples are ambiguous
w.r.t. their underlying rule, and one critical example disambiguates the task
demonstrated. The question is whether conventional IA methods can identify such
an example in interpreting the inductive reasoning process in ICL. Our
experiments provide several practical findings; for example, a certain simple
IA method works the best, and the larger the model, the generally harder it is
to interpret the ICL with gradient-based IA methods.",220,2412.15628v1,cs.CL,cs.CL,synthetic biology,2024-12-20,2024-12-23T21:07:01.217766
A Deep Probabilistic Framework for Continuous Time Dynamic Graph Generation,"Recent advancements in graph representation learning have shifted attention
towards dynamic graphs, which exhibit evolving topologies and features over
time. The increased use of such graphs creates a paramount need for generative
models suitable for applications such as data augmentation, obfuscation, and
anomaly detection. However, there are few generative techniques that handle
continuously changing temporal graph data; existing work largely relies on
augmenting static graphs with additional temporal information to model dynamic
interactions between nodes. In this work, we propose a fundamentally different
approach: We instead directly model interactions as a joint probability of an
edge forming between two nodes at a given time. This allows us to
autoregressively generate new synthetic dynamic graphs in a largely assumption
free, scalable, and inductive manner. We formalize this approach as DG-Gen, a
generative framework for continuous time dynamic graphs, and demonstrate its
effectiveness over five datasets. Our experiments demonstrate that DG-Gen not
only generates higher fidelity graphs compared to traditional methods but also
significantly advances link prediction tasks.",214,2412.15582v1,cs.LG,cs.LG,synthetic biology,2024-12-20,2024-12-23T21:07:01.218763
Understanding When and Why Graph Attention Mechanisms Work via Node Classification,"Despite the growing popularity of graph attention mechanisms, their
theoretical understanding remains limited. This paper aims to explore the
conditions under which these mechanisms are effective in node classification
tasks through the lens of Contextual Stochastic Block Models (CSBMs). Our
theoretical analysis reveals that incorporating graph attention mechanisms is
\emph{not universally beneficial}. Specifically, by appropriately defining
\emph{structure noise} and \emph{feature noise} in graphs, we show that graph
attention mechanisms can enhance classification performance when structure
noise exceeds feature noise. Conversely, when feature noise predominates,
simpler graph convolution operations are more effective. Furthermore, we
examine the over-smoothing phenomenon and show that, in the high
signal-to-noise ratio (SNR) regime, graph convolutional networks suffer from
over-smoothing, whereas graph attention mechanisms can effectively resolve this
issue. Building on these insights, we propose a novel multi-layer Graph
Attention Network (GAT) architecture that significantly outperforms
single-layer GATs in achieving \emph{perfect node classification} in CSBMs,
relaxing the SNR requirement from $ \omega(\sqrt{\log n}) $ to $
\omega(\sqrt{\log n} / \sqrt[3]{n}) $. To our knowledge, this is the first
study to delineate the conditions for perfect node classification using
multi-layer GATs. Our theoretical contributions are corroborated by extensive
experiments on both synthetic and real-world datasets, highlighting the
practical implications of our findings.",328,2412.15496v1,cs.LG,"cs.LG,stat.ML",synthetic biology,2024-12-20,2024-12-23T21:07:01.219760
GCA-3D: Towards Generalized and Consistent Domain Adaptation of 3D Generators,"Recently, 3D generative domain adaptation has emerged to adapt the
pre-trained generator to other domains without collecting massive datasets and
camera pose distributions. Typically, they leverage large-scale pre-trained
text-to-image diffusion models to synthesize images for the target domain and
then fine-tune the 3D model. However, they suffer from the tedious pipeline of
data generation, which inevitably introduces pose bias between the source
domain and synthetic dataset. Furthermore, they are not generalized to support
one-shot image-guided domain adaptation, which is more challenging due to the
more severe pose bias and additional identity bias introduced by the single
image reference. To address these issues, we propose GCA-3D, a generalized and
consistent 3D domain adaptation method without the intricate pipeline of data
generation. Different from previous pipeline methods, we introduce multi-modal
depth-aware score distillation sampling loss to efficiently adapt 3D generative
models in a non-adversarial manner. This multi-modal loss enables GCA-3D in
both text prompt and one-shot image prompt adaptation. Besides, it leverages
per-instance depth maps from the volume rendering module to mitigate the
overfitting problem and retain the diversity of results. To enhance the pose
and identity consistency, we further propose a hierarchical spatial consistency
loss to align the spatial structure between the generated images in the source
and target domain. Experiments demonstrate that GCA-3D outperforms previous
methods in terms of efficiency, generalization, pose accuracy, and identity
consistency.",311,2412.15491v1,cs.CV,cs.CV,synthetic biology,2024-12-20,2024-12-23T21:07:01.220757
Toward Appearance-based Autonomous Landing Site Identification for Multirotor Drones in Unstructured Environments,"A remaining challenge in multirotor drone flight is the autonomous
identification of viable landing sites in unstructured environments. One
approach to solve this problem is to create lightweight, appearance-based
terrain classifiers that can segment a drone's RGB images into safe and unsafe
regions. However, such classifiers require data sets of images and masks that
can be prohibitively expensive to create. We propose a pipeline to
automatically generate synthetic data sets to train these classifiers,
leveraging modern drones' ability to survey terrain automatically and the
ability to automatically calculate landing safety masks from terrain models
derived from such surveys. We then train a U-Net on the synthetic data set,
test it on real-world data for validation, and demonstrate it on our drone
platform in real-time.",157,2412.15486v1,cs.CV,"cs.CV,cs.LG,cs.RO",synthetic biology,2024-12-20,2024-12-23T21:07:01.220757
From your Block to our Block: How to Find Shared Structure between Stochastic Block Models over Multiple Graphs,"Stochastic Block Models (SBMs) are a popular approach to modeling single
real-world graphs. The key idea of SBMs is to partition the vertices of the
graph into blocks with similar edge densities within, as well as between
different blocks. However, what if we are given not one but multiple graphs
that are unaligned and of different sizes? How can we find out if these graphs
share blocks with similar connectivity structures? In this paper, we propose
the shared stochastic block modeling (SSBM) problem, in which we model $n$
graphs using SBMs that share parameters of $s$ blocks. We show that fitting an
SSBM is NP-hard, and consider two approaches to fit good models in practice. In
the first, we directly maximize the likelihood of the shared model using a
Markov chain Monte Carlo algorithm. In the second, we first fit an SBM for each
graph and then select which blocks to share. We propose an integer linear
program to find the optimal shared blocks and to scale to large numbers of
blocks, we propose a fast greedy algorithm. Through extensive empirical
evaluation on synthetic and real-world data, we show that our methods work well
in practice.",245,2412.15476v1,cs.SI,cs.SI,synthetic biology,2024-12-20,2024-12-23T21:07:01.221755
Computing the Non-Dominated Flexible Skyline in Vertically Distributed Datasets with No Random Access,"In today's data-driven world, algorithms operating with vertically
distributed datasets are crucial due to the increasing prevalence of
large-scale, decentralized data storage. These algorithms enhance data privacy
by processing data locally, reducing the need for data transfer and minimizing
exposure to breaches. They also improve scalability, as they can handle vast
amounts of data spread across multiple locations without requiring centralized
access. Top-k queries have been studied extensively under this lens, and are
particularly suitable in applications involving healthcare, finance, and IoT,
where data is often sensitive and distributed across various sources. Classical
top-k algorithms are based on the availability of two kinds of access to
sources: sorted access, i.e., a sequential scan in the internal sort order, one
tuple at a time, of the dataset; random access, which provides all the
information available at a data source for a tuple whose id is known. However,
in scenarios where data retrieval costs are high or data is streamed in
real-time or, simply, data are from external sources that only offer sorted
access, random access may become impractical or impossible, due to latency
issues or data access constraints. Fortunately, a long tradition of algorithms
designed for the ""no random access"" (NRA) scenario exists for classical top-k
queries. Yet, these do not cover the recent advances in ranking queries,
proposing hybridizations of top-k queries (which are preference-aware and
control the output size) and skyline queries (which are preference-agnostic and
have uncontrolled output size). The non-dominated flexible skyline (ND) is one
such proposal. We introduce an algorithm for computing ND in the NRA scenario,
prove its correctness and optimality within its class, and provide an
experimental evaluation covering a wide range of cases, with both synthetic and
real datasets.",392,2412.15468v1,cs.DB,cs.DB,synthetic biology,2024-12-20,2024-12-23T21:07:01.222752
Observational Signatures of Disk Winds in Protoplanetary Disks: Differentiating Magnetized and Photoevaporative Outflows With Fully Coupled Thermochemistry,"Magnetized winds and photoevaporative winds are critical in shaping
protoplanetary disk evolution. Using 2D axisymmetric (magneto-)hydrodynamic
simulations with Athena++ implementing fully coupled thermochemistry, we
investigate the signatures of the two winds in CO and [C~I] ALMA observations,
and examine the potential to distinguish the origins. Our simulations reveal
fundamental differences between the two winds: magnetized winds are colder and
denser, exhibiting super-Keplerian rotation with small poloidal velocities of
$\lesssim 1~{\rm km~s}^{-1}$ in the atmosphere ($z/R\gtrsim0.45$), while
photoevaporative winds are hotter and less dense, exhibiting sub-Keplerian
rotation with higher poloidal velocity of several ${\rm km~s}^{-1}$. In
addition to previously identified factors like thermal pressure gradient and
disk's self-gravity, we demonstrate that magnetic tension/pressure and
advection significantly influence rotational velocities of the gas in the wind,
which lead to emission patterns that are distinct from Keplerian rotation in
synthetic ALMA observations. Magnetized winds are visible in CO channel maps
when wind loss rates are $\gtrsim10^{-8}~M_\odot~{\rm yr}^{-1}$. When wind loss
rates are lower, magnetized winds produce subtle perturbations in channel maps,
which resemble the so-called ``velocity kinks'' produced by protoplanets. While
photoevaporative winds dissociate CO through strong XUV radiation and thus are
weaker in CO, they can create observable ring-like substructures. [C~I]
emission is optically thin and could be most effective at detecting both winds
in disks with high gas mass and/or high [C~I] abundance. Due to the spatially
extended nature of the winds, using a large beam ($\simeq0.4$"" for disks in
nearby star-forming regions) will be helpful regardless of the tracer used.",452,2412.15371v1,astro-ph.EP,astro-ph.EP,synthetic biology,2024-12-19,2024-12-23T21:07:01.223750
EnvGS: Modeling View-Dependent Appearance with Environment Gaussian,"Reconstructing complex reflections in real-world scenes from 2D images is
essential for achieving photorealistic novel view synthesis. Existing methods
that utilize environment maps to model reflections from distant lighting often
struggle with high-frequency reflection details and fail to account for
near-field reflections. In this work, we introduce EnvGS, a novel approach that
employs a set of Gaussian primitives as an explicit 3D representation for
capturing reflections of environments. These environment Gaussian primitives
are incorporated with base Gaussian primitives to model the appearance of the
whole scene. To efficiently render these environment Gaussian primitives, we
developed a ray-tracing-based renderer that leverages the GPU's RT core for
fast rendering. This allows us to jointly optimize our model for high-quality
reconstruction while maintaining real-time rendering speeds. Results from
multiple real-world and synthetic datasets demonstrate that our method produces
significantly more detailed reflections, achieving the best rendering quality
in real-time novel view synthesis.",207,2412.15215v1,cs.CV,cs.CV,synthetic biology,2024-12-19,2024-12-23T21:07:01.224747
Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation,"Reconstructing the geometry and appearance of objects from photographs taken
in different environments is difficult as the illumination and therefore the
object appearance vary across captured images. This is particularly challenging
for more specular objects whose appearance strongly depends on the viewing
direction. Some prior approaches model appearance variation across images using
a per-image embedding vector, while others use physically-based rendering to
recover the materials and per-image illumination. Such approaches fail at
faithfully recovering view-dependent appearance given significant variation in
input illumination and tend to produce mostly diffuse results. We present an
approach that reconstructs objects from images taken under different
illuminations by first relighting the images under a single reference
illumination with a multiview relighting diffusion model and then
reconstructing the object's geometry and appearance with a radiance field
architecture that is robust to the small remaining inconsistencies among the
relit images. We validate our proposed approach on both synthetic and real
datasets and demonstrate that it greatly outperforms existing techniques at
reconstructing high-fidelity appearance from images taken under extreme
illumination variation. Moreover, our approach is particularly effective at
recovering view-dependent ""shiny"" appearance which cannot be reconstructed by
prior methods.",248,2412.15211v1,cs.CV,cs.CV,synthetic biology,2024-12-19,2024-12-23T21:07:01.225747
EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues,"Automated analysis of vast Earth observation data via interactive
Vision-Language Models (VLMs) can unlock new opportunities for environmental
monitoring, disaster response, and resource management. Existing generic VLMs
do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs
remain restricted to a fixed resolution and few sensor modalities. In this
paper, we introduce EarthDial, a conversational assistant specifically designed
for Earth Observation (EO) data, transforming complex, multi-sensory Earth
observations into interactive, natural language dialogues. EarthDial supports
multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide
range of remote sensing tasks, including classification, detection, captioning,
question answering, visual reasoning, and visual grounding. To achieve this, we
introduce an extensive instruction tuning dataset comprising over 11.11M
instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and
multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,
EarthDial handles bi-temporal and multi-temporal sequence analysis for
applications like change detection. Our extensive experimental results on 37
downstream applications demonstrate that EarthDial outperforms existing generic
and domain-specific models, achieving better generalization across various EO
tasks.",260,2412.15190v1,cs.CV,cs.CV,synthetic biology,2024-12-19,2024-12-23T21:07:01.226743
OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization,"In recent years, the field of text-to-video (T2V) generation has made
significant strides. Despite this progress, there is still a gap between
theoretical advancements and practical application, amplified by issues like
degraded image quality and flickering artifacts. Recent advancements in
enhancing the video diffusion model (VDM) through feedback learning have shown
promising results. However, these methods still exhibit notable limitations,
such as misaligned feedback and inferior scalability. To tackle these issues,
we introduce OnlineVPO, a more efficient preference learning approach tailored
specifically for video diffusion models. Our method features two novel designs,
firstly, instead of directly using image-based reward feedback, we leverage the
video quality assessment (VQA) model trained on synthetic data as the reward
model to provide distribution and modality-aligned feedback on the video
diffusion model. Additionally, we introduce an online DPO algorithm to address
the off-policy optimization and scalability issue in existing video preference
learning frameworks. By employing the video reward model to offer concise video
feedback on the fly, OnlineVPO offers effective and efficient preference
guidance. Extensive experiments on the open-source video-diffusion model
demonstrate OnlineVPO as a simple yet effective and more importantly scalable
preference learning algorithm for video diffusion models, offering valuable
insights for future advancements in this domain.",269,2412.15159v1,cs.CV,cs.CV,synthetic biology,2024-12-19,2024-12-23T21:07:01.227740
A Full Transformer-based Framework for Automatic Pain Estimation using Videos,"The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.",108,2412.15095v1,cs.CV,"cs.CV,cs.AI,cs.LG",synthetic biology,2024-12-19,2024-12-23T21:07:01.227740
Coherent spin-1 dynamics encoded in the rotational states of ultracold molecules,"The rotational states of ultracold polar molecules possess long radiative
lifetimes, microwave-domain coupling, and tunable dipolar interactions.
Coherent dynamics between pairs of rotational states have been used to
demonstrate simple models of quantum magnetism and to manipulate quantum
information stored as qubits. The availability of numerous rotational states
has led to many proposals to implement more complicated models of quantum
magnetism, higher-dimensional qudits, and intricate state networks as synthetic
dimensions; however, these are yet to be experimentally realised. The primary
issue limiting their implementation is the detrimental effect of the optical
trapping environment on coherence, which is not easily mitigated for systems
beyond two levels. To address this challenge, we investigate the applicability
of magic-wavelength optical tweezer traps to facilitate multitransition
coherence between rotational states. We demonstrate simultaneous second-scale
coherence between three rotational states. Utilising this extended coherence,
we perform multiparameter estimation using a generalised Ramsey sequence and
demonstrate coherent spin-1 dynamics encoded in the rotational states. Our work
paves the way to implementing proposed quantum simulation, computation, and
metrology schemes that exploit the rich rotational structure of ultracold polar
molecules.",252,2412.15088v1,physics.atom-ph,"physics.atom-ph,cond-mat.quant-gas,quant-ph",synthetic biology,2024-12-19,2024-12-23T21:07:01.228737
Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation,"We consider the conditional generation of 3D drug-like molecules with
\textit{explicit control} over molecular properties such as drug-like
properties (e.g., Quantitative Estimate of Druglikeness or Synthetic
Accessibility score) and effectively binding to specific protein sites. To
tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and
factorize the latent space of our generative model into two disentangled
aspects: molecular properties and the remaining structural context of 3D
molecules. Our model ensures explicit control over these molecular attributes
while maintaining equivariance of coordinate representation and invariance of
data likelihood. Furthermore, we introduce a novel alignment-based coordinate
loss to adapt equivariant networks for auto-regressive de-novo 3D molecule
generation from scratch. Extensive experiments validate our model's
effectiveness on property-guided and context-guided molecule generation, both
for de-novo 3D molecule design and structure-based drug discovery against
protein targets.",208,2412.15086v1,cs.LG,"cs.LG,cs.AI",synthetic biology,2024-12-19,2024-12-23T21:07:01.229735
AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling,"In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We will release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath",299,2412.15084v1,cs.CL,"cs.CL,cs.AI,cs.LG",synthetic biology,2024-12-19,2024-12-23T21:07:01.230732
Assessing treatment effects in observational data with missing confounders: A comparative study of practical doubly-robust and traditional missing data methods,"In pharmacoepidemiology, safety and effectiveness are frequently evaluated
using readily available administrative and electronic health records data. In
these settings, detailed confounder data are often not available in all data
sources and therefore missing on a subset of individuals. Multiple imputation
(MI) and inverse-probability weighting (IPW) are go-to analytical methods to
handle missing data and are dominant in the biomedical literature.
Doubly-robust methods, which are consistent under fewer assumptions, can be
more efficient with respect to mean-squared error. We discuss two
practical-to-implement doubly-robust estimators, generalized raking and inverse
probability-weighted targeted maximum likelihood estimation (TMLE), which are
both currently under-utilized in biomedical studies. We compare their
performance to IPW and MI in a detailed numerical study for a variety of
synthetic data-generating and missingness scenarios, including scenarios with
rare outcomes and a high missingness proportion. Further, we consider plasmode
simulation studies that emulate the complex data structure of a large
electronic health records cohort in order to compare anti-depressant therapies
in a rare-outcome setting where a key confounder is prone to more than 50\%
missingness. We provide guidance on selecting a missing data analysis approach,
based on which methods excelled with respect to the bias-variance trade-off
across the different scenarios studied.",290,2412.15012v1,stat.ME,stat.ME,synthetic biology,2024-12-19,2024-12-23T21:07:01.231729
Observation of liquid-solid transition of nanoconfined water at ambient temperature,"Nanoconfined water plays an indispensable role in various phenomena in
biology, chemistry, and engineering. It exhibits many abnormal properties
compared to bulk water, especially under strong confinement. However, the
origin of those anomalies is still elusive due to the lack of structural
information on hydrogen-bonding networks. Considering the inhomogeneity of the
nanocavity and the tiny amount of water molecules, conventional optical
spectroscopies and nuclear magnetic resonance (NMR) fail to realize the
structure analysis of nanoconfined water. Here, we addressed this issue by
combining scanning probe microscopy (SPM) with advanced quantum sensing(QS)
based on an atomic-size quantum sensor like nitrogen-vacancy (NV) center in
diamond, which can apply the nanoscale-NMR for characterizing both the dynamics
and structure of confined water at ambient conditions. We built a
two-dimensional (2D) nanoconfined water system with a hexagonal-boron nitride
(hBN) flake and a hydrophilic diamond surface. By using the SPM tip to measure
the confinement size precisely, we observed a critical confinement size of ~2
nm, below which the water diffusion was significantly suppressed and the
hydrogen-bonding network of water showed an ordered structure. Meanwhile,
molecular dynamics (MD) simulation revealed a solid-like water contact layer on
the diamond surface under strong confinement, which also reproduced the
measured nanoscale-NMR spectra and confirmed the liquid-solid phase transition
observed in the experiments. Notably, with this new SPM-QS platform, our
results showed a promising way to elucidate the abnormal properties of
nanoconfined water in future applications.",350,2412.15001v1,cond-mat.mes-hall,cond-mat.mes-hall,synthetic biology,2024-12-19,2024-12-23T21:07:01.232727
MRWeb: An Exploration of Generating Multi-Page Resource-Aware Web Code from UI Designs,"Multi-page websites dominate modern web development. However, existing
design-to-code methods rely on simplified assumptions, limiting to single-page,
self-contained webpages without external resource connection. To address this
gap, we introduce the Multi-Page Resource-Aware Webpage (MRWeb) generation
task, which transforms UI designs into multi-page, functional web UIs with
internal/external navigation, image loading, and backend routing. We propose a
novel resource list data structure to track resources, links, and design
components. Our study applies existing methods to the MRWeb problem using a
newly curated dataset of 500 websites (300 synthetic, 200 real-world).
Specifically, we identify the best metric to evaluate the similarity of the web
UI, assess the impact of the resource list on MRWeb generation, analyze MLLM
limitations, and evaluate the effectiveness of the MRWeb tool in real-world
workflows. The results show that resource lists boost navigation functionality
from 0% to 66%-80% while facilitating visual similarity. Our proposed metrics
and evaluation framework provide new insights into MLLM performance on MRWeb
tasks. We release the MRWeb tool, dataset, and evaluation framework to promote
further research.",255,2412.15310v1,cs.SE,"cs.SE,cs.AI,cs.IR",synthetic biology,2024-12-19,2024-12-23T21:07:01.232727
Diffusion priors for Bayesian 3D reconstruction from incomplete measurements,"Many inverse problems are ill-posed and need to be complemented by prior
information that restricts the class of admissible models. Bayesian approaches
encode this information as prior distributions that impose generic properties
on the model such as sparsity, non-negativity or smoothness. However, in case
of complex structured models such as images, graphs or three-dimensional (3D)
objects,generic prior distributions tend to favor models that differ largely
from those observed in the real world. Here we explore the use of diffusion
models as priors that are combined with experimental data within a Bayesian
framework. We use 3D point clouds to represent 3D objects such as household
items or biomolecular complexes formed from proteins and nucleic acids. We
train diffusion models that generate coarse-grained 3D structures at a medium
resolution and integrate these with incomplete and noisy experimental data. To
demonstrate the power of our approach, we focus on the reconstruction of
biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which
is an important inverse problem in structural biology. We find that posterior
sampling with diffusion model priors allows for 3D reconstruction from very
sparse, low-resolution and partial observations.",244,2412.14897v1,cs.LG,cs.LG,synthetic biology,2024-12-19,2024-12-23T21:07:01.233724
Characterising the dynamics of unlabelled temporal networks,"Networks model the architecture backbone of complex systems. The backbone
itself can change over time leading to what is called `temporal networks'.
Interpreting temporal networks as trajectories in graph space of a latent graph
dynamics has recently enabled the extension of concepts and tools from
dynamical systems and time series to networks. Here we address temporal
networks with unlabelled nodes, a case that has received relatively little
attention so far. Situations in which node labelling cannot be tracked over
time often emerge in practice due to technical challenges, or privacy
constraints. In unlabelled temporal networks there is no one-to-one matching
between a network snapshot and its adjacency matrix. Characterizing the
dynamical properties of such unlabelled network trajectories is nontrivial. We
here exploit graph invariants to extend to the unlabelled setting
network-dynamical quantifiers of linear correlations and dynamical instability.
In particular, we focus on autocorrelation functions and the sensitive
dependence on initial conditions. We show with synthetic graph dynamics that
the measures are capable of recovering and estimating these dynamical
fingerprints even when node labels are unavailable. We also validate the
methods for some empirical temporal networks with removed node labels.",252,2412.14864v1,physics.soc-ph,"physics.soc-ph,nlin.CD,physics.data-an",synthetic biology,2024-12-19,2024-12-23T21:07:01.234721
DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis,"Recently developed large language models (LLMs) have presented promising new
avenues to address data scarcity in low-resource scenarios. In few-shot
aspect-based sentiment analysis (ABSA), previous efforts have explored data
augmentation techniques, which prompt LLMs to generate new samples by modifying
existing ones. However, these methods fail to produce adequately diverse data,
impairing their effectiveness. Besides, some studies apply in-context learning
for ABSA by using specific instructions and a few selected examples as prompts.
Though promising, LLMs often yield labels that deviate from task requirements.
To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data
synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize
data from two complementary perspectives: \textit{key-point-driven} and
\textit{instance-driven}, which effectively generate diverse and high-quality
ABSA samples in low-resource settings. Furthermore, a \textit{label refinement}
module is integrated to improve the synthetic labels. Extensive experiments
demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA
solutions and other LLM-oriented data generation methods.",258,2412.14849v1,cs.CL,cs.CL,synthetic biology,2024-12-19,2024-12-23T21:07:01.235719
Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas,"The analysis of political biases in large language models (LLMs) has
primarily examined these systems as single entities with fixed viewpoints.
While various methods exist for measuring such biases, the impact of
persona-based prompting on LLMs' political orientation remains unexplored. In
this work we leverage PersonaHub, a collection of synthetic persona
descriptions, to map the political distribution of persona-based prompted LLMs
using the Political Compass Test (PCT). We then examine whether these initial
compass distributions can be manipulated through explicit ideological prompting
towards diametrically opposed political orientations: right-authoritarian and
left-libertarian. Our experiments reveal that synthetic personas predominantly
cluster in the left-libertarian quadrant, with models demonstrating varying
degrees of responsiveness when prompted with explicit ideological descriptors.
While all models demonstrate significant shifts towards right-authoritarian
positions, they exhibit more limited shifts towards left-libertarian positions,
suggesting an asymmetric response to ideological manipulation that may reflect
inherent biases in model training.",203,2412.14843v2,cs.CL,"cs.CL,cs.AI",synthetic biology,2024-12-19,2024-12-23T21:07:01.235719
Federated Heavy Hitter Analytics with Local Differential Privacy,"Federated heavy hitter analytics enables service providers to better
understand the preferences of cross-party users by analyzing the most frequent
items. As with federated learning, it faces challenges of privacy concerns,
statistical heterogeneity, and expensive communication. Local differential
privacy (LDP), as the \textit{de facto} standard for privacy-preserving data
collection, solves the privacy challenge by letting each user perturb her data
locally and report the sanitized version. However, in federated settings,
applying LDP complicates the other two challenges, due to the deteriorated
utility by the injected LDP noise or increasing communication/computation costs
by perturbation mechanism. To tackle these problems, we propose a novel
target-aligning prefix tree mechanism satisfying $\epsilon$-LDP, for federated
heavy hitter analytics. In particular, we propose an adaptive extension
strategy to address the inconsistencies between covering necessary prefixes and
estimating heavy hitters within a party to enhance the utility. We also present
a consensus-based pruning strategy that utilizes noisy prior knowledge from
other parties to further align the inconsistency between finding heavy hitters
in each party and providing reasonable frequency information to identify the
global ones. To the best of our knowledge, our study is the first solution to
the federated heavy hitter analytics in a cross-party setting while satisfying
the stringent $\epsilon$-LDP. Comprehensive experiments on both real-world and
synthetic datasets confirm the effectiveness of our proposed mechanism.",313,2412.14832v1,cs.CR,"cs.CR,cs.DB",synthetic biology,2024-12-19,2024-12-23T21:07:01.236716
MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data,"In healthcare, the integration of multimodal data is pivotal for developing
comprehensive diagnostic and predictive models. However, managing missing data
remains a significant challenge in real-world applications. We introduce MARIA
(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based
deep learning model designed to address these challenges through an
intermediate fusion strategy. Unlike conventional approaches that depend on
imputation, MARIA utilizes a masked self-attention mechanism, which processes
only the available data without generating synthetic values. This approach
enables it to effectively handle incomplete datasets, enhancing robustness and
minimizing biases introduced by imputation methods. We evaluated MARIA against
10 state-of-the-art machine learning and deep learning models across 8
diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms
existing methods in terms of performance and resilience to varying levels of
data incompleteness, underscoring its potential for critical healthcare
applications.",193,2412.14810v1,cs.LG,"cs.LG,cs.AI",synthetic biology,2024-12-19,2024-12-23T21:07:01.237713
ResoFilter: Fine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis,"Large language models (LLMs) have shown remarkable effectiveness across
various domains, with data augmentation methods utilizing GPT for synthetic
data generation becoming prevalent. However, the quality and utility of
augmented data remain questionable, and current methods lack clear metrics for
evaluating data characteristics. To address these challenges, we propose
ResoFilter, a novel method that integrates models, data, and tasks to refine
datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter
features for data selection, offering improved interpretability by representing
data characteristics through model weights. Our experiments demonstrate that
ResoFilter achieves comparable results to full-scale fine-tuning using only
half the data in mathematical tasks and exhibits strong generalization across
different models and domains. This method provides valuable insights for
constructing synthetic datasets and evaluating high-quality data, offering a
promising solution for enhancing data augmentation techniques and improving
training dataset quality for LLMs. For reproducibility, we will release our
code and data upon acceptance.",214,2412.14809v2,cs.CL,cs.CL,synthetic biology,2024-12-19,2024-12-23T21:07:01.237713
ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine,"Large Language Models (LLMs) have demonstrated remarkable potential in
diverse domains, yet their application in the legal sector, particularly in
low-resource contexts, remains limited. This study addresses the challenges of
adapting LLMs to the Palestinian legal domain, where political instability,
fragmented legal frameworks, and limited AI resources hinder effective
machine-learning applications. We present a fine-tuned model based on a
quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set
derived from Palestinian legal texts. Using smaller-scale models and
strategically generated question-answer pairs, we achieve a cost-effective,
locally sustainable solution that provides accurate and contextually relevant
legal guidance. Our experiments demonstrate promising performance on various
query types, ranging from yes/no questions and narrative explanations to
complex legal differentiations, while highlighting areas for improvement, such
as handling calculation-based inquiries and structured list formatting. This
work provides a pathway for the deployment of AI-driven legal assistance tools
tailored to the needs of resource-constrained environments.",211,2412.14771v1,cs.CL,"cs.CL,cs.AI,cs.LG",synthetic biology,2024-12-19,2024-12-23T21:07:01.238711
Ground State Phases and Topological Excitations of Spin-1 Bose-Einstein Condensate in Twisted Optical Lattices,"Recently, the simulation of moir\'e physics using cold atom platforms has
gained significant attention. These platforms provide an opportunity to explore
novel aspects of moir\'e physics that go beyond the limits of traditional
condensed matter systems. Building on recent experimental advancements in
creating twisted bilayer spin-dependent optical lattices for pseudospin-1/2
Bose gases, we extend this concept to a trilayer optical lattice for spin-1
Bose gases. Unlike conventional moir\'e patterns, which are typically induced
by interlayer tunneling or interspin coupling, the moir\'e pattern in this
trilayer system arises from inter-species atomic interactions. We investigate
the ground state of Bose-Einstein condensates loaded in this spin-1 twisted
optical lattice under both ferromagnetic and antiferromagnetic interactions. We
find that the ground state forms a periodic pattern of distinct phases in the
homogeneous case, including ferromagnetic, antiferromagnetic, polar, and broken
axial symmetry phases. Additionally, by quenching the optical lattice potential
strength, we examine the quench dynamics of the system above the ground state
and observe the emergence of topological excitations such as vortex pairs. This
study provides a pathway for exploring the rich physics of spin-1 twisted
optical lattices and expands our understanding of moir\'e systems in synthetic
quantum platforms.",286,2412.14731v1,cond-mat.quant-gas,cond-mat.quant-gas,synthetic biology,2024-12-19,2024-12-23T21:07:01.239708
Generative AI for Banks: Benchmarks and Algorithms for Synthetic Financial Transaction Data,"The banking sector faces challenges in using deep learning due to data
sensitivity and regulatory constraints, but generative AI may offer a solution.
Thus, this study identifies effective algorithms for generating synthetic
financial transaction data and evaluates five leading models - Conditional
Tabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN),
Wasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational
AutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality,
efficiency, privacy, and graph structure. While none of the algorithms is able
to replicate the real data's graph structure, each excels in specific areas:
DGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data
replication and augmentation, and CTGAN achieves a balance across all five
criteria, making it suitable for general applications with moderate privacy
concerns. As a result, our findings offer valuable insights for choosing the
most suitable algorithm.",202,2412.14730v1,cs.LG,cs.LG,synthetic biology,2024-12-19,2024-12-23T21:07:01.239708
How to Synthesize Text Data without Model Collapse?,"Model collapse in synthetic data indicates that iterative training on
self-generated data leads to a gradual decline in performance. With the
proliferation of AI models, synthetic data will fundamentally reshape the web
data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend
of synthetic and human-produced data. In this paper, we focus on two questions:
what is the impact of synthetic data on language model training, and how to
synthesize data without model collapse? We first pre-train language models
across different proportions of synthetic data, revealing a negative
correlation between the proportion of synthetic data and model performance. We
further conduct statistical analysis on synthetic data to uncover
distributional shift phenomenon and over-concentration of n-gram features.
Inspired by the above findings, we propose token editing on human-produced data
to obtain semi-synthetic data. As a proof of concept, we theoretically
demonstrate that token-level editing can prevent model collapse, as the test
error is constrained by a finite upper bound. We conduct extensive experiments
on pre-training from scratch, continual pre-training, and supervised
fine-tuning. The results validate our theoretical proof that token-level
editing improves data quality and enhances model performance.",252,2412.14689v1,cs.CL,"cs.CL,cs.AI,cs.LG",synthetic biology,2024-12-19,2024-12-23T21:07:01.240705
Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines,"As the demand for artificial intelligence (AI) grows to address complex
real-world tasks, single models are often insufficient, requiring the
integration of multiple models into pipelines. This paper introduces Bel
Esprit, a conversational agent designed to construct AI model pipelines based
on user-defined requirements. Bel Esprit employs a multi-agent framework where
subagents collaborate to clarify requirements, build, validate, and populate
pipelines with appropriate models. We demonstrate the effectiveness of this
framework in generating pipelines from ambiguous user queries, using both
human-curated and synthetic data. A detailed error analysis highlights ongoing
challenges in pipeline construction. Bel Esprit is available for a free trial
at https://belesprit.aixplain.com.",158,2412.14684v1,cs.AI,"cs.AI,cs.HC,cs.MA",synthetic biology,2024-12-19,2024-12-23T21:07:01.241208
MUSTER: Longitudinal Deformable Registration by Composition of Consecutive Deformations,"Longitudinal imaging allows for the study of structural changes over time.
One approach to detecting such changes is by non-linear image registration.
This study introduces Multi-Session Temporal Registration (MUSTER), a novel
method that facilitates longitudinal analysis of changes in extended series of
medical images. MUSTER improves upon conventional pairwise registration by
incorporating more than two imaging sessions to recover longitudinal
deformations. Longitudinal analysis at a voxel-level is challenging due to
effects of a changing image contrast as well as instrumental and environmental
sources of bias between sessions. We show that local normalized
cross-correlation as an image similarity metric leads to biased results and
propose a robust alternative. We test the performance of MUSTER on a synthetic
multi-site, multi-session neuroimaging dataset and show that, in various
scenarios, using MUSTER significantly enhances the estimated deformations
relative to pairwise registration. Additionally, we apply MUSTER on a sample of
older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.
The results show that MUSTER can effectively identify patterns of
neuro-degeneration from T1-weighted images and that these changes correlate
with changes in cognition, matching the performance of state of the art
segmentation methods. By leveraging GPU acceleration, MUSTER efficiently
handles large datasets, making it feasible also in situations with limited
computational resources.",275,2412.14671v1,cs.CV,"cs.CV,cs.NA,math.NA",synthetic biology,2024-12-19,2024-12-23T21:07:01.242208
FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors Guided by Facial Recognizers,"Previous Deepfake detection methods perform well within their training
domains, but their effectiveness diminishes significantly with new synthesis
techniques. Recent studies have revealed that detection models often create
decision boundaries based on facial identity rather than synthetic artifacts,
resulting in poor performance on cross-domain datasets. To address this
limitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a
novel training method that mitigates facial identity influence using a face
recognizer. Specifically, we first train a face recognizer using the same
backbone as the Deepfake detector. The recognizer is then frozen and employed
during the detector's training to reduce facial identity information. This is
achieved by feeding input images into both the recognizer and the detector, and
minimizing the similarity of their feature embeddings through our Facial
Identity Attenuating loss. This process encourages the detector to generate
embeddings distinct from the recognizer, effectively reducing the impact of
facial identity. Extensive experiments demonstrate that our approach
significantly enhances detection performance on both in-domain and cross-domain
datasets.",221,2412.14623v1,cs.CV,"cs.CV,cs.CR",synthetic biology,2024-12-19,2024-12-23T21:07:01.243204
Persistent current in a non-Hermitian Hatano-Nelson ring: Disorder-induced amplification,"Non-reciprocal hopping induces a synthetic magnetic flux which leads to the
non-Hermitian Aharonov-Bohm effect. Since non-Hermitian Hamiltonians possess
both real and imaginary eigenvalues, this effect allows the observation of real
and imaginary persistent currents in a ring threaded by the synthetic
flux~\cite{nrh8}. Motivated by this, we investigate the behavior of persistent
currents in a disordered Hatano-Nelson ring with anti-Hermitian intradimer
hopping. The disorder is diagonal and we explore three distinct models, namely
the Aubry-Andr\'{e}-Harper model, the Fibonacci model, both representing
correlated disorder, and an uncorrelated (random) model. We conduct a detailed
analysis of the energy spectrum and examine the real and imaginary parts of the
persistent current under various conditions such as different ring sizes and
filling factors. Interestingly, we find that real and imaginary persistent
currents exhibit amplification in the presence of correlated disorder. This
amplification is also observed in certain individual random configurations but
vanishes after configuration averaging. Additionally, we observe both
diamagnetic and paramagnetic responses in the current behavior and investigate
aspects of persistent currents in the absence of disorder that have not been
previously explored. Interestingly, we find that the intradimer bonds host only
imaginary currents, while the interdimer bonds carry only real currents.",290,2412.14593v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.dis-nn,cond-mat.str-el,physics.comp-ph,quant-ph",synthetic biology,2024-12-19,2024-12-23T21:07:01.244200
A Large-scale Empirical Study on Large Language Models for Election Prediction,"Can Large Language Models (LLMs) accurately predict election outcomes? While
LLMs have demonstrated impressive performance in healthcare, legal analysis,
and creative applications, their capabilities in election forecasting remain
uncertain. Notably, election prediction poses unique challenges: limited
voter-level data, evolving political contexts, and the complexity of modeling
human behavior. In the first part of this paper, we explore and introduce a
multi-step reasoning framework for election prediction, which systematically
integrates demographic, ideological, and time-sensitive factors. Validated on
2016 and 2020 real-world data and extensive synthetic personas, our approach
adapts to changing political landscapes, reducing bias and significantly
improving predictive accuracy. We further apply our pipeline to the 2024 U.S.
presidential election, illustrating its ability to generalize beyond observed
historical data. Beyond enhancing accuracy, the second part of the paper
provides insights into the broader implications of LLM-based election
forecasting. We identify potential political biases embedded in pretrained
corpora, examine how demographic patterns can become exaggerated, and suggest
strategies for mitigating these issues. Together, this project, a large-scale
LLM empirical study, advances the accuracy of election predictions and
establishes directions for more balanced, transparent, and context-aware
modeling in political science research and practice.",262,2412.15291v1,cs.CL,"cs.CL,cs.SI",synthetic biology,2024-12-19,2024-12-23T21:07:01.244200
Guided Diffusion Model for Sensor Data Obfuscation,"Sensor data collected by Internet of Things (IoT) devices carries detailed
information about individuals in their vicinity. Sharing this data with a
semi-trusted service provider may compromise the individuals' privacy, as
sensitive information can be extracted by powerful machine learning models.
Data obfuscation empowered by generative models is a promising approach to
generate synthetic sensor data such that the useful information contained in
the original data is preserved and the sensitive information is obscured. This
newly generated data will then be shared with the service provider instead of
the original sensor data. In this work, we propose PrivDiffuser, a novel data
obfuscation technique based on a denoising diffusion model that attains a
superior trade-off between data utility and privacy through effective guidance
techniques. Specifically, we extract latent representations that contain
information about public and private attributes from sensor data to guide the
diffusion model, and impose mutual information-based regularization when
learning the latent representations to alleviate the entanglement of public and
private attributes, thereby increasing the effectiveness of guidance.
Evaluation on three real-world datasets containing different sensing modalities
reveals that PrivDiffuser yields a better privacy-utility trade-off than the
state-of-the-art obfuscation model, decreasing the utility loss by up to
$1.81\%$ and the privacy loss by up to $3.42\%$. Moreover, we showed that users
with diverse privacy needs can use PrivDiffuser to protect their privacy
without having to retrain the model.",310,2412.14499v1,cs.CR,"cs.CR,cs.LG",synthetic biology,2024-12-19,2024-12-23T21:07:01.245198
Treatment Effects Estimation on Networked Observational Data using Disentangled Variational Graph Autoencoder,"Estimating individual treatment effect (ITE) from observational data has
gained increasing attention across various domains, with a key challenge being
the identification of latent confounders affecting both treatment and outcome.
Networked observational data offer new opportunities to address this issue by
utilizing network information to infer latent confounders. However, most
existing approaches assume observed variables and network information serve
only as proxy variables for latent confounders, which often fails in practice,
as some variables influence treatment but not outcomes, and vice versa. Recent
advances in disentangled representation learning, which disentangle latent
factors into instrumental, confounding, and adjustment factors, have shown
promise for ITE estimation. Building on this, we propose a novel disentangled
variational graph autoencoder that learns disentangled factors for treatment
effect estimation on networked observational data. Our graph encoder further
ensures factor independence using the Hilbert-Schmidt Independence Criterion.
Extensive experiments on two semi-synthetic datasets derived from real-world
social networks and one synthetic dataset demonstrate that our method achieves
state-of-the-art performance.",237,2412.14497v1,cs.LG,"cs.LG,cs.AI,stat.ML",synthetic biology,2024-12-19,2024-12-23T21:07:01.246195
Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles,"The recent advent of large-scale 3D data, e.g. Objaverse, has led to
impressive progress in training pose-conditioned diffusion models for novel
view synthesis. However, due to the synthetic nature of such 3D data, their
performance drops significantly when applied to real-world images. This paper
consolidates a set of good practices to finetune large pretrained models for a
real-world task -- harvesting vehicle assets for autonomous driving
applications. To this end, we delve into the discrepancies between the
synthetic data and real driving data, then develop several strategies to
account for them properly. Specifically, we start with a virtual camera
rotation of real images to ensure geometric alignment with synthetic data and
consistency with the pose manifold defined by pretrained models. We also
identify important design choices in object-centric data curation to account
for varying object distances in real driving scenes -- learn across varying
object scales with fixed camera focal length. Further, we perform
occlusion-aware training in latent spaces to account for ubiquitous occlusions
in real data, and handle large viewpoint changes by leveraging a symmetric
prior. Our insights lead to effective finetuning that results in a $68.8\%$
reduction in FID for novel view synthesis over prior arts.",265,2412.14494v1,cs.CV,cs.CV,synthetic biology,2024-12-19,2024-12-23T21:07:01.247192
Graph-Structured Topic Modeling for Documents with Spatial or Covariate Dependencies,"We address the challenge of incorporating document-level metadata into topic
modeling to improve topic mixture estimation. To overcome the computational
complexity and lack of theoretical guarantees in existing Bayesian methods, we
extend probabilistic latent semantic indexing (pLSI), a frequentist framework
for topic modeling, by incorporating document-level covariates or known
similarities between documents through a graph formalism. Modeling documents as
nodes and edges denoting similarities, we propose a new estimator based on a
fast graph-regularized iterative singular value decomposition (SVD) that
encourages similar documents to share similar topic mixture proportions. We
characterize the estimation error of our proposed method by deriving
high-probability bounds and develop a specialized cross-validation method to
optimize our regularization parameters. We validate our model through
comprehensive experiments on synthetic datasets and three real-world corpora,
demonstrating improved performance and faster inference compared to existing
Bayesian methods.",194,2412.14477v1,cs.LG,"cs.LG,stat.ME",synthetic biology,2024-12-19,2024-12-23T21:07:01.248190
TraianProt: a user-friendly R shiny application for wide format proteomics data downstream analysis,"Summary: Mass spectrometry coupled to liquid chromatography (LC-MS/MS) is a
powerful technique for the charac-terisation of proteomes. However, the diverse
software platforms available for processing the raw proteomics data, each
produce their own output format, making the extraction of meaningful and
interpretable results a difficult task. We present TraianProt, a web-based,
user-friendly proteomics data analysis platform, that enables the analysis of
both label-free and labeled data from Data-Dependent or Data-Independent
Acquisition mass spectrometry mode support-ing different computational
platforms such as MaxQuant, MSFragger, DIA-NN, ProteoScape and Proteome
Discoverer output formats. TraianProt provides a dynamic framework that
includes several processing modules allowing the user to perform a complete
downstream analysis covering the stages of data pre-processing, differential
expression analy-sis, functional analysis and protein-protein interaction
analysis. Data output includes a wide range of high-quality, cus-tomisable
graphs such as heatmap, volcano plot, boxplot and barplot. This allows users to
extract biological insights from proteomic data without any programming skills.
Availability and implementation: TraianProt is implemented in R. Its code and
documentation are available on GitHub at
https://github.com/SamueldelaCamaraFuentes/TraianProt along with a step-by-step
tutorial incorporated in the repository. Contact: sdelacam@ucm.es Supplementary
information: Supplementary data are available at Bioinformatics online",341,2412.15806v1,stat.AP,"stat.AP,q-bio.QM",bioinformatics,2024-12-20,2024-12-23T21:07:02.243170
The IBEX Imaging Knowledge-Base: A Community Resource Enabling Adoption and Development of Immunofluoresence Imaging Methods,"The iterative bleaching extends multiplexity (IBEX) Knowledge-Base is a
central portal for researchers adopting IBEX and related 2D and 3D
immunofluorescence imaging methods. The design of the Knowledge-Base is modeled
after efforts in the open-source software community and includes three facets:
a development platform (GitHub), static website, and service for data
archiving. The Knowledge-Base facilitates the practice of open science
throughout the research life cycle by providing validation data for recommended
and non-recommended reagents, e.g., primary and secondary antibodies. In
addition to reporting negative data, the Knowledge-Base empowers method
adoption and evolution by providing a venue for sharing protocols, videos,
datasets, software, and publications. A dedicated discussion forum fosters a
sense of community among researchers while addressing questions not covered in
published manuscripts. Together, scientists from around the world are advancing
scientific discovery at a faster pace, reducing wasted time and effort, and
instilling greater confidence in the resulting data.",214,2412.12965v1,q-bio.TO,"q-bio.TO,eess.IV",bioinformatics,2024-12-17,2024-12-23T21:07:02.243674
Accelerating Sparse Graph Neural Networks with Tensor Core Optimization,"Graph neural networks (GNNs) have seen extensive application in domains such
as social networks, bioinformatics, and recommendation systems. However, the
irregularity and sparsity of graph data challenge traditional computing
methods, which are insufficient to meet the performance demands of GNNs. Recent
research has explored parallel acceleration using CUDA Cores and Tensor Cores,
but significant challenges persist: (1) kernel fusion leads to false high
utilization, failing to treat CUDA and Tensor Cores as independent resources,
and (2) heterogeneous cores have distinct computation preferences, causing
inefficiencies. To address these issues, this paper proposes FTC-GNN, a novel
acceleration framework that efficiently utilizes CUDA and Tensor Cores for GNN
computation. FTC-GNN introduces (1) a collaborative design that enables the
parallel utilization of CUDA and Tensor Cores and (2) a sparse-to-dense
transformation strategy that assigns dense matrix operations to Tensor Cores
while leveraging CUDA Cores for data management and sparse edge processing.
This design optimizes GPU resource utilization and improves computational
efficiency. Experimental results demonstrate the effectiveness of FTC-GNN using
GCN and AGNN models across various datasets. For GCN, FTC-GNN achieves speedups
of 4.90x, 7.10x, and 1.17x compared to DGL, PyG, and TC-GNN, respectively. For
AGNN, it achieves speedups of 5.32x, 2.92x, and 1.02x, establishing its
superiority in accelerating GNN computations.",333,2412.12218v1,cs.LG,"cs.LG,cs.AR",bioinformatics,2024-12-16,2024-12-23T21:07:02.244673
Diffusion Model with Representation Alignment for Protein Inverse Folding,"Protein inverse folding is a fundamental problem in bioinformatics, aiming to
recover the amino acid sequences from a given protein backbone structure.
Despite the success of existing methods, they struggle to fully capture the
intricate inter-residue relationships critical for accurate sequence
prediction. We propose a novel method that leverages diffusion models with
representation alignment (DMRA), which enhances diffusion-based inverse folding
by (1) proposing a shared center that aggregates contextual information from
the entire protein structure and selectively distributes it to each residue;
and (2) aligning noisy hidden representations with clean semantic
representations during the denoising process. This is achieved by predefined
semantic representations for amino acid types and a representation alignment
method that utilizes type embeddings as semantic feedback to normalize each
residue. In experiments, we conduct extensive evaluations on the CATH4.2
dataset to demonstrate that DMRA outperforms leading methods, achieving
state-of-the-art performance and exhibiting strong generalization capabilities
on the TS50 and TS500 datasets.",219,2412.09380v1,cs.LG,"cs.LG,cs.AI",bioinformatics,2024-12-12,2024-12-23T21:07:02.245670
"Analyzing the Performance Portability of SYCL across CPUs, GPUs, and Hybrid Systems with Protein Database Search","The high-performance computing (HPC) landscape is undergoing rapid
transformation, with an increasing emphasis on energy-efficient and
heterogeneous computing environments. This comprehensive study extends our
previous research on SYCL's performance portability by evaluating its
effectiveness across a broader spectrum of computing architectures, including
CPUs, GPUs, and hybrid CPU-GPU configurations from NVIDIA, Intel, and AMD. Our
analysis covers single-GPU, multi-GPU, single-CPU, and CPU-GPU hybrid setups,
using the SW\# protein database search application as a case study. The results
demonstrate SYCL's versatility across different architectures, maintaining
comparable performance to CUDA on NVIDIA GPUs while achieving similar
architectural efficiency rates on most CPU configurations. Although SYCL showed
excellent functional portability in hybrid CPU-GPU configurations, performance
varied significantly based on specific hardware combinations. Some performance
limitations were identified in multi-GPU and CPU-GPU configurations, primarily
attributed to workload distribution strategies rather than SYCL-specific
constraints. These findings position SYCL as a promising unified programming
model for heterogeneous computing environments, particularly for bioinformatic
applications.",247,2412.08308v1,cs.DC,cs.DC,bioinformatics,2024-12-11,2024-12-23T21:07:02.245670
MHSA: A Multi-scale Hypergraph Network for Mild Cognitive Impairment Detection via Synchronous and Attentive Fusion,"The precise detection of mild cognitive impairment (MCI) is of significant
importance in preventing the deterioration of patients in a timely manner.
Although hypergraphs have enhanced performance by learning and analyzing brain
networks, they often only depend on vector distances between features at a
single scale to infer interactions. In this paper, we deal with a more arduous
challenge, hypergraph modelling with synchronization between brain regions, and
design a novel framework, i.e., A Multi-scale Hypergraph Network for MCI
Detection via Synchronous and Attentive Fusion (MHSA), to tackle this
challenge. Specifically, our approach employs the Phase-Locking Value (PLV) to
calculate the phase synchronization relationship in the spectrum domain of
regions of interest (ROIs) and designs a multi-scale feature fusion mechanism
to integrate dynamic connectivity features of functional magnetic resonance
imaging (fMRI) from both the temporal and spectrum domains. To evaluate and
optimize the direct contribution of each ROI to phase synchronization in the
temporal domain, we structure the PLV coefficients dynamically adjust strategy,
and the dynamic hypergraph is modelled based on a comprehensive
temporal-spectrum fusion matrix. Experiments on the real-world dataset indicate
the effectiveness of our strategy. The code is available at
https://github.com/Jia-Weiming/MHSA.",284,2412.12149v1,cs.LG,"cs.LG,cs.AI,cs.CV",bioinformatics,2024-12-11,2024-12-23T21:07:02.246667
Systematically Examining Reproducibility: A Case Study for High Throughput Sequencing using the PRIMAD Model and BioCompute Object,"The reproducibility of computational pipelines is an expectation in
biomedical science, particularly in critical domains like human health. In this
context, reporting next generation genome sequencing methods used in precision
medicine spurred the development of the IEEE 2791-2020 standard for
Bioinformatics Analyses Generated by High Throughput Sequencing (HTS), known as
the BioCompute Object (BCO). Championed by the USA's Food and Drug
Administration, the BCO is a pragmatic framework for documenting pipelines;
however, it has not been systematically assessed for its reproducibility
claims.
  This study uses the PRIMAD model, a conceptual framework for describing
computational experiments for reproducibility purposes, to systematically
review the BCO for depth and coverage. A meticulous mapping of BCO and PRIMAD
elements onto a published BCO use case reveals potential omissions and
necessary extensions within both frameworks. This underscores the significance
of systematically validating claims of reproducibility for published digital
objects, thereby enhancing the reliability of scientific research in bioscience
and related disciplines.
  This study, along with its artifacts, is reported as a RO-Crate, providing a
structured reporting approach, which is available at
https://doi.org/10.5281/zenodo.14317922.",272,2412.07502v1,cs.CE,cs.CE,bioinformatics,2024-12-10,2024-12-23T21:07:02.247665
A Review on the Applications of Transformer-based language models for Nucleotide Sequence Analysis,"In recent times, Transformer-based language models are making quite an impact
in the field of natural language processing. As relevant parallels can be drawn
between biological sequences and natural languages, the models used in NLP can
be easily extended and adapted for various applications in bioinformatics. In
this regard, this paper introduces the major developments of Transformer-based
models in the recent past in the context of nucleotide sequences. We have
reviewed and analysed a large number of application-based papers on this
subject, giving evidence of the main characterizing features and to different
approaches that may be adopted to customize such powerful computational
machines. We have also provided a structured description of the functioning of
Transformers, that may enable even first time users to grab the essence of such
complex architectures. We believe this review will help the scientific
community in understanding the various applications of Transformer-based
language models to nucleotide sequences. This work will motivate the readers to
build on these methodologies to tackle also various other problems in the field
of bioinformatics.",217,2412.07201v1,cs.CL,"cs.CL,cs.AI",bioinformatics,2024-12-10,2024-12-23T21:07:02.247665
A4-Unet: Deformable Multi-Scale Attention Network for Brain Tumor Segmentation,"Brain tumor segmentation models have aided diagnosis in recent years.
However, they face MRI complexity and variability challenges, including
irregular shapes and unclear boundaries, leading to noise, misclassification,
and incomplete segmentation, thereby limiting accuracy. To address these
issues, we adhere to an outstanding Convolutional Neural Networks (CNNs) design
paradigm and propose a novel network named A4-Unet. In A4-Unet, Deformable
Large Kernel Attention (DLKA) is incorporated in the encoder, allowing for
improved capture of multi-scale tumors. Swin Spatial Pyramid Pooling (SSPP)
with cross-channel attention is employed in a bottleneck further to study
long-distance dependencies within images and channel relationships. To enhance
accuracy, a Combined Attention Module (CAM) with Discrete Cosine Transform
(DCT) orthogonality for channel weighting and convolutional element-wise
multiplication is introduced for spatial weighting in the decoder. Attention
gates (AG) are added in the skip connection to highlight the foreground while
suppressing irrelevant background information. The proposed network is
evaluated on three authoritative MRI brain tumor benchmarks and a proprietary
dataset, and it achieves a 94.4% Dice score on the BraTS 2020 dataset, thereby
establishing multiple new state-of-the-art benchmarks. The code is available
here: https://github.com/WendyWAAAAANG/A4-Unet.",304,2412.06088v1,cs.CV,"cs.CV,cs.AI",bioinformatics,2024-12-08,2024-12-23T21:07:02.248662
ProtBoost: protein function prediction with Py-Boost and Graph Neural Networks -- CAFA5 top2 solution,"Predicting protein properties, functions and localizations are important
tasks in bioinformatics. Recent progress in machine learning offers an
opportunities for improving existing methods. We developed a new approach
called ProtBoost, which relies on the strength of pretrained protein language
models, the new Py-Boost gradient boosting method and Graph Neural Networks
(GCN). The ProtBoost method was ranked second best model in the recent Critical
Assessment of Functional Annotation (CAFA5) international challenge with more
than 1600 participants. Py-Boost is the first gradient boosting method capable
of predicting thousands of targets simultaneously, making it an ideal fit for
tasks like the CAFA challange. Our GCN-based approach performs stacking of many
individual models and boosts the performance significantly. Notably, it can be
applied to any task where targets are arranged in a hierarchical structure,
such as Gene Ontology. Additionally, we introduced new methods for leveraging
the graph structure of targets and present an analysis of protein language
models for protein function prediction task. ProtBoost is publicly available
at: https://github.com/btbpanda/CAFA5-protein-function-prediction-2nd-place.",255,2412.04529v1,q-bio.QM,q-bio.QM,bioinformatics,2024-12-05,2024-12-23T21:07:02.249659
Does your model understand genes? A benchmark of gene properties for biological and text models,"The application of deep learning methods, particularly foundation models, in
biological research has surged in recent years. These models can be text-based
or trained on underlying biological data, especially omics data of various
types. However, comparing the performance of these models consistently has
proven to be a challenge due to differences in training data and downstream
tasks. To tackle this problem, we developed an architecture-agnostic
benchmarking approach that, instead of evaluating the models directly,
leverages entity representation vectors from each model and trains simple
predictive models for each benchmarking task. This ensures that all types of
models are evaluated using the same input and output types. Here we focus on
gene properties collected from professionally curated bioinformatics databases.
These gene properties are categorized into five major groups: genomic
properties, regulatory functions, localization, biological processes, and
protein properties. Overall, we define hundreds of tasks based on these
databases, which include binary, multi-label, and multi-class classification
tasks. We apply these benchmark tasks to evaluate expression-based models,
large language models, protein language models, DNA-based models, and
traditional baselines. Our findings suggest that text-based models and protein
language models generally outperform expression-based models in genomic
properties and regulatory functions tasks, whereas expression-based models
demonstrate superior performance in localization tasks. These results should
aid in the development of more informed artificial intelligence strategies for
biological understanding and therapeutic discovery. To ensure the
reproducibility and transparency of our findings, we have made the source code
and benchmark data publicly accessible for further investigation and expansion
at github.com/BiomedSciAI/gene-benchmark.",342,2412.04075v1,cs.AI,cs.AI,bioinformatics,2024-12-05,2024-12-23T21:07:02.250657
A Semi-Supervised Approach with Error Reflection for Echocardiography Segmentation,"Segmenting internal structure from echocardiography is essential for the
diagnosis and treatment of various heart diseases. Semi-supervised learning
shows its ability in alleviating annotations scarcity. While existing
semi-supervised methods have been successful in image segmentation across
various medical imaging modalities, few have attempted to design methods
specifically addressing the challenges posed by the poor contrast, blurred edge
details and noise of echocardiography. These characteristics pose challenges to
the generation of high-quality pseudo-labels in semi-supervised segmentation
based on Mean Teacher. Inspired by human reflection on erroneous practices, we
devise an error reflection strategy for echocardiography semi-supervised
segmentation architecture. The process triggers the model to reflect on
inaccuracies in unlabeled image segmentation, thereby enhancing the robustness
of pseudo-label generation. Specifically, the strategy is divided into two
steps. The first step is called reconstruction reflection. The network is
tasked with reconstructing authentic proxy images from the semantic masks of
unlabeled images and their auxiliary sketches, while maximizing the structural
similarity between the original inputs and the proxies. The second step is
called guidance correction. Reconstruction error maps decouple unreliable
segmentation regions. Then, reliable data that are more likely to occur near
high-density areas are leveraged to guide the optimization of unreliable data
potentially located around decision boundaries. Additionally, we introduce an
effective data augmentation strategy, termed as multi-scale mixing up strategy,
to minimize the empirical distribution gap between labeled and unlabeled images
and perceive diverse scales of cardiac anatomical structures. Extensive
experiments demonstrate the competitiveness of the proposed method.",336,2412.00715v1,eess.IV,"eess.IV,cs.CV",bioinformatics,2024-12-01,2024-12-23T21:07:02.251654
The lifex library version 2.0,"This article presents updates to lifex [Africa, SoftwareX (2022)], a C++
library for high-performance finite element simulations of multiphysics,
multiscale and multidomain problems. In this release, we introduce an
additional intergrid transfer method for non-matching multiphysics coupling on
the same domain, significantly optimize nearest-neighbor point searches and
interface coupling utilities, extend the support for 2D and mixed-dimensional
problems, and provide improved facilities for input/output and simulation
serialization and restart. These advancements also propagate to the previously
released modules of lifex specifically designed for cardiac modeling and
simulation, namely lifex-fiber [Africa et al., BMC Bioinformatics (2023)],
lifex-ep [Africa et al., BMC Bioinformatics (2023)] and lifex-cfd [Africa et
al., Computer Physics Communications (2024)]. The changes introduced in this
release aim at consolidating lifex's position as a valuable and versatile tool
for the simulation of multiphysics systems.",229,2411.19624v1,math.NA,"math.NA,cs.NA",bioinformatics,2024-11-29,2024-12-23T21:07:02.251654
Extracting Information in a Low-resource Setting: Case Study on Bioinformatics Workflows,"Bioinformatics workflows are essential for complex biological data analyses
and are often described in scientific articles with source code in public
repositories. Extracting detailed workflow information from articles can
improve accessibility and reusability but is hindered by limited annotated
corpora. To address this, we framed the problem as a low-resource extraction
task and tested four strategies: 1) creating a tailored annotated corpus, 2)
few-shot named-entity recognition (NER) with an autoregressive language model,
3) NER using masked language models with existing and new corpora, and 4)
integrating workflow knowledge into NER models. Using BioToFlow, a new corpus
of 52 articles annotated with 16 entities, a SciBERT-based NER model achieved a
70.4 F-measure, comparable to inter-annotator agreement. While knowledge
integration improved performance for specific entities, it was less effective
across the entire information schema. Our results demonstrate that
high-performance information extraction for bioinformatics workflows is
achievable.",227,2411.19295v1,cs.CL,cs.CL,bioinformatics,2024-11-28,2024-12-23T21:07:02.252651
A Data-Driven Approach to Dataflow-Aware Online Scheduling for Graph Neural Network Inference,"Graph Neural Networks (GNNs) have shown significant promise in various
domains, such as recommendation systems, bioinformatics, and network analysis.
However, the irregularity of graph data poses unique challenges for efficient
computation, leading to the development of specialized GNN accelerator
architectures that surpass traditional CPU and GPU performance. Despite this,
the structural diversity of input graphs results in varying performance across
different GNN accelerators, depending on their dataflows. This variability in
performance due to differing dataflows and graph properties remains largely
unexplored, limiting the adaptability of GNN accelerators. To address this, we
propose a data-driven framework for dataflow-aware latency prediction in GNN
inference. Our approach involves training regressors to predict the latency of
executing specific graphs on particular dataflows, using simulations on
synthetic graphs. Experimental results indicate that our regressors can predict
the optimal dataflow for a given graph with up to 91.28% accuracy and a Mean
Absolute Percentage Error (MAPE) of 3.78%. Additionally, we introduce an online
scheduling algorithm that uses these regressors to enhance scheduling
decisions. Our experiments demonstrate that this algorithm achieves up to
$3.17\times$ speedup in mean completion time and $6.26\times$ speedup in mean
execution time compared to the best feasible baseline across all datasets.",287,2411.16342v1,cs.LG,"cs.LG,cs.AR",bioinformatics,2024-11-25,2024-12-23T21:07:02.253648
From Exponential to Polynomial Complexity: Efficient Permutation Counting with Subword Constraints,"Counting distinct permutations with replacement, especially when involving
multiple subwords, is a longstanding challenge in combinatorial analysis, with
critical applications in cryptography, bioinformatics, and statistical
modeling. This paper introduces a novel framework that presents closed-form
formulas for calculating distinct permutations with replacement, fundamentally
reducing the time complexity from exponential to linear relative to the
sequence length for single-subword calculations. We then extend our
foundational formula to handle multiple subwords through the development of an
additional formula. Unlike traditional methods relying on brute-force
enumeration or recursive algorithms, our approach leverages novel combinatorial
constructs and advanced mathematical techniques to achieve unprecedented
efficiency. This comprehensive advancement in reducing computational complexity
not only simplifies permutation counting but also establishes a new benchmark
for scalability and versatility. We also demonstrate the practical utility of
our formulas through diverse applications, including the simultaneous
identification of multiple genetic motifs in DNA sequences and complex pattern
analysis in cryptographic systems, using a computer program that runs the
proposed formulae.",216,2411.16744v1,cs.CR,"cs.CR,q-bio.GN",bioinformatics,2024-11-23,2024-12-23T21:07:02.253648
Prob-cGAN: A Probabilistic Conditional Generative Adversarial Network for LSD1 Inhibitor Activity Prediction,"The inhibition of Lysine-Specific Histone Demethylase 1 (LSD1) is a promising
strategy for cancer treatment and targeting epigenetic mechanisms. This paper
introduces a Probabilistic Conditional Generative Adversarial Network
(Prob-cGAN), designed to predict the activity of LSD1 inhibitors. The Prob-cGAN
was evaluated against state-of-the-art models using the ChEMBL database,
demonstrating superior performance. Specifically, it achieved a top-1 $R^2$ of
0.739, significantly outperforming the Smiles-Transformer model at 0.591 and
the baseline cGAN at 0.488. Furthermore, it recorded a lower $RMSE$ of 0.562,
compared to 0.708 and 0.791 for the Smiles-Transformer and cGAN models
respectively. These results highlight the potential of Prob-cGAN to enhance
drug design and advance our understanding of complex biological systems through
machine learning and bioinformatics.",216,2411.15483v1,cs.CE,cs.CE,bioinformatics,2024-11-23,2024-12-23T21:07:02.254646
Multi-modal Representation Learning Enables Accurate Protein Function Prediction in Low-Data Setting,"In this study, we propose HOPER (HOlistic ProtEin Representation), a novel
multimodal learning framework designed to enhance protein function prediction
(PFP) in low-data settings. The challenge of predicting protein functions is
compounded by the limited availability of labeled data. Traditional machine
learning models already struggle in such cases, and while deep learning models
excel with abundant data, they also face difficulties when data is scarce.
HOPER addresses this issue by integrating three distinct modalities - protein
sequences, biomedical text, and protein-protein interaction (PPI) networks - to
create a comprehensive protein representation. The model utilizes autoencoders
to generate holistic embeddings, which are then employed for PFP tasks using
transfer learning. HOPER outperforms existing methods on a benchmark dataset
across all Gene Ontology categories, i.e., molecular function, biological
process, and cellular component. Additionally, we demonstrate its practical
utility by identifying new immune-escape proteins in lung adenocarcinoma,
offering insights into potential therapeutic targets. Our results highlight the
effectiveness of multimodal representation learning for overcoming data
limitations in biological research, potentially enabling more accurate and
scalable protein function prediction. HOPER source code and datasets are
available at https://github.com/kansil/HOPER",271,2412.08649v1,q-bio.BM,"q-bio.BM,cs.LG",bioinformatics,2024-11-22,2024-12-23T21:07:02.255643
OpenMS WebApps: Building User-Friendly Solutions for MS Analysis,"Liquid Chromatography Mass Spectrometry (LC-MS) is an indispensable
analytical technique in proteomics, metabolomics, and other life sciences.
While OpenMS provides advanced open-source software for MS data analysis, its
complexity can be challenging for non-experts. To address this, we have
developed OpenMS WebApps, a framework for creating user-friendly MS web
applications based on the Streamlit Python package. OpenMS WebApps simplifies
MS data analysis through an intuitive graphical user interface, interactive
result visualizations, and support for both local and online execution. Key
features include workspaces management, automatic generation of input widgets,
and parallel execution of tools resulting in highperformance and ready-to-use
solutions for online and local deployment. This framework benefits both
researchers and developers: scientists can focus on their research without the
burden of complex software setups, and developers can rapidly create and
distribute custom WebApps with novel algorithms. Several applications built on
the OpenMS WebApps template demonstrate its utility across diverse MS-related
fields, enhancing the OpenMS eco-system for developers and a wider range of
users. Furthermore, it integrates seamlessly with third-party software,
extending benefits to developers beyond the OpenMS community.",263,2411.13189v1,q-bio.BM,"q-bio.BM,cs.HC",bioinformatics,2024-11-20,2024-12-23T21:07:02.255643
Quantized symbolic time series approximation,"Time series are ubiquitous in numerous science and engineering domains, e.g.,
signal processing, bioinformatics, and astronomy. Previous work has verified
the efficacy of symbolic time series representation in a variety of engineering
applications due to its storage efficiency and numerosity reduction. The most
recent symbolic aggregate approximation technique, ABBA, has been shown to
preserve essential shape information of time series and improve downstream
applications, e.g., neural network inference regarding prediction and anomaly
detection in time series.
  Motivated by the emergence of high-performance hardware which enables
efficient computation for low bit-width representations, we present a new
quantization-based ABBA symbolic approximation technique, QABBA, which exhibits
improved storage efficiency while retaining the original speed and accuracy of
symbolic reconstruction. We prove an upper bound for the error arising from
quantization and discuss how the number of bits should be chosen to balance
this with other errors.
  An application of QABBA with large language models (LLMs) for time series
regression is also presented, and its utility is investigated. By representing
the symbolic chain of patterns on time series, QABBA not only avoids the
training of embedding from scratch, but also achieves a new state-of-the-art on
Monash regression dataset. The symbolic approximation to the time series offers
a more efficient way to fine-tune LLMs on the time series regression task which
contains various application domains. We further present a set of extensive
experiments performed across various well-established datasets to demonstrate
the advantages of the QABBA method for symbolic approximation.",321,2411.15209v1,cs.LG,"cs.LG,eess.SP,stat.ML",bioinformatics,2024-11-20,2024-12-23T21:07:02.256641
Integrating Secondary Structures Information into Triangular Spatial Relationships (TSR) for Advanced Protein Classification,"Protein structures represent the key to deciphering biological functions. The
more detailed form of similarity among these proteins is sometimes overlooked
by the conventional structural comparison methods. In contrast, further
advanced methods, such as Triangular Spatial Relationship (TSR), have been
demonstrated to make finer differentiations. Still, the classical
implementation of TSR does not provide for the integration of secondary
structure information, which is important for a more detailed understanding of
the folding pattern of a protein. To overcome these limitations, we developed
the SSE-TSR approach. The proposed method integrates secondary structure
elements (SSEs) into TSR-based protein representations. This allows an enriched
representation of protein structures by considering 18 different combinations
of helix, strand, and coil arrangements. Our results show that using SSEs
improves the accuracy and reliability of protein classification to varying
degrees. We worked with two large protein datasets of 9.2K and 7.8K samples,
respectively. We applied the SSE-TSR approach and used a neural network model
for classification. Interestingly, introducing SSEs improved performance
statistics for Dataset 1, with accuracy moving from 96.0% to 98.3%. For Dataset
2, where the performance statistics were already good, further small
improvements were found with the introduction of SSE, giving an accuracy of
99.5% compared to 99.4%. These results show that SSE integration can
dramatically improve TSR key discrimination, with significant benefits in
datasets with low initial accuracies and only incremental gains in those with
high baseline performance. Thus, SSE-TSR is a powerful bioinformatics tool that
improves protein classification and understanding of protein function and
interaction.",347,2411.12853v1,cs.LG,"cs.LG,q-bio.BM",bioinformatics,2024-11-19,2024-12-23T21:07:02.257638
Sorted Consecutive Occurrence Queries in Substrings,"The string indexing problem is a fundamental computational problem with
numerous applications, including information retrieval and bioinformatics. It
aims to efficiently solve the pattern matching problem: given a text $T$ of
length $n$ for preprocessing and a pattern $P$ of length $m$ as a query, the
goal is to report all occurrences of $P$ as substrings of $T$. Navarro and
Thankachan [CPM 2015, Theor. Comput. Sci. 2016] introduced a variant of this
problem called the gap-bounded consecutive occurrence query, which reports
pairs of consecutive occurrences of $P$ in $T$ such that their gaps (i.e., the
distances between them) lie within a query-specified range $[g_1, g_2]$.
Recently, Bille et al. [FSTTCS 2020, Theor. Comput. Sci. 2022] proposed the
top-$k$ close consecutive occurrence query, which reports the $k$ closest
consecutive occurrences of $P$ in $T$, sorted in non-descending order of
distance. Both problems are optimally solved in query time with $O(n \log
n)$-space data structures.
  In this paper, we generalize these problems to the range query model, which
focuses only on occurrences of $P$ in a specified substring $T[a.. b]$ of $T$.
Our contributions are as follows: (1) We propose an $O(n \log^2 n)$-space data
structure that answers the range top-$k$ consecutive occurrence query in $O(|P|
+ \log\log n + k)$ time. (2) We propose an $O(n \log^{2+\epsilon} n)$-space
data structure that answers the range gap-bounded consecutive occurrence query
in $O(|P| + \log\log n + \mathit{output})$ time, where $\epsilon$ is a positive
constant and $\mathit{output}$ denotes the number of outputs. Additionally, as
by-products, we present algorithms for geometric problems involving weighted
horizontal segments in a 2D plane, which are of independent interest.",472,2411.12099v2,cs.DS,"cs.DS,cs.CG",bioinformatics,2024-11-18,2024-12-23T21:07:02.258636
A Modular Open Source Framework for Genomic Variant Calling,"Variant calling is a fundamental task in genomic research, essential for
detecting genetic variations such as single nucleotide polymorphisms (SNPs) and
insertions or deletions (indels). This paper presents an enhancement to
DeepChem, a widely used open-source drug discovery framework, through the
integration of DeepVariant. In particular, we introduce a variant calling
pipeline that leverages DeepVariant's convolutional neural network (CNN)
architecture to improve the accuracy and reliability of variant detection. The
implemented pipeline includes stages for realignment of sequencing reads,
candidate variant detection, and pileup image generation, followed by variant
classification using a modified Inception v3 model. Our work adds a modular and
extensible variant calling framework to the DeepChem framework and enables
future work integrating DeepChem's drug discovery infrastructure more tightly
with bioinformatics pipelines.",187,2411.11513v1,q-bio.QM,"q-bio.QM,cs.LG",bioinformatics,2024-11-18,2024-12-23T21:07:02.259634
Stealing Training Graphs from Graph Neural Networks,"Graph Neural Networks (GNNs) have shown promising results in modeling graphs
in various tasks. The training of GNNs, especially on specialized tasks such as
bioinformatics, demands extensive expert annotations, which are expensive and
usually contain sensitive information of data providers. The trained GNN models
are often shared for deployment in the real world. As neural networks can
memorize the training samples, the model parameters of GNNs have a high risk of
leaking private training data. Our theoretical analysis shows the strong
connections between trained GNN parameters and the training graphs used,
confirming the training graph leakage issue. However, explorations into
training data leakage from trained GNNs are rather limited. Therefore, we
investigate a novel problem of stealing graphs from trained GNNs. To obtain
high-quality graphs that resemble the target training set, a graph diffusion
model with diffusion noise optimization is deployed as a graph generator.
Furthermore, we propose a selection method that effectively leverages GNN model
parameters to identify training graphs from samples generated by the graph
diffusion model. Extensive experiments on real-world datasets demonstrate the
effectiveness of the proposed framework in stealing training graphs from the
trained GNN.",242,2411.11197v1,cs.LG,"cs.LG,cs.CR",bioinformatics,2024-11-17,2024-12-23T21:07:02.260630
Hardness Results on Characteristics for Elastic-Degenerated Strings,"Generalizations of plain strings have been proposed as a compact way to
represent a collection of nearly identical sequences or to express uncertainty
at specific text positions by enumerating all possibilities. While a plain
string stores a character at each of its positions, generalizations consider a
set of characters (indeterminate strings), a set of strings of equal length
(generalized degenerate strings, or shortly GD strings), or a set of strings of
arbitrary lengths (elastic-degenerate strings, or shortly ED strings). These
generalizations are of importance to compactly represent such type of data, and
find applications in bioinformatics for representing and maintaining a set of
genetic sequences of the same taxonomy or a multiple sequence alignment. To be
of use, attention has been drawn to answering various query types such as
pattern matching or measuring similarity of ED strings by generalizing
techniques known to plain strings. However, for some types of queries, it has
been shown that a generalization of a polynomial-time solvable query on classic
strings becomes NP-hard on ED strings, e.g. [Russo et al.,2022]. In that light,
we wonder about other types of queries, which are of particular interest to
bioinformatics: the search for the longest repeating factor, unique substrings,
absent words, anti-powers, and longest previous factors. While we obtain a
polynomial time algorithm for the first problem on ED strings, we show that all
others are NP-hard to compute, some of them even under the restriction that the
input can be modelled as an indeterminate or GD string.",332,2411.10653v1,cs.DS,"cs.DS,cs.CC",bioinformatics,2024-11-16,2024-12-23T21:07:02.261630
"Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks","Graph Neural Networks (GNNs) have recently gained traction in transportation,
bioinformatics, language and image processing, but research on their
application to supply chain management remains limited. Supply chains are
inherently graph-like, making them ideal for GNN methodologies, which can
optimize and solve complex problems. The barriers include a lack of proper
conceptual foundations, familiarity with graph applications in SCM, and
real-world benchmark datasets for GNN-based supply chain research. To address
this, we discuss and connect supply chains with graph structures for effective
GNN application, providing detailed formulations, examples, mathematical
definitions, and task guidelines. Additionally, we present a multi-perspective
real-world benchmark dataset from a leading FMCG company in Bangladesh,
focusing on supply chain planning. We discuss various supply chain tasks using
GNNs and benchmark several state-of-the-art models on homogeneous and
heterogeneous graphs across six supply chain analytics tasks. Our analysis
shows that GNN-based models consistently outperform statistical Machine
Learning and other Deep Learning models by around 10-30% in regression, 10-30%
in classification and detection tasks, and 15-40% in anomaly detection tasks on
designated metrics. With this work, we lay the groundwork for solving supply
chain problems using GNNs, supported by conceptual discussions, methodological
insights, and a comprehensive dataset.",291,2411.08550v1,cs.LG,"cs.LG,cs.CE,stat.ML",bioinformatics,2024-11-13,2024-12-23T21:07:02.262626
Classification of Keratitis from Eye Corneal Photographs using Deep Learning,"Keratitis is an inflammatory corneal condition responsible for 10% of visual
impairment in low- and middle-income countries (LMICs), with bacteria, fungi,
or amoeba as the most common infection etiologies. While an accurate and timely
diagnosis is crucial for the selected treatment and the patients' sight
outcomes, due to the high cost and limited availability of laboratory
diagnostics in LMICs, diagnosis is often made by clinical observation alone,
despite its lower accuracy. In this study, we investigate and compare different
deep learning approaches to diagnose the source of infection: 1) three separate
binary models for infection type predictions; 2) a multitask model with a
shared backbone and three parallel classification layers (Multitask V1); and,
3) a multitask model with a shared backbone and a multi-head classification
layer (Multitask V2). We used a private Brazilian cornea dataset to conduct the
empirical evaluation. We achieved the best results with Multitask V2, with an
area under the receiver operating characteristic curve (AUROC) confidence
intervals of 0.7413-0.7740 (bacteria), 0.8395-0.8725 (fungi), and 0.9448-0.9616
(amoeba). A statistical analysis of the impact of patient features on models'
performance revealed that sex significantly affects amoeba infection
prediction, and age seems to affect fungi and bacteria predictions.",304,2411.08935v1,cs.CV,"cs.CV,cs.LG",bioinformatics,2024-11-13,2024-12-23T21:07:02.262626
Large Language Model in Medical Informatics: Direct Classification and Enhanced Text Representations for Automatic ICD Coding,"Addressing the complexity of accurately classifying International
Classification of Diseases (ICD) codes from medical discharge summaries is
challenging due to the intricate nature of medical documentation. This paper
explores the use of Large Language Models (LLM), specifically the LLAMA
architecture, to enhance ICD code classification through two methodologies:
direct application as a classifier and as a generator of enriched text
representations within a Multi-Filter Residual Convolutional Neural Network
(MultiResCNN) framework. We evaluate these methods by comparing them against
state-of-the-art approaches, revealing LLAMA's potential to significantly
improve classification outcomes by providing deep contextual insights into
medical texts.",139,2411.06823v1,cs.LG,"cs.LG,cs.IR",bioinformatics,2024-11-11,2024-12-23T21:07:02.263622
TourSynbio-Search: A Large Language Model Driven Agent Framework for Unified Search Method for Protein Engineering,"The exponential growth in protein-related databases and scientific
literature, combined with increasing demands for efficient biological
information retrieval, has created an urgent need for unified and accessible
search methods in protein engineering research. We present TourSynbio-Search, a
novel bioinformatics search agent framework powered by the TourSynbio-7B
protein multimodal large language model (LLM), designed to address the growing
challenges of information retrieval across rapidly expanding protein databases
and corresponding online research literature. The agent's dual-module
architecture consists of PaperSearch and ProteinSearch components, enabling
comprehensive exploration of both scientific literature and protein data across
multiple biological databases. At its core, TourSynbio-Search employs an
intelligent agent system that interprets natural language queries, optimizes
search parameters, and executes search operations across major platforms
including UniProt, PDB, ArXiv, and BioRxiv. The agent's ability to process
intuitive natural language queries reduces technical barriers, allowing
researchers to efficiently access and analyze complex biological data without
requiring extensive bioinformatics expertise. Through detailed case studies in
literature retrieval and protein structure visualization, we demonstrate
TourSynbio-Search's effectiveness in streamlining biological information
retrieval and enhancing research productivity. This framework represents an
advancement in bridging the accessibility gap between complex biological
databases and researchers, potentially accelerating progress in protein
engineering applications. Our codes are available at:
https://github.com/tsynbio/Toursynbio-Search",307,2411.06024v1,q-bio.QM,q-bio.QM,bioinformatics,2024-11-09,2024-12-23T21:07:02.265617
DWFL: Enhancing Federated Learning through Dynamic Weighted Averaging,"Federated Learning (FL) is a distributed learning technique that maintains
data privacy by providing a decentralized training method for machine learning
models using distributed big data. This promising Federated Learning approach
has also gained popularity in bioinformatics, where the privacy of biomedical
data holds immense importance, especially when patient data is involved.
Despite the successful implementation of Federated learning in biological
sequence analysis, rigorous consideration is still required to improve accuracy
in a way that data privacy should not be compromised. Additionally, the optimal
integration of federated learning, especially in protein sequence analysis, has
not been fully explored. We propose a deep feed-forward neural network-based
enhanced federated learning method for protein sequence classification to
overcome these challenges. Our method introduces novel enhancements to improve
classification accuracy. We introduce dynamic weighted federated learning
(DWFL) which is a federated learning-based approach, where local model weights
are adjusted using weighted averaging based on their performance metrics. By
assigning higher weights to well-performing models, we aim to create a more
potent initial global model for the federated learning process, leading to
improved accuracy. We conduct experiments using real-world protein sequence
datasets to assess the effectiveness of DWFL. The results obtained using our
proposed approach demonstrate significant improvements in model accuracy,
making federated learning a preferred, more robust, and privacy-preserving
approach for collaborative machine-learning tasks.",285,2411.05173v1,cs.LG,cs.LG,bioinformatics,2024-11-07,2024-12-23T21:07:02.266614
EPIC: Enhancing Privacy through Iterative Collaboration,"Advancements in genomics technology lead to a rising volume of viral (e.g.,
SARS-CoV-2) sequence data, resulting in increased usage of machine learning
(ML) in bioinformatics. Traditional ML techniques require centralized data
collection and processing, posing challenges in realistic healthcare scenarios.
Additionally, privacy, ownership, and stringent regulation issues exist when
pooling medical data into centralized storage to train a powerful deep learning
(DL) model. The Federated learning (FL) approach overcomes such issues by
setting up a central aggregator server and a shared global model. It also
facilitates data privacy by extracting knowledge while keeping the actual data
private. This work proposes a cutting-edge Privacy enhancement through
Iterative Collaboration (EPIC) architecture. The network is divided and
distributed between local and centralized servers. We demonstrate the EPIC
approach to resolve a supervised classification problem to estimate SARS-CoV-2
genomic sequence data lineage without explicitly transferring raw sequence
data. We aim to create a universal decentralized optimization framework that
allows various data holders to work together and converge to a single
predictive model. The findings demonstrate that privacy-preserving strategies
can be successfully used with aggregation approaches without materially
altering the degree of learning convergence. Finally, we highlight a few
potential issues and prospects for study in FL-based approaches to healthcare
applications.",273,2411.05167v1,cs.LG,"cs.LG,cs.CR",bioinformatics,2024-11-07,2024-12-23T21:07:02.267612
Localized KBO with genetic dynamics for multi-modal optimization,"In this paper, we introduce a novel approach to multi-modal optimization by
enhancing the recently developed kinetic-based optimization (KBO) method with
genetic dynamics (GKBO). The proposed method targets objective functions with
multiple global minima, addressing a critical need in fields like engineering
design, machine learning, and bioinformatics. By incorpo rating leader-follower
dynamics and localized interactions, the algorithm efficiently navigates
high-dimensional search spaces to detect multiple optimal solutions. After
providing a binary description, a mean-field approximation is derived, and
different numerical experiments are conducted to validate the results.",126,2411.04840v2,math.NA,"math.NA,cs.NA",bioinformatics,2024-11-07,2024-12-23T21:07:02.267612
Automating Exploratory Proteomics Research via Language Models,"With the development of artificial intelligence, its contribution to science
is evolving from simulating a complex problem to automating entire research
processes and producing novel discoveries. Achieving this advancement requires
both specialized general models grounded in real-world scientific data and
iterative, exploratory frameworks that mirror human scientific methodologies.
In this paper, we present PROTEUS, a fully automated system for scientific
discovery from raw proteomics data. PROTEUS uses large language models (LLMs)
to perform hierarchical planning, execute specialized bioinformatics tools, and
iteratively refine analysis workflows to generate high-quality scientific
hypotheses. The system takes proteomics datasets as input and produces a
comprehensive set of research objectives, analysis results, and novel
biological hypotheses without human intervention. We evaluated PROTEUS on 12
proteomics datasets collected from various biological samples (e.g. immune
cells, tumors) and different sample types (single-cell and bulk), generating
191 scientific hypotheses. These were assessed using both automatic LLM-based
scoring on 5 metrics and detailed reviews from human experts. Results
demonstrate that PROTEUS consistently produces reliable, logically coherent
results that align well with existing literature while also proposing novel,
evaluable hypotheses. The system's flexible architecture facilitates seamless
integration of diverse analysis tools and adaptation to different proteomics
data types. By automating complex proteomics analysis workflows and hypothesis
generation, PROTEUS has the potential to considerably accelerate the pace of
scientific discovery in proteomics research, enabling researchers to
efficiently explore large-scale datasets and uncover biological insights.",347,2411.03743v1,cs.AI,"cs.AI,q-bio.QM",bioinformatics,2024-11-06,2024-12-23T21:07:02.268609
DP-HLS: A High-Level Synthesis Framework for Accelerating Dynamic Programming Algorithms in Bioinformatics,"Dynamic programming (DP) based algorithms are essential yet compute-intensive
parts of numerous bioinformatics pipelines, which typically involve populating
a 2-D scoring matrix based on a recursive formula, optionally followed by a
traceback step to get the optimal alignment path. DP algorithms are used in a
wide spectrum of bioinformatics tasks, including read assembly, homology
search, gene annotation, basecalling, and phylogenetic inference. So far,
specialized hardware like ASICs and FPGAs have provided massive speedup for
these algorithms. However, these solutions usually represent a single design
point in the DP algorithmic space and typically require months of manual effort
to implement using low-level hardware description languages (HDLs). This paper
introduces DP-HLS, a novel framework based on High-Level Synthesis (HLS) that
simplifies and accelerates the development of a broad set of bioinformatically
relevant DP algorithms in hardware. DP-HLS features an easy-to-use template
with integrated HLS directives, enabling efficient hardware solutions without
requiring hardware design knowledge. In our experience, DP-HLS significantly
reduced the development time of new kernels (months to days) and produced
designs with comparable resource utilization to open-source hand-coded
HDL-based implementations and performance within 7.7-16.8% margin. DP-HLS is
compatible with AWS EC2 F1 FPGA instances. To demonstrate the versatility of
the DP-HLS framework, we implemented 15 diverse DP kernels, achieving 1.3-32x
improved throughput over state-of-the-art GPU and CPU baselines and providing
the first open-source FPGA implementation for several of them. The DP-HLS
codebase is available freely under the MIT license and its detailed wiki is
available to assist new users.",397,2411.03398v1,cs.AR,"cs.AR,cs.DC",bioinformatics,2024-11-05,2024-12-23T21:07:02.269606
gggenomes: effective and versatile visualizations for comparative genomics,"The effective visualization of genomic data is crucial for exploring and
interpreting complex relationships within and across genes and genomes. Despite
advances in developing dedicated bioinformatics software, common visualization
tools often fail to efficiently integrate the diverse datasets produced in
comparative genomics, lack intuitive interfaces to construct complex plots and
are missing functionalities to inspect the underlying data iteratively and at
scale. Here, we introduce gggenomes, a versatile R package designed to overcome
these challenges by extending the widely used ggplot2 framework for comparative
genomics. gggenomes is available from CRAN and GitHub, accompanied by detailed
and user-friendly documentation (https://thackl.github.io/gggenomes).",157,2411.13556v1,q-bio.GN,q-bio.GN,bioinformatics,2024-11-05,2024-12-23T21:07:02.270603
Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval,"The field of bioinformatics has seen significant progress, making the
cross-modal text-molecule retrieval task increasingly vital. This task focuses
on accurately retrieving molecule structures based on textual descriptions, by
effectively aligning textual descriptions and molecules to assist researchers
in identifying suitable molecular candidates. However, many existing approaches
overlook the details inherent in molecule sub-structures. In this work, we
introduce the Optimal TRansport-based Multi-grained Alignments model (ORMA), a
novel approach that facilitates multi-grained alignments between textual
descriptions and molecules. Our model features a text encoder and a molecule
encoder. The text encoder processes textual descriptions to generate both
token-level and sentence-level representations, while molecules are modeled as
hierarchical heterogeneous graphs, encompassing atom, motif, and molecule nodes
to extract representations at these three levels. A key innovation in ORMA is
the application of Optimal Transport (OT) to align tokens with motifs, creating
multi-token representations that integrate multiple token alignments with their
corresponding motifs. Additionally, we employ contrastive learning to refine
cross-modal alignments at three distinct scales: token-atom, multitoken-motif,
and sentence-molecule, ensuring that the similarities between correctly matched
text-molecule pairs are maximized while those of unmatched pairs are minimized.
To our knowledge, this is the first attempt to explore alignments at both the
motif and multi-token levels. Experimental results on the ChEBI-20 and PCdes
datasets demonstrate that ORMA significantly outperforms existing
state-of-the-art (SOTA) models.",336,2411.11875v1,cs.IR,"cs.IR,cs.AI,cs.CL,q-bio.BM",bioinformatics,2024-11-04,2024-12-23T21:07:02.271601
A General Recipe for Contractive Graph Neural Networks -- Technical Report,"Graph Neural Networks (GNNs) have gained significant popularity for learning
representations of graph-structured data due to their expressive power and
scalability. However, despite their success in domains such as social network
analysis, recommendation systems, and bioinformatics, GNNs often face
challenges related to stability, generalization, and robustness to noise and
adversarial attacks. Regularization techniques have shown promise in addressing
these challenges by controlling model complexity and improving robustness.
Building on recent advancements in contractive GNN architectures, this paper
presents a novel method for inducing contractive behavior in any GNN through
SVD regularization. By deriving a sufficient condition for contractiveness in
the update step and applying constraints on network parameters, we demonstrate
the impact of SVD regularization on the Lipschitz constant of GNNs. Our
findings highlight the role of SVD regularization in enhancing the stability
and generalization of GNNs, contributing to the development of more robust
graph-based learning algorithms dynamics.",208,2411.01717v1,cs.LG,"cs.LG,stat.ML",bioinformatics,2024-11-04,2024-12-23T21:07:02.271601
Diceplot: A package for high dimensional categorical data visualization,"Visualization of multidimensional, categorical data is a common challenge
across scientific areas and, in particular, the life sciences. The goal is to
create a comprehensive overview of the underlying data which allows to assess
multiple variables intuitively. One application where such visualizations are
particularly useful is pathway analysis, where we check for dysregulation in
known biological regulatory mechanisms and functions across multiple
conditions. Here, we propose a new visualization approach that codes such data
in a comprehensive and intuitive representation: Dice plots visualize up to
four distinct categorical classes in a single view that consist of multiple
elements resembling the faces of dice, whereas domino plots add an additional
layer of information for binary comparison. The code is available as the
diceplot R package, as pydiceplot on pip and at https://github.com/maflot.",178,2410.23897v1,q-bio.QM,q-bio.QM,bioinformatics,2024-10-30,2024-12-23T21:07:02.272598
A robust optimization approach to flow decomposition,"In this paper, we consider a variant of the so-called minimum flow
decomposition (MFD) problem in which uncertainty regarding edge capacities is
taken into account from a robustness perspective. In the classical flow
decomposition problem, a network flow is decomposed into a set of weighted
paths from a fixed source node to a fixed sink node that precisely represents
the flow distribution across all edges. While MFDs are often used in
bioinformatics applications, they are also applicable in other fields, such as
flows of goods or passengers in distribution networks, where the decomposition
represents the vehicles and corresponding capacities needed to cover these
flows. We generalize this problem to the weighted inexact case with lower and
upper bounds on the flow values, provide a detailed analysis, and explore
different variants that are solvable in polynomial time. Moreover, we introduce
the concept of robust flow decomposition by incorporating uncertain flows and
applying different robustness concepts to handle the uncertainty. Finally, we
present two different adjustable problem formulations and perform computational
experiments illustrating the benefit of adjustability in the uncertain case in
a subsequent computational study.",222,2410.21140v1,math.OC,math.OC,bioinformatics,2024-10-28,2024-12-23T21:07:02.273596
Peptide-GPT: Generative Design of Peptides using Generative Pre-trained Transformers and Bio-informatic Supervision,"In recent years, natural language processing (NLP) models have demonstrated
remarkable capabilities in various domains beyond traditional text generation.
In this work, we introduce PeptideGPT, a protein language model tailored to
generate protein sequences with distinct properties: hemolytic activity,
solubility, and non-fouling characteristics. To facilitate a rigorous
evaluation of these generated sequences, we established a comprehensive
evaluation pipeline consisting of ideas from bioinformatics to retain valid
proteins with ordered structures. First, we rank the generated sequences based
on their perplexity scores, then we filter out those lying outside the
permissible convex hull of proteins. Finally, we predict the structure using
ESMFold and select the proteins with pLDDT values greater than 70 to ensure
ordered structure. The properties of generated sequences are evaluated using
task-specific classifiers - PeptideBERT and HAPPENN. We achieved an accuracy of
76.26% in hemolytic, 72.46% in non-hemolytic, 78.84% in non-fouling, and 68.06%
in solubility protein generation. Our experimental results demonstrate the
effectiveness of PeptideGPT in de novo protein design and underscore the
potential of leveraging NLP-based approaches for paving the way for future
innovations and breakthroughs in synthetic biology and bioinformatics. Codes,
models, and data used in this study are freely available at:
https://github.com/aayush-shah14/PeptideGPT.",305,2410.19222v1,cs.LG,"cs.LG,q-bio.QM",bioinformatics,2024-10-25,2024-12-23T21:07:02.273596
Leveraging CORAL-Correlation Consistency Network for Semi-Supervised Left Atrium MRI Segmentation,"Semi-supervised learning (SSL) has been widely used to learn from both a few
labeled images and many unlabeled images to overcome the scarcity of labeled
samples in medical image segmentation. Most current SSL-based segmentation
methods use pixel values directly to identify similar features in labeled and
unlabeled data. They usually fail to accurately capture the intricate
attachment structures in the left atrium, such as the areas of inconsistent
density or exhibit outward curvatures, adding to the complexity of the task. In
this paper, we delve into this issue and introduce an effective solution,
CORAL(Correlation-Aligned)-Correlation Consistency Network (CORN), to capture
the global structure shape and local details of Left Atrium. Diverging from
previous methods focused on each local pixel value, the CORAL-Correlation
Consistency Module (CCM) in the CORN leverages second-order statistical
information to capture global structural features by minimizing the
distribution discrepancy between labeled and unlabeled samples in feature
space. Yet, direct construction of features from unlabeled data frequently
results in ``Sample Selection Bias'', leading to flawed supervision. We thus
further propose the Dynamic Feature Pool (DFP) for the CCM, which utilizes a
confidence-based filtering strategy to remove incorrectly selected features and
regularize both teacher and student models by constraining the similarity
matrix to be consistent. Extensive experiments on the Left Atrium dataset have
shown that the proposed CORN outperforms previous state-of-the-art
semi-supervised learning methods.",315,2410.15916v1,cs.CV,"cs.CV,I.4.6",bioinformatics,2024-10-21,2024-12-23T21:07:02.274593
Tensor-Fused Multi-View Graph Contrastive Learning,"Graph contrastive learning (GCL) has emerged as a promising approach to
enhance graph neural networks' (GNNs) ability to learn rich representations
from unlabeled graph-structured data. However, current GCL models face
challenges with computational demands and limited feature utilization, often
relying only on basic graph properties like node degrees and edge attributes.
This constrains their capacity to fully capture the complex topological
characteristics of real-world phenomena represented by graphs. To address these
limitations, we propose Tensor-Fused Multi-View Graph Contrastive Learning
(TensorMV-GCL), a novel framework that integrates extended persistent homology
(EPH) with GCL representations and facilitates multi-scale feature extraction.
Our approach uniquely employs tensor aggregation and compression to fuse
information from graph and topological features obtained from multiple
augmented views of the same graph. By incorporating tensor concatenation and
contraction modules, we reduce computational overhead by separating feature
tensor aggregation and transformation. Furthermore, we enhance the quality of
learned topological features and model robustness through noise-injected EPH.
Experiments on molecular, bioinformatic, and social network datasets
demonstrate TensorMV-GCL's superiority, outperforming 15 state-of-the-art
methods in graph classification tasks across 9 out of 11 benchmarks while
achieving comparable results on the remaining two. The code for this paper is
publicly available at https://github.com/CS-SAIL/Tensor-MV-GCL.git.",307,2410.15247v1,cs.LG,"cs.LG,cs.AI",bioinformatics,2024-10-20,2024-12-23T21:07:02.275591
D-SarcNet: A Dual-stream Deep Learning Framework for Automatic Analysis of Sarcomere Structures in Fluorescently Labeled hiPSC-CMs,"Human-induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) are a
powerful tool in advancing cardiovascular research and clinical applications.
The maturation of sarcomere organization in hiPSC-CMs is crucial, as it
supports the contractile function and structural integrity of these cells.
Traditional methods for assessing this maturation like manual annotation and
feature extraction are labor-intensive, time-consuming, and unsuitable for
high-throughput analysis. To address this, we propose D-SarcNet, a dual-stream
deep learning framework that takes fluorescent hiPSC-CM single-cell images as
input and outputs the stage of the sarcomere structural organization on a scale
from 1.0 to 5.0. The framework also integrates Fast Fourier Transform (FFT),
deep learning-generated local patterns, and gradient magnitude to capture
detailed structural information at both global and local levels. Experiments on
a publicly available dataset from the Allen Institute for Cell Science show
that the proposed approach not only achieves a Spearman correlation of 0.868
marking a 3.7% improvement over the previous state-of-the-art but also
significantly enhances other key performance metrics, including MSE, MAE, and
R2 score. Beyond establishing a new state-of-the-art in sarcomere structure
assessment from hiPSC-CM images, our ablation studies highlight the
significance of integrating global and local information to enhance deep
learning networks ability to discern and learn vital visual features of
sarcomere structure.",318,2410.14983v1,cs.CV,cs.CV,bioinformatics,2024-10-19,2024-12-23T21:07:02.276589
TransBox: EL++-closed Ontology Embedding,"OWL (Web Ontology Language) ontologies, which are able to represent both
relational and type facts as standard knowledge graphs and complex domain
knowledge in Description Logic (DL) axioms, are widely adopted in domains such
as healthcare and bioinformatics. Inspired by the success of knowledge graph
embeddings, embedding OWL ontologies has gained significant attention in recent
years. Current methods primarily focus on learning embeddings for atomic
concepts and roles, enabling the evaluation based on normalized axioms through
specially designed score functions. However, they often neglect the embedding
of complex concepts, making it difficult to infer with more intricate axioms.
This limitation reduces their effectiveness in advanced reasoning tasks, such
as Ontology Learning and ontology-mediated Query Answering. In this paper, we
propose EL++-closed ontology embeddings which are able to represent any logical
expressions in DL via composition. Furthermore, we develop TransBox, an
effective EL++-closed ontology embedding method that can handle many-to-one,
one-to-many and many-to-many relations. Our extensive experiments demonstrate
that TransBox often achieves state-of-the-art performance across various
real-world datasets for predicting complex axioms.",262,2410.14571v1,cs.AI,cs.AI,bioinformatics,2024-10-18,2024-12-23T21:07:02.277585
A Bioinformatic Approach Validated Utilizing Machine Learning Algorithms to Identify Relevant Biomarkers and Crucial Pathways in Gallbladder Cancer,"Gallbladder cancer (GBC) is the most frequent cause of disease among biliary
tract neoplasms. Identifying the molecular mechanisms and biomarkers linked to
GBC progression has been a significant challenge in scientific research. Few
recent studies have explored the roles of biomarkers in GBC. Our study aimed to
identify biomarkers in GBC using machine learning (ML) and bioinformatics
techniques. We compared GBC tumor samples with normal samples to identify
differentially expressed genes (DEGs) from two microarray datasets (GSE100363,
GSE139682) obtained from the NCBI GEO database. A total of 146 DEGs were found,
with 39 up-regulated and 107 down-regulated genes. Functional enrichment
analysis of these DEGs was performed using Gene Ontology (GO) terms and
REACTOME pathways through DAVID. The protein-protein interaction network was
constructed using the STRING database. To identify hub genes, we applied three
ranking algorithms: Degree, MNC, and Closeness Centrality. The intersection of
hub genes from these algorithms yielded 11 hub genes. Simultaneously, two
feature selection methods (Pearson correlation and recursive feature
elimination) were used to identify significant gene subsets. We then developed
ML models using SVM and RF on the GSE100363 dataset, with validation on
GSE139682, to determine the gene subset that best distinguishes GBC samples.
The hub genes outperformed the other gene subsets. Finally, NTRK2, COL14A1,
SCN4B, ATP1A2, SLC17A7, SLIT3, COL7A1, CLDN4, CLEC3B, ADCYAP1R1, and MFAP4 were
identified as crucial genes, with SLIT3, COL7A1, and CLDN4 being strongly
linked to GBC development and prediction.",393,2410.14433v1,q-bio.GN,"q-bio.GN,cs.LG",bioinformatics,2024-10-18,2024-12-23T21:07:02.278583
Shape Transformation Driven by Active Contour for Class-Imbalanced Semi-Supervised Medical Image Segmentation,"Annotating 3D medical images demands expert knowledge and is time-consuming.
As a result, semi-supervised learning (SSL) approaches have gained significant
interest in 3D medical image segmentation. The significant size differences
among various organs in the human body lead to imbalanced class distribution,
which is a major challenge in the real-world application of these SSL
approaches. To address this issue, we develop a novel Shape Transformation
driven by Active Contour (STAC), that enlarges smaller organs to alleviate
imbalanced class distribution across different organs. Inspired by curve
evolution theory in active contour methods, STAC employs a signed distance
function (SDF) as the level set function, to implicitly represent the shape of
organs, and deforms voxels in the direction of the steepest descent of SDF
(i.e., the normal vector). To ensure that the voxels far from expansion organs
remain unchanged, we design an SDF-based weight function to control the degree
of deformation for each voxel. We then use STAC as a data-augmentation process
during the training stage. Experimental results on two benchmark datasets
demonstrate that the proposed method significantly outperforms some
state-of-the-art methods. Source code is publicly available at
https://github.com/GuGuLL123/STAC.",284,2410.14210v1,cs.CV,"cs.CV,cs.NE",bioinformatics,2024-10-18,2024-12-23T21:07:02.279580
scFusionTTT: Single-cell transcriptomics and proteomics fusion with Test-Time Training layers,"Single-cell multi-omics (scMulti-omics) refers to the paired multimodal data,
such as Cellular Indexing of Transcriptomes and Epitopes by Sequencing
(CITE-seq), where the regulation of each cell was measured from different
modalities, i.e. genes and proteins. scMulti-omics can reveal heterogeneity
inside tumors and understand the distinct genetic properties of diverse cell
types, which is crucial to targeted therapy. Currently, deep learning methods
based on attention structures in the bioinformatics area face two challenges.
The first challenge is the vast number of genes in a single cell. Traditional
attention-based modules struggled to effectively leverage all gene information
due to their limited capacity for long-context learning and high-complexity
computing. The second challenge is that genes in the human genome are ordered
and influence each other's expression. Most of the methods ignored this
sequential information. The recently introduced Test-Time Training (TTT) layer
is a novel sequence modeling approach, particularly suitable for handling long
contexts like genomics data because TTT layer is a linear complexity sequence
modeling structure and is better suited to data with sequential relationships.
In this paper, we propose scFusionTTT, a novel method for Single-Cell
multimodal omics Fusion with TTT-based masked autoencoder. Of note, we combine
the order information of genes and proteins in the human genome with the TTT
layer, fuse multimodal omics, and enhance unimodal omics analysis. Finally, the
model employs a three-stage training strategy, which yielded the best
performance across most metrics in four multimodal omics datasets and four
unimodal omics datasets, demonstrating the superior performance of our model.
The dataset and code will be available on
https://github.com/DM0815/scFusionTTT.",399,2410.13257v1,cs.LG,"cs.LG,cs.AI",bioinformatics,2024-10-17,2024-12-23T21:07:02.280577
Improved Kernelization and Fixed-parameter Algorithms for Bicluster Editing,"Given a bipartite graph $G$, the \textsc{Bicluster Editing} problem asks for
the minimum number of edges to insert or delete in $G$ so that every connected
component is a bicluster, i.e. a complete bipartite graph. This has several
applications, including in bioinformatics and social network analysis. In this
work, we study the parameterized complexity under the natural parameter $k$,
which is the number of allowed modified edges. We first show that one can
obtain a kernel with $4.5k$ vertices, an improvement over the previously known
quadratic kernel. We then propose an algorithm that runs in time
$O^*(2.581^k)$. Our algorithm has the advantage of being conceptually simple
and should be easy to implement.",174,2410.13123v2,cs.DS,cs.DS,bioinformatics,2024-10-17,2024-12-23T21:07:02.281575
Incremental computation of the set of period sets,"Overlaps between words are crucial in many areas of computer science, such as
code design, stringology, and bioinformatics. A self overlapping word is
characterized by its periods and borders. A period of a word $u$ is the
starting position of a suffix of $u$ that is also a prefix $u$, and such a
suffix is called a border. Each word of length, say $n>0$, has a set of
periods, but not all combinations of integers are sets of periods. Computing
the period set of a word $u$ takes linear time in the length of $u$. We address
the question of computing, the set, denoted $\Gamma_n$, of all period sets of
words of length $n$. Although period sets have been characterized, there is no
formula to compute the cardinality of $\Gamma_n$ (which is exponential in $n$),
and the known dynamic programming algorithm to enumerate $\Gamma_n$ suffers
from its space complexity. We present an incremental approach to compute
$\Gamma_n$ from $\Gamma_{n-1}$, which reduces the space complexity, and then a
constructive certification algorithm useful for verification purposes. The
incremental approach defines a parental relation between sets in $\Gamma_{n-1}$
and $\Gamma_n$, enabling one to investigate the dynamics of period sets, and
their intriguing statistical properties. Moreover, the period set of a word $u$
is the key for computing the absence probability of $u$ in random texts. Thus,
knowing $\Gamma_n$ is useful to assess the significance of word statistics,
such as the number of missing words in a random text.",361,2410.12077v2,cs.DM,"cs.DM,05-06,G.2.1",bioinformatics,2024-10-15,2024-12-23T21:07:02.281575
A Learning Search Algorithm for the Restricted Longest Common Subsequence Problem,"This paper addresses the Restricted Longest Common Subsequence (RLCS)
problem, an extension of the well-known Longest Common Subsequence (LCS)
problem. This problem has significant applications in bioinformatics,
particularly for identifying similarities and discovering mutual patterns and
important motifs among DNA, RNA, and protein sequences. Building on recent
advancements in solving this problem through a general search framework, this
paper introduces two novel heuristic approaches designed to enhance the search
process by steering it towards promising regions in the search space. The first
heuristic employs a probabilistic model to evaluate partial solutions during
the search process. The second heuristic is based on a neural network model
trained offline using a genetic algorithm. A key aspect of this approach is
extracting problem-specific features of partial solutions and the complete
problem instance. An effective hybrid method, referred to as the learning beam
search, is developed by combining the trained neural network model with a beam
search framework. An important contribution of this paper is found in the
generation of real-world instances where scientific abstracts serve as input
strings, and a set of frequently occurring academic words from the literature
are used as restricted patterns. Comprehensive experimental evaluations
demonstrate the effectiveness of the proposed approaches in solving the RLCS
problem. Finally, an empirical explainability analysis is applied to the
obtained results. In this way, key feature combinations and their respective
contributions to the success or failure of the algorithms across different
problem types are identified.",302,2410.12031v1,cs.AI,cs.AI,bioinformatics,2024-10-15,2024-12-23T21:07:02.282572
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.109493
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",computational social science,2024-12-20,2024-12-23T21:07:03.110490
Can Generative Video Models Help Pose Estimation?,"Pairwise pose estimation from images with little or no overlap is an open
challenge in computer vision. Existing methods, even those trained on
large-scale datasets, struggle in these scenarios due to the lack of
identifiable correspondences or visual overlap. Inspired by the human ability
to infer spatial relationships from diverse scenes, we propose a novel
approach, InterPose, that leverages the rich priors encoded within pre-trained
generative video models. We propose to use a video model to hallucinate
intermediate frames between two input images, effectively creating a dense,
visual transition, which significantly simplifies the problem of pose
estimation. Since current video models can still produce implausible motion or
inconsistent geometry, we introduce a self-consistency score that evaluates the
consistency of pose predictions from sampled videos. We demonstrate that our
approach generalizes among three state-of-the-art video models and show
consistent improvements over the state-of-the-art DUSt3R on four diverse
datasets encompassing indoor, outdoor, and object-centric scenes. Our findings
suggest a promising avenue for improving pose estimation models by leveraging
large generative models trained on vast amounts of video data, which is more
readily available than 3D data. See our project page for results:
https://inter-pose.github.io/.",271,2412.16155v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.110490
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",computational social science,2024-12-20,2024-12-23T21:07:03.111486
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.112484
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.112484
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset.",207,2412.16146v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.113481
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",computational social science,2024-12-20,2024-12-23T21:07:03.114478
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",computational social science,2024-12-20,2024-12-23T21:07:03.115475
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.115475
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",computational social science,2024-12-20,2024-12-23T21:07:03.116473
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",computational social science,2024-12-20,2024-12-23T21:07:03.117470
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",computational social science,2024-12-20,2024-12-23T21:07:03.118468
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",computational social science,2024-12-20,2024-12-23T21:07:03.118468
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",computational social science,2024-12-20,2024-12-23T21:07:03.119465
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,computational social science,2024-12-20,2024-12-23T21:07:03.120462
LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical Analysis of Diffeomorphism,"Image registration is a core task in computational anatomy that establishes
correspondences between images. Invertible deformable registration, which
computes a deformation field and handles complex, non-linear transformation, is
essential for tracking anatomical variations, especially in neuroimaging
applications where inter-subject differences and longitudinal changes are key.
Analyzing the deformation fields is challenging due to their non-linearity,
limiting statistical analysis. However, traditional approaches for analyzing
deformation fields are computationally expensive, sensitive to initialization,
and prone to numerical errors, especially when the deformation is far from the
identity. To address these limitations, we propose the Log-Euclidean
Diffeomorphic Autoencoder (LEDA), an innovative framework designed to compute
the principal logarithm of deformation fields by efficiently predicting
consecutive square roots. LEDA operates within a linearized latent space that
adheres to the diffeomorphisms group action laws, enhancing our model's
robustness and applicability. We also introduce a loss function to enforce
inverse consistency, ensuring accurate latent representations of deformation
fields. Extensive experiments with the OASIS-1 dataset demonstrate the
effectiveness of LEDA in accurately modeling and analyzing complex non-linear
deformations while maintaining inverse consistency. Additionally, we evaluate
its ability to capture and incorporate clinical variables, enhancing its
relevance for clinical applications.",271,2412.16129v1,cs.CV,"cs.CV,cs.LG",computational social science,2024-12-20,2024-12-23T21:07:03.121460
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",computational social science,2024-12-20,2024-12-23T21:07:03.122457
Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli,"As humans perceive and actively engage with the world, we adjust our
decisions in response to shifting group dynamics and are influenced by social
interactions. This study aims to identify which aspects of interaction affect
cooperation-defection choices. Specifically, we investigate human cooperation
within the Prisoner's Dilemma game, using the Drift-Diffusion Model to describe
the decision-making process. We introduce a novel Bayesian model for the
evolution of the model's parameters based on the nature of interactions
experienced with other players. This approach enables us to predict the
evolution of the population's expected cooperation rate. We successfully
validate our model using an unseen test dataset and apply it to explore three
strategic scenarios: co-player manipulation, use of rewards and punishments,
and time pressure. These results support the potential of our model as a
foundational tool for developing and testing strategies aimed at enhancing
cooperation, ultimately contributing to societal welfare.",182,2412.16121v1,physics.soc-ph,physics.soc-ph,computational social science,2024-12-20,2024-12-23T21:07:03.122457
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,computational social science,2024-12-20,2024-12-23T21:07:03.123454
Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts,"This study investigates the potential of Large Language Models (LLMs),
particularly GPT-4o, for Optical Character Recognition (OCR) in low-resource
scripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.
Using a meticulously curated dataset of 2,520 images incorporating controlled
variations in text length, font size, background color, and blur, the research
simulates diverse real-world challenges. Results emphasize the limitations of
zero-shot LLM-based OCR, particularly for linguistically complex scripts,
highlighting the need for annotated datasets and fine-tuned models. This work
underscores the urgency of addressing accessibility gaps in text digitization,
paving the way for inclusive and robust OCR solutions for underserved
languages.",164,2412.16119v1,cs.LG,"cs.LG,cs.CV,eess.IV",computational social science,2024-12-20,2024-12-23T21:07:03.123454
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",computational social science,2024-12-20,2024-12-23T21:07:03.125451
PruneVid: Visual Token Pruning for Efficient Video Large Language Models,"In this paper, we introduce PruneVid, a visual token pruning method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing pruning methods. Code:
https://github.com/Visual-AI/PruneVid.",204,2412.16117v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.126448
The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse,"There is an ongoing debate about how to moderate toxic speech on social media
and how content moderation affects online discourse. We propose and validate a
methodology for measuring the content-moderation-induced distortions in online
discourse using text embeddings from computational linguistics. We test our
measure on a representative dataset of 5 million US political Tweets and find
that removing toxic Tweets distorts online content. This finding is consistent
across different embedding models, toxicity metrics, and samples. Importantly,
we demonstrate that content-moderation-induced distortions are not caused by
the toxic language. Instead, we show that, as a side effect, content moderation
shifts the mean and variance of the embedding space, distorting the topic
composition of online content. Finally, we propose an alternative approach to
content moderation that uses generative Large Language Models to rephrase toxic
Tweets to preserve their salvageable content rather than removing them
entirely. We demonstrate that this rephrasing strategy reduces toxicity while
minimizing distortions in online content.",220,2412.16114v1,cs.SI,cs.SI,computational social science,2024-12-20,2024-12-23T21:07:03.127444
CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up,"Diffusion Transformers (DiT) have become a leading architecture in image
generation. However, the quadratic complexity of attention mechanisms, which
are responsible for modeling token-wise relationships, results in significant
latency when generating high-resolution images. To address this issue, we aim
at a linear attention mechanism in this paper that reduces the complexity of
pre-trained DiTs to linear. We begin our exploration with a comprehensive
summary of existing efficient attention mechanisms and identify four key
factors crucial for successful linearization of pre-trained DiTs: locality,
formulation consistency, high-rank attention maps, and feature integrity. Based
on these insights, we introduce a convolution-like local attention strategy
termed CLEAR, which limits feature interactions to a local window around each
query token, and thus achieves linear complexity. Our experiments indicate
that, by fine-tuning the attention layer on merely 10K self-generated samples
for 10K iterations, we can effectively transfer knowledge from a pre-trained
DiT to a student model with linear complexity, yielding results comparable to
the teacher model. Simultaneously, it reduces attention computations by 99.5%
and accelerates generation by 6.3 times for generating 8K-resolution images.
Furthermore, we investigate favorable properties in the distilled attention
layers, such as zero-shot generalization cross various models and plugins, and
improved support for multi-GPU parallel inference. Models and codes are
available here: https://github.com/Huage001/CLEAR.",308,2412.16112v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.129439
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",computational social science,2024-12-20,2024-12-23T21:07:03.130438
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",computational social science,2024-12-20,2024-12-23T21:07:03.132432
Logical Consistency of Large Language Models in Fact-checking,"In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses -- a meaning-preserving change in the input query
results in an inconsistent response and attributes to vulnerabilities of LLMs
such as hallucination, jailbreaking, etc. Consequently, existing research
focuses on simple paraphrasing-based consistency assessment of LLMs, and
ignores complex queries that necessitates an even better understanding of
logical reasoning by an LLM. Our work therefore addresses the logical
inconsistency of LLMs under complex logical queries with primitive logical
operators, e.g., negation, conjunction, and disjunction. As a test bed, we
consider retrieval-augmented LLMs on a fact-checking task involving
propositional logic queries from real-world knowledge graphs (KGs). Our
contributions are three-fold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries as input and demonstrate that existing LLMs lack
logical consistency, specially on complex queries. Improvement: We employ
supervised fine-tuning to improve the logical consistency of LLMs on the
complex fact-checking task with KG contexts.",309,2412.16100v1,cs.CL,cs.CL,computational social science,2024-12-20,2024-12-23T21:07:03.134427
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",computational social science,2024-12-20,2024-12-23T21:07:03.134427
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",computational social science,2024-12-20,2024-12-23T21:07:03.135425
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,computational social science,2024-12-20,2024-12-23T21:07:03.136423
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",computational social science,2024-12-20,2024-12-23T21:07:03.136423
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,computational social science,2024-12-20,2024-12-23T21:07:03.137419
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,computational social science,2024-12-20,2024-12-23T21:07:03.138416
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",computational social science,2024-12-20,2024-12-23T21:07:03.138416
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",computational social science,2024-12-20,2024-12-23T21:07:03.139414
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",computational social science,2024-12-20,2024-12-23T21:07:03.140411
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",computational social science,2024-12-20,2024-12-23T21:07:03.140411
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",computational social science,2024-12-20,2024-12-23T21:07:03.141409
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",computational social science,2024-12-20,2024-12-23T21:07:03.142407
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,computational social science,2024-12-20,2024-12-23T21:07:03.143404
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,computational social science,2024-12-20,2024-12-23T21:07:03.144401
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,computational social science,2024-12-20,2024-12-23T21:07:03.144401
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",computational social science,2024-12-20,2024-12-23T21:07:03.145900
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",computational social science,2024-12-20,2024-12-23T21:07:03.146899
Correct implied volatility shapes and reliable pricing in the rough Heston model,"We use modifications of the Adams method and very fast and accurate
sinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and
Levendorski\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for
options of moderate and long maturities and strikes not very far from the spot,
thousands of prices can be calculated in several msec. with relative errors of
the order of 0.5\% and smaller running Matlab on a Mac with moderate
characteristics. We demonstrate that for the calibrated set of parameters in
Euch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility
surface is significantly flatter and fits the data very poorly, hence, the
calibration results in op.cit. is an example of the {\em ghost calibration}
(M.Boyarchenko and Levendorki\u{i}, Quantitative Finance 2015, v. 15): the
errors of the model and numerical method almost cancel one another. We explain
how calibration errors of this sort are generated by each of popular versions
of numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)
with prefixed parameters of a numerical method, resulting in spurious
volatility smiles and skews. We suggest a general {\em Conformal Bootstrap
principle} which allows one to avoid ghost calibration errors. We outline
schemes of application of Conformal Bootstrap principle and the method of the
paper to the design of accurate and fast calibration procedures.",339,2412.16067v1,q-fin.MF,"q-fin.MF,q-fin.CP,60-08, 60E10, 60G10, 60G22, 65C20, 65D30, 65G20, 91G20, 91G60",computational social science,2024-12-20,2024-12-23T21:07:03.147896
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",computational social science,2024-12-20,2024-12-23T21:07:03.147896
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",computational social science,2024-12-20,2024-12-23T21:07:03.148894
Adaptable TeaStore,"Adaptability is a fundamental requirement for modern Cloud software
architectures to ensure robust performance in the face of diverse known and
unforeseen events inherent to distributed systems. State-of-the-art Cloud
systems frequently adopt microservices or serverless architectures. Among
these, TeaStore is a recognised microservice reference architecture that offers
a benchmarking framework for modelling and resource management techniques.
However, TeaStore's original configuration lacks the flexibility necessary to
address the varied scenarios encountered in real-world applications. To
overcome this limitation, we propose an enhanced variant of TeaStore that
distinguishes between mandatory and optional services while incorporating
third-party service integration. Core services such as WebUI, Image Provider,
and Persistence are designated as mandatory to maintain essential
functionality, whereas optional services, such as Recommender and Auth, extend
the architecture's feature set. We outline the design and configuration
possibilities of this adaptable TeaStore variant, aimed at enabling a broader
spectrum of configurability and operational resilience.",213,2412.16060v1,cs.DC,cs.DC,computational social science,2024-12-20,2024-12-23T21:07:03.149891
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",computational social science,2024-12-20,2024-12-23T21:07:03.150888
A vector logic for extensional formal semantics,"This paper proves a homomorphism between extensional formal semantics and
distributional vector space semantics, demonstrating structural compatibility.
Formal semantics models meaning as reference, using logical structures to map
linguistic expressions to truth conditions, while distributional semantics
represents meaning through word vectors derived from contextual usage. By
constructing injective mappings that preserve semantic relationships, we show
that every semantic function in an extensional model corresponds to a
compatible vector space operation. This result respects compositionality and
extends to function compositions, constant interpretations, and $n$-ary
relations. Rather than pursuing unification, we highlight a mathematical
foundation for hybrid cognitive models that integrate symbolic and sub-symbolic
reasoning and semantics. These findings support multimodal language processing,
aligning `meaning as reference' (Frege, Tarski) with `meaning as use'
(Wittgenstein, Firth).",174,2412.16152v1,math.LO,"math.LO,03C55, 03B38, 91F20, 68T50, 03B65,F.3.2; F.4.1",cognitive science,2024-12-20,2024-12-23T21:07:03.903440
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",cognitive science,2024-12-20,2024-12-23T21:07:03.904438
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,cognitive science,2024-12-20,2024-12-23T21:07:03.905435
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",cognitive science,2024-12-20,2024-12-23T21:07:03.906432
How connection probability shapes fluctuations of neural population dynamics,"Mean-field models of neuronal populations in the brain have proven extremely
useful to understand network dynamics and response to stimuli, but these models
generally lack a faithful description of the fluctuations in the biologically
relevant case of finite network size and connection probabilities $p<1$
(non-full connectivity). To gain insight into the different fluctuation
mechanisms underlying the neural variability of populations of spiking neurons,
we derive and analyze a stochastic mean-field model for finite-size networks of
Poisson neurons with random, non-full connectivity, external noise and
disordered mean inputs. We treat the quenched disorder of the connectivity by
an annealed approximation that enables a reduction to a low-dimensional closed
system of coupled Langevin equations for the mean and variance of the neuronal
membrane potentials as well as a variable capturing finite-size fluctuations
arising specifically in the case $p<1$. Comparing to microscopic simulations,
we find that the mesoscopic model describes the fluctuations and nonlinearities
well and outperforms previous mesoscopic models that neglected the recurrent
noise effect caused by the non-full connectivity. This effect can be
analytically understood by a softening of the effective nonlinearity and the
multiplicative character of finite-size spiking noise. The mesoscopic theory
shows that quenched disorder can stabilize the asynchronous state, and it
correctly predicts large quantitiative and non-trivial qualitative effects of
connection probability on the variance of the population firing rate and its
dependence on stimulus strength. Our theory thus elucidates how disordered
connectivity shapes nonlinear dynamics and fluctuations of neural populations
at the mesoscopic scale and showcases a useful mean-field method to treat
non-full connectivity in finite-size, spiking neural networks.",370,2412.16111v1,q-bio.NC,q-bio.NC,cognitive science,2024-12-20,2024-12-23T21:07:03.907430
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",cognitive science,2024-12-20,2024-12-23T21:07:03.908427
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",cognitive science,2024-12-20,2024-12-23T21:07:03.909426
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.910422
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",cognitive science,2024-12-20,2024-12-23T21:07:03.911419
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",cognitive science,2024-12-20,2024-12-23T21:07:03.912416
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",cognitive science,2024-12-20,2024-12-23T21:07:03.912416
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,cognitive science,2024-12-20,2024-12-23T21:07:03.913414
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,cognitive science,2024-12-20,2024-12-23T21:07:03.914411
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",cognitive science,2024-12-20,2024-12-23T21:07:03.914411
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,cognitive science,2024-12-20,2024-12-23T21:07:03.915408
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",cognitive science,2024-12-20,2024-12-23T21:07:03.915408
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.916406
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,cognitive science,2024-12-20,2024-12-23T21:07:03.917403
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",cognitive science,2024-12-20,2024-12-23T21:07:03.917403
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",cognitive science,2024-12-20,2024-12-23T21:07:03.918400
First Constraint on the Diffuse Supernova Neutrino Background through the CE$ν$NS process from the LZ experiment,"We report the limits on the diffuse supernova neutrino background (DSNB) flux
and the fundamental DSNB parameters measured from the first science run of the
LUX-ZEPLIN (LZ) experiment, a dual-phase xenon detector located at the Sanford
Underground Research Facility in Lead, South Dakota, USA. This is the first
time the DSNB limit is measured through the process of the coherent elastic
neutrino-nucleus scattering (CE$\nu$NS). Using an exposure of 60~live days and
a fiducial mass of 5.5~t, the upper limit on the DSNB $\nu_x$ (each of
$\nu_\mu$, $\nu_\tau$, $\bar\nu_\mu$, $\bar\nu_\tau$) flux is
$686-826$~cm$^{-2}$s$^{-1}$ at the 90\% confidence level for neutrino energies
E$>$19.3~MeV, assuming the flux for each $\nu_x$ flavor is the same. The
interval accounts for the uncertainty in existing DSNB models. The present
result is comparable to the existing best limit and further improvements are
expected after collecting data from an estimated 1,000-day exposure in the
future.",281,2412.15886v1,hep-ex,hep-ex,cognitive science,2024-12-20,2024-12-23T21:07:03.918400
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.919398
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.920395
On the Power of Strategic Corpus Enrichment in Content Creation Games,"Search and recommendation ecosystems exhibit competition among content
creators. This competition has been tackled in a variety of game-theoretic
frameworks. Content creators generate documents with the aim of being
recommended by a content ranker for various information needs. In order for the
ecosystem, modeled as a content ranking game, to be effective and maximize user
welfare, it should guarantee stability, where stability is associated with the
existence of pure Nash equilibrium in the corresponding game. Moreover, if the
contents' ranking algorithm possesses a game in which any best-response
learning dynamics of the content creators converge to equilibrium of high
welfare, the system is considered highly attractive. However, as classical
content ranking algorithms, employed by search and recommendation systems, rank
documents by their distance to information needs, it has been shown that they
fail to provide such stability properties. As a result, novel content ranking
algorithms have been devised. In this work, we offer an alternative approach:
corpus enrichment with a small set of fixed dummy documents. It turns out that,
with the right design, such enrichment can lead to pure Nash equilibrium and
even to the convergence of any best-response dynamics to a high welfare result,
where we still employ the classical/current content ranking approach. We show
two such corpus enrichment techniques with tight bounds on the number of
documents needed to obtain the desired results. Interestingly, our study is a
novel extension of Borel's Colonel Blotto game.",287,2412.15878v1,cs.GT,cs.GT,cognitive science,2024-12-20,2024-12-23T21:07:03.920395
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",cognitive science,2024-12-20,2024-12-23T21:07:03.921392
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,cognitive science,2024-12-20,2024-12-23T21:07:03.922390
Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science,"Understanding how transportation networks work is important for improving
connectivity, efficiency, and safety. In Brazil, where road transport is a
significant portion of freight and passenger movement, network science can
provide valuable insights into the structural properties of the infrastructure,
thus helping decision makers responsible for proposing improvements to the
system. This paper models the federal road network as weighted networks, with
the intent to unveil its topological characteristics and identify key locations
(cities) that play important roles for the country through 75,000 kilometres of
roads. We start with a simple network to examine basic connectivity and
topology, where weights are the distance of the road segment. We then
incorporate other weights representing number of incidents, population, and
number of cities in-between each segment. We then focus on community detection
as a way to identify clusters of cities that form cohesive groups within a
network. Our findings aim to bring clarity to the overall structure of federal
roads in Brazil, thus providing actionable insights for improving
infrastructure planning and prioritising resources to enhance network
resilience.",208,2412.15865v1,physics.soc-ph,"physics.soc-ph,cs.CY",cognitive science,2024-12-20,2024-12-23T21:07:03.923387
Enriching Social Science Research via Survey Item Linking,"Questions within surveys, called survey items, are used in the social
sciences to study latent concepts, such as the factors influencing life
satisfaction. Instead of using explicit citations, researchers paraphrase the
content of the survey items they use in-text. However, this makes it
challenging to find survey items of interest when comparing related work.
Automatically parsing and linking these implicit mentions to survey items in a
knowledge base can provide more fine-grained references. We model this task,
called Survey Item Linking (SIL), in two stages: mention detection and entity
disambiguation. Due to an imprecise definition of the task, existing datasets
used for evaluating the performance for SIL are too small and of low-quality.
We argue that latent concepts and survey item mentions should be
differentiated. To this end, we create a high-quality and richly annotated
dataset consisting of 20,454 English and German sentences. By benchmarking deep
learning systems for each of the two stages independently and sequentially, we
demonstrate that the task is feasible, but observe that errors propagate from
the first stage, leading to a lower overall task performance. Moreover,
mentions that require the context of multiple sentences are more challenging to
identify for models in the first stage. Modeling the entire context of a
document and combining the two stages into an end-to-end system could mitigate
these problems in future work, and errors could additionally be reduced by
collecting more diverse data and by improving the quality of the knowledge
base. The data and code are available at https://github.com/e-tornike/SIL .",338,2412.15831v1,cs.DL,"cs.DL,cs.CL",cognitive science,2024-12-20,2024-12-23T21:07:03.923387
SUBMASSIVE: Resolving Subclass Cycles in Very Large Knowledge Graphs,"Large knowledge graphs capture information of a large number of entities and
their relations. Among the many relations they capture, class subsumption
assertions are usually present and expressed using the \texttt{rdfs:subClassOf}
construct. From our examination, publicly available knowledge graphs contain
many potentially erroneous cyclic subclass relations, a problem that can be
exacerbated when different knowledge graphs are integrated as Linked Open Data.
In this paper, we present an automatic approach for resolving such cycles at
scale using automated reasoning by encoding the problem of cycle-resolving to a
MAXSAT solver. The approach is tested on the LOD-a-lot dataset, and compared
against a semi-automatic version of our algorithm. We show how the number of
removed triples is a trade-off against the efficiency of the algorithm.",170,2412.15829v1,cs.LO,"cs.LO,cs.SC,math.OC,68T27, 68T20, 68T09,F.3.0; I.2.1; I.2.4",cognitive science,2024-12-20,2024-12-23T21:07:03.924384
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",cognitive science,2024-12-20,2024-12-23T21:07:03.925383
Precision ICU Resource Planning: A Multimodal Model for Brain Surgery Outcomes,"Although advances in brain surgery techniques have led to fewer postoperative
complications requiring Intensive Care Unit (ICU) monitoring, the routine
transfer of patients to the ICU remains the clinical standard, despite its high
cost. Predictive Gradient Boosted Trees based on clinical data have attempted
to optimize ICU admission by identifying key risk factors pre-operatively;
however, these approaches overlook valuable imaging data that could enhance
prediction accuracy. In this work, we show that multimodal approaches that
combine clinical data with imaging data outperform the current clinical data
only baseline from 0.29 [F1] to 0.30 [F1], when only pre-operative clinical
data is used and from 0.37 [F1] to 0.41 [F1], for pre- and post-operative data.
This study demonstrates that effective ICU admission prediction benefits from
multimodal data fusion, especially in contexts of severe class imbalance.",189,2412.15818v1,eess.IV,"eess.IV,cs.CV,q-bio.NC",cognitive science,2024-12-20,2024-12-23T21:07:03.926380
Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis,"Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.",134,2412.15814v1,cs.CR,"cs.CR,cs.DC,cs.LO",cognitive science,2024-12-20,2024-12-23T21:07:03.927377
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",cognitive science,2024-12-20,2024-12-23T21:07:03.928374
Learning from Impairment: Leveraging Insights from Clinical Linguistics in Language Modelling Research,"This position paper investigates the potential of integrating insights from
language impairment research and its clinical treatment to develop
human-inspired learning strategies and evaluation frameworks for language
models (LMs). We inspect the theoretical underpinnings underlying some
influential linguistically motivated training approaches derived from
neurolinguistics and, particularly, aphasiology, aimed at enhancing the
recovery and generalization of linguistic skills in aphasia treatment, with a
primary focus on those targeting the syntactic domain. We highlight how these
insights can inform the design of rigorous assessments for LMs, specifically in
their handling of complex syntactic phenomena, as well as their implications
for developing human-like learning strategies, aligning with efforts to create
more sustainable and cognitively plausible natural language processing (NLP)
models.",160,2412.15785v1,cs.CL,cs.CL,cognitive science,2024-12-20,2024-12-23T21:07:03.928374
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",cognitive science,2024-12-20,2024-12-23T21:07:03.929371
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",cognitive science,2024-12-20,2024-12-23T21:07:03.930368
Building Bridges: AI Custom Chatbots as Mediators between Mathematics and Physics,"This work explores the integration of AI custom chatbots in educational
settings, with a particular focus on their applicability in the context of
mathematics and physics. In view of the increasing deployment of AI tools such
as ChatGPT in educational contexts, the present study examines their potential
as personalized tutoring systems. The study assesses the impact of AI-generated
learning materials on the learning experiences and performance of sixth-grade
students, with a particular focus on proportional relationships in mathematical
and physical contexts. The randomized controlled study with N = 214 students
compared traditional textbook materials with explanations generated by a custom
chatbot. The results demonstrated that while AI-generated materials had an
indefinite impact on learning outcomes, they significantly enhanced
positive-activating emotions, situational interest, and self-efficacy, while
reducing intrinsic and extrinsic cognitive load. These findings underscore the
potential of AI to transform educational practices by fostering a superior
learning experience. However, further research is required to clarify its
impact on learning performance and long-term learning outcomes. The study
highlights the importance of careful integration and customization of AI tools
to maximize their benefits in physics education.",233,2412.15747v1,physics.ed-ph,physics.ed-ph,cognitive science,2024-12-20,2024-12-23T21:07:03.931366
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,cognitive science,2024-12-20,2024-12-23T21:07:03.932365
Distribution-Free Normal Modal Logics,"This article initiates the semantic study of distribution-free normal modal
logic systems, laying the semantic foundations and anticipating further
research in the area. The article explores roughly the same area, though taking
a different approach, with a recent article by Bezhanishvili, de Groot,
Dmitrieva and Morachini, who studied a distribution-free version of Dunn's
Positive Modal Logic (PML). Unlike PML, we consider logics that may drop
distribution and which are equipped with both an implication connective and
modal operators. We adopt a uniform relational semantics approach, relying on
recent results on representation and duality for normal lattice expansions. We
prove canonicity and completeness in the relational semantics of the minimal
distribution-free normal modal logic, assuming just the K-axiom, as well as of
its axiomatic extensions obtained by adding any of the D, T, B, S4 or S5
axioms. Adding distribution can be easily accommodated and, as a side result,
we also obtain a new semantic treatment of Intuitionistic Modal Logic.",224,2412.15736v1,cs.LO,"cs.LO,math.LO",cognitive science,2024-12-20,2024-12-23T21:07:03.933362
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.933362
Safe Spaces or Toxic Places? Content Moderation and Social Dynamics of Online Eating Disorder Communities,"Social media platforms have become critical spaces for discussing mental
health concerns, including eating disorders. While these platforms can provide
valuable support networks, they may also amplify harmful content that glorifies
disordered cognition and self-destructive behaviors. While social media
platforms have implemented various content moderation strategies, from
stringent to laissez-faire approaches, we lack a comprehensive understanding of
how these different moderation practices interact with user engagement in
online communities around these sensitive mental health topics. This study
addresses this knowledge gap through a comparative analysis of eating disorder
discussions across Twitter/X, Reddit, and TikTok. Our findings reveal that
while users across all platforms engage similarly in expressing concerns and
seeking support, platforms with weaker moderation (like Twitter/X) enable the
formation of toxic echo chambers that amplify pro-anorexia rhetoric. These
results demonstrate how moderation strategies significantly influence the
development and impact of online communities, particularly in contexts
involving mental health and self-harm.",201,2412.15721v1,cs.SI,"cs.SI,cs.CY,cs.HC",cognitive science,2024-12-20,2024-12-23T21:07:03.934360
Does the brain behave like a (complex) network? I. Dynamics,"Graph theory is now becoming a standard tool in system-level neuroscience.
However, endowing observed brain anatomy and dynamics with a complex network
structure does not entail that the brain actually works as a network. Asking
whether the brain behaves as a network means asking whether network properties
count. From the viewpoint of neurophysiology and, possibly, of brain physics,
the most substantial issues a network structure may be instrumental in
addressing relate to the influence of network properties on brain dynamics and
to whether these properties ultimately explain some aspects of brain function.
Here, we address the dynamical implications of complex network, examining which
aspects and scales of brain activity may be understood to genuinely behave as a
network. To do so, we first define the meaning of networkness, and analyse some
of its implications. We then examine ways in which brain anatomy and dynamics
can be endowed with a network structure and discuss possible ways in which
network structure may be shown to represent a genuine organisational principle
of brain activity, rather than just a convenient description of its anatomy and
dynamics.",213,2412.15711v1,q-bio.NC,"q-bio.NC,cond-mat.dis-nn,nlin.AO",cognitive science,2024-12-20,2024-12-23T21:07:03.935357
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.935357
Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion,"This paper addresses the question of whether or not uncoupled online learning
algorithms converge to the Nash equilibrium in pricing competition or whether
they can learn to collude. Algorithmic collusion has been debated among
competition regulators, and it is a highly relevant phenomenon for buyers and
sellers on online retail platforms. We analyze formally if mean-based
algorithms, a class of bandit algorithms relevant to algorithmic pricing,
converge to the Nash equilibrium in repeated Bertrand oligopolies. Bandit
algorithms only learn the profit of the agent for the price set in each step.
In addition, we provide results of extensive experiments with different types
of multi-armed bandit algorithms used for algorithmic pricing. In a
mathematical proof, we show that mean-based algorithms converge to correlated
rational strategy profiles, which coincide with the Nash equilibrium in
versions of the Bertrand competition. Learning algorithms do not converge to a
Nash equilibrium in general, and the fact that Bertrand pricing games are
learnable with bandit algorithms is remarkable. Our numerical results suggest
that wide-spread bandit algorithms that are not mean-based also converge to
equilibrium and that algorithmic collusion only arises with symmetric
implementations of UCB or Q-learning, but not if different algorithms are used
by sellers. In addition, the level of supra-competitive prices decreases with
increasing numbers of sellers. Supra-competitive prices decrease consumer
welfare. If algorithms lead to algorithmic collusion, this is important for
consumers, sellers, and regulators to understand. We show that for the
important class of multi-armed bandit algorithms such fears are overrated
unless all sellers agree on a symmetric implementation of certain collusive
algorithms.",330,2412.15707v1,cs.GT,cs.GT,cognitive science,2024-12-20,2024-12-23T21:07:03.936354
High-efficiency fast pinching radiation of electron beams in nonuniform plasma,"The continuous development of bright x/gamma-ray sources has opened up new
frontiers of science and advanced applications. Currently, there is still a
lack of efficient approaches to produce gamma-rays with photon energies up to
GeV and with high peak brilliance comparable to modern free-electron lasers.
Here we report a novel mechanism called beam fast pinching radiation burst to
generate such gamma-ray sources. It is achieved by injecting a GeV electron
beam into a submillimeter plasma with an upramp density profile, enabling
violent beam pinching to occur rapidly. During this process, a burst of
collimated gamma-rays is efficiently produced with photon energy up to GeV,
energy conversion efficiency exceeding $30\%$, and peak brilliance exceeding
$10^{28}$ photons s$^{-1}$ mm$^{-2}$ mrad$^{-2}$ per $0.1\%$ bandwidth. All of
these are several orders of magnitude higher than existing gamma-ray sources.
This opens a novel avenue for the development of extremely bright gamma-ray
sources for both fundamental research and cutting-edge applications.",238,2412.15706v1,physics.plasm-ph,"physics.plasm-ph,physics.acc-ph",cognitive science,2024-12-20,2024-12-23T21:07:03.937352
High-Dimensional Bayesian Optimisation with Large-Scale Constraints via Latent Space Gaussian Processes,"Design optimisation offers the potential to develop lightweight aircraft
structures with reduced environmental impact. Due to the high number of design
variables and constraints, these challenges are typically addressed using
gradient-based optimisation methods to maintain efficiency. However, this
approach often results in a local solution, overlooking the global design
space. Moreover, gradients are frequently unavailable. Bayesian Optimisation
presents a promising alternative, enabling sample-efficient global optimisation
through probabilistic surrogate models that do not depend on gradients.
Although Bayesian Optimisation has shown its effectiveness for problems with a
small number of design variables, it struggles to scale to high-dimensional
problems, particularly when incorporating large-scale constraints. This
challenge is especially pronounced in aeroelastic tailoring, where directional
stiffness properties are integrated into the structural design to manage
aeroelastic deformations and enhance both aerodynamic and structural
performance. Ensuring the safe operation of the system requires simultaneously
addressing constraints from various analysis disciplines, making global design
space exploration even more complex. This study seeks to address this issue by
employing high-dimensional Bayesian Optimisation combined with a dimensionality
reduction technique to tackle the optimisation challenges in aeroelastic
tailoring. The proposed approach is validated through experiments on a
well-known benchmark case with black-box constraints, as well as its
application to the aeroelastic tailoring problem, demonstrating the feasibility
of Bayesian Optimisation for high-dimensional problems with large-scale
constraints.",301,2412.15679v1,cs.CE,cs.CE,cognitive science,2024-12-20,2024-12-23T21:07:03.937352
Two-Dimensional Graphene: Theoretical Study of Multi-photon Non-linear Absorption Coefficient of a Strong Electromagnetic Wave by Using Quantum Kinetic Equation,"Based on the quantum kinetic equation for electrons, we theoretically study
the quantum multi-photon non-linear absorption of a strong electromagnetic wave
(EMW) in two-dimensional graphene. Two cases of the electron scattering
mechanism are considered: Electron-optical phonon scattering and
electron-acoustic phonon scattering. The general multi-photon absorption
coefficient is presented as a function of the temperature, the external
magnetic field, the photon energy and the amplitude of external EMW. These
analytical expressions for multi-photon non-linear absorption coefficient
(MNAC) are numerically calculated and the results are discussed in both the
absence and presence of a magnetic field perpendicular to the graphene sheet.
The results show that there is no absorption peak in the absence of the
magnetic field, which contrasts with previous results in 2D systems such as
quantum wells or superlattices. However, when there is a strong magnetic field
along the direction perpendicular to the 2D graphene, absorption spectral lines
appear consistent with the magneto-phonon resonance conditions. Our
calculations show that the MPA's effect is stronger than mono-photon
absorption. Besides, the quantum multi-photon non-linear absorption phenomenon
has been studied from low to high temperatures. This transcends the limits of
the classical BKE which is studied in the high-temperature domain. The
computational results show that the dependence of MNAC on the above quantities
is consistent with the previous theoretical investigation. Another novel
feature of this work is that the general analytic expression for MNAC shows the
Half Width at Half Maximum dependence on the magnetic field which is in good
agreement with the previous experimental observations. Thus, our estimation
might give a critical prediction for future experimental observations in 2D
graphene.",347,2412.15638v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",cognitive science,2024-12-20,2024-12-23T21:07:03.938349
Modeling Autonomous Shifts Between Focus State and Mind-Wandering Using a Predictive-Coding-Inspired Variational RNN Model,"The current study investigates possible neural mechanisms underling
autonomous shifts between focus state and mind-wandering by conducting model
simulation experiments. On this purpose, we modeled perception processes of
continuous sensory sequences using our previous proposed variational RNN model
which was developed based on the free energy principle. The current study
extended this model by introducing an adaptation mechanism of a meta-level
parameter, referred to as the meta-prior $\mathbf{w}$, which regulates the
complexity term in the free energy. Our simulation experiments demonstrated
that autonomous shifts between focused perception and mind-wandering take place
when $\mathbf{w}$ switches between low and high values associated with decrease
and increase of the average reconstruction error over the past window. In
particular, high $\mathbf{w}$ prioritized top-down predictions while low
$\mathbf{w}$ emphasized bottom-up sensations. This paper explores how our
experiment results align with existing studies and highlights their potential
for future research.",201,2412.15620v1,q-bio.NC,"q-bio.NC,cs.AI",cognitive science,2024-12-20,2024-12-23T21:07:03.939346
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",cognitive science,2024-12-20,2024-12-23T21:07:03.940344
Synaptic plasticity alters the nature of chaos transition in neural networks,"In realistic neural circuits, both neurons and synapses are coupled in
dynamics with separate time scales. The circuit functions are intimately
related to these coupled dynamics. However, it remains challenging to
understand the intrinsic properties of the coupled dynamics. Here, we develop
the neuron-synapse coupled quasi-potential method to demonstrate how learning
induces the qualitative change in macroscopic behaviors of recurrent neural
networks. We find that under the Hebbian learning, a large Hebbian strength
will alter the nature of the chaos transition, from a continuous type to a
discontinuous type, where the onset of chaos requires a smaller synaptic gain
compared to the non-plastic counterpart network. In addition, our theory
predicts that under feedback and homeostatic learning, the location and type of
chaos transition are retained, and only the chaotic fluctuation is adjusted.
Our theoretical calculations are supported by numerical simulations.",188,2412.15592v1,q-bio.NC,"q-bio.NC,cond-mat.dis-nn,cond-mat.stat-mech",cognitive science,2024-12-20,2024-12-23T21:07:03.940344
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.909884
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",data science,2024-12-20,2024-12-23T21:07:04.909884
Can Generative Video Models Help Pose Estimation?,"Pairwise pose estimation from images with little or no overlap is an open
challenge in computer vision. Existing methods, even those trained on
large-scale datasets, struggle in these scenarios due to the lack of
identifiable correspondences or visual overlap. Inspired by the human ability
to infer spatial relationships from diverse scenes, we propose a novel
approach, InterPose, that leverages the rich priors encoded within pre-trained
generative video models. We propose to use a video model to hallucinate
intermediate frames between two input images, effectively creating a dense,
visual transition, which significantly simplifies the problem of pose
estimation. Since current video models can still produce implausible motion or
inconsistent geometry, we introduce a self-consistency score that evaluates the
consistency of pose predictions from sampled videos. We demonstrate that our
approach generalizes among three state-of-the-art video models and show
consistent improvements over the state-of-the-art DUSt3R on four diverse
datasets encompassing indoor, outdoor, and object-centric scenes. Our findings
suggest a promising avenue for improving pose estimation models by leveraging
large generative models trained on vast amounts of video data, which is more
readily available than 3D data. See our project page for results:
https://inter-pose.github.io/.",271,2412.16155v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.910881
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.911878
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.912875
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset.",207,2412.16146v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.912875
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",data science,2024-12-20,2024-12-23T21:07:04.913873
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",data science,2024-12-20,2024-12-23T21:07:04.914870
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.914870
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",data science,2024-12-20,2024-12-23T21:07:04.915867
Role of the ratio of tangential to normal stiffness coefficient on the behaviour of vibrofluidised particles,"The selection of parameters in the contact law for inter-particle
interactions affects the results of simulations of flowing granular materials.
The present study aims to understand the effect of the ratio of tangential to
normal spring stiffness coefficient ($\kappa$) on inter-particle contact
behaviour in terms of the rotational coefficient of restitution determined
using data obtained from multi-particle simulations. The effect of $\kappa$ on
the profiles of the micro- and macroscopic properties of particles in a
vibrofluidised bed is also investigated. The Discrete Element Method (DEM) is
used to simulate a vertically vibrated fluidised bed using the open-source
software LAMMPS. The inter-particle and wall-particle contact forces are
determined using the linear spring-dashpot (LSD) model. The distribution of the
mean co-ordination number, force during the contact, contact regimes, and
rotational coefficient of restitution are determined from the data obtained
from simulations. It was shown that $\kappa$ plays a significant role in the
distribution of inter-particle contacts between different regimes and, thereby,
the velocity distribution and profiles of statistically averaged properties of
the vibrofluidised particles. Our results show that for particles with surface
friction coefficient $\mu>0.1$, the commonly used value $\kappa=\frac{2}{7}$
results in quantitatively different results from those obtained using $0.67 \le
\kappa < 1$, a range consistent with the realistic values of Poisson ratios for
simple materials.",315,2412.16133v1,cond-mat.soft,cond-mat.soft,data science,2024-12-20,2024-12-23T21:07:04.916865
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",data science,2024-12-20,2024-12-23T21:07:04.917862
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,data science,2024-12-20,2024-12-23T21:07:04.918859
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,data science,2024-12-20,2024-12-23T21:07:04.918859
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",data science,2024-12-20,2024-12-23T21:07:04.919857
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,data science,2024-12-20,2024-12-23T21:07:04.920855
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",data science,2024-12-20,2024-12-23T21:07:04.921851
PruneVid: Visual Token Pruning for Efficient Video Large Language Models,"In this paper, we introduce PruneVid, a visual token pruning method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing pruning methods. Code:
https://github.com/Visual-AI/PruneVid.",204,2412.16117v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.921851
Weighted nonlocal operators and their applications in labeled learning,"Motivated by problems in machine learning, we study a class of variational
problems characterized by nonlocal operators. These operators are characterized
by power-type weights, which are singular at a portion of the boundary. We
identify a range of exponents on these weights for which the variational
Dirichlet problem is well-posed. This range is determined by the ambient
dimension of the problem, the growth rate of the nonlocal functional, and the
dimension of the boundary portion on which the Dirichlet data is prescribed. We
show the variational convergence of solutions to solutions of local weighted
Sobolev functionals in the event of vanishing nonlocality.",136,2412.16109v1,math.AP,math.AP,data science,2024-12-20,2024-12-23T21:07:04.922849
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",data science,2024-12-20,2024-12-23T21:07:04.923846
Quantifying the benefit of load uncertainty reduction for the design of district energy systems under grid constraints using the Value of Information,"Load uncertainty must be accounted for during design to ensure building
energy systems can meet energy demands during operation. Reducing building load
uncertainty allows for improved designs with less compromise to be identified,
reducing the cost of decarbonizing energy usage. However, the building
monitoring required to reduce load uncertainty is costly. This study quantifies
the economic benefit of practical building monitoring for supporting energy
system design decisions, to determine if its benefits outweigh its cost. Value
of Information analysis (VoI) is a numerical framework for quantifying the
benefit of uncertainty reduction to support decision making. An extension of
the framework, termed 'On-Policy' VoI, is proposed, which admits complex
decision making tasks where decision policies are required. This is applied to
a case study district energy system design problem, where a Linear Program
model is used to size solar-battery systems and grid connection capacity under
uncertain building loads, modelled using historic electricity metering data.
Load uncertainty is found to have a significant impact on both system operating
costs (\pm30%) and the optimal system design (\pm20%). However, using building
monitoring is found to reduce overall costs by less than 2% on average, less
than the cost of measurement, and is therefore not economically worthwhile.
This provides the first numerical evidence to support the sufficiency of using
standard building load profiles for energy system design. Further, reducing
only uncertainty in mean load is found to provide all available decision
support benefit, meaning using hourly measurement data provides no benefit for
energy retrofit design.",312,2412.16105v1,eess.SY,"eess.SY,cs.SY",data science,2024-12-20,2024-12-23T21:07:04.923846
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",data science,2024-12-20,2024-12-23T21:07:04.925842
Interleaved Speech-Text Language Models are Simple Streaming Text to Speech Synthesizers,"This paper introduces Interleaved Speech-Text Language Model (IST-LM) for
streaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,
IST-LM is directly trained on interleaved sequences of text and speech tokens
with a fixed ratio, eliminating the need for additional efforts in duration
prediction and grapheme-to-phoneme alignment. The ratio of text chunk size to
speech chunk size is crucial for the performance of IST-LM. To explore this, we
conducted a comprehensive series of statistical analyses on the training data
and performed correlation analysis with the final performance, uncovering
several key factors: 1) the distance between speech tokens and their
corresponding text tokens, 2) the number of future text tokens accessible to
each speech token, and 3) the frequency of speech tokens precedes their
corresponding text tokens. Experimental results demonstrate how to achieve an
optimal streaming TTS system without complicated engineering, which we show has
a limited gap with the non-streaming system. IST-LM is conceptually simple and
empirically powerful, paving the way for streaming TTS with minimal overhead
while largely maintaining performance.",239,2412.16102v1,eess.AS,eess.AS,data science,2024-12-20,2024-12-23T21:07:04.926839
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",data science,2024-12-20,2024-12-23T21:07:04.926839
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",data science,2024-12-20,2024-12-23T21:07:04.927835
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",data science,2024-12-20,2024-12-23T21:07:04.927835
The Evolution of LLM Adoption in Industry Data Curation Practices,"As large language models (LLMs) grow increasingly adept at processing
unstructured text data, they offer new opportunities to enhance data curation
workflows. This paper explores the evolution of LLM adoption among
practitioners at a large technology company, evaluating the impact of LLMs in
data curation tasks through participants' perceptions, integration strategies,
and reported usage scenarios. Through a series of surveys, interviews, and user
studies, we provide a timely snapshot of how organizations are navigating a
pivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess
LLM adoption in industry for development tasks (N=84), and facilitated expert
interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we
explored practitioners' current and anticipated LLM usage through a user study
involving two LLM-based prototypes (N=12). While each study addressed distinct
research goals, they revealed a broader narrative about evolving LLM usage in
aggregate. We discovered an emerging shift in data understanding from
heuristic-first, bottom-up approaches to insights-first, top-down workflows
supported by LLMs. Furthermore, to respond to a more complex data landscape,
data practitioners now supplement traditional subject-expert-created 'golden
datasets' with LLM-generated 'silver' datasets and rigorously validated 'super
golden' datasets curated by diverse experts. This research sheds light on the
transformative role of LLMs in large-scale analysis of unstructured data and
highlights opportunities for further tool development.",328,2412.16089v1,cs.HC,"cs.HC,cs.AI",data science,2024-12-20,2024-12-23T21:07:04.928833
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",data science,2024-12-20,2024-12-23T21:07:04.929830
Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation,"The increasing demand for privacy-preserving data analytics in finance
necessitates solutions for synthetic data generation that rigorously uphold
privacy standards. We introduce DP-Fed-FinDiff framework, a novel integration
of Differential Privacy, Federated Learning and Denoising Diffusion
Probabilistic Models designed to generate high-fidelity synthetic tabular data.
This framework ensures compliance with stringent privacy regulations while
maintaining data utility. We demonstrate the effectiveness of DP-Fed-FinDiff on
multiple real-world financial datasets, achieving significant improvements in
privacy guarantees without compromising data quality. Our empirical evaluations
reveal the optimal trade-offs between privacy budgets, client configurations,
and federated optimization strategies. The results affirm the potential of
DP-Fed-FinDiff to enable secure data sharing and robust analytics in highly
regulated domains, paving the way for further advances in federated learning
and privacy-preserving data synthesis.",186,2412.16083v1,cs.LG,"cs.LG,q-fin.ST",data science,2024-12-20,2024-12-23T21:07:04.930827
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",data science,2024-12-20,2024-12-23T21:07:04.930827
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",data science,2024-12-20,2024-12-23T21:07:04.931825
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.932822
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",data science,2024-12-20,2024-12-23T21:07:04.932822
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",data science,2024-12-20,2024-12-23T21:07:04.933819
Correct implied volatility shapes and reliable pricing in the rough Heston model,"We use modifications of the Adams method and very fast and accurate
sinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and
Levendorski\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for
options of moderate and long maturities and strikes not very far from the spot,
thousands of prices can be calculated in several msec. with relative errors of
the order of 0.5\% and smaller running Matlab on a Mac with moderate
characteristics. We demonstrate that for the calibrated set of parameters in
Euch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility
surface is significantly flatter and fits the data very poorly, hence, the
calibration results in op.cit. is an example of the {\em ghost calibration}
(M.Boyarchenko and Levendorki\u{i}, Quantitative Finance 2015, v. 15): the
errors of the model and numerical method almost cancel one another. We explain
how calibration errors of this sort are generated by each of popular versions
of numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)
with prefixed parameters of a numerical method, resulting in spurious
volatility smiles and skews. We suggest a general {\em Conformal Bootstrap
principle} which allows one to avoid ghost calibration errors. We outline
schemes of application of Conformal Bootstrap principle and the method of the
paper to the design of accurate and fast calibration procedures.",339,2412.16067v1,q-fin.MF,"q-fin.MF,q-fin.CP,60-08, 60E10, 60G10, 60G22, 65C20, 65D30, 65G20, 91G20, 91G60",data science,2024-12-20,2024-12-23T21:07:04.934818
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",data science,2024-12-20,2024-12-23T21:07:04.935815
Examining Entropic Unbalanced Optimal Transport and Sinkhorn Divergences for Spatial Forecast Verification,"An optimal transport (OT) problem seeks to find the cheapest mapping between
two distributions with equal total density, given the cost of transporting
density from one place to another. Unbalanced OT allows for different total
density in each distribution. This is the typical setting for precipitation
forecast and observation data, when considering the densities as accumulated
rainfall, or intensity. In this work, entropic unbalanced OT and its associated
Sinkhorn divergence are examined as a spatial forecast verification method for
precipitation data. It offers many attractive features, such as morphing one
field into another, defining a distance between fields and providing feature
based optimal assignment. It is found that the Sinkhorn divergence is robust
against the common double penalty problem (a form of phase error), aligns with
expert assessments of model performance, and allows for a variety of novel
pictorial illustrations of error. It provides informative summary scores, and
has few limitations to its application. Combined, these findings place
unbalanced entropy regularised optimal transport and the Sinkhorn divergence as
an informative method which follows geometric intuition.",221,2412.16063v1,math.OC,math.OC,data science,2024-12-20,2024-12-23T21:07:04.936812
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,data science,2024-12-20,2024-12-23T21:07:04.937810
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",data science,2024-12-20,2024-12-23T21:07:04.937810
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",data science,2024-12-20,2024-12-23T21:07:04.938807
Rare multi-nucleon decays with the full data sets of the Majorana Demonstrator,"The Majorana Demonstrator was an ultra-low-background experiment designed for
neutrinoless double-beta decay ($0\nu\beta\beta$) investigation in $^{76}$Ge.
Located at the Sanford Underground Research Facility in Lead, South Dakota, the
Demonstrator utilized modular high-purity Ge detector arrays within shielded
vacuum cryostats, operating deep underground. The arrays, with a capacity of up
to 40.4 kg (27.2 kg enriched to $\sim 88\%$ in $^{76}$Ge), have accumulated the
full data set, totaling 64.5 kg yr of enriched active exposure and 27.4 kg yr
of exposure for natural detectors. Our updated search improves previously
explored three-nucleon decay modes in Ge isotopes, setting new half-life limits
of $1.27\times10^{26}$ years (90\% confidence level) for $^{76}$Ge($ppp$)
$\rightarrow$ $^{73}$Cu e$^+\pi^+\pi^+$ and $^{76}$Ge($ppn$) $\rightarrow$
$^{73}$Zn e$^+\pi^+$. The half-life limit for the invisible tri-proton decay
mode of $^{76}$Ge is found to be $1.4\times10^{25}$ yr. Furthermore, we have
updated limits for corresponding multi-nucleon decays.",330,2412.16047v1,nucl-ex,nucl-ex,data science,2024-12-20,2024-12-23T21:07:04.939804
Segmentation of arbitrary features in very high resolution remote sensing imagery,"Very high resolution (VHR) mapping through remote sensing (RS) imagery
presents a new opportunity to inform decision-making and sustainable practices
in countless domains. Efficient processing of big VHR data requires automated
tools applicable to numerous geographic regions and features. Contemporary RS
studies address this challenge by employing deep learning (DL) models for
specific datasets or features, which limits their applicability across
contexts.
  The present research aims to overcome this limitation by introducing
EcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.
EcoMapper fully automates processing of geospatial data, DL model training, and
inference. Models trained with EcoMapper successfully segmented two distinct
features in a real-world UAV dataset, achieving scores competitive with prior
studies which employed context-specific models.
  To evaluate EcoMapper, many additional models were trained on permutations of
principal field survey characteristics (FSCs). A relationship was discovered
allowing derivation of optimal ground sampling distance from feature size,
termed Cording Index (CI). A comprehensive methodology for field surveys was
developed to ensure DL methods can be applied effectively to collected data.
  The EcoMapper code accompanying this work is available at
https://github.com/hcording/ecomapper .",264,2412.16046v1,cs.CV,cs.CV,data science,2024-12-20,2024-12-23T21:07:04.940802
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",data science,2024-12-20,2024-12-23T21:07:04.941799
X-ray polarization of the magnetar 1E 1841-045 in outburst,"We report on IXPE and NuSTAR observations that began forty days following the
onset of the 2024 outburst of the magnetar 1E 1841-045, marking the first ever
IXPE observation of a magnetar in an enhanced state. Our spectropolarimetric
analysis indicates that a non-thermal double power-law (PL) spectral model can
fit the phase-averaged intensity data well, with the soft and hard components
dominating below and above around 5 keV, respectively. We find that the soft PL
exhibits a polarization degree (PD) of about 20% while the hard X-ray PL
displays a PD of about 50%; both components have a polarization angle (PA)
compatible with 0 degree. These results are supported through model-independent
polarization analysis which shows an increasing PD from about 15% to 70% in the
2-3 keV and 6-8 keV ranges, respectively, while the PA remains consistent with
0 degree. We find marginal evidence for variability in the polarization
properties with pulse phase, namely a higher PD at spin phases coinciding with
the peak in the hard X-ray pulse. We compare the hard X-ray PL to the
expectation from direct resonant inverse Compton scattering (RICS) and
secondary pair cascade synchrotron radiation from primary high-energy RICS
photons, finding that both can provide reasonable spectropolarimetric agreement
with the data, yet, the latter more naturally. Finally, we suggest that the
soft power law X-ray component may be emission emanating from a Comptonized
corona in the inner magnetosphere.",327,2412.16036v1,astro-ph.HE,astro-ph.HE,data science,2024-12-20,2024-12-23T21:07:04.942798
A Framework for Streaming Event-Log Prediction in Business Processes,"We present a Python-based framework for event-log prediction in streaming
mode, enabling predictions while data is being generated by a business process.
The framework allows for easy integration of streaming algorithms, including
language models like n-grams and LSTMs, and for combining these predictors
using ensemble methods.
  Using our framework, we conducted experiments on various well-known
process-mining data sets and compared classical batch with streaming mode.
Though, in batch mode, LSTMs generally achieve the best performance, there is
often an n-gram whose accuracy comes very close. Combining basic models in
ensemble methods can even outperform LSTMs. The value of basic models with
respect to LSTMs becomes even more apparent in streaming mode, where LSTMs
generally lack accuracy in the early stages of a prediction run, while basic
methods make sensible predictions immediately.",173,2412.16032v1,cs.AI,"cs.AI,cs.LG",data science,2024-12-20,2024-12-23T21:07:04.942798
Learning sparsity-promoting regularizers for linear inverse problems,"This paper introduces a novel approach to learning sparsity-promoting
regularizers for solving linear inverse problems. We develop a bilevel
optimization framework to select an optimal synthesis operator, denoted as $B$,
which regularizes the inverse problem while promoting sparsity in the solution.
The method leverages statistical properties of the underlying data and
incorporates prior knowledge through the choice of $B$. We establish the
well-posedness of the optimization problem, provide theoretical guarantees for
the learning process, and present sample complexity bounds. The approach is
demonstrated through examples, including compact perturbations of a known
operator and the problem of learning the mother wavelet, showcasing its
flexibility in incorporating prior knowledge into the regularization framework.
This work extends previous efforts in Tikhonov regularization by addressing
non-differentiable norms and proposing a data-driven approach for sparse
regularization in infinite dimensions.",181,2412.16031v1,stat.ML,"stat.ML,cs.LG,math.ST,stat.TH,65J22, 68T05",data science,2024-12-20,2024-12-23T21:07:04.943795
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,data science,2024-12-20,2024-12-23T21:07:04.944791
Dimension-8 operators in $W^+W^-$ production via gluon fusion,"We investigate the impact of dimension-8 operators on $W^+W^-$ production at
the LHC for the incoming gluon-gluon channel. To this end, we have identified
all dimension-8 CP-even operators contributing to the process in question, and
computed the corresponding tree-level helicity amplitudes for fully-leptonic
decays of the $W$ bosons. These are implemented in the program MCFM-RE, which
automatically incorporates the effect of a jet-veto to reduce the otherwise
overwhelming $t\bar t$ background. We find that, unless we break the hierarchy
of the effective field theory (EFT), the interference of the dimension-8
operators with the Standard Model is negligible across the considered
distributions. This justifies including the square of dimension-6 operators
when performing EFT fits with this channel. We then present new constraints on
CP-even and CP-odd dimension-6 operators within the EFT regime. Lastly, we
postulate a scenario in which the hierarchy of the EFT is broken, justified by
the strong constraints on dimension-6 operators from existing on-shell Higgs
data. In this scenario, we discuss the constraints that can be reasonably set
on CP-even dimension-8 operators with current and future data. We remark that
the effect of the jet-veto on the ability to constrain new physics in the
$W^+W^-$ channel is quite dramatic and must be properly taken into account.",309,2412.16020v1,hep-ph,"hep-ph,hep-ex",data science,2024-12-20,2024-12-23T21:07:04.945788
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",data science,2024-12-20,2024-12-23T21:07:04.945788
Discovery of a non-Hermitian phase transition in a bulk condensed-matter system,"Phase transitions are fundamental in nature. A small parameter change near a
critical point leads to a qualitative change in system properties. Across a
regular phase transition, the system remains in thermal equilibrium and,
therefore, experiences a change of static properties, like the emergence of a
magnetisation upon cooling a ferromagnet below the Curie temperature. When
driving a system far from equilibrium, novel, otherwise inaccessible quantum
states of matter may arise. Such states are typically non-Hermitian, that is,
their dynamics break time-reversal symmetry, a basic law of equilibrium
physics. Phase transitions in non-Hermitian systems are of fundamentally new
nature in that the dynamical behaviour rather than static properties may
undergo a qualitative change at a critical, here called exceptional point. Here
we experimentally realize a non-Hermitian phase transition in a bulk
condensed-matter system. Optical excitation creates charge carriers in the
ferromagnetic semiconductor EuO. In a temperature-dependent interplay with the
Hermitian transition to ferromagnetic order, a non-Hermitian change of the
relaxation dynamics occurs, manifesting in our time-resolved reflection data as
a transition from bi-exponential real to single-exponential complex decay. Our
theory models this behavior and predicts non-Hermitian phase transitions for a
large class of condensed-matter systems, where they may be exploited to
sensitively control bulk-dynamic properties.",289,2412.16012v1,cond-mat.str-el,cond-mat.str-el,data science,2024-12-20,2024-12-23T21:07:04.946786
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",climate science,2024-12-20,2024-12-23T21:07:05.683893
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,climate science,2024-12-20,2024-12-23T21:07:05.684890
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",climate science,2024-12-20,2024-12-23T21:07:05.685887
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",climate science,2024-12-20,2024-12-23T21:07:05.686885
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",climate science,2024-12-20,2024-12-23T21:07:05.687882
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.688880
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",climate science,2024-12-20,2024-12-23T21:07:05.688880
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",climate science,2024-12-20,2024-12-23T21:07:05.689877
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",climate science,2024-12-20,2024-12-23T21:07:05.690874
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,climate science,2024-12-20,2024-12-23T21:07:05.691871
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,climate science,2024-12-20,2024-12-23T21:07:05.692870
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",climate science,2024-12-20,2024-12-23T21:07:05.693867
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,climate science,2024-12-20,2024-12-23T21:07:05.693867
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",climate science,2024-12-20,2024-12-23T21:07:05.694863
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.694863
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,climate science,2024-12-20,2024-12-23T21:07:05.695861
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",climate science,2024-12-20,2024-12-23T21:07:05.696858
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",climate science,2024-12-20,2024-12-23T21:07:05.697855
First Constraint on the Diffuse Supernova Neutrino Background through the CE$ν$NS process from the LZ experiment,"We report the limits on the diffuse supernova neutrino background (DSNB) flux
and the fundamental DSNB parameters measured from the first science run of the
LUX-ZEPLIN (LZ) experiment, a dual-phase xenon detector located at the Sanford
Underground Research Facility in Lead, South Dakota, USA. This is the first
time the DSNB limit is measured through the process of the coherent elastic
neutrino-nucleus scattering (CE$\nu$NS). Using an exposure of 60~live days and
a fiducial mass of 5.5~t, the upper limit on the DSNB $\nu_x$ (each of
$\nu_\mu$, $\nu_\tau$, $\bar\nu_\mu$, $\bar\nu_\tau$) flux is
$686-826$~cm$^{-2}$s$^{-1}$ at the 90\% confidence level for neutrino energies
E$>$19.3~MeV, assuming the flux for each $\nu_x$ flavor is the same. The
interval accounts for the uncertainty in existing DSNB models. The present
result is comparable to the existing best limit and further improvements are
expected after collecting data from an estimated 1,000-day exposure in the
future.",281,2412.15886v1,hep-ex,hep-ex,climate science,2024-12-20,2024-12-23T21:07:05.697855
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.698852
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.699850
On the Power of Strategic Corpus Enrichment in Content Creation Games,"Search and recommendation ecosystems exhibit competition among content
creators. This competition has been tackled in a variety of game-theoretic
frameworks. Content creators generate documents with the aim of being
recommended by a content ranker for various information needs. In order for the
ecosystem, modeled as a content ranking game, to be effective and maximize user
welfare, it should guarantee stability, where stability is associated with the
existence of pure Nash equilibrium in the corresponding game. Moreover, if the
contents' ranking algorithm possesses a game in which any best-response
learning dynamics of the content creators converge to equilibrium of high
welfare, the system is considered highly attractive. However, as classical
content ranking algorithms, employed by search and recommendation systems, rank
documents by their distance to information needs, it has been shown that they
fail to provide such stability properties. As a result, novel content ranking
algorithms have been devised. In this work, we offer an alternative approach:
corpus enrichment with a small set of fixed dummy documents. It turns out that,
with the right design, such enrichment can lead to pure Nash equilibrium and
even to the convergence of any best-response dynamics to a high welfare result,
where we still employ the classical/current content ranking approach. We show
two such corpus enrichment techniques with tight bounds on the number of
documents needed to obtain the desired results. Interestingly, our study is a
novel extension of Borel's Colonel Blotto game.",287,2412.15878v1,cs.GT,cs.GT,climate science,2024-12-20,2024-12-23T21:07:05.700847
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",climate science,2024-12-20,2024-12-23T21:07:05.700847
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,climate science,2024-12-20,2024-12-23T21:07:05.701844
Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science,"Understanding how transportation networks work is important for improving
connectivity, efficiency, and safety. In Brazil, where road transport is a
significant portion of freight and passenger movement, network science can
provide valuable insights into the structural properties of the infrastructure,
thus helping decision makers responsible for proposing improvements to the
system. This paper models the federal road network as weighted networks, with
the intent to unveil its topological characteristics and identify key locations
(cities) that play important roles for the country through 75,000 kilometres of
roads. We start with a simple network to examine basic connectivity and
topology, where weights are the distance of the road segment. We then
incorporate other weights representing number of incidents, population, and
number of cities in-between each segment. We then focus on community detection
as a way to identify clusters of cities that form cohesive groups within a
network. Our findings aim to bring clarity to the overall structure of federal
roads in Brazil, thus providing actionable insights for improving
infrastructure planning and prioritising resources to enhance network
resilience.",208,2412.15865v1,physics.soc-ph,"physics.soc-ph,cs.CY",climate science,2024-12-20,2024-12-23T21:07:05.702842
Geographic distribution of the global agricultural workforce every decade for the years 2000-2100,"Agricultural workers play a vital role in the global economy and food
security by cultivating, transporting, and processing food for populations
worldwide. Despite their importance, detailed spatial data on the global
agricultural workforce have remained scarce. Here, we present a new gridded
dataset that maps the global distribution of agricultural workers for every
decade over the years 2000-2100, distributed at 0.083$\times$0.083 degrees
resolution, roughly $\sim$10km$\times$10km at the Equator. The dataset is
developed using an empirical modeling framework relying on generalized additive
mixed models (GAMMs) that integrate socioeconomic variables, including gross
domestic product per capita, total population, rural population size, and
agricultural land use. The predictions are consistent with Shared
Socio-economic Pathways and we distribute full time series data for all SSPs 1
to 5. This dataset opens new avenues for future research on labour force
health, productivity and risk, and could be very useful for developing
informed, forward-looking strategies that address the challenges of climate
resilience in agriculture. The dataset and code for reproducing it are
available for the user community [publicly available on publication at DOI:
10.5281/zenodo.14443333].",262,2412.15841v1,stat.AP,stat.AP,climate science,2024-12-20,2024-12-23T21:07:05.703839
Enriching Social Science Research via Survey Item Linking,"Questions within surveys, called survey items, are used in the social
sciences to study latent concepts, such as the factors influencing life
satisfaction. Instead of using explicit citations, researchers paraphrase the
content of the survey items they use in-text. However, this makes it
challenging to find survey items of interest when comparing related work.
Automatically parsing and linking these implicit mentions to survey items in a
knowledge base can provide more fine-grained references. We model this task,
called Survey Item Linking (SIL), in two stages: mention detection and entity
disambiguation. Due to an imprecise definition of the task, existing datasets
used for evaluating the performance for SIL are too small and of low-quality.
We argue that latent concepts and survey item mentions should be
differentiated. To this end, we create a high-quality and richly annotated
dataset consisting of 20,454 English and German sentences. By benchmarking deep
learning systems for each of the two stages independently and sequentially, we
demonstrate that the task is feasible, but observe that errors propagate from
the first stage, leading to a lower overall task performance. Moreover,
mentions that require the context of multiple sentences are more challenging to
identify for models in the first stage. Modeling the entire context of a
document and combining the two stages into an end-to-end system could mitigate
these problems in future work, and errors could additionally be reduced by
collecting more diverse data and by improving the quality of the knowledge
base. The data and code are available at https://github.com/e-tornike/SIL .",338,2412.15831v1,cs.DL,"cs.DL,cs.CL",climate science,2024-12-20,2024-12-23T21:07:05.703839
SUBMASSIVE: Resolving Subclass Cycles in Very Large Knowledge Graphs,"Large knowledge graphs capture information of a large number of entities and
their relations. Among the many relations they capture, class subsumption
assertions are usually present and expressed using the \texttt{rdfs:subClassOf}
construct. From our examination, publicly available knowledge graphs contain
many potentially erroneous cyclic subclass relations, a problem that can be
exacerbated when different knowledge graphs are integrated as Linked Open Data.
In this paper, we present an automatic approach for resolving such cycles at
scale using automated reasoning by encoding the problem of cycle-resolving to a
MAXSAT solver. The approach is tested on the LOD-a-lot dataset, and compared
against a semi-automatic version of our algorithm. We show how the number of
removed triples is a trade-off against the efficiency of the algorithm.",170,2412.15829v1,cs.LO,"cs.LO,cs.SC,math.OC,68T27, 68T20, 68T09,F.3.0; I.2.1; I.2.4",climate science,2024-12-20,2024-12-23T21:07:05.704836
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",climate science,2024-12-20,2024-12-23T21:07:05.705834
Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis,"Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.",134,2412.15814v1,cs.CR,"cs.CR,cs.DC,cs.LO",climate science,2024-12-20,2024-12-23T21:07:05.705834
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",climate science,2024-12-20,2024-12-23T21:07:05.706831
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",climate science,2024-12-20,2024-12-23T21:07:05.707828
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",climate science,2024-12-20,2024-12-23T21:07:05.708827
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,climate science,2024-12-20,2024-12-23T21:07:05.709824
Coexistence Options and Performance Analysis of 100 Gbit/s Coherent PON in Brownfield DWDM Networks,"We study system architectures for the coexistence of future coherent PON and
DWDM networks. Considering deployed optical filters, we observe filtering
penalties < 1dB at a laser frequency accuracy < 12GHz when using a
cost-effective architecture.",53,2412.15743v1,eess.SP,"eess.SP,cs.NI",climate science,2024-12-20,2024-12-23T21:07:05.709824
Distribution-Free Normal Modal Logics,"This article initiates the semantic study of distribution-free normal modal
logic systems, laying the semantic foundations and anticipating further
research in the area. The article explores roughly the same area, though taking
a different approach, with a recent article by Bezhanishvili, de Groot,
Dmitrieva and Morachini, who studied a distribution-free version of Dunn's
Positive Modal Logic (PML). Unlike PML, we consider logics that may drop
distribution and which are equipped with both an implication connective and
modal operators. We adopt a uniform relational semantics approach, relying on
recent results on representation and duality for normal lattice expansions. We
prove canonicity and completeness in the relational semantics of the minimal
distribution-free normal modal logic, assuming just the K-axiom, as well as of
its axiomatic extensions obtained by adding any of the D, T, B, S4 or S5
axioms. Adding distribution can be easily accommodated and, as a side result,
we also obtain a new semantic treatment of Intuitionistic Modal Logic.",224,2412.15736v1,cs.LO,"cs.LO,math.LO",climate science,2024-12-20,2024-12-23T21:07:05.710821
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.711818
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.711818
Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion,"This paper addresses the question of whether or not uncoupled online learning
algorithms converge to the Nash equilibrium in pricing competition or whether
they can learn to collude. Algorithmic collusion has been debated among
competition regulators, and it is a highly relevant phenomenon for buyers and
sellers on online retail platforms. We analyze formally if mean-based
algorithms, a class of bandit algorithms relevant to algorithmic pricing,
converge to the Nash equilibrium in repeated Bertrand oligopolies. Bandit
algorithms only learn the profit of the agent for the price set in each step.
In addition, we provide results of extensive experiments with different types
of multi-armed bandit algorithms used for algorithmic pricing. In a
mathematical proof, we show that mean-based algorithms converge to correlated
rational strategy profiles, which coincide with the Nash equilibrium in
versions of the Bertrand competition. Learning algorithms do not converge to a
Nash equilibrium in general, and the fact that Bertrand pricing games are
learnable with bandit algorithms is remarkable. Our numerical results suggest
that wide-spread bandit algorithms that are not mean-based also converge to
equilibrium and that algorithmic collusion only arises with symmetric
implementations of UCB or Q-learning, but not if different algorithms are used
by sellers. In addition, the level of supra-competitive prices decreases with
increasing numbers of sellers. Supra-competitive prices decrease consumer
welfare. If algorithms lead to algorithmic collusion, this is important for
consumers, sellers, and regulators to understand. We show that for the
important class of multi-armed bandit algorithms such fears are overrated
unless all sellers agree on a symmetric implementation of certain collusive
algorithms.",330,2412.15707v1,cs.GT,cs.GT,climate science,2024-12-20,2024-12-23T21:07:05.712815
High-efficiency fast pinching radiation of electron beams in nonuniform plasma,"The continuous development of bright x/gamma-ray sources has opened up new
frontiers of science and advanced applications. Currently, there is still a
lack of efficient approaches to produce gamma-rays with photon energies up to
GeV and with high peak brilliance comparable to modern free-electron lasers.
Here we report a novel mechanism called beam fast pinching radiation burst to
generate such gamma-ray sources. It is achieved by injecting a GeV electron
beam into a submillimeter plasma with an upramp density profile, enabling
violent beam pinching to occur rapidly. During this process, a burst of
collimated gamma-rays is efficiently produced with photon energy up to GeV,
energy conversion efficiency exceeding $30\%$, and peak brilliance exceeding
$10^{28}$ photons s$^{-1}$ mm$^{-2}$ mrad$^{-2}$ per $0.1\%$ bandwidth. All of
these are several orders of magnitude higher than existing gamma-ray sources.
This opens a novel avenue for the development of extremely bright gamma-ray
sources for both fundamental research and cutting-edge applications.",238,2412.15706v1,physics.plasm-ph,"physics.plasm-ph,physics.acc-ph",climate science,2024-12-20,2024-12-23T21:07:05.713812
Climate Impact Assessment Requires Weighting: Introducing the Weighted Climate Dataset,"High-resolution gridded climate data are readily available from multiple
sources, yet climate research and decision-making increasingly require country
and region-specific climate information weighted by socio-economic factors.
Moreover, the current landscape of disparate data sources and inconsistent
weighting methodologies exacerbates the reproducibility crisis and undermines
scientific integrity. To address these issues, we have developed a globally
comprehensive dataset at both country (GADM0) and region (GADM1) levels,
encompassing various climate indicators (precipitation, temperature, SPEI, wind
gust). Our methodology involves weighting gridded climate data by population
density, night-time light intensity, cropland area, and concurrent population
count -- all proxies for socio-economic activity -- before aggregation. We
process data from multiple sources, offering daily, monthly, and annual climate
variables spanning from 1900 to 2023. A unified framework streamlines our
preprocessing steps, and rigorous validation against leading climate impact
studies ensures data reliability. The resulting Weighted Climate Dataset is
publicly accessible through an online dashboard at
https://weightedclimatedata.streamlit.app/.",237,2412.15699v1,stat.AP,stat.AP,climate science,2024-12-20,2024-12-23T21:07:05.713812
High-Dimensional Bayesian Optimisation with Large-Scale Constraints via Latent Space Gaussian Processes,"Design optimisation offers the potential to develop lightweight aircraft
structures with reduced environmental impact. Due to the high number of design
variables and constraints, these challenges are typically addressed using
gradient-based optimisation methods to maintain efficiency. However, this
approach often results in a local solution, overlooking the global design
space. Moreover, gradients are frequently unavailable. Bayesian Optimisation
presents a promising alternative, enabling sample-efficient global optimisation
through probabilistic surrogate models that do not depend on gradients.
Although Bayesian Optimisation has shown its effectiveness for problems with a
small number of design variables, it struggles to scale to high-dimensional
problems, particularly when incorporating large-scale constraints. This
challenge is especially pronounced in aeroelastic tailoring, where directional
stiffness properties are integrated into the structural design to manage
aeroelastic deformations and enhance both aerodynamic and structural
performance. Ensuring the safe operation of the system requires simultaneously
addressing constraints from various analysis disciplines, making global design
space exploration even more complex. This study seeks to address this issue by
employing high-dimensional Bayesian Optimisation combined with a dimensionality
reduction technique to tackle the optimisation challenges in aeroelastic
tailoring. The proposed approach is validated through experiments on a
well-known benchmark case with black-box constraints, as well as its
application to the aeroelastic tailoring problem, demonstrating the feasibility
of Bayesian Optimisation for high-dimensional problems with large-scale
constraints.",301,2412.15679v1,cs.CE,cs.CE,climate science,2024-12-20,2024-12-23T21:07:05.714810
A District-level Ensemble Model to Enhance Dengue Prediction and Control for the Mekong Delta Region of Vietnam,"The Mekong Delta Region of Vietnam faces increasing dengue risks driven by
urbanization, globalization, and climate change. This study introduces a
probabilistic forecasting model for predicting dengue incidence and outbreaks
with one to three month lead times, integrating meteorological,
sociodemographic, preventive, and epidemiological data. Seventy-two models were
evaluated, and an ensemble combining top-performing spatiotemporal, supervised
PCA, and semi-mechanistic hhh4 frameworks was developed. Using data from
2004-2022 for training, validation, and evaluation, the ensemble model
demonstrated 69% accuracy at a 3-month horizon, outperforming a baseline model.
While effective, its performance declined in years with atypical seasonality,
such as 2019 and 2022. The model provides critical lead time for targeted
dengue prevention and control measures, addressing a growing public health need
in the region.",192,2412.15645v1,stat.AP,stat.AP,climate science,2024-12-20,2024-12-23T21:07:05.715807
Two-Dimensional Graphene: Theoretical Study of Multi-photon Non-linear Absorption Coefficient of a Strong Electromagnetic Wave by Using Quantum Kinetic Equation,"Based on the quantum kinetic equation for electrons, we theoretically study
the quantum multi-photon non-linear absorption of a strong electromagnetic wave
(EMW) in two-dimensional graphene. Two cases of the electron scattering
mechanism are considered: Electron-optical phonon scattering and
electron-acoustic phonon scattering. The general multi-photon absorption
coefficient is presented as a function of the temperature, the external
magnetic field, the photon energy and the amplitude of external EMW. These
analytical expressions for multi-photon non-linear absorption coefficient
(MNAC) are numerically calculated and the results are discussed in both the
absence and presence of a magnetic field perpendicular to the graphene sheet.
The results show that there is no absorption peak in the absence of the
magnetic field, which contrasts with previous results in 2D systems such as
quantum wells or superlattices. However, when there is a strong magnetic field
along the direction perpendicular to the 2D graphene, absorption spectral lines
appear consistent with the magneto-phonon resonance conditions. Our
calculations show that the MPA's effect is stronger than mono-photon
absorption. Besides, the quantum multi-photon non-linear absorption phenomenon
has been studied from low to high temperatures. This transcends the limits of
the classical BKE which is studied in the high-temperature domain. The
computational results show that the dependence of MNAC on the above quantities
is consistent with the previous theoretical investigation. Another novel
feature of this work is that the general analytic expression for MNAC shows the
Half Width at Half Maximum dependence on the magnetic field which is in good
agreement with the previous experimental observations. Thus, our estimation
might give a critical prediction for future experimental observations in 2D
graphene.",347,2412.15638v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",climate science,2024-12-20,2024-12-23T21:07:05.716804
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",climate science,2024-12-20,2024-12-23T21:07:05.716804
Room-temperature nonlinear transport and microwave rectification in antiferromagnetic MnBi$_2$Te$_4$ films,"The discovery of the nonlinear Hall effect provides an avenue for studying
the interplay among symmetry, topology, and phase transitions, with potential
applications in signal doubling and high-frequency rectification. However,
practical applications require devices fabricated on large area thin film as
well as room-temperature operation. Here, we demonstrate robust
room-temperature nonlinear transverse response and microwave rectification in
MnBi$_2$Te$_4$ films grown by molecular beam epitaxy. We observe multiple
sign-reversals in the nonlinear response by tuning the chemical potential.
Through theoretical analysis, we identify skew scattering and side jump,
arising from extrinsic spin-orbit scattering, as the main mechanisms underlying
the observed nonlinear signals. Furthermore, we demonstrate radio frequency
(RF) rectification in the range of 1-8 gigahertz at 300 K. These findings not
only enhance our understanding of the relationship between nonlinear response
and magnetism, but also expand the potential applications as energy harvesters
and detectors in high-frequency scenarios.",210,2412.15591v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,climate science,2024-12-20,2024-12-23T21:07:05.717802
J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM,"Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made
available the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image
archive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive
serves as a valuable resource for researchers and scholars interested in
deep-sea imagery. The dataset comprises images and videos of deep-sea
phenomena, predominantly of marine organisms, but also of the seafloor and
physical processes. In this study, we propose J-EDI QA, a benchmark for
understanding images of deep-sea organisms using a multimodal large language
model (LLM). The benchmark is comprised of 100 images, accompanied by questions
and answers with four options by JAMSTEC researchers for each image. The QA
pairs are provided in Japanese, and the benchmark assesses the ability to
understand deep-sea species in Japanese. In the evaluation presented in this
paper, OpenAI o1 achieved a 50% correct response rate. This result indicates
that even with the capabilities of state-of-the-art models as of December 2024,
deep-sea species comprehension is not yet at an expert level. Further advances
in deep-sea species-specific LLMs are therefore required.",277,2412.15574v1,cs.CV,cs.CV,climate science,2024-12-20,2024-12-23T21:07:05.718799
Thin films as practical quantum materials: a status quo and beyond,"Quantum materials have been in the limelight for several years now. These
materials exhibit intriguing quantum phenomena, which when harnessed properly,
promise extraordinary advancements across various scientific and technological
domains. To fully exploit their potential, it is imperative to synthesize such
quantum materials in thin film form so that they are compatible with
well-established device fabrication techniques. In this perspective, an
overview of the current status and future directions of thin film quantum
material synthesis is provided. The criteria for quantum materials are
discussed, as well as the many benefits of preparing them as thin films.
Prominent deposition techniques such as molecular beam epitaxy and chemical
vapor deposition are reviewed along with potential contenders. Despite
challenges, progress in thin film quantum material technology holds the
potential to realize practical devices with unprecedented functionalities.",158,2412.15565v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,climate science,2024-12-20,2024-12-23T21:07:05.718799
Spatial Clustering of Citizen Science Data Improves Downstream Species Distribution Models,"Citizen science biodiversity data present great opportunities for ecology and
conservation across vast spatial and temporal scales. However, the
opportunistic nature of these data lacks the sampling structure required by
modeling methodologies that address a pervasive challenge in ecological data
collection: imperfect detection, i.e., the likelihood of under-observing
species on field surveys. Occupancy modeling is an example of an approach that
accounts for imperfect detection by explicitly modeling the observation process
separately from the biological process of habitat selection. This produces
species distribution models that speak to the pattern of the species on a
landscape after accounting for imperfect detection in the data, rather than the
pattern of species observations corrupted by errors. To achieve this benefit,
occupancy models require multiple surveys of a site across which the site's
status (i.e., occupied or not) is assumed constant. Since citizen science data
are not collected under the required repeated-visit protocol, observations may
be grouped into sites post hoc. Existing approaches for constructing sites
discard some observations and/or consider only geographic distance and not
environmental similarity. In this study, we compare ten approaches for site
construction in terms of their impact on downstream species distribution models
for 31 bird species in Oregon, using observations recorded in the eBird
database. We find that occupancy models built on sites constructed by spatial
clustering algorithms perform better than existing alternatives.",280,2412.15559v1,cs.LG,cs.LG,climate science,2024-12-20,2024-12-23T21:07:05.719796
Influence of Phase Segregation on the Hysteresis of Perovskite Solar Cells,"Organic-inorganic hybrid perovskite solar cells (PSC) have demonstrated
impressive performance improvement. Among the various characteristics, the
time-dependent current-voltage (J-V) hysteresis allows a direct exploration of
various critical phenomena that affect the stability of PSCs. The hysteresis is
associated with various spatial heterogeneity-related phenomena, including
lifetime, bandgap, and phase segregation. We investigate these phenomena
through numerical simulations and quantify how the spatial non-uniformity in
the perovskite active layer impacts the hysteresis. Further, we correlate the
time dependent device degradation with the hysteresis trends in terms of ion
density and effective carrier lifetime.",149,2412.15558v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,climate science,2024-12-20,2024-12-23T21:07:05.719796
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",atmospheric science,2024-12-20,2024-12-23T21:07:06.521755
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,atmospheric science,2024-12-20,2024-12-23T21:07:06.522752
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",atmospheric science,2024-12-20,2024-12-23T21:07:06.523749
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",atmospheric science,2024-12-20,2024-12-23T21:07:06.524747
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",atmospheric science,2024-12-20,2024-12-23T21:07:06.524747
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.525746
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",atmospheric science,2024-12-20,2024-12-23T21:07:06.526742
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",atmospheric science,2024-12-20,2024-12-23T21:07:06.527739
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",atmospheric science,2024-12-20,2024-12-23T21:07:06.528737
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,atmospheric science,2024-12-20,2024-12-23T21:07:06.529734
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,atmospheric science,2024-12-20,2024-12-23T21:07:06.530731
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",atmospheric science,2024-12-20,2024-12-23T21:07:06.530731
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,atmospheric science,2024-12-20,2024-12-23T21:07:06.531728
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",atmospheric science,2024-12-20,2024-12-23T21:07:06.531728
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.532726
Three new hot hydrogen-deficient pre-white dwarfs,"We have detected three new hydrogen-deficient (H < 0.001 mass fraction)
pre-white dwarfs (WDs) with helium-dominated atmospheres. The first object is a
relatively cool PG1159 star (effective temperature Teff = 72,000 K) that has
the lowest surface gravity of any PG1159 star known (log g = 4.8). It is a
PG1159 star in the earliest pre-WD phase. The second object is a hot subdwarf O
(sdO) star (Teff = 50,000 K, log g = 5.3) with high carbon and oxygen
abundances. It is only the third known member of the recently established
CO-sdO spectral class, which comprises stars that are thought to be formed by a
merger of a disrupted low-mass CO WD with a higher-mass He WD. The third object
is one of the rare stars of spectral type O(He) (Teff = 90,000 K, log g = 5.5).",216,2412.15984v1,astro-ph.SR,astro-ph.SR,atmospheric science,2024-12-20,2024-12-23T21:07:06.533723
Adding interferometric lightning detection to the Pierre Auger Observatory,"The Pierre Auger Observatory has detected downward terrestrial gamma-ray
flashes (TGFs) with its Surface Detector. A key to understanding this
high-energy radiation in thunderstorms is to combine such measurements with
measurements of lightning processes in their earliest stages. With eleven
modified Auger Engineering Radio Array (AERA) stations we can build an
interferometric lightning detection array working in the bandwidth between 30 -
80 MHz inside the Surface Detector array to precisely measure lightning stepped
leaders in 3D. These measurements allow us to decipher the cause of TGFs and
clarify the reason for the observed high-energy particles in thunderstorms. We
will present the current status of the detection plans including the
configuration of the interferometric lightning detection array and the steps to
take as well as the reconstruction characteristics obtained with AERA.",166,2412.15972v1,astro-ph.IM,"astro-ph.IM,astro-ph.HE",atmospheric science,2024-12-20,2024-12-23T21:07:06.533723
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,atmospheric science,2024-12-20,2024-12-23T21:07:06.534721
Propagation of untwisting solar jets from the low-beta corona into the super-Alfvénic wind: Testing a solar origin scenario for switchbacks,"Parker Solar Probe's (PSP) discovery of the prevalence of switchbacks (SBs),
localised magnetic deflections in the nascent solar wind, has sparked interest
in uncovering their origins. A prominent theory suggests these SBs originate in
the lower corona through magnetic reconnection processes, closely linked to
solar jet phenomena. Jets are impulsive events, observed across scales and
solar atmosphere layers, associated with the release of magnetic twist and
helicity. This study examines whether self-consistent jets can form and
propagate into the super-Alfv\'enic wind, assesses the impact of distinct
Parker solar wind profiles on jet dynamics, and determines if jet-induced
magnetic untwisting waves display signatures typical of SBs. We employed
parametric 3D numerical MHD simulations using the ARMS code to model the
self-consistent generation of solar jets. Our study focuses on the propagation
of solar jets in distinct atmospheric plasma $\beta$ and Alfv\'en velocity
profiles, including a Parker solar wind. Our findings show that self-consistent
coronal jets can form and propagate into the super-Alfv\'enic wind. Notable
structures such as the leading Alfv\'enic wave and trailing dense-jet region
were consistently observed across diverse plasma $\beta$ atmospheres. The jet
propagation dynamics are significantly influenced by atmospheric variations,
with changes in Alfv\'en velocity profiles affecting the group velocity and
propagation ratio of the leading and trailing structures. U-loops, prevalent at
jet onset, do not persist in the low-$\beta$ corona, but magnetic untwisting
waves associated with jets show SB-like signatures. However, full-reversal SBs
were not observed. These findings may explain the absence of full reversal SBs
in the sub-Alfv\'enic wind and illustrate the propagation of magnetic
deflections through jet-like events, shedding light on possible SB formation
processes.",399,2412.15930v1,astro-ph.SR,"astro-ph.SR,physics.plasm-ph,physics.space-ph",atmospheric science,2024-12-20,2024-12-23T21:07:06.535717
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",atmospheric science,2024-12-20,2024-12-23T21:07:06.536715
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",atmospheric science,2024-12-20,2024-12-23T21:07:06.536715
First Constraint on the Diffuse Supernova Neutrino Background through the CE$ν$NS process from the LZ experiment,"We report the limits on the diffuse supernova neutrino background (DSNB) flux
and the fundamental DSNB parameters measured from the first science run of the
LUX-ZEPLIN (LZ) experiment, a dual-phase xenon detector located at the Sanford
Underground Research Facility in Lead, South Dakota, USA. This is the first
time the DSNB limit is measured through the process of the coherent elastic
neutrino-nucleus scattering (CE$\nu$NS). Using an exposure of 60~live days and
a fiducial mass of 5.5~t, the upper limit on the DSNB $\nu_x$ (each of
$\nu_\mu$, $\nu_\tau$, $\bar\nu_\mu$, $\bar\nu_\tau$) flux is
$686-826$~cm$^{-2}$s$^{-1}$ at the 90\% confidence level for neutrino energies
E$>$19.3~MeV, assuming the flux for each $\nu_x$ flavor is the same. The
interval accounts for the uncertainty in existing DSNB models. The present
result is comparable to the existing best limit and further improvements are
expected after collecting data from an estimated 1,000-day exposure in the
future.",281,2412.15886v1,hep-ex,hep-ex,atmospheric science,2024-12-20,2024-12-23T21:07:06.537712
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.537712
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.538710
On the Power of Strategic Corpus Enrichment in Content Creation Games,"Search and recommendation ecosystems exhibit competition among content
creators. This competition has been tackled in a variety of game-theoretic
frameworks. Content creators generate documents with the aim of being
recommended by a content ranker for various information needs. In order for the
ecosystem, modeled as a content ranking game, to be effective and maximize user
welfare, it should guarantee stability, where stability is associated with the
existence of pure Nash equilibrium in the corresponding game. Moreover, if the
contents' ranking algorithm possesses a game in which any best-response
learning dynamics of the content creators converge to equilibrium of high
welfare, the system is considered highly attractive. However, as classical
content ranking algorithms, employed by search and recommendation systems, rank
documents by their distance to information needs, it has been shown that they
fail to provide such stability properties. As a result, novel content ranking
algorithms have been devised. In this work, we offer an alternative approach:
corpus enrichment with a small set of fixed dummy documents. It turns out that,
with the right design, such enrichment can lead to pure Nash equilibrium and
even to the convergence of any best-response dynamics to a high welfare result,
where we still employ the classical/current content ranking approach. We show
two such corpus enrichment techniques with tight bounds on the number of
documents needed to obtain the desired results. Interestingly, our study is a
novel extension of Borel's Colonel Blotto game.",287,2412.15878v1,cs.GT,cs.GT,atmospheric science,2024-12-20,2024-12-23T21:07:06.539708
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",atmospheric science,2024-12-20,2024-12-23T21:07:06.539708
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,atmospheric science,2024-12-20,2024-12-23T21:07:06.540705
Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science,"Understanding how transportation networks work is important for improving
connectivity, efficiency, and safety. In Brazil, where road transport is a
significant portion of freight and passenger movement, network science can
provide valuable insights into the structural properties of the infrastructure,
thus helping decision makers responsible for proposing improvements to the
system. This paper models the federal road network as weighted networks, with
the intent to unveil its topological characteristics and identify key locations
(cities) that play important roles for the country through 75,000 kilometres of
roads. We start with a simple network to examine basic connectivity and
topology, where weights are the distance of the road segment. We then
incorporate other weights representing number of incidents, population, and
number of cities in-between each segment. We then focus on community detection
as a way to identify clusters of cities that form cohesive groups within a
network. Our findings aim to bring clarity to the overall structure of federal
roads in Brazil, thus providing actionable insights for improving
infrastructure planning and prioritising resources to enhance network
resilience.",208,2412.15865v1,physics.soc-ph,"physics.soc-ph,cs.CY",atmospheric science,2024-12-20,2024-12-23T21:07:06.541702
Time-dependent modelling of short-term variability in the TeV-blazar VER J0521+211 during the major flare in 2020,"The BL Lacertae object VER J0521+211 underwent a notable flaring episode in
February 2020. A short-term monitoring campaign, led by the MAGIC (Major
Atmospheric Gamma Imaging Cherenkov) collaboration, covering a wide energy
range from radio to very-high-energy (VHE, 100 GeV < E < 100 TeV) gamma rays
was organised to study its evolution. These observations resulted in a
consistent detection of the source over six consecutive nights in the VHE
gamma-ray domain. Combining these nightly observations with an extensive set of
multiwavelength data made modelling of the blazar's spectral energy
distribution (SED) possible during the flare. This modelling was performed with
a focus on two plausible emission mechanisms: i) a leptonic two-zone
synchrotron-self-Compton scenario, and ii) a lepto-hadronic one-zone scenario.
Both models effectively replicated the observed SED from radio to the VHE
gamma-ray band. Furthermore, by introducing a set of evolving parameters, both
models were successful in reproducing the evolution of the fluxes measured in
different bands throughout the observing campaign. Notably, the lepto-hadronic
model predicts enhanced photon and neutrino fluxes at ultra-high energies (E >
100 TeV). While the photon component, generated via decay of neutral pions, is
not directly observable as it is subject to intense pair production (and
therefore extinction) through interactions with the cosmic microwave background
photons, neutrino detectors (e.g. IceCube) can probe the predicted neutrino
component. Finally, the analysis of the gamma-ray spectra, as observed by MAGIC
and the Fermi-LAT telescopes, yielded a conservative 95\% confidence upper
limit of z \leq 0.244 for the redshift of this blazar.",396,2412.15836v1,astro-ph.HE,astro-ph.HE,atmospheric science,2024-12-20,2024-12-23T21:07:06.542702
AIFS-CRPS: Ensemble forecasting using a model trained with a loss function based on the Continuous Ranked Probability Score,"Over the last three decades, ensemble forecasts have become an integral part
of forecasting the weather. They provide users with more complete information
than single forecasts as they permit to estimate the probability of weather
events by representing the sources of uncertainties and accounting for the
day-to-day variability of error growth in the atmosphere. This paper presents a
novel approach to obtain a weather forecast model for ensemble forecasting with
machine-learning. AIFS-CRPS is a variant of the Artificial Intelligence
Forecasting System (AIFS) developed at ECMWF. Its loss function is based on a
proper score, the Continuous Ranked Probability Score (CRPS). For the loss, the
almost fair CRPS is introduced because it approximately removes the bias in the
score due to finite ensemble size yet avoids a degeneracy of the fair CRPS. The
trained model is stochastic and can generate as many exchangeable members as
desired and computationally feasible in inference. For medium-range forecasts
AIFS-CRPS outperforms the physics-based Integrated Forecasting System (IFS)
ensemble for the majority of variables and lead times. For subseasonal
forecasts, AIFS-CRPS outperforms the IFS ensemble before calibration and is
competitive with the IFS ensemble when forecasts are evaluated as anomalies to
remove the influence of model biases.",282,2412.15832v1,physics.ao-ph,physics.ao-ph,atmospheric science,2024-12-20,2024-12-23T21:07:06.543698
Enriching Social Science Research via Survey Item Linking,"Questions within surveys, called survey items, are used in the social
sciences to study latent concepts, such as the factors influencing life
satisfaction. Instead of using explicit citations, researchers paraphrase the
content of the survey items they use in-text. However, this makes it
challenging to find survey items of interest when comparing related work.
Automatically parsing and linking these implicit mentions to survey items in a
knowledge base can provide more fine-grained references. We model this task,
called Survey Item Linking (SIL), in two stages: mention detection and entity
disambiguation. Due to an imprecise definition of the task, existing datasets
used for evaluating the performance for SIL are too small and of low-quality.
We argue that latent concepts and survey item mentions should be
differentiated. To this end, we create a high-quality and richly annotated
dataset consisting of 20,454 English and German sentences. By benchmarking deep
learning systems for each of the two stages independently and sequentially, we
demonstrate that the task is feasible, but observe that errors propagate from
the first stage, leading to a lower overall task performance. Moreover,
mentions that require the context of multiple sentences are more challenging to
identify for models in the first stage. Modeling the entire context of a
document and combining the two stages into an end-to-end system could mitigate
these problems in future work, and errors could additionally be reduced by
collecting more diverse data and by improving the quality of the knowledge
base. The data and code are available at https://github.com/e-tornike/SIL .",338,2412.15831v1,cs.DL,"cs.DL,cs.CL",atmospheric science,2024-12-20,2024-12-23T21:07:06.544695
SUBMASSIVE: Resolving Subclass Cycles in Very Large Knowledge Graphs,"Large knowledge graphs capture information of a large number of entities and
their relations. Among the many relations they capture, class subsumption
assertions are usually present and expressed using the \texttt{rdfs:subClassOf}
construct. From our examination, publicly available knowledge graphs contain
many potentially erroneous cyclic subclass relations, a problem that can be
exacerbated when different knowledge graphs are integrated as Linked Open Data.
In this paper, we present an automatic approach for resolving such cycles at
scale using automated reasoning by encoding the problem of cycle-resolving to a
MAXSAT solver. The approach is tested on the LOD-a-lot dataset, and compared
against a semi-automatic version of our algorithm. We show how the number of
removed triples is a trade-off against the efficiency of the algorithm.",170,2412.15829v1,cs.LO,"cs.LO,cs.SC,math.OC,68T27, 68T20, 68T09,F.3.0; I.2.1; I.2.4",atmospheric science,2024-12-20,2024-12-23T21:07:06.544695
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",atmospheric science,2024-12-20,2024-12-23T21:07:06.545692
Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis,"Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.",134,2412.15814v1,cs.CR,"cs.CR,cs.DC,cs.LO",atmospheric science,2024-12-20,2024-12-23T21:07:06.546689
IXPE detection of highly polarized X-rays from the magnetar 1E 1841-045,"The Imaging X-ray Polarimetry Explorer (IXPE) observed for the first time
highly polarized X-ray emission from the magnetar 1E 1841-045, targeted after a
burst-active phase in August 2024. To date, IXPE has observed four other
magnetars during quiescent periods, highlighting substantially different
polarization properties. 1E 1841-045 exhibits a high, energy-dependent
polarization degree, which increases monotonically from ~15% at 2-3 keV up to
~55% at 5.5-8 keV, while the polarization angle, aligned with the celestial
North, remains fairly constant. The broadband spectrum (2-79 keV) obtained by
combining simultaneous IXPE and NuSTAR data is well modeled by a blackbody and
two power-law components. The unabsorbed 2-8 keV flux (~2E-11 erg/cm2/s) is
about 10% higher than that obtained from archival XMM-Newton and NuSTAR
observations. The polarization of the soft, thermal component does not exceed
~25%, and may be produced by a condensed surface or a bombarded atmosphere. The
intermediate power law is polarized at around 30%, consistent with predictions
for resonant Compton scattering in the star magnetosphere; while, the hard
power law exhibits a polarization degree exceeding 65%, pointing to a
synchrotron/curvature origin.",297,2412.15811v1,astro-ph.HE,astro-ph.HE,atmospheric science,2024-12-20,2024-12-23T21:07:06.547686
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",atmospheric science,2024-12-20,2024-12-23T21:07:06.548684
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",atmospheric science,2024-12-20,2024-12-23T21:07:06.548684
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",atmospheric science,2024-12-20,2024-12-23T21:07:06.549681
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,atmospheric science,2024-12-20,2024-12-23T21:07:06.550678
Distribution-Free Normal Modal Logics,"This article initiates the semantic study of distribution-free normal modal
logic systems, laying the semantic foundations and anticipating further
research in the area. The article explores roughly the same area, though taking
a different approach, with a recent article by Bezhanishvili, de Groot,
Dmitrieva and Morachini, who studied a distribution-free version of Dunn's
Positive Modal Logic (PML). Unlike PML, we consider logics that may drop
distribution and which are equipped with both an implication connective and
modal operators. We adopt a uniform relational semantics approach, relying on
recent results on representation and duality for normal lattice expansions. We
prove canonicity and completeness in the relational semantics of the minimal
distribution-free normal modal logic, assuming just the K-axiom, as well as of
its axiomatic extensions obtained by adding any of the D, T, B, S4 or S5
axioms. Adding distribution can be easily accommodated and, as a side result,
we also obtain a new semantic treatment of Intuitionistic Modal Logic.",224,2412.15736v1,cs.LO,"cs.LO,math.LO",atmospheric science,2024-12-20,2024-12-23T21:07:06.551676
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.551676
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.552673
Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion,"This paper addresses the question of whether or not uncoupled online learning
algorithms converge to the Nash equilibrium in pricing competition or whether
they can learn to collude. Algorithmic collusion has been debated among
competition regulators, and it is a highly relevant phenomenon for buyers and
sellers on online retail platforms. We analyze formally if mean-based
algorithms, a class of bandit algorithms relevant to algorithmic pricing,
converge to the Nash equilibrium in repeated Bertrand oligopolies. Bandit
algorithms only learn the profit of the agent for the price set in each step.
In addition, we provide results of extensive experiments with different types
of multi-armed bandit algorithms used for algorithmic pricing. In a
mathematical proof, we show that mean-based algorithms converge to correlated
rational strategy profiles, which coincide with the Nash equilibrium in
versions of the Bertrand competition. Learning algorithms do not converge to a
Nash equilibrium in general, and the fact that Bertrand pricing games are
learnable with bandit algorithms is remarkable. Our numerical results suggest
that wide-spread bandit algorithms that are not mean-based also converge to
equilibrium and that algorithmic collusion only arises with symmetric
implementations of UCB or Q-learning, but not if different algorithms are used
by sellers. In addition, the level of supra-competitive prices decreases with
increasing numbers of sellers. Supra-competitive prices decrease consumer
welfare. If algorithms lead to algorithmic collusion, this is important for
consumers, sellers, and regulators to understand. We show that for the
important class of multi-armed bandit algorithms such fears are overrated
unless all sellers agree on a symmetric implementation of certain collusive
algorithms.",330,2412.15707v1,cs.GT,cs.GT,atmospheric science,2024-12-20,2024-12-23T21:07:06.552673
High-efficiency fast pinching radiation of electron beams in nonuniform plasma,"The continuous development of bright x/gamma-ray sources has opened up new
frontiers of science and advanced applications. Currently, there is still a
lack of efficient approaches to produce gamma-rays with photon energies up to
GeV and with high peak brilliance comparable to modern free-electron lasers.
Here we report a novel mechanism called beam fast pinching radiation burst to
generate such gamma-ray sources. It is achieved by injecting a GeV electron
beam into a submillimeter plasma with an upramp density profile, enabling
violent beam pinching to occur rapidly. During this process, a burst of
collimated gamma-rays is efficiently produced with photon energy up to GeV,
energy conversion efficiency exceeding $30\%$, and peak brilliance exceeding
$10^{28}$ photons s$^{-1}$ mm$^{-2}$ mrad$^{-2}$ per $0.1\%$ bandwidth. All of
these are several orders of magnitude higher than existing gamma-ray sources.
This opens a novel avenue for the development of extremely bright gamma-ray
sources for both fundamental research and cutting-edge applications.",238,2412.15706v1,physics.plasm-ph,"physics.plasm-ph,physics.acc-ph",atmospheric science,2024-12-20,2024-12-23T21:07:06.554173
Is the Spin of the Black Hole in GX 339-4 Negative?,"We have studied the accreting black hole binary GX 339-4 using two highly
accurate broad-band X-ray data sets in very soft spectral states from
simultaneous NICER and NuSTAR observations. Simultaneous fitting of both data
sets with relativistic models of the disk, its Comptonization and reflection
allows us to relatively accurately determine the black-hole mass and spin, and
the distance and inclination. However, we find the measured values strongly
depend on the used disk model. With the widely used thin-disk Kerr models
kerrbb and kerrbb2 (which employ color corrections), we find relatively low
masses and strongly negative spins. Then, the models utilizing detailed disk
atmospheric spectra, bhspec and slimbh, predict moderately positive spins and
high masses. When adding a warm corona above the disk (as proposed before for
both AGNs and accreting binaries), we find the spin is weakly constrained, but
consistent with zero. In all cases, the fitted inclination is low,
$\approx$30-$34^\circ$. For the spin aligned with the binary orbit, the mass
function for this binary implies large values of the mass, consistent only with
those obtained with either slimbh or warm corona. We also test the disk models
for an assumed set of mass, distance and inclination. We find that, e.g.,
kerrbb yields values of the spin parameter lower than bhspec or slimbh by
$\sim$0.2-0.3. Our results confirm previously found strong disk-model
dependencies of the measured black-hole spin, now for a low-mass X-ray binary.",339,2412.15705v1,astro-ph.HE,astro-ph.HE,atmospheric science,2024-12-20,2024-12-23T21:07:06.554173
GraphDOP: Towards skilful data-driven medium-range weather forecasts learnt and initialised directly from observations,"We introduce GraphDOP, a new data-driven, end-to-end forecast system
developed at the European Centre for Medium-Range Weather Forecasts (ECMWF)
that is trained and initialised exclusively from Earth System observations,
with no physics-based (re)analysis inputs or feedbacks. GraphDOP learns the
correlations between observed quantities - such as brightness temperatures from
polar orbiters and geostationary satellites - and geophysical quantities of
interest (that are measured by conventional observations), to form a coherent
latent representation of Earth System state dynamics and physical processes,
and is capable of producing skilful predictions of relevant weather parameters
up to five days into the future.",142,2412.15687v1,physics.ao-ph,"physics.ao-ph,cs.LG",atmospheric science,2024-12-20,2024-12-23T21:07:06.555171
High-Dimensional Bayesian Optimisation with Large-Scale Constraints via Latent Space Gaussian Processes,"Design optimisation offers the potential to develop lightweight aircraft
structures with reduced environmental impact. Due to the high number of design
variables and constraints, these challenges are typically addressed using
gradient-based optimisation methods to maintain efficiency. However, this
approach often results in a local solution, overlooking the global design
space. Moreover, gradients are frequently unavailable. Bayesian Optimisation
presents a promising alternative, enabling sample-efficient global optimisation
through probabilistic surrogate models that do not depend on gradients.
Although Bayesian Optimisation has shown its effectiveness for problems with a
small number of design variables, it struggles to scale to high-dimensional
problems, particularly when incorporating large-scale constraints. This
challenge is especially pronounced in aeroelastic tailoring, where directional
stiffness properties are integrated into the structural design to manage
aeroelastic deformations and enhance both aerodynamic and structural
performance. Ensuring the safe operation of the system requires simultaneously
addressing constraints from various analysis disciplines, making global design
space exploration even more complex. This study seeks to address this issue by
employing high-dimensional Bayesian Optimisation combined with a dimensionality
reduction technique to tackle the optimisation challenges in aeroelastic
tailoring. The proposed approach is validated through experiments on a
well-known benchmark case with black-box constraints, as well as its
application to the aeroelastic tailoring problem, demonstrating the feasibility
of Bayesian Optimisation for high-dimensional problems with large-scale
constraints.",301,2412.15679v1,cs.CE,cs.CE,atmospheric science,2024-12-20,2024-12-23T21:07:06.556168
Two-Dimensional Graphene: Theoretical Study of Multi-photon Non-linear Absorption Coefficient of a Strong Electromagnetic Wave by Using Quantum Kinetic Equation,"Based on the quantum kinetic equation for electrons, we theoretically study
the quantum multi-photon non-linear absorption of a strong electromagnetic wave
(EMW) in two-dimensional graphene. Two cases of the electron scattering
mechanism are considered: Electron-optical phonon scattering and
electron-acoustic phonon scattering. The general multi-photon absorption
coefficient is presented as a function of the temperature, the external
magnetic field, the photon energy and the amplitude of external EMW. These
analytical expressions for multi-photon non-linear absorption coefficient
(MNAC) are numerically calculated and the results are discussed in both the
absence and presence of a magnetic field perpendicular to the graphene sheet.
The results show that there is no absorption peak in the absence of the
magnetic field, which contrasts with previous results in 2D systems such as
quantum wells or superlattices. However, when there is a strong magnetic field
along the direction perpendicular to the 2D graphene, absorption spectral lines
appear consistent with the magneto-phonon resonance conditions. Our
calculations show that the MPA's effect is stronger than mono-photon
absorption. Besides, the quantum multi-photon non-linear absorption phenomenon
has been studied from low to high temperatures. This transcends the limits of
the classical BKE which is studied in the high-temperature domain. The
computational results show that the dependence of MNAC on the above quantities
is consistent with the previous theoretical investigation. Another novel
feature of this work is that the general analytic expression for MNAC shows the
Half Width at Half Maximum dependence on the magnetic field which is in good
agreement with the previous experimental observations. Thus, our estimation
might give a critical prediction for future experimental observations in 2D
graphene.",347,2412.15638v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",atmospheric science,2024-12-20,2024-12-23T21:07:06.557166
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",atmospheric science,2024-12-20,2024-12-23T21:07:06.557166
Room-temperature nonlinear transport and microwave rectification in antiferromagnetic MnBi$_2$Te$_4$ films,"The discovery of the nonlinear Hall effect provides an avenue for studying
the interplay among symmetry, topology, and phase transitions, with potential
applications in signal doubling and high-frequency rectification. However,
practical applications require devices fabricated on large area thin film as
well as room-temperature operation. Here, we demonstrate robust
room-temperature nonlinear transverse response and microwave rectification in
MnBi$_2$Te$_4$ films grown by molecular beam epitaxy. We observe multiple
sign-reversals in the nonlinear response by tuning the chemical potential.
Through theoretical analysis, we identify skew scattering and side jump,
arising from extrinsic spin-orbit scattering, as the main mechanisms underlying
the observed nonlinear signals. Furthermore, we demonstrate radio frequency
(RF) rectification in the range of 1-8 gigahertz at 300 K. These findings not
only enhance our understanding of the relationship between nonlinear response
and magnetism, but also expand the potential applications as energy harvesters
and detectors in high-frequency scenarios.",210,2412.15591v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,atmospheric science,2024-12-20,2024-12-23T21:07:06.558164
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,environmental science,2024-12-20,2024-12-23T21:07:07.263846
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,environmental science,2024-12-20,2024-12-23T21:07:07.264843
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",environmental science,2024-12-20,2024-12-23T21:07:07.265840
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",environmental science,2024-12-20,2024-12-23T21:07:07.266837
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,environmental science,2024-12-20,2024-12-23T21:07:07.267835
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",environmental science,2024-12-20,2024-12-23T21:07:07.268832
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",environmental science,2024-12-20,2024-12-23T21:07:07.270827
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",environmental science,2024-12-20,2024-12-23T21:07:07.271824
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",environmental science,2024-12-20,2024-12-23T21:07:07.271824
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.272822
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",environmental science,2024-12-20,2024-12-23T21:07:07.273819
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",environmental science,2024-12-20,2024-12-23T21:07:07.273819
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",environmental science,2024-12-20,2024-12-23T21:07:07.274816
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,environmental science,2024-12-20,2024-12-23T21:07:07.276811
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",environmental science,2024-12-20,2024-12-23T21:07:07.277808
Electric Vehicle Charging Stations Placement Optimization in Vietnam Using Mixed-Integer Nonlinear Programming Model,"Vietnam is viewed as one of the promising markets for electric vehicles
(EVs), especially automobiles when it is predicted to reach 1 million in 2028
and 3.5 million in 2040. However, the lack of charging station infrastructure
has hindered the growth rate of EVs in this country. This study aims to propose
an optimization model using Mixed-Integer Nonlinear Programming (MINLP) to
implement an optimal location strategy for EVs charging stations in Ho Chi Minh
(HCM) City. The problem is solved by a solver named Gurobi and using the
Brand-and-Cut method. There are 2 perspectives including Charging Station
Operators and EV users. In addition, 7 kinds of costs considered include
installation cost, land rental cost, maintenance cost, operational cost,
charging cost, waiting cost, and traveling cost. From 1509 Point of Interest
and 199 residential areas, 134 POIs were chosen with 923 charging stations
including 592 Level-2 chargers and 331 Level-3 chargers to fully satisfy the
customer demand. Furthermore, the effectiveness of the proposed model is proved
by a minor MIP Gap and running in a short time with full feasibility.",234,2412.16025v1,cs.CE,cs.CE,environmental science,2024-12-20,2024-12-23T21:07:07.277808
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",environmental science,2024-12-20,2024-12-23T21:07:07.278805
"MAD-NG, a standalone multiplatform tool for linear and non-linear optics design and optimisation","The presentation will provide an overview of the capabilities of the
Methodical Accelerator Design Next Generation (MAD-NG) tool. MAD-NG is a
standalone, all-in-one, multi-platform tool well-suited for linear and
nonlinear optics design and optimization, and has already been used in
large-scale studies such as HiLumi-LHC or FCC-ee. It embeds LuaJIT, an
extremely fast tracing just-in-time compiler for the Lua programming language,
delivering exceptional versatility and performance for the forefront of
computational physics. The core of MAD-NG relies on the fast Generalized
Truncated Power Series Algebra (GTPSA) library, which has been specially
developed to handle many parameters and high-order differential algebra,
including Lie map operators. This ecosystem offers powerful features for the
analysis and optimization of linear and nonlinear optics, thanks to the fast
parametric nonlinear normal forms and the polyvalent matching command. A few
examples and results will complete this presentation of MAD-NG.",205,2412.16006v1,cs.CE,cs.CE,environmental science,2024-12-20,2024-12-23T21:07:07.279803
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",environmental science,2024-12-20,2024-12-23T21:07:07.279803
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.280800
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,environmental science,2024-12-20,2024-12-23T21:07:07.281797
Less is More: Towards Green Code Large Language Models via Unified Structural Pruning,"The extensive application of Large Language Models (LLMs) in generative
coding tasks has raised concerns due to their high computational demands and
energy consumption. Unlike previous structural pruning methods designed for
classification models that deal with lowdimensional classification logits,
generative Code LLMs produce high-dimensional token logit sequences, making
traditional pruning objectives inherently limited. Moreover, existing single
component pruning approaches further constrain the effectiveness when applied
to generative Code LLMs. In response, we propose Flab-Pruner, an innovative
unified structural pruning method that combines vocabulary, layer, and
Feed-Forward Network (FFN) pruning. This approach effectively reduces model
parameters while maintaining performance. Additionally, we introduce a
customized code instruction data strategy for coding tasks to enhance the
performance recovery efficiency of the pruned model. Through extensive
evaluations on three state-of-the-art Code LLMs across multiple generative
coding tasks, the results demonstrate that Flab-Pruner retains 97% of the
original performance after pruning 22% of the parameters and achieves the same
or even better performance after post-training. The pruned models exhibit
significant improvements in storage, GPU usage, computational efficiency, and
environmental impact, while maintaining well robustness. Our research provides
a sustainable solution for green software engineering and promotes the
efficient deployment of LLMs in real-world generative coding intelligence
applications.",294,2412.15921v1,cs.SE,"cs.SE,cs.AI",environmental science,2024-12-20,2024-12-23T21:07:07.282795
Dynamic heterogeneity in the self-induced spin glass state of elemental neodymium,"Spin glasses are magnetic materials exhibiting numerous magnetization
patterns, that randomly vary both in real space and in time. To date, it is
still not well understood what the nature of these spatiotemporal dynamics is,
namely if they are completely random or if there are links between given time
and length scales. Here we show the ubiquitous behavior of dynamic
heterogeneity in the self-induced spin glass state of elemental neodymium. We
used spin-polarized scanning tunneling microscopy in combination with atomistic
spin dynamics simulations to image the locally ordered magnetic patterns in the
glass state, and tracked the induced spatiotemporal dynamics in response to
external perturbations. We observed that the real space magnetization exhibited
a coexistence of slow and fast dynamics reminiscent of dynamic heterogeneity in
structural glasses. Furthermore, we found that zero-field cooling imprints a
specific set of metastable periodicities into the spin glass, which evolved
during aging and could be thermally reinitialized. These results demonstrate
the importance of local length scales for the understanding of aging dynamics
in spin glasses and provide a link to the more general picture of true glasses.",240,2412.15916v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.dis-nn,cond-mat.mes-hall",environmental science,2024-12-20,2024-12-23T21:07:07.282795
Topological junctions for one-dimensional systems,"We study and classify the emergence of protected edge modes at the junction
of one-dimensional materials. Using symmetries of Lagrangian planes in boundary
symplectic spaces, we present a novel proof of the periodic table of
topological insulators in one dimension. We show that edge modes necessarily
arise at the junction of two materials having different topological indices.
Our approach provides a systematic framework for understanding
symmetry-protected modes in one-dimension. It does not rely on periodic nor
ergodicity and covers a wide range of operators which includes both continuous
and discrete models.",117,2412.15887v1,math-ph,"math-ph,cond-mat.mtrl-sci,math.MP,34L40, 34B09, 53D12,",environmental science,2024-12-20,2024-12-23T21:07:07.283792
First Constraint on the Diffuse Supernova Neutrino Background through the CE$ν$NS process from the LZ experiment,"We report the limits on the diffuse supernova neutrino background (DSNB) flux
and the fundamental DSNB parameters measured from the first science run of the
LUX-ZEPLIN (LZ) experiment, a dual-phase xenon detector located at the Sanford
Underground Research Facility in Lead, South Dakota, USA. This is the first
time the DSNB limit is measured through the process of the coherent elastic
neutrino-nucleus scattering (CE$\nu$NS). Using an exposure of 60~live days and
a fiducial mass of 5.5~t, the upper limit on the DSNB $\nu_x$ (each of
$\nu_\mu$, $\nu_\tau$, $\bar\nu_\mu$, $\bar\nu_\tau$) flux is
$686-826$~cm$^{-2}$s$^{-1}$ at the 90\% confidence level for neutrino energies
E$>$19.3~MeV, assuming the flux for each $\nu_x$ flavor is the same. The
interval accounts for the uncertainty in existing DSNB models. The present
result is comparable to the existing best limit and further improvements are
expected after collecting data from an estimated 1,000-day exposure in the
future.",281,2412.15886v1,hep-ex,hep-ex,environmental science,2024-12-20,2024-12-23T21:07:07.283792
Direct measurement of the local electrocaloric effect in 2D ferroelectric In${}_2$Se${}_3$ by Scanning Electrocaloric Thermometry,"The electrocaloric effect refers to the temperature change in a material when
an electric field is applied or removed. Significant breakthroughs revealed its
potential for solid-state cooling technologies in past decades. These devices
offer a sustainable alternative to traditional vapor compression refrigeration,
with advantages such as compactness, silent operation, and the absence of
moving parts or refrigerants.
  Electrocaloric effects are typically studied using indirect methods using
polarization data, and which suffer from inaccuracies related to assumptions
about heat capacity. Direct methods, although more precise, require device
fabrication and face challenges in studying meso- or nanoscale systems, like 2D
materials, and materials with non-uniform polarization textures where high
spatial resolution is required.
  In this study, a novel technique, Scanning Electrocaloric Thermometry, is
introduced for characterizing the local electrocaloric effect in nanomaterials.
This approach achieves high spatial resolution by locally applying electric
fields and by simultaneously measuring the resulting temperature change. By
employing AC excitation, the measurement sensitivity is further enhanced and
the electrocaloric effect is disentangled from other heating mechanisms such as
Joule heating and dielectric losses. The effectiveness of the method is
demonstrated by examining electrocaloric and heat dissipation phenomena in
two-dimensional In${}_2$Se${}_3$ micrometer-sized flakes.",288,2412.15884v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.284789
Observation of distorted tilted conical phase at the surface of a bulk chiral magnet with resonant elastic x-ray scattering,"We report on various magnetic configurations including spirals and skyrmions
at the surface of the magnetic insulator Cu$_2$OSeO$_3$ at low temperatures
with a magnetic field applied along <100> using resonant elastic X-ray
scattering (REXS). We observe a well-ordered surface state referred to as a
distorted tilted conical spiral (TC) phase over a wide range of magnetic
fields. The distorted TC phase shows characteristic higher harmonic magnetic
satellites in the REXS reciprocal space maps. Skyrmions emerge following static
magnetic field cycling and appear to coexist with the distorted TC phase. Our
results indicate that this phase represents a distinct and stable surface state
that does not disappear with field cycling and persists until the field
strength is increased sufficiently to create the field-polarized state.",166,2412.15882v1,cond-mat.str-el,"cond-mat.str-el,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.285787
On the Power of Strategic Corpus Enrichment in Content Creation Games,"Search and recommendation ecosystems exhibit competition among content
creators. This competition has been tackled in a variety of game-theoretic
frameworks. Content creators generate documents with the aim of being
recommended by a content ranker for various information needs. In order for the
ecosystem, modeled as a content ranking game, to be effective and maximize user
welfare, it should guarantee stability, where stability is associated with the
existence of pure Nash equilibrium in the corresponding game. Moreover, if the
contents' ranking algorithm possesses a game in which any best-response
learning dynamics of the content creators converge to equilibrium of high
welfare, the system is considered highly attractive. However, as classical
content ranking algorithms, employed by search and recommendation systems, rank
documents by their distance to information needs, it has been shown that they
fail to provide such stability properties. As a result, novel content ranking
algorithms have been devised. In this work, we offer an alternative approach:
corpus enrichment with a small set of fixed dummy documents. It turns out that,
with the right design, such enrichment can lead to pure Nash equilibrium and
even to the convergence of any best-response dynamics to a high welfare result,
where we still employ the classical/current content ranking approach. We show
two such corpus enrichment techniques with tight bounds on the number of
documents needed to obtain the desired results. Interestingly, our study is a
novel extension of Borel's Colonel Blotto game.",287,2412.15878v1,cs.GT,cs.GT,environmental science,2024-12-20,2024-12-23T21:07:07.285787
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",environmental science,2024-12-20,2024-12-23T21:07:07.286784
Controlled polymorphic competition -- a path to tough and hard ceramics,"From nanoscale devices including sensors, electronics, or biocompatible
coatings to macroscale structural, automotive or aerospace components,
fundamental understanding of plasticity and fracture can guide the realization
of materials that ensure safe and durable performance. Identifying the role of
atomic-scale plasticity is crucial, especially for applications relying on
brittle ceramics. Here, stress-intensity-controlled atomistic simulations of
fracture in cubic Ti$_{1-x}$Al$_{x}$N model systems demonstrate how
$\overset{\lower.5em\circ}{\mathrm{A}}$-scale plasticity - manifested as
lattice distortions, phase transformation, nucleation and emission of
dislocations - substantially affects the macroscale fracture toughness
(K$_{Ic}$) and fracture strength (${\sigma}$$_{f}$) of brittle ceramics. The
extent of plastic deformation in Ti$_{1-x}$Al$_{x}$N increases monotonically
with the Al content (x), due to a corresponding decrease in cubic $\rightarrow$
hexagonal polymorph transition energy. Overall, plasticity positively affects
the mechanical properties, resulting in optimal combinations of strength and
toughness for x~0.6. However, for x exceeding ~0.7, the benefits of plasticity
diminish. The initial rise followed by a decline in K$_{Ic}$(x) and
${\sigma}$$_{f}$(x) is explained based on the interplay between phase
transformation and tensile cleavage on the easiest fracture plane. The results
highlight the impact of atomic-scale plasticity on observable properties and
point to strategies for toughening ceramics through control of polymorph
competition.",382,2412.15874v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,environmental science,2024-12-20,2024-12-23T21:07:07.287781
Understanding the Structure and Resilience of the Brazilian Federal Road Network Through Network Science,"Understanding how transportation networks work is important for improving
connectivity, efficiency, and safety. In Brazil, where road transport is a
significant portion of freight and passenger movement, network science can
provide valuable insights into the structural properties of the infrastructure,
thus helping decision makers responsible for proposing improvements to the
system. This paper models the federal road network as weighted networks, with
the intent to unveil its topological characteristics and identify key locations
(cities) that play important roles for the country through 75,000 kilometres of
roads. We start with a simple network to examine basic connectivity and
topology, where weights are the distance of the road segment. We then
incorporate other weights representing number of incidents, population, and
number of cities in-between each segment. We then focus on community detection
as a way to identify clusters of cities that form cohesive groups within a
network. Our findings aim to bring clarity to the overall structure of federal
roads in Brazil, thus providing actionable insights for improving
infrastructure planning and prioritising resources to enhance network
resilience.",208,2412.15865v1,physics.soc-ph,"physics.soc-ph,cs.CY",environmental science,2024-12-20,2024-12-23T21:07:07.288779
Efficient Curation of Invertebrate Image Datasets Using Feature Embeddings and Automatic Size Comparison,"The amount of image datasets collected for environmental monitoring purposes
has increased in the past years as computer vision assisted methods have gained
interest. Computer vision applications rely on high-quality datasets, making
data curation important. However, data curation is often done ad-hoc and the
methods used are rarely published. We present a method for curating large-scale
image datasets of invertebrates that contain multiple images of the same taxa
and/or specimens and have relatively uniform background in the images. Our
approach is based on extracting feature embeddings with pretrained deep neural
networks, and using these embeddings to find visually most distinct images by
comparing their embeddings to the group prototype embedding. Also, we show that
a simple area-based size comparison approach is able to find a lot of common
erroneous images, such as images containing detached body parts and
misclassified samples. In addition to the method, we propose using novel
metrics for evaluating human-in-the-loop outlier detection methods. The
implementations of the proposed curation methods, as well as a benchmark
dataset containing annotated erroneous images, are publicly available in
https://github.com/mikkoim/taxonomist-studio.",266,2412.15844v1,cs.CV,cs.CV,environmental science,2024-12-20,2024-12-23T21:07:07.288779
Enriching Social Science Research via Survey Item Linking,"Questions within surveys, called survey items, are used in the social
sciences to study latent concepts, such as the factors influencing life
satisfaction. Instead of using explicit citations, researchers paraphrase the
content of the survey items they use in-text. However, this makes it
challenging to find survey items of interest when comparing related work.
Automatically parsing and linking these implicit mentions to survey items in a
knowledge base can provide more fine-grained references. We model this task,
called Survey Item Linking (SIL), in two stages: mention detection and entity
disambiguation. Due to an imprecise definition of the task, existing datasets
used for evaluating the performance for SIL are too small and of low-quality.
We argue that latent concepts and survey item mentions should be
differentiated. To this end, we create a high-quality and richly annotated
dataset consisting of 20,454 English and German sentences. By benchmarking deep
learning systems for each of the two stages independently and sequentially, we
demonstrate that the task is feasible, but observe that errors propagate from
the first stage, leading to a lower overall task performance. Moreover,
mentions that require the context of multiple sentences are more challenging to
identify for models in the first stage. Modeling the entire context of a
document and combining the two stages into an end-to-end system could mitigate
these problems in future work, and errors could additionally be reduced by
collecting more diverse data and by improving the quality of the knowledge
base. The data and code are available at https://github.com/e-tornike/SIL .",338,2412.15831v1,cs.DL,"cs.DL,cs.CL",environmental science,2024-12-20,2024-12-23T21:07:07.289776
SUBMASSIVE: Resolving Subclass Cycles in Very Large Knowledge Graphs,"Large knowledge graphs capture information of a large number of entities and
their relations. Among the many relations they capture, class subsumption
assertions are usually present and expressed using the \texttt{rdfs:subClassOf}
construct. From our examination, publicly available knowledge graphs contain
many potentially erroneous cyclic subclass relations, a problem that can be
exacerbated when different knowledge graphs are integrated as Linked Open Data.
In this paper, we present an automatic approach for resolving such cycles at
scale using automated reasoning by encoding the problem of cycle-resolving to a
MAXSAT solver. The approach is tested on the LOD-a-lot dataset, and compared
against a semi-automatic version of our algorithm. We show how the number of
removed triples is a trade-off against the efficiency of the algorithm.",170,2412.15829v1,cs.LO,"cs.LO,cs.SC,math.OC,68T27, 68T20, 68T09,F.3.0; I.2.1; I.2.4",environmental science,2024-12-20,2024-12-23T21:07:07.290773
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",environmental science,2024-12-20,2024-12-23T21:07:07.291771
Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis,"Stablecoins are digital assets designed to maintain a stable value, typically
pegged to traditional currencies. Despite their growing prominence, many
stablecoins have struggled to consistently meet stability expectations, and
their underlying mechanisms often remain opaque and challenging to analyze.
This paper focuses on the DAI stablecoin, which combines
crypto-collateralization and algorithmic mechanisms. We propose a formal
logic-based framework for representing the policies and operations of DAI,
implemented in Prolog and released as open-source software. Our framework
enables detailed analysis and simulation of DAI's stability mechanisms,
providing a foundation for understanding its robustness and identifying
potential vulnerabilities.",134,2412.15814v1,cs.CR,"cs.CR,cs.DC,cs.LO",environmental science,2024-12-20,2024-12-23T21:07:07.292770
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",environmental science,2024-12-20,2024-12-23T21:07:07.293767
A detailed examination of polysilicon resistivity incorporating the grain size distribution,"Current transport in polysilicon is a complicated process with many factors
to consider. The inhomogeneous nature of polysilicon with its differently
shaped and sized grains is one such consideration. We have developed a method
that enhances existing resistivity models with a two-dimensional extension that
incorporates the grain size distribution using a Voronoi-based resistor
network. We obtain grain size distributions both from our growth simulations
(700 K, 800 K, and 900 K) and experimental analysis. Applying our method, we
investigate the effect that variation in grain size produces with cases of
different average grain sizes (2 nm to 3 $\mu$m). For example, the resistivity
of polysilicon with an average grain size of 175 nm drops from 11 k$\Omega$
$\cdot$ cm to 4.5 k$\Omega$ $\cdot$ cm when compared to conventional
one-dimensional modeling. Our study highlights the strong effect of grain size
variation on resistivity, revealing that wider distributions result in
significant resistivity reductions of up to more than 50%. Due to the larger
grains present with a grain size distribution, current transport encounters
fewer grain boundaries while the average grain size remains the same resulting
in fewer barriers along the current transport path. Incorporating the grain
structure into the resistivity modeling facilitates a more detailed and
comprehensive characterization of the electrical properties of polysilicon.",282,2412.15784v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,physics.comp-ph",environmental science,2024-12-20,2024-12-23T21:07:07.294763
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",environmental science,2024-12-20,2024-12-23T21:07:07.295762
Dynamic Learning Rate Decay for Stochastic Variational Inference,"Like many optimization algorithms, Stochastic Variational Inference (SVI) is
sensitive to the choice of the learning rate. If the learning rate is too
small, the optimization process may be slow, and the algorithm might get stuck
in local optima. On the other hand, if the learning rate is too large, the
algorithm may oscillate or diverge, failing to converge to a solution. Adaptive
learning rate methods such as Adam, AdaMax, Adagrad, or RMSprop automatically
adjust the learning rate based on the history of gradients. Nevertheless, if
the base learning rate is too large, the variational parameters might still
oscillate around the optimal solution. With learning rate schedules, the
learning rate can be reduced gradually to mitigate this problem. However, the
amount at which the learning rate should be decreased in each iteration is not
known a priori, which can significantly impact the performance of the
optimization. In this work, we propose a method to decay the learning rate
based on the history of the variational parameters. We use an empirical measure
to quantify the amount of oscillations against the progress of the variational
parameters to adapt the learning rate. The approach requires little memory and
is computationally efficient. We demonstrate in various numerical examples that
our method reduces the sensitivity of the optimization performance to the
learning rate and that it can also be used in combination with other adaptive
learning rate methods.",290,2412.15745v1,cs.CE,cs.CE,environmental science,2024-12-20,2024-12-23T21:07:07.296758
Distribution-Free Normal Modal Logics,"This article initiates the semantic study of distribution-free normal modal
logic systems, laying the semantic foundations and anticipating further
research in the area. The article explores roughly the same area, though taking
a different approach, with a recent article by Bezhanishvili, de Groot,
Dmitrieva and Morachini, who studied a distribution-free version of Dunn's
Positive Modal Logic (PML). Unlike PML, we consider logics that may drop
distribution and which are equipped with both an implication connective and
modal operators. We adopt a uniform relational semantics approach, relying on
recent results on representation and duality for normal lattice expansions. We
prove canonicity and completeness in the relational semantics of the minimal
distribution-free normal modal logic, assuming just the K-axiom, as well as of
its axiomatic extensions obtained by adding any of the D, T, B, S4 or S5
axioms. Adding distribution can be easily accommodated and, as a side result,
we also obtain a new semantic treatment of Intuitionistic Modal Logic.",224,2412.15736v1,cs.LO,"cs.LO,math.LO",environmental science,2024-12-20,2024-12-23T21:07:07.296758
Electrically-tunable ultra-flat bands and $π$-electron magnetism in graphene nanoribbons,"Atomically thin crystals hosting flat electronic bands have been recently
identified as a rich playground for exploring and engineering strongly
correlated phases. Yet, their variety remains limited, primarily to
two-dimensional moir\'e superlattices. Here, we predict the formation of
reversible, electrically-induced ultra-flat bands and $\pi$-electron magnetism
in one-dimensional chevron graphene nanoribbons. Our $ab$ $initio$ calculations
show that the application of a transverse electric field to these nanoribbons
generates a pair of isolated, nearly perfectly flat bands with widths of
approximately 1 meV around the Fermi level. Upon charge doping, these flat
bands undergo a Stoner-like electronic instability, resulting in the
spontaneous emergence of local magnetic moments at the edges of the otherwise
non-magnetic nanoribbon, akin to a one-dimensional spin-$\frac{1}{2}$ chain.
Our findings expand the class of carbon-based nanostructures exhibiting flat
bands and establish a novel route for inducing correlated electronic phases in
chevron graphene nanoribbons.",233,2412.15729v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.297756
Active nitrogen flux measurement during GaN growth based on the transmitted signal detected with a pyrometer,"A novel approach for the measurement of the Nitrogen active species generated
by a plasma source in the molecular beam epitaxy environment is here presented.
The method is based on the analysis of the variations in the optical signal
measured by a pyrometer during a two step, Gallium rich and Nitrogen
controlled, growth modes. The method permits a precise, quantitative and direct
measurement of the flux of active species as a function of the plasma
generation parameters of the cell: nitrogen gas flux and RF-power.",101,2412.15710v1,physics.ins-det,"physics.ins-det,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.298755
Online Optimization Algorithms in Repeated Price Competition: Equilibrium Learning and Algorithmic Collusion,"This paper addresses the question of whether or not uncoupled online learning
algorithms converge to the Nash equilibrium in pricing competition or whether
they can learn to collude. Algorithmic collusion has been debated among
competition regulators, and it is a highly relevant phenomenon for buyers and
sellers on online retail platforms. We analyze formally if mean-based
algorithms, a class of bandit algorithms relevant to algorithmic pricing,
converge to the Nash equilibrium in repeated Bertrand oligopolies. Bandit
algorithms only learn the profit of the agent for the price set in each step.
In addition, we provide results of extensive experiments with different types
of multi-armed bandit algorithms used for algorithmic pricing. In a
mathematical proof, we show that mean-based algorithms converge to correlated
rational strategy profiles, which coincide with the Nash equilibrium in
versions of the Bertrand competition. Learning algorithms do not converge to a
Nash equilibrium in general, and the fact that Bertrand pricing games are
learnable with bandit algorithms is remarkable. Our numerical results suggest
that wide-spread bandit algorithms that are not mean-based also converge to
equilibrium and that algorithmic collusion only arises with symmetric
implementations of UCB or Q-learning, but not if different algorithms are used
by sellers. In addition, the level of supra-competitive prices decreases with
increasing numbers of sellers. Supra-competitive prices decrease consumer
welfare. If algorithms lead to algorithmic collusion, this is important for
consumers, sellers, and regulators to understand. We show that for the
important class of multi-armed bandit algorithms such fears are overrated
unless all sellers agree on a symmetric implementation of certain collusive
algorithms.",330,2412.15707v1,cs.GT,cs.GT,environmental science,2024-12-20,2024-12-23T21:07:07.299749
High-efficiency fast pinching radiation of electron beams in nonuniform plasma,"The continuous development of bright x/gamma-ray sources has opened up new
frontiers of science and advanced applications. Currently, there is still a
lack of efficient approaches to produce gamma-rays with photon energies up to
GeV and with high peak brilliance comparable to modern free-electron lasers.
Here we report a novel mechanism called beam fast pinching radiation burst to
generate such gamma-ray sources. It is achieved by injecting a GeV electron
beam into a submillimeter plasma with an upramp density profile, enabling
violent beam pinching to occur rapidly. During this process, a burst of
collimated gamma-rays is efficiently produced with photon energy up to GeV,
energy conversion efficiency exceeding $30\%$, and peak brilliance exceeding
$10^{28}$ photons s$^{-1}$ mm$^{-2}$ mrad$^{-2}$ per $0.1\%$ bandwidth. All of
these are several orders of magnitude higher than existing gamma-ray sources.
This opens a novel avenue for the development of extremely bright gamma-ray
sources for both fundamental research and cutting-edge applications.",238,2412.15706v1,physics.plasm-ph,"physics.plasm-ph,physics.acc-ph",environmental science,2024-12-20,2024-12-23T21:07:07.299749
High-Dimensional Bayesian Optimisation with Large-Scale Constraints via Latent Space Gaussian Processes,"Design optimisation offers the potential to develop lightweight aircraft
structures with reduced environmental impact. Due to the high number of design
variables and constraints, these challenges are typically addressed using
gradient-based optimisation methods to maintain efficiency. However, this
approach often results in a local solution, overlooking the global design
space. Moreover, gradients are frequently unavailable. Bayesian Optimisation
presents a promising alternative, enabling sample-efficient global optimisation
through probabilistic surrogate models that do not depend on gradients.
Although Bayesian Optimisation has shown its effectiveness for problems with a
small number of design variables, it struggles to scale to high-dimensional
problems, particularly when incorporating large-scale constraints. This
challenge is especially pronounced in aeroelastic tailoring, where directional
stiffness properties are integrated into the structural design to manage
aeroelastic deformations and enhance both aerodynamic and structural
performance. Ensuring the safe operation of the system requires simultaneously
addressing constraints from various analysis disciplines, making global design
space exploration even more complex. This study seeks to address this issue by
employing high-dimensional Bayesian Optimisation combined with a dimensionality
reduction technique to tackle the optimisation challenges in aeroelastic
tailoring. The proposed approach is validated through experiments on a
well-known benchmark case with black-box constraints, as well as its
application to the aeroelastic tailoring problem, demonstrating the feasibility
of Bayesian Optimisation for high-dimensional problems with large-scale
constraints.",301,2412.15679v1,cs.CE,cs.CE,environmental science,2024-12-20,2024-12-23T21:07:07.300747
SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control,"Synthesizing natural human motion that adapts to complex environments while
allowing creative control remains a fundamental challenge in motion synthesis.
Existing models often fall short, either by assuming flat terrain or lacking
the ability to control motion semantics through text. To address these
limitations, we introduce SCENIC, a diffusion model designed to generate human
motion that adapts to dynamic terrains within virtual scenes while enabling
semantic control through natural language. The key technical challenge lies in
simultaneously reasoning about complex scene geometry while maintaining text
control. This requires understanding both high-level navigation goals and
fine-grained environmental constraints. The model must ensure physical
plausibility and precise navigation across varied terrain, while also
preserving user-specified text control, such as ``carefully stepping over
obstacles"" or ``walking upstairs like a zombie."" Our solution introduces a
hierarchical scene reasoning approach. At its core is a novel scene-dependent,
goal-centric canonicalization that handles high-level goal constraint, and is
complemented by an ego-centric distance field that captures local geometric
details. This dual representation enables our model to generate physically
plausible motion across diverse 3D scenes. By implementing frame-wise text
alignment, our system achieves seamless transitions between different motion
styles while maintaining scene constraints. Experiments demonstrate our novel
diffusion model generates arbitrarily long human motions that both adapt to
complex scenes with varying terrain surfaces and respond to textual prompts.
Additionally, we show SCENIC can generalize to four real-scene datasets. Our
code, dataset, and models will be released at
\url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.",338,2412.15664v1,cs.CV,cs.CV,environmental science,2024-12-20,2024-12-23T21:07:07.301744
Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning,"In multi-agent reinforcement learning (MARL), the centralized training with
decentralized execution (CTDE) framework has gained widespread adoption due to
its strong performance. However, the further development of CTDE faces two key
challenges. First, agents struggle to autonomously assess the relevance of
input information for cooperative tasks, impairing their decision-making
abilities. Second, in communication-limited scenarios with partial
observability, agents are unable to access global information, restricting
their ability to collaborate effectively from a global perspective. To address
these challenges, we introduce a novel cooperative MARL framework based on
information selection and tacit learning. In this framework, agents gradually
develop implicit coordination during training, enabling them to infer the
cooperative behavior of others in a discrete space without communication,
relying solely on local information. Moreover, we integrate gating and
selection mechanisms, allowing agents to adaptively filter information based on
environmental changes, thereby enhancing their decision-making capabilities.
Experiments on popular MARL benchmarks show that our framework can be
seamlessly integrated with state-of-the-art algorithms, leading to significant
performance improvements.",227,2412.15639v1,cs.MA,"cs.MA,cs.AI,cs.LG",environmental science,2024-12-20,2024-12-23T21:07:07.302741
Two-Dimensional Graphene: Theoretical Study of Multi-photon Non-linear Absorption Coefficient of a Strong Electromagnetic Wave by Using Quantum Kinetic Equation,"Based on the quantum kinetic equation for electrons, we theoretically study
the quantum multi-photon non-linear absorption of a strong electromagnetic wave
(EMW) in two-dimensional graphene. Two cases of the electron scattering
mechanism are considered: Electron-optical phonon scattering and
electron-acoustic phonon scattering. The general multi-photon absorption
coefficient is presented as a function of the temperature, the external
magnetic field, the photon energy and the amplitude of external EMW. These
analytical expressions for multi-photon non-linear absorption coefficient
(MNAC) are numerically calculated and the results are discussed in both the
absence and presence of a magnetic field perpendicular to the graphene sheet.
The results show that there is no absorption peak in the absence of the
magnetic field, which contrasts with previous results in 2D systems such as
quantum wells or superlattices. However, when there is a strong magnetic field
along the direction perpendicular to the 2D graphene, absorption spectral lines
appear consistent with the magneto-phonon resonance conditions. Our
calculations show that the MPA's effect is stronger than mono-photon
absorption. Besides, the quantum multi-photon non-linear absorption phenomenon
has been studied from low to high temperatures. This transcends the limits of
the classical BKE which is studied in the high-temperature domain. The
computational results show that the dependence of MNAC on the above quantities
is consistent with the previous theoretical investigation. Another novel
feature of this work is that the general analytic expression for MNAC shows the
Half Width at Half Maximum dependence on the magnetic field which is in good
agreement with the previous experimental observations. Thus, our estimation
might give a critical prediction for future experimental observations in 2D
graphene.",347,2412.15638v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",environmental science,2024-12-20,2024-12-23T21:07:07.303738
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",environmental science,2024-12-20,2024-12-23T21:07:07.304736
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",quantum information science,2024-12-20,2024-12-23T21:07:08.473776
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,quantum information science,2024-12-20,2024-12-23T21:07:08.474774
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,quantum information science,2024-12-20,2024-12-23T21:07:08.475771
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",quantum information science,2024-12-20,2024-12-23T21:07:08.476769
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",quantum information science,2024-12-20,2024-12-23T21:07:08.477765
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",quantum information science,2024-12-20,2024-12-23T21:07:08.478763
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",quantum information science,2024-12-20,2024-12-23T21:07:08.478763
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",quantum information science,2024-12-20,2024-12-23T21:07:08.479759
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",quantum information science,2024-12-20,2024-12-23T21:07:08.480757
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",quantum information science,2024-12-20,2024-12-23T21:07:08.480757
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",quantum information science,2024-12-20,2024-12-23T21:07:08.481754
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,quantum information science,2024-12-20,2024-12-23T21:07:08.482752
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",quantum information science,2024-12-20,2024-12-23T21:07:08.483749
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",quantum information science,2024-12-20,2024-12-23T21:07:08.484746
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",quantum information science,2024-12-20,2024-12-23T21:07:08.484746
The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse,"There is an ongoing debate about how to moderate toxic speech on social media
and how content moderation affects online discourse. We propose and validate a
methodology for measuring the content-moderation-induced distortions in online
discourse using text embeddings from computational linguistics. We test our
measure on a representative dataset of 5 million US political Tweets and find
that removing toxic Tweets distorts online content. This finding is consistent
across different embedding models, toxicity metrics, and samples. Importantly,
we demonstrate that content-moderation-induced distortions are not caused by
the toxic language. Instead, we show that, as a side effect, content moderation
shifts the mean and variance of the embedding space, distorting the topic
composition of online content. Finally, we propose an alternative approach to
content moderation that uses generative Large Language Models to rephrase toxic
Tweets to preserve their salvageable content rather than removing them
entirely. We demonstrate that this rephrasing strategy reduces toxicity while
minimizing distortions in online content.",220,2412.16114v1,cs.SI,cs.SI,quantum information science,2024-12-20,2024-12-23T21:07:08.485743
Quantifying the benefit of load uncertainty reduction for the design of district energy systems under grid constraints using the Value of Information,"Load uncertainty must be accounted for during design to ensure building
energy systems can meet energy demands during operation. Reducing building load
uncertainty allows for improved designs with less compromise to be identified,
reducing the cost of decarbonizing energy usage. However, the building
monitoring required to reduce load uncertainty is costly. This study quantifies
the economic benefit of practical building monitoring for supporting energy
system design decisions, to determine if its benefits outweigh its cost. Value
of Information analysis (VoI) is a numerical framework for quantifying the
benefit of uncertainty reduction to support decision making. An extension of
the framework, termed 'On-Policy' VoI, is proposed, which admits complex
decision making tasks where decision policies are required. This is applied to
a case study district energy system design problem, where a Linear Program
model is used to size solar-battery systems and grid connection capacity under
uncertain building loads, modelled using historic electricity metering data.
Load uncertainty is found to have a significant impact on both system operating
costs (\pm30%) and the optimal system design (\pm20%). However, using building
monitoring is found to reduce overall costs by less than 2% on average, less
than the cost of measurement, and is therefore not economically worthwhile.
This provides the first numerical evidence to support the sufficiency of using
standard building load profiles for energy system design. Further, reducing
only uncertainty in mean load is found to provide all available decision
support benefit, meaning using hourly measurement data provides no benefit for
energy retrofit design.",312,2412.16105v1,eess.SY,"eess.SY,cs.SY",quantum information science,2024-12-20,2024-12-23T21:07:08.486741
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",quantum information science,2024-12-20,2024-12-23T21:07:08.487738
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",quantum information science,2024-12-20,2024-12-23T21:07:08.487738
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",quantum information science,2024-12-20,2024-12-23T21:07:08.487738
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",quantum information science,2024-12-20,2024-12-23T21:07:08.488736
Dual-Polarized Beyond Diagonal RIS,"Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of
RIS architectures more flexible than conventional RIS. While BD-RIS has been
primarily analyzed assuming uni-polarized systems, modern wireless deployments
are dual-polarized. To address this gap, this paper investigates the
fundamental limits of dual-polarized BD-RIS-aided systems. We derive the
scaling laws governing the performance of BD-RIS and the Pareto frontier of the
trade-off between performance and circuit complexity enabled by BD-RIS.
Theoretical results show that the group-connected RIS with group size 2
provides remarkable gains over conventional RIS in both Rayleigh and
line-of-sight (LoS) channels, while maintaining a reduced circuit complexity.",166,2412.16097v1,cs.IT,"cs.IT,eess.SP,math.IT",quantum information science,2024-12-20,2024-12-23T21:07:08.489733
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,quantum information science,2024-12-20,2024-12-23T21:07:08.489733
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,quantum information science,2024-12-20,2024-12-23T21:07:08.490730
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",quantum information science,2024-12-20,2024-12-23T21:07:08.490730
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",quantum information science,2024-12-20,2024-12-23T21:07:08.491727
Chiral phase-imaging meta-sensors,"Light waves possess multiple degrees of freedom besides intensity, including
phase and polarization, that often contain important information but require
complex and bulky systems for their measurement. Here we report a pair of
compact multifunctional photodetectors that can selectively measure the local
phase gradient of, respectively, the right and left circular-polarization
component of any incident wave. These devices employ a chiral pair of
integrated plasmonic metasurfaces to introduce a sharp dependence of
responsivity on local direction of propagation of the desired polarization
component. An order-of-magnitude polarization selectivity with respect to phase
gradient is demonstrated with both devices. Using the measured device
characteristics, we also describe computationally a pixel array that allows for
the simultaneous separate mapping of the right and left circularly-polarized
incident wavefronts in a particularly simple imaging setup. These unique
capabilities may be exploited to enable new functionalities for applications in
chemical sensing, biomedical microscopy, and machine vision.",201,2412.16084v1,physics.optics,physics.optics,quantum information science,2024-12-20,2024-12-23T21:07:08.492726
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,quantum information science,2024-12-20,2024-12-23T21:07:08.493724
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",quantum information science,2024-12-20,2024-12-23T21:07:08.493724
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",quantum information science,2024-12-20,2024-12-23T21:07:08.494720
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",quantum information science,2024-12-20,2024-12-23T21:07:08.495717
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,quantum information science,2024-12-20,2024-12-23T21:07:08.495717
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,quantum information science,2024-12-20,2024-12-23T21:07:08.496714
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,quantum information science,2024-12-20,2024-12-23T21:07:08.497711
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",quantum information science,2024-12-20,2024-12-23T21:07:08.497711
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",quantum information science,2024-12-20,2024-12-23T21:07:08.498709
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",quantum information science,2024-12-20,2024-12-23T21:07:08.499706
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",quantum information science,2024-12-20,2024-12-23T21:07:08.500703
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",quantum information science,2024-12-20,2024-12-23T21:07:08.501701
Examining Entropic Unbalanced Optimal Transport and Sinkhorn Divergences for Spatial Forecast Verification,"An optimal transport (OT) problem seeks to find the cheapest mapping between
two distributions with equal total density, given the cost of transporting
density from one place to another. Unbalanced OT allows for different total
density in each distribution. This is the typical setting for precipitation
forecast and observation data, when considering the densities as accumulated
rainfall, or intensity. In this work, entropic unbalanced OT and its associated
Sinkhorn divergence are examined as a spatial forecast verification method for
precipitation data. It offers many attractive features, such as morphing one
field into another, defining a distance between fields and providing feature
based optimal assignment. It is found that the Sinkhorn divergence is robust
against the common double penalty problem (a form of phase error), aligns with
expert assessments of model performance, and allows for a variety of novel
pictorial illustrations of error. It provides informative summary scores, and
has few limitations to its application. Combined, these findings place
unbalanced entropy regularised optimal transport and the Sinkhorn divergence as
an informative method which follows geometric intuition.",221,2412.16063v1,math.OC,math.OC,quantum information science,2024-12-20,2024-12-23T21:07:08.501701
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",quantum information science,2024-12-20,2024-12-23T21:07:08.502698
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",quantum information science,2024-12-20,2024-12-23T21:07:08.502698
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,quantum information science,2024-12-20,2024-12-23T21:07:08.503695
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",quantum information science,2024-12-20,2024-12-23T21:07:08.504693
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",quantum information science,2024-12-20,2024-12-23T21:07:08.504693
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,quantum information science,2024-12-20,2024-12-23T21:07:08.505690
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",quantum information science,2024-12-20,2024-12-23T21:07:08.506688
Segmentation of arbitrary features in very high resolution remote sensing imagery,"Very high resolution (VHR) mapping through remote sensing (RS) imagery
presents a new opportunity to inform decision-making and sustainable practices
in countless domains. Efficient processing of big VHR data requires automated
tools applicable to numerous geographic regions and features. Contemporary RS
studies address this challenge by employing deep learning (DL) models for
specific datasets or features, which limits their applicability across
contexts.
  The present research aims to overcome this limitation by introducing
EcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.
EcoMapper fully automates processing of geospatial data, DL model training, and
inference. Models trained with EcoMapper successfully segmented two distinct
features in a real-world UAV dataset, achieving scores competitive with prior
studies which employed context-specific models.
  To evaluate EcoMapper, many additional models were trained on permutations of
principal field survey characteristics (FSCs). A relationship was discovered
allowing derivation of optimal ground sampling distance from feature size,
termed Cording Index (CI). A comprehensive methodology for field surveys was
developed to ensure DL methods can be applied effectively to collected data.
  The EcoMapper code accompanying this work is available at
https://github.com/hcording/ecomapper .",264,2412.16046v1,cs.CV,cs.CV,quantum information science,2024-12-20,2024-12-23T21:07:08.507685
Millikelvin Nb nanoSQUID-embedded tuneable resonator fabricated with a neon focused-ion-beam,"SQUID-embedded superconducting resonators are of great interest due to their
potential for coupling highly scalable superconducting circuits with quantum
memories based on solid-state spin ensembles. Such an application requires a
high-$Q$, frequency-tuneable resonator which is both resilient to magnetic
field, and able to operate at millikelvin temperatures. These requirements
motivate the use of a higher $H_{c}$ metal such as niobium, however the
challenge then becomes to sufficiently reduce the operating temperature. We
address this by presenting a monolithic Nb nanoSQUID-embedded resonator, where
neon focused-ion-beam fabrication of the nanoSQUID results in a device
displaying frequency tuneability at $T = 16$ mK. In order to assess the
applicability of the device for coupling to small spin clusters, we
characterise the flux sensitivity as a function of microwave drive power and
externally applied magnetic field, and find that the noise is dominated by
dielectric noise in the resonator. Finally, we discuss improvements to the
device design which can dramatically improve the flux sensitivity, which
highlights the promise of Nb SQUID-embedded resonators for hybrid
superconductor-spin applications.",261,2412.16045v1,quant-ph,"quant-ph,cond-mat.supr-con",quantum information science,2024-12-20,2024-12-23T21:07:08.508683
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",quantum information science,2024-12-20,2024-12-23T21:07:08.508683
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.281738
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.282735
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.283732
Terbium under High Pressure: First-Principles Dynamical Mean-Field Theory Study,"Elemental rare-earth metals provide a playground for studying novel electron
correlation effects and complex magnetism. However, ab initio simulations of
these systems remain challenging. Here, we employ fully charge self-consistent
density functional theory and dynamical mean-field theory (DFT+DMFT) to
investigate terbium (Tb) metal under pressure. We show that Tb exhibits a
strong band renormalization due to correlation effects, with the calculated
electron density of states in good agreement with the experiments. At higher
pressures, the correlated electronic structures persist but with modulation in
the Hubbard gap, highlighting the tunability of effective Coulomb interactions
and kinetic energies. Our DFT+DMFT calculations further indicate a
ferromagnetic ground state of Tb at low pressure and low temperature, as well
as a transition from ferromagnetism to paramagnetism at elevated temperatures.
These ab initio results also align with the experiments. Our study paves the
way for exploring heavy lanthanides via advanced first-principles simulations.",212,2412.16125v1,cond-mat.str-el,cond-mat.str-el,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.284729
Observational Properties of Harmonic EMIC waves: Statistical Study,"Electromagnetic ion cyclotron (EMIC) waves are discrete electromagnetic
emissions separated by multiple ion gyrofrequencies. Harmonic EMIC waves are
defined as waves with a strong electric or magnetic field (or both) at the
harmonics of the fundamental EMIC mode. In this paper, for the first time, we
present a statistical study on harmonic EMIC waves by the Van Allen Probes. The
EMIC waves are categorized into three types based on their harmonics: (1)
fundamental mode only (without higher harmonics), (2) electrostatic (ES)
harmonics, and (3) electromagnetic (EM) harmonics. Our statistical study shows
that ES and EM harmonic EMIC waves predominantly occur on the dayside, outside
the plasmasphere with $L >5$ and are associated with a low $f_{pe}/f_{ce}$, a
high proton $\beta_H$, and a strong fundamental EMIC mode. The results will
advance our understanding of harmonic EMIC waves and their generation
mechanisms.",217,2412.16124v1,physics.space-ph,physics.space-ph,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.284729
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.285726
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.286723
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.287721
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.287721
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.288718
Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation,"The increasing demand for privacy-preserving data analytics in finance
necessitates solutions for synthetic data generation that rigorously uphold
privacy standards. We introduce DP-Fed-FinDiff framework, a novel integration
of Differential Privacy, Federated Learning and Denoising Diffusion
Probabilistic Models designed to generate high-fidelity synthetic tabular data.
This framework ensures compliance with stringent privacy regulations while
maintaining data utility. We demonstrate the effectiveness of DP-Fed-FinDiff on
multiple real-world financial datasets, achieving significant improvements in
privacy guarantees without compromising data quality. Our empirical evaluations
reveal the optimal trade-offs between privacy budgets, client configurations,
and federated optimization strategies. The results affirm the potential of
DP-Fed-FinDiff to enable secure data sharing and robust analytics in highly
regulated domains, paving the way for further advances in federated learning
and privacy-preserving data synthesis.",186,2412.16083v1,cs.LG,"cs.LG,q-fin.ST",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.289716
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.289716
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.290713
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.290713
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.292710
Fuzzy-Space Engineering,"The techniques developed for matrix models and fuzzy geometry are powerful
tools for representing strings and membranes in quantum physics. We study the
representation of fuzzy surfaces using these techniques. This involves
constructing graphs and writing their coordinates and connectivity into
matrices. To construct arbitrary graphs and quickly change them, we use 3D
software. A script generates the three matrices from the graphs. These matrices
are then processed in Wolfram Mathematica to calculate the zero modes of the
Dirac operator. Our first result shows the quantization of a two-dimensional
Trefoil knot. Additional examples illustrate various properties and behaviors
of this process. This helps us to gain a deeper understanding of fuzzy spaces
and zero-mode surfaces. This work contributes to advancing the understanding of
visualization aspects in fuzzy geometry.",153,2412.16011v1,hep-th,hep-th,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.293705
Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling,"Conversational assistants are increasingly popular across diverse real-world
applications, highlighting the need for advanced multimodal speech modeling.
Speech, as a natural mode of communication, encodes rich user-specific
characteristics such as speaking rate and pitch, making it critical for
effective interaction. Our work introduces a data-centric customization
approach for efficiently enhancing multimodal understanding in conversational
speech modeling. Central to our contributions is a novel multi-task learning
paradigm that involves designing auxiliary tasks to utilize a small amount of
speech data. Our approach achieves state-of-the-art performance on the
Spoken-SQuAD benchmark, using only 10% of the training data with open-weight
models, establishing a robust and efficient framework for audio-centric
conversational modeling. We also introduce ASK-QA, the first dataset for
multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation
inputs. Code and data forthcoming.",187,2412.15995v1,cs.CL,"cs.CL,cs.AI,cs.SD,eess.AS",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.293705
Investigating the Interplay between Spin-Polarization and Magnetic Damping in $\mathrm{Co}_{x}\mathrm{Fe}_{80-x}\mathrm{B}_{20}$ for Magnonics Applications,"For magnonics and spintronics applications, the spin polarization ($P$) of a
transport current and the magnetic damping ($\alpha$) play a crucial role, e.g.
for magnetization dynamics and magnetization switching applications. In
particular, $P$ in a glassy (amorphous) 3d transition ferromagnet such as CoFeB
and $\alpha$ are both strongly affected by $s-d$ scattering mechanisms. Hence,
a correlation can be expected which is a priori difficult to predict. In this
work, $P$ and $\alpha$ are measured using current-induced Doppler shifts using
propagating spin-wave spectroscopy and broadband ferromagnetic resonance
techniques in blanket films and current-carrying $Co_{\rm x}Fe_{\rm
{80-x}}B_{\rm 20}$ alloy microstrips. The measured $P$ ranges from 0.18 $\pm$
0.05 to 0.39 $\pm$ 0.05 and $\alpha$ ranges from $(4.0\pm 0.2)\cdot10^{-3}$ to
$(9.7\pm 0.6)\cdot10^{-3}$. We find that for increasing $P$ a systematic drop
in $\alpha$ is observed, indicating an interplay between magnetic damping and
the spin polarization of the transport current which suggests that interband
scattering dominates in $Co_{\rm x}Fe_{\rm {80-x}}B_{\rm 20}$. Our results may
guide future experiments, theory, and applications in advancing spintronics and
metal magnonics.",368,2412.15954v1,physics.app-ph,physics.app-ph,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.294702
Watertox: The Art of Simplicity in Universal Attacks A Cross-Model Framework for Robust Adversarial Generation,"Contemporary adversarial attack methods face significant limitations in
cross-model transferability and practical applicability. We present Watertox,
an elegant adversarial attack framework achieving remarkable effectiveness
through architectural diversity and precision-controlled perturbations. Our
two-stage Fast Gradient Sign Method combines uniform baseline perturbations
($\epsilon_1 = 0.1$) with targeted enhancements ($\epsilon_2 = 0.4$). The
framework leverages an ensemble of complementary architectures, from VGG to
ConvNeXt, synthesizing diverse perspectives through an innovative voting
mechanism. Against state-of-the-art architectures, Watertox reduces model
accuracy from 70.6% to 16.0%, with zero-shot attacks achieving up to 98.8%
accuracy reduction against unseen architectures. These results establish
Watertox as a significant advancement in adversarial methodologies, with
promising applications in visual security systems and CAPTCHA generation.",205,2412.15924v1,cs.CV,"cs.CV,cs.AI,cs.CR",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.295700
RiTTA: Modeling Event Relations in Text-to-Audio Generation,"Despite significant advancements in Text-to-Audio (TTA) generation models
achieving high-fidelity audio with fine-grained context understanding, they
struggle to model the relations between audio events described in the input
text. However, previous TTA methods have not systematically explored audio
event relation modeling, nor have they proposed frameworks to enhance this
capability. In this work, we systematically study audio event relation modeling
in TTA generation models. We first establish a benchmark for this task by: 1.
proposing a comprehensive relation corpus covering all potential relations in
real-world scenarios; 2. introducing a new audio event corpus encompassing
commonly heard audios; and 3. proposing new evaluation metrics to assess audio
event relation modeling from various perspectives. Furthermore, we propose a
finetuning framework to enhance existing TTA models ability to model audio
events relation. Code is available at: https://github.com/yuhanghe01/RiTTA",193,2412.15922v1,cs.LG,"cs.LG,cs.SD,eess.AS",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.295700
Self-supervised Spatial-Temporal Learner for Precipitation Nowcasting,"Nowcasting, the short-term prediction of weather, is essential for making
timely and weather-dependent decisions. Specifically, precipitation nowcasting
aims to predict precipitation at a local level within a 6-hour time frame. This
task can be framed as a spatial-temporal sequence forecasting problem, where
deep learning methods have been particularly effective. However, despite
advancements in self-supervised learning, most successful methods for
nowcasting remain fully supervised. Self-supervised learning is advantageous
for pretraining models to learn representations without requiring extensive
labeled data. In this work, we leverage the benefits of self-supervised
learning and integrate it with spatial-temporal learning to develop a novel
model, SpaT-SparK. SpaT-SparK comprises a CNN-based encoder-decoder structure
pretrained with a masked image modeling (MIM) task and a translation network
that captures temporal relationships among past and future precipitation maps
in downstream tasks. We conducted experiments on the NL-50 dataset to evaluate
the performance of SpaT-SparK. The results demonstrate that SpaT-SparK
outperforms existing baseline supervised models, such as SmaAt-UNet, providing
more accurate nowcasting predictions.",242,2412.15917v1,cs.LG,cs.LG,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.296697
CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR Sequences,"Neural distance fields (NDF) have emerged as a powerful tool for addressing
challenges in 3D computer vision and graphics downstream problems. While
significant progress has been made to learn NDF from various kind of sensor
data, a crucial aspect that demands attention is the supervision of neural
fields during training as the ground-truth NDFs are not available for
large-scale outdoor scenes. Previous works have utilized various forms of
expected signed distance to guide model learning. Yet, these approaches often
need to pay more attention to critical considerations of surface geometry and
are limited to small-scale implementations. To this end, we propose a novel
methodology leveraging second-order derivatives of the signed distance field
for improved neural field learning. Our approach addresses limitations by
accurately estimating signed distance, offering a more comprehensive
understanding of underlying geometry. To assess the efficacy of our
methodology, we conducted comparative evaluations against prevalent methods for
mapping and localization tasks, which are primary application areas of NDF. Our
results demonstrate the superiority of the proposed approach, highlighting its
potential for advancing the capabilities of neural distance fields in computer
vision and graphics applications.",222,2412.15909v1,cs.CV,"cs.CV,cs.GR",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.297694
Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model,"Background: Recent advances in large language models highlight the need for
high-quality multilingual medical datasets. While Japan leads globally in CT
scanner deployment and utilization, the lack of large-scale Japanese radiology
datasets has hindered the development of specialized language models for
medical imaging analysis. Objective: To develop a comprehensive Japanese CT
report dataset through machine translation and establish a specialized language
model for structured finding classification. Additionally, to create a
rigorously validated evaluation dataset through expert radiologist review.
Methods: We translated the CT-RATE dataset (24,283 CT reports from 21,304
patients) into Japanese using GPT-4o mini. The training dataset consisted of
22,778 machine-translated reports, while the validation dataset included 150
radiologist-revised reports. We developed CT-BERT-JPN based on
""tohoku-nlp/bert-base-japanese-v3"" architecture for extracting 18 structured
findings from Japanese radiology reports. Results: Translation metrics showed
strong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores
ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression
sections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in
11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular
septal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1
scores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in
four conditions. Conclusions: Our study establishes a robust Japanese CT report
dataset and demonstrates the effectiveness of a specialized language model for
structured finding classification. The hybrid approach of machine translation
and expert validation enables the creation of large-scale medical datasets
while maintaining high quality.",398,2412.15907v1,cs.CL,"cs.CL,cs.AI",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.299690
A Digital Phantom for 3D MR Spectroscopy Data Simulation,"Simulated data is increasingly valued by researchers for validating MRS and
MRSI processing and analysis algorithms. However, there is no consensus on the
optimal approaches for simulation models and parameters. This study introduces
a novel 3D MRS digital brain phantom framework, providing a comprehensive and
modular foundation for MRS and MRSI data simulation. We generate a digital
brain phantom by combining anatomical and tissue label information with
metabolite data from the literature. This phantom contains all necessary
information for simulating spectral data. We integrate the phantom with a
signal-based model to demonstrate its functionality and usability in generating
various spectral datasets. Outputs are saved in the NIfTI-MRS format, enabling
their use in downstream applications. We successfully implemented and tested
the 3D MRS digital brain phantom framework using two different anatomical
models at two resolutions. The resulting metabolite maps and spectral datasets
demonstrate realistic data quality, flexibility based on user inputs, and
reasonable computational efficiency. This innovative 3D digital brain phantom
framework provides a clear and structured approach to simulating MRS and MRSI
data. Its modular design establishes a strong, adaptable foundation for future
advancements in MRS and MRSI simulation, allowing researchers to extend and
refine the model to meet the field's evolving needs.",250,2412.15869v1,physics.med-ph,"physics.med-ph,physics.bio-ph",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.300686
Using matrix-product states for time-series machine learning,"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling
quantum many-body physics. For many applications, and particularly in
one-dimension, they capture relevant quantum correlations in many-body
wavefunctions while remaining tractable to store and manipulate on a classical
computer. This has motivated researchers to also apply the MPS ansatz to
machine learning (ML) problems where capturing complex correlations in datasets
is also a key requirement. Here, we develop and apply an MPS-based algorithm,
MPSTime, for learning a joint probability distribution underlying an observed
time-series dataset, and show how it can be used to tackle important
time-series ML problems, including classification and imputation. MPSTime can
efficiently learn complicated time-series probability distributions directly
from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$,
with values for our applications ranging between $\chi_{\rm max} = 20-150$, and
can be trained for both classification and imputation tasks under a single
logarithmic loss function. Using synthetic and publicly available real-world
datasets, spanning applications in medicine, energy, and astronomy, we
demonstrate performance competitive with state-of-the-art ML approaches, but
with the key advantage of encoding the full joint probability distribution
learned from the data. By sampling from the joint probability distribution and
calculating its conditional entanglement entropy, we show how its underlying
structure can be uncovered and interpreted. This manuscript is supplemented
with the release of a publicly available code package MPSTime that implements
our approach. The efficiency of the MPS-based ansatz for learning complex
correlation structures from time-series data is likely to underpin
interpretable advances to challenging time-series ML problems across science,
industry, and medicine.",371,2412.15826v1,stat.ML,"stat.ML,cs.LG,quant-ph",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.301684
Precision ICU Resource Planning: A Multimodal Model for Brain Surgery Outcomes,"Although advances in brain surgery techniques have led to fewer postoperative
complications requiring Intensive Care Unit (ICU) monitoring, the routine
transfer of patients to the ICU remains the clinical standard, despite its high
cost. Predictive Gradient Boosted Trees based on clinical data have attempted
to optimize ICU admission by identifying key risk factors pre-operatively;
however, these approaches overlook valuable imaging data that could enhance
prediction accuracy. In this work, we show that multimodal approaches that
combine clinical data with imaging data outperform the current clinical data
only baseline from 0.29 [F1] to 0.30 [F1], when only pre-operative clinical
data is used and from 0.37 [F1] to 0.41 [F1], for pre- and post-operative data.
This study demonstrates that effective ICU admission prediction benefits from
multimodal data fusion, especially in contexts of severe class imbalance.",189,2412.15818v1,eess.IV,"eess.IV,cs.CV,q-bio.NC",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.301684
Efficient Hamiltonian Simulation: A Utility Scale Perspective for Covalent Inhibitor Reactivity Prediction,"Quantum computing applications in the noisy intermediate-scale quantum (NISQ)
era demand algorithms capable of generating shallower circuits that are
feasible to run on today's quantum systems. This is a challenge, particularly
for quantum chemistry applications, considering the inherent complexity of
molecular systems. In this paper, we demonstrate advancements that expand the
size of chemistry problems that can be run on today's quantum systems by
applying hardware-efficient approaches, such as Quantum-Centric Data-Driven
Research and Development (QDDRD), optimized algorithms with reduced circuit
depth, and execute the experiments with middleware-supported quantum error
mitigation. We report up to a 29-fold reduction in circuit depth for covalent
drug molecules, enabling Hamiltonian dynamics for reactivity predictions,
assuming all-to-all connectivity of quantum hardware. When employed on IBMQ's
Heron architecture, we see up to a 16-fold reduction. The overarching impact of
this work is that it highlights promising methods that allow researchers to
explore the dynamics of commercially relevant chemistry on real quantum
hardware via Hamiltonian simulation.",223,2412.15804v1,quant-ph,quant-ph,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.302682
WebLLM: A High-Performance In-Browser LLM Inference Engine,"Advancements in large language models (LLMs) have unlocked remarkable
capabilities. While deploying these models typically requires server-grade GPUs
and cloud-based inference, the recent emergence of smaller open-source models
and increasingly powerful consumer devices have made on-device deployment
practical. The web browser as a platform for on-device deployment is
universally accessible, provides a natural agentic environment, and
conveniently abstracts out the different backends from diverse device vendors.
To address this opportunity, we introduce WebLLM, an open-source JavaScript
framework that enables high-performance LLM inference entirely within web
browsers. WebLLM provides an OpenAI-style API for seamless integration into web
applications, and leverages WebGPU for efficient local GPU acceleration and
WebAssembly for performant CPU computation. With machine learning compilers
MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming
the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM
can retain up to 80% native performance on the same device, with room to
further close the gap. WebLLM paves the way for universally accessible,
privacy-preserving, personalized, and locally powered LLM applications in web
browsers. The code is available at: https://github.com/mlc-ai/web-llm.",291,2412.15803v1,cs.LG,"cs.LG,cs.AI",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.303678
Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning,"Despite recent advances in large language models, open-source models often
struggle to consistently perform well on complex reasoning tasks. Existing
ensemble methods, whether applied at the token or output levels, fail to
address these challenges. In response, we present Language model Ensemble with
Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level
ensembling of language models. LE-MCTS formulates step-by-step reasoning with
an ensemble of language models as a Markov decision process. In this framework,
states represent intermediate reasoning paths, while actions consist of
generating the next reasoning step using one of the language models selected
from a predefined pool. Guided by a process-based reward model, LE-MCTS
performs a tree search over the reasoning steps generated by different language
models, identifying the most accurate reasoning chain. Experimental results on
five mathematical reasoning benchmarks demonstrate that our approach
outperforms both single language model decoding algorithms and language model
ensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the
MATH and MQA datasets, respectively, highlighting its effectiveness in solving
complex reasoning problems.",238,2412.15797v1,cs.CL,cs.CL,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.304675
Debiasing of Two-Line Element Sets for Batch Least Squares Pseudo-Orbit Determination in MEO and GEO,"The availability of accurate and timely state predictions for objects in
near-Earth orbits is becoming increasingly important due to the growing
congestion in key orbital regimes. The Two-line Element Set (TLE) catalogue
remains, to this day, one of the few publicly-available, comprehensive sources
of near-Earth object ephemerides. At the same time, TLEs are affected by
measurement noise and are limited by the low accuracy of the SGP4 theory,
introducing significant uncertainty into state predictions. Previous literature
has shown that filtering TLEs with batch least squares methods can yield
significant improvements in long-term state prediction accuracy. However, this
process can be highly sensitive to TLE quality which can vary throughout the
year. In this study, it is shown that either extended-duration fit windows of
the order of months, or the removal of systematic biases in along-track
position prior to state estimation can produce significant reductions in
post-fit position errors. Simple models for estimating these systematic biases
are shown to be effective without introducing the need for high-complexity
Machine Learning (ML) models. Furthermore, by establishing a TLE-based error
metric, the need for high accuracy ephemerides is removed when creating these
models. For selected satellites in the Medium Earth Orbit (MEO) regime,
post-fit position errors are reduced by up to 80 %, from approximately 5 km to
1 km; meanwhile, for selected satellites in the Geostationary Earth Orbit
(GEO)/Geosynchronous Earth Orbit (GSO) regime, large oscillations in post-fit
position error can be suppressed.",328,2412.15793v1,astro-ph.EP,"astro-ph.EP,astro-ph.IM,eess.SP,math.OC",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.304675
On the optimal growth of autocatalytic subnetworks: A Mathematical Optimization Approach,"Chemical reaction networks (CRNs) are essential for modeling and analyzing
complex systems across fields, from biochemistry to economics. Autocatalytic
reaction network -- networks where certain species catalyze their own
production -- are particularly significant for understanding self-replication
dynamics in biological systems and serve as foundational elements in
formalizing the concept of a circular economy. In a previous study, we
developed a mixed-integer linear optimization-based procedure to enumerate all
minimal autocatalytic subnetworks within a network. In this work, we define the
maximum growth factor (MGF) of an autocatalytic subnetwork, develop
mathematical optimization approaches to compute this metric, and explore its
implications in the field of economics and dynamical systems. We develop exact
approaches to determine the MGF of any subnetwork based on an iterative
procedure with guaranteed convergence, which allows for identifying
autocatalytic subnetworks with the highest MGF. We report the results of
computational experiments on synthetic CRNs and two well-known datasets, namely
the Formose and E. coli reaction networks, identifying their autocatalytic
subnetworks and exploring their scientific ramifications. Using advanced
optimization techniques and interdisciplinary applications, our framework adds
an essential resource to analyze complex systems modeled as reaction networks.",265,2412.15776v1,math.OC,"math.OC,cs.CE",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.305673
Coherent Interactions of Free Electrons and Matter: Toward Tunable Compact X-ray Sources,"Compact laboratory-scale X-ray sources still rely on the same fundamental
principles as in the first X-ray tubes developed more than a century ago. In
recent years, significant research and development have focused on large-scale
X-ray sources such as synchrotrons and free-electron lasers, leading to the
generation of high-brightness coherent X-rays. However, the large size and high
costs of such sources prevent their widespread use. The quest for a compact and
coherent Xray source has long been a critical objective in modern physics,
gaining further importance in recent years for industrial applications and
fundamental scientific research. Here, we review the physical mechanisms
governing compact coherent X-ray generation. Of current interest are coherent
periodic interactions of free electrons in crystalline materials, creating hard
X-rays via a mechanism known as parametric X-ray radiation (PXR). Over the past
decade, X-ray sources leveraging this mechanism have demonstrated
state-of-the-art tunability, directionality, and broad spatial coherence,
enabling X-ray phase-contrast imaging on a compact scale. The coming years are
expected to show substantial miniaturization of compact X-ray sources,
facilitated by progress in electron beam technologies. This review compares the
most promising mechanisms used for hard-X-ray generation, contrasting
parametric X-ray radiation with inverse Compton scattering and characteristic
radiation from a liquid-jet anode. We cover the most recent advancements,
including the development of new materials, innovative geometrical designs, and
specialized optimization techniques, aiming toward X-ray flux levels suitable
for medical imaging and X-ray spectroscopy in compact scales.",334,2412.15775v1,physics.app-ph,"physics.app-ph,physics.optics",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.306670
Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease Detection based on Spontaneous Speech,"Alzheimer's Disease (AD) is a significant and growing public health concern.
Investigating alterations in speech and language patterns offers a promising
path towards cost-effective and non-invasive early detection of AD on a large
scale. Large language models (LLMs), such as GPT, have enabled powerful new
possibilities for semantic text analysis. In this study, we leverage GPT-4 to
extract five semantic features from transcripts of spontaneous patient speech.
The features capture known symptoms of AD, but they are difficult to quantify
effectively using traditional methods of computational linguistics. We
demonstrate the clinical significance of these features and further validate
one of them (""Word-Finding Difficulties"") against a proxy measure and human
raters. When combined with established linguistic features and a Random Forest
classifier, the GPT-derived features significantly improve the detection of AD.
Our approach proves effective for both manually transcribed and automatically
generated transcripts, representing a novel and impactful use of recent
advancements in LLMs for AD speech analysis.",206,2412.15772v1,cs.CL,"cs.CL,cs.AI",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.307668
VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in Large Vision-Language Models,"Large Vision-Language Models (LVLMs) have made remarkable developments along
with the recent surge of large language models. Despite their advancements,
LVLMs have a tendency to generate plausible yet inaccurate or inconsistent
information based on the provided source content. This phenomenon, also known
as ``hallucinations"" can have serious downstream implications during the
deployment of LVLMs. To address this, we present VORD a simple and effective
method that alleviates hallucinations by calibrating token predictions based on
ordinal relationships between modified image pairs. VORD is presented in two
forms: 1.) a minimalist training-free variant which eliminates implausible
tokens from modified image pairs, and 2.) a trainable objective function that
penalizes unlikely tokens. Our experiments demonstrate that VORD delivers
better calibration and effectively mitigates object hallucinations on a
wide-range of LVLM benchmarks.",195,2412.15739v1,cs.CV,cs.CV,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.307668
The Role of Recurrency in Image Segmentation for Noisy and Limited Sample Settings,"The biological brain has inspired multiple advances in machine learning.
However, most state-of-the-art models in computer vision do not operate like
the human brain, simply because they are not capable of changing or improving
their decisions/outputs based on a deeper analysis. The brain is recurrent,
while these models are not. It is therefore relevant to explore what would be
the impact of adding recurrent mechanisms to existing state-of-the-art
architectures and to answer the question of whether recurrency can improve
existing architectures. To this end, we build on a feed-forward segmentation
model and explore multiple types of recurrency for image segmentation. We
explore self-organizing, relational, and memory retrieval types of recurrency
that minimize a specific energy function. In our experiments, we tested these
models on artificial and medical imaging data, while analyzing the impact of
high levels of noise and few-shot learning settings. Our results do not
validate our initial hypothesis that recurrent models should perform better in
these settings, suggesting that these recurrent architectures, by themselves,
are not sufficient to surpass state-of-the-art feed-forward versions and that
additional work needs to be done on the topic.",254,2412.15734v1,cs.CV,"cs.CV,cs.LG",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.308667
High-efficiency fast pinching radiation of electron beams in nonuniform plasma,"The continuous development of bright x/gamma-ray sources has opened up new
frontiers of science and advanced applications. Currently, there is still a
lack of efficient approaches to produce gamma-rays with photon energies up to
GeV and with high peak brilliance comparable to modern free-electron lasers.
Here we report a novel mechanism called beam fast pinching radiation burst to
generate such gamma-ray sources. It is achieved by injecting a GeV electron
beam into a submillimeter plasma with an upramp density profile, enabling
violent beam pinching to occur rapidly. During this process, a burst of
collimated gamma-rays is efficiently produced with photon energy up to GeV,
energy conversion efficiency exceeding $30\%$, and peak brilliance exceeding
$10^{28}$ photons s$^{-1}$ mm$^{-2}$ mrad$^{-2}$ per $0.1\%$ bandwidth. All of
these are several orders of magnitude higher than existing gamma-ray sources.
This opens a novel avenue for the development of extremely bright gamma-ray
sources for both fundamental research and cutting-edge applications.",238,2412.15706v1,physics.plasm-ph,"physics.plasm-ph,physics.acc-ph",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.309673
Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration,"Recent advancements in language models (LMs) have sparked growing interest in
developing LM agents. While fully autonomous agents could excel in many
scenarios, numerous use cases inherently require them to collaborate with
humans due to humans' latent preferences, domain expertise, or need for
control. To facilitate the study of human-agent collaboration, we present
Collaborative Gym (Co-Gym), a general framework enabling asynchronous,
tripartite interaction among agents, humans, and task environments. We
instantiate Co-Gym with three representative tasks in both simulated and
real-world conditions, and propose an evaluation framework that assesses both
the collaboration outcomes and processes. Our findings reveal that
collaborative agents consistently outperform their fully autonomous
counterparts in task performance within those delivered cases, achieving win
rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related
Work when evaluated by real users. However, our study also highlights
significant challenges in developing collaborative agents, requiring
advancements in core aspects of intelligence -- communication capabilities,
situational awareness, and balancing autonomy and human control.",223,2412.15701v1,cs.AI,"cs.AI,cs.CL,cs.HC",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.310661
Effect of three-orifice baffles orientation on the flow and thermal-hydraulic performance: experimental analysis for net and oscillatory flows,"Three-orifice baffles equally spaced along a circular tube are investigated
as a means for heat transfer enhancement under net, oscillatory and compound
flows. An unprecedented, systematic analysis of the relative orientation of
consecutive baffles -- aligned or opposed -- is accomplished to assess the
changes induced on the flow structure and their impact on the thermal-hydraulic
performance. The results cover the Nusselt number, the net and oscillatory
friction factors and the instantaneous velocity fields using PIV in an
experimental campaign with a 32 mm tube diameter. The study is conducted in the
range of net Reynolds numbers $50 < Re_n < 1000$ and oscillatory Reynolds
numbers $0 < Re_{osc}< 750$, for a dimensionless amplitude $x_0/D = 0.5$ and
$Pr=65$. In absence of oscillatory flow, opposed baffles advance the transition
to turbulence from $Re_n = 100$ to $50$, increasing the net friction factor (40
%) for $Re_n > 50$ and the Nusselt number (maximum of 27 %) for $Re_n < 150$.
When an oscillatory flow is applied, augmentations caused by opposed baffles
are only observed for $Re_n < 150$ and $Re_{osc} < 150$. Above $Re_n$,
$Re_{osc}>150$, opposed baffles are not recommended for the promotion of heat
transfer, owing to friction penalties. However, the chaotic mixing and lack of
short-circuiting between baffles observed with flow velocimetry over a wide
range of operational conditions point out the interest of this configuration to
achieve plug flow.",368,2412.15682v1,physics.flu-dyn,physics.flu-dyn,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.311657
A survey on FPGA-based accelerator for ML models,"This paper thoroughly surveys machine learning (ML) algorithms acceleration
in hardware accelerators, focusing on Field-Programmable Gate Arrays (FPGAs).
It reviews 287 out of 1138 papers from the past six years, sourced from four
top FPGA conferences. Such selection underscores the increasing integration of
ML and FPGA technologies and their mutual importance in technological
advancement. Research clearly emphasises inference acceleration (81\%) compared
to training acceleration (13\%). Additionally, the findings reveals that CNN
dominates current FPGA acceleration research while emerging models like GNN
show obvious growth trends. The categorization of the FPGA research papers
reveals a wide range of topics, demonstrating the growing relevance of ML in
FPGA research. This comprehensive analysis provides valuable insights into the
current trends and future directions of FPGA research in the context of ML
applications.",179,2412.15666v1,cs.AR,"cs.AR,cs.LG,68W25, 68M20, 68Q25,I.5.4; C.1.3; B.5.2",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.312654
SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training,"Recent advancements highlight the potential of end-to-end real-time spoken
dialogue systems, showcasing their low latency and high quality. In this paper,
we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction
system with single-stage training. SLAM-Omni achieves zero-shot timbre control
by modeling spoken language with semantic tokens and decoupling speaker
information to a vocoder. By predicting grouped speech semantic tokens at each
step, our method significantly reduces the sequence length of audio tokens,
accelerating both training and inference. Additionally, we propose historical
text prompting to compress dialogue history, facilitating efficient multi-round
interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior
models of similar scale, requiring only 15 hours of training on 4 GPUs with
limited data. Notably, it is the first spoken dialogue system to achieve
competitive performance with a single-stage training approach, eliminating the
need for pre-training on TTS or ASR tasks. Further experiments validate its
multilingual and multi-turn dialogue capabilities on larger datasets.",232,2412.15649v1,eess.AS,eess.AS,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.313652
Lecture Notes on High Dimensional Linear Regression,"These lecture notes cover advanced topics in linear regression, with an
in-depth exploration of the existence, uniqueness, relations, computation, and
non-asymptotic properties of the most prominent estimators in this setting. The
covered estimators include least squares, ridgeless, ridge, and lasso. The
content follows a proposition-proof structure, making it suitable for students
seeking a formal and rigorous understanding of the statistical theory
underlying machine learning methods.",95,2412.15633v1,stat.ME,"stat.ME,stat.CO,stat.ML",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.313652
Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems,"The paper presents a framework of microservices-based architecture dedicated
to enhancing the performance of real-time travel reservation systems using the
power of predictive analytics. Traditional monolithic systems are bad at
scaling and performing with high loads, causing backup resources to be
underutilized along with delays. To overcome the above-stated problems, we
adopt a modularization approach in decoupling system components into
independent services that can grow or shrink according to demand. Our framework
also includes real-time predictive analytics, through machine learning models,
that optimize forecasting customer demand, dynamic pricing, as well as system
performance. With an experimental evaluation applying the approach, we could
show that the framework impacts metrics of performance such as response time,
throughput, transaction rate of success, and prediction accuracy compared to
their conventional counterparts. Not only does the microservices approach
improve scalability and fault tolerance like a usual architecture, but it also
brings along timely and accurate predictions, which imply a greater customer
satisfaction and efficiency of operation. The integration of real-time
analytics would lead to more intelligent decision-making, thereby improving the
response of the system along with the reliability it holds. A scalable,
efficient framework is offered by such a system to address the modern
challenges imposed by any form of travel reservation system while considering
other complex, data-driven industries as future applications. Future work will
be an investigation of advanced AI models and edge processing to further
improve the performance and robustness of the systems employed.",303,2412.15616v1,cs.IT,"cs.IT,cs.AI,cs.CE,cs.LG,math.IT",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.314652
Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage,"The advancement of large language models (LLMs) prompts the development of
multi-modal agents, which are used as a controller to call external tools,
providing a feasible way to solve practical tasks. In this paper, we propose a
multi-modal agent tuning method that automatically generates multi-modal
tool-usage data and tunes a vision-language model (VLM) as the controller for
powerful tool-usage reasoning. To preserve the data quality, we prompt the
GPT-4o mini model to generate queries, files, and trajectories, followed by
query-file and trajectory verifiers. Based on the data synthesis pipeline, we
collect the MM-Traj dataset that contains 20K tasks with trajectories of tool
usage. Then, we develop the T3-Agent via \underline{T}rajectory
\underline{T}uning on VLMs for \underline{T}ool usage using MM-Traj.
Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently
achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},
which outperforms untrained VLMs by $20\%$, showing the effectiveness of the
proposed data synthesis pipeline, leading to high-quality data for tool-usage
capabilities.",298,2412.15606v1,cs.AI,"cs.AI,cs.CV",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.315646
Advanced Control Strategy to Compensate Power Sharing Error and DC Circulating Current in Parallel Single-Phase Inverters,"This paper proposes an advanced control strategy to eliminate both current
sharing error and DC circulating current caused by line impedance mismatched
and measurement errors in islanded AC microgrid system. The proposed adaptive
virtual impedance scheme is developed with the aid of a low bandwidth
communication and a simple PI compensator to adaptively adjust the virtual
impedance value so that active and reactive power will be shared equally among
inverters. Meanwhile, the DC offset in output voltage measurement is detected
by using the voltage ripple information of DC link voltage of inverter. The
proposed control strategy can be implemented directly without any pre-knowledge
of the feeder impedances. The proposed compensation scheme is simple and easy
to implement, and it also does not require extra hardware circuit or sensors.
The effectiveness of the proposed solution is verified by simulation and
experimental results of 6 kVA microgrid system.",179,2412.15604v1,eess.SY,"eess.SY,cs.SY",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.316644
SemDP: Semantic-level Differential Privacy Protection for Face Datasets,"While large-scale face datasets have advanced deep learning-based face
analysis, they also raise privacy concerns due to the sensitive personal
information they contain. Recent schemes have implemented differential privacy
to protect face datasets. However, these schemes generally treat each image as
a separate database, which does not fully meet the core requirements of
differential privacy. In this paper, we propose a semantic-level differential
privacy protection scheme that applies to the entire face dataset. Unlike
pixel-level differential privacy approaches, our scheme guarantees that
semantic privacy in faces is not compromised. The key idea is to convert
unstructured data into structured data to enable the application of
differential privacy. Specifically, we first extract semantic information from
the face dataset to build an attribute database, then apply differential
perturbations to obscure this attribute data, and finally use an image
synthesis model to generate a protected face dataset. Extensive experimental
results show that our scheme can maintain visual naturalness and balance the
privacy-utility trade-off compared to the mainstream schemes.",208,2412.15590v1,cs.CV,"cs.CV,cs.CR",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.316644
Limit theorems for critical branching processes in a finite state space Markovian environment,"Let $(Z_n)_{n\geq 0}$ be a critical branching process in a random environment
defined by a Markov chain $(X_n)_{n\geq 0}$ with values in a finite state space
$\mathbb X$. Let $ S_n = \sum_{k=1}^n \ln f_{X_k}'(1)$ be the Markov walk
associated to $(X_n)_{n\geq 0}$, where $f_i$ is the offspring generating
function when the environment is $i \in \mathbb X$. Conditioned on the event
$\{ Z_n>0\}$, we show the non degeneracy of limit law of the normalized number
of particles ${Z_n}/{e^{S_n}}$ and determine the limit of the law of
$\frac{S_n}{\sqrt{n}} $ jointly with $X_n$. Based on these results we establish
a Yaglom-type theorem which specifies the limit of the joint law of $ \log Z_n$
and $X_n$ given $Z_n>0$.",270,2412.15585v1,math.PR,"math.PR,60J80, 60F05, 60J10",advanced manufacturing,2024-12-20,2024-12-23T21:07:09.317641
Tracking the 2024 US Presidential Election Chatter on TikTok: A Public Multimodal Dataset,"This paper presents the TikTok 2024 U.S. Presidential Election Dataset, a
large-scale, resource designed to advance research into political communication
and social media dynamics. The dataset comprises 3.14 million videos published
on TikTok between November 1, 2023, and October 16, 2024, encompassing video
ids and transcripts. Data collection was conducted using the TikTok Research
API with a comprehensive set of election-related keywords and hashtags,
supplemented by third-party tools to address API limitations and expand content
coverage, enabling analysis of hashtag co-occurrence networks that reveal
politically aligned hashtags based on ideological affiliations, the evolution
of top hashtags over time, and summary statistics that highlight the dataset's
scale and richness. This dataset offers insights into TikTok's role in shaping
electoral discourse by providing a multimodal view of election-related content.
It enables researchers to explore critical topics such as coordinated
messaging, misinformation spread, audience engagement, and linguistic trends.
The TikTok 2024 U.S. Presidential Election Dataset is publicly available and
aims to contribute to the broader understanding of social media's impact on
democracy and public opinion.",253,2412.15583v1,cs.SI,cs.SI,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.318638
A Deep Probabilistic Framework for Continuous Time Dynamic Graph Generation,"Recent advancements in graph representation learning have shifted attention
towards dynamic graphs, which exhibit evolving topologies and features over
time. The increased use of such graphs creates a paramount need for generative
models suitable for applications such as data augmentation, obfuscation, and
anomaly detection. However, there are few generative techniques that handle
continuously changing temporal graph data; existing work largely relies on
augmenting static graphs with additional temporal information to model dynamic
interactions between nodes. In this work, we propose a fundamentally different
approach: We instead directly model interactions as a joint probability of an
edge forming between two nodes at a given time. This allows us to
autoregressively generate new synthetic dynamic graphs in a largely assumption
free, scalable, and inductive manner. We formalize this approach as DG-Gen, a
generative framework for continuous time dynamic graphs, and demonstrate its
effectiveness over five datasets. Our experiments demonstrate that DG-Gen not
only generates higher fidelity graphs compared to traditional methods but also
significantly advances link prediction tasks.",214,2412.15582v1,cs.LG,cs.LG,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.318638
J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM,"Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made
available the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image
archive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive
serves as a valuable resource for researchers and scholars interested in
deep-sea imagery. The dataset comprises images and videos of deep-sea
phenomena, predominantly of marine organisms, but also of the seafloor and
physical processes. In this study, we propose J-EDI QA, a benchmark for
understanding images of deep-sea organisms using a multimodal large language
model (LLM). The benchmark is comprised of 100 images, accompanied by questions
and answers with four options by JAMSTEC researchers for each image. The QA
pairs are provided in Japanese, and the benchmark assesses the ability to
understand deep-sea species in Japanese. In the evaluation presented in this
paper, OpenAI o1 achieved a 50% correct response rate. This result indicates
that even with the capabilities of state-of-the-art models as of December 2024,
deep-sea species comprehension is not yet at an expert level. Further advances
in deep-sea species-specific LLMs are therefore required.",277,2412.15574v1,cs.CV,cs.CV,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.319635
Thin films as practical quantum materials: a status quo and beyond,"Quantum materials have been in the limelight for several years now. These
materials exhibit intriguing quantum phenomena, which when harnessed properly,
promise extraordinary advancements across various scientific and technological
domains. To fully exploit their potential, it is imperative to synthesize such
quantum materials in thin film form so that they are compatible with
well-established device fabrication techniques. In this perspective, an
overview of the current status and future directions of thin film quantum
material synthesis is provided. The criteria for quantum materials are
discussed, as well as the many benefits of preparing them as thin films.
Prominent deposition techniques such as molecular beam epitaxy and chemical
vapor deposition are reviewed along with potential contenders. Despite
challenges, progress in thin film quantum material technology holds the
potential to realize practical devices with unprecedented functionalities.",158,2412.15565v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,advanced manufacturing,2024-12-20,2024-12-23T21:07:09.320633
Stochastic Variational Inference for Structured Additive Distributional Regression,"In structured additive distributional regression, the conditional
distribution of the response variables given the covariate information and the
vector of model parameters is modelled using a P-parametric probability density
function where each parameter is modelled through a linear predictor and a
bijective response function that maps the domain of the predictor into the
domain of the parameter. We present a method to perform inference in structured
additive distributional regression using stochastic variational inference. We
propose two strategies for constructing a multivariate Gaussian variational
distribution to estimate the posterior distribution of the regression
coefficients. The first strategy leverages covariate information and
hyperparameters to learn both the location vector and the precision matrix. The
second strategy tackles the complexity challenges of the first by initially
assuming independence among all smooth terms and then introducing correlations
through an additional set of variational parameters. Furthermore, we present
two approaches for estimating the smoothing parameters. The first treats them
as free parameters and provides point estimates, while the second accounts for
uncertainty by applying a variational approximation to the posterior
distribution. Our model was benchmarked against state-of-the-art competitors in
logistic and gamma regression simulation studies. Finally, we validated our
approach by comparing its posterior estimates to those obtained using Markov
Chain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics
and semiconductor/computer sectors.",283,2412.10038v1,stat.CO,"stat.CO,62,G.3",biotechnology,2024-12-13,2024-12-23T21:07:10.159484
Use of diverse data sources to control which topics emerge in a science map,"Traditional science maps visualize topics by clustering documents, but they
are inherently biased toward clustering certain topics over others. If these
topics could be chosen, then the science maps could be tailored for different
needs. In this paper, we explore the use of document networks from diverse data
sources as a tool to control the topic clustering bias of a science map. We
analyze this by evaluating the clustering effectiveness of several topic
categories over two traditional and six non-traditional data sources. We found
that the topics favored in each non-traditional data source are about: Health
for Facebook users, biotechnology for patent families, government and social
issues for policy documents, food for Twitter conversations, nursing for
Twitter users, and geographical entities for document authors (the favoring in
this latter source was particularly strong). Our results show that diverse data
sources can be used to control topic bias, which opens up the possibility of
creating science maps tailored for different needs.",190,2412.07550v1,cs.DL,cs.DL,biotechnology,2024-12-10,2024-12-23T21:07:10.160482
Novel Magnetic Actuation Strategies for Precise Ferrofluid Marble Manipulation in Magnetic Digital Microfluidics: Position Control and Applications,"Precise manipulation of liquid marbles has significant potential in various
applications such as lab-on-a-chip systems, drug delivery, and biotechnology
and has been a challenge for researchers. Ferrofluid marble (FM) is a marble
with a ferrofluid core that can easily be manipulated by a magnetic field.
Although FMs have great potential for accurate positioning and manipulation,
these marbles have not been precisely controlled in magnetic digital
microfluidics, so far. In this study for the first time, a novel method of
magnetic actuation is proposed using a pair of Helmholtz coils and permanent
magnets. The governing equations for controlling the FM position are
investigated, and it is shown that there are three different strategies for
adjusting the applied magnetic force. Then, experiments are conducted to
demonstrate the capability of the proposed method. To this aim, different
magnetic setups are proposed for manipulating FMs. These setups are compared in
terms of energy consumption and tracking ability across various frequencies.
The study showcases several applications of precise FM position control,
including controllable reciprocal positioning, simultaneous position control of
two FMs, the transport of non-magnetic liquid marbles using the FMs, and sample
extraction method from the liquid core of the FM.",254,2412.02859v1,cs.RO,"cs.RO,cs.SY,eess.SY",biotechnology,2024-12-03,2024-12-23T21:07:10.160482
SA-GNAS: Seed Architecture Expansion for Efficient Large-scale Graph Neural Architecture Search,"GNAS (Graph Neural Architecture Search) has demonstrated great effectiveness
in automatically designing the optimal graph neural architectures for multiple
downstream tasks, such as node classification and link prediction. However,
most existing GNAS methods cannot efficiently handle large-scale graphs
containing more than million-scale nodes and edges due to the expensive
computational and memory overhead. To scale GNAS on large graphs while
achieving better performance, we propose SA-GNAS, a novel framework based on
seed architecture expansion for efficient large-scale GNAS. Similar to the cell
expansion in biotechnology, we first construct a seed architecture and then
expand the seed architecture iteratively. Specifically, we first propose a
performance ranking consistency-based seed architecture selection method, which
selects the architecture searched on the subgraph that best matches the
original large-scale graph. Then, we propose an entropy minimization-based seed
architecture expansion method to further improve the performance of the seed
architecture. Extensive experimental results on five large-scale graphs
demonstrate that the proposed SA-GNAS outperforms human-designed
state-of-the-art GNN architectures and existing graph NAS methods. Moreover,
SA-GNAS can significantly reduce the search time, showing better search
efficiency. For the largest graph with billion edges, SA-GNAS can achieve 2.8
times speedup compared to the SOTA large-scale GNAS method GAUSS. Additionally,
since SA-GNAS is inherently parallelized, the search efficiency can be further
improved with more GPUs. SA-GNAS is available at
https://github.com/PasaLab/SAGNAS.",328,2412.02196v1,cs.LG,cs.LG,biotechnology,2024-12-03,2024-12-23T21:07:10.161478
"The Updated Genome Warehouse: Enhancing Data Value, Security, and Usability to Address Data Expansion","The Genome Warehouse (GWH), accessible at https://ngdc.cncb.ac.cn/gwh, is an
extensively utilized public repository dedicated to the deposition, management
and sharing of genome assembly sequences, annotations, and metadata. This paper
highlights noteworthy enhancements to the GWH since the 2021 version,
emphasizing substantial advancements in web interfaces for data submission,
database functionality updates, and resource integration. Key updates include
the reannotation of released prokaryotic genomes, mirroring of genome resources
from National Center for Biotechnology Information (NCBI) GenBank and RefSeq,
integration of Poxviridae sequences, implementation of an online batch
submission system, enhancements to the quality control system, advanced search
capabilities, and the introduction of a controlled-access mechanism for human
genome data. These improvements collectively augment the ease and security of
data submission and access as well as genome data value, thereby fostering
heightened convenience and utility for researchers in the genomic field.",209,2411.15467v1,q-bio.GN,q-bio.GN,biotechnology,2024-11-23,2024-12-23T21:07:10.162978
Energy-based generative models for monoclonal antibodies,"Since the approval of the first antibody drug in 1986, a total of 162
antibodies have been approved for a wide range of therapeutic areas, including
cancer, autoimmune, infectious, or cardiovascular diseases. Despite advances in
biotechnology that accelerated the development of antibody drugs, the drug
discovery process for this modality remains lengthy and costly, requiring
multiple rounds of optimizations before a drug candidate can progress to
preclinical and clinical trials. This multi-optimization problem involves
increasing the affinity of the antibody to the target antigen while refining
additional biophysical properties that are essential to drug development such
as solubility, thermostability or aggregation propensity. Additionally,
antibodies that resemble natural human antibodies are particularly desirable,
as they are likely to offer improved profiles in terms of safety, efficacy, and
reduced immunogenicity, further supporting their therapeutic potential. In this
article, we explore the use of energy-based generative models to optimize a
candidate monoclonal antibody. We identify tradeoffs when optimizing for
multiple properties, concentrating on solubility, humanness and affinity and
use the generative model we develop to generate candidate antibodies that lie
on an optimal Pareto front that satisfies these constraints.",248,2411.13390v1,q-bio.BM,q-bio.BM,biotechnology,2024-11-20,2024-12-23T21:07:10.162978
Silanization Strategies for Tailoring Peptide Functionalization on Silicon Surfaces: Implications for Enhancing Stem Cell Adhesion,"Biomaterial surface engineering and integrating cell-adhesive ligands are
crucial in biological research and biotechnological applications. The interplay
between cells and their microenvironment, influenced by chemical and physical
cues, impacts cellular behavior. Surface modification of biomaterials
profoundly affects cellular responses, especially at the cell-surface
interface. This work focuses on enhancing cellular activities through material
manipulation, emphasizing silanization for further functionalization with
bioactive molecules like RGD peptides to improve cell adhesion. The grafting of
three distinct silanes onto silicon wafers using both spin coating and
immersion methods was investigated. This study sheds light on the effects of
different alkyl chain lengths and protecting groups on cellular behavior,
providing valuable insights into optimizing silane-based self-assembled
monolayers (SAMs) before peptide or protein grafting for the first time.
Specifically, it challenges the common use of APTES molecules in this context.
These findings advance our understanding of surface modification strategies,
paving the way for tailoring biomaterial surfaces to modulate cellular behavior
for diverse biotechnological applications.",230,2411.12332v1,q-bio.CB,q-bio.CB,biotechnology,2024-11-19,2024-12-23T21:07:10.163977
Enhancing reinforcement learning for population setpoint tracking in co-cultures,"Efficient multiple setpoint tracking can enable advanced biotechnological
applications, such as maintaining desired population levels in co-cultures for
optimal metabolic division of labor. In this study, we employ reinforcement
learning as a control method for population setpoint tracking in co-cultures,
focusing on policy-gradient techniques where the control policy is
parameterized by neural networks. However, achieving accurate tracking across
multiple setpoints is a significant challenge in reinforcement learning, as the
agent must effectively balance the contributions of various setpoints to
maximize the expected system performance. Traditional return functions, such as
those based on a quadratic cost, often yield suboptimal performance due to
their inability to efficiently guide the agent toward the simultaneous
satisfaction of all setpoints. To overcome this, we propose a novel return
function that rewards the simultaneous satisfaction of multiple setpoints and
diminishes overall reward gains otherwise, accounting for both stage and
terminal system performance. This return function includes parameters to
fine-tune the desired smoothness and steepness of the learning process. We
demonstrate our approach considering an $\textit{Escherichia coli}$ co-culture
in a chemostat with optogenetic control over amino acid synthesis pathways,
leveraging auxotrophies to modulate growth.",255,2411.09177v1,eess.SY,"eess.SY,cs.SY",biotechnology,2024-11-14,2024-12-23T21:07:10.164974
Biomass phenotyping of oilseed rape through UAV multi-view oblique imaging with 3DGS and SAM model,"Biomass estimation of oilseed rape is crucial for optimizing crop
productivity and breeding strategies. While UAV-based imaging has advanced
high-throughput phenotyping, current methods often rely on orthophoto images,
which struggle with overlapping leaves and incomplete structural information in
complex field environments. This study integrates 3D Gaussian Splatting (3DGS)
with the Segment Anything Model (SAM) for precise 3D reconstruction and biomass
estimation of oilseed rape. UAV multi-view oblique images from 36 angles were
used to perform 3D reconstruction, with the SAM module enhancing point cloud
segmentation. The segmented point clouds were then converted into point cloud
volumes, which were fitted to ground-measured biomass using linear regression.
The results showed that 3DGS (7k and 30k iterations) provided high accuracy,
with peak signal-to-noise ratios (PSNR) of 27.43 and 29.53 and training times
of 7 and 49 minutes, respectively. This performance exceeded that of structure
from motion (SfM) and mipmap Neural Radiance Fields (Mip-NeRF), demonstrating
superior efficiency. The SAM module achieved high segmentation accuracy, with a
mean intersection over union (mIoU) of 0.961 and an F1-score of 0.980.
Additionally, a comparison of biomass extraction models found the point cloud
volume model to be the most accurate, with an determination coefficient (R2) of
0.976, root mean square error (RMSE) of 2.92 g/plant, and mean absolute
percentage error (MAPE) of 6.81%, outperforming both the plot crop volume and
individual crop volume models. This study highlights the potential of combining
3DGS with multi-view UAV imaging for improved biomass phenotyping.",371,2411.08453v1,cs.CV,cs.CV,biotechnology,2024-11-13,2024-12-23T21:07:10.165972
Safe Paths and Sequences for Scalable ILPs in RNA Transcript Assembly Problems,"A common step at the core of many RNA transcript assembly tools is to find a
set of weighted paths that best explain the weights of a DAG. While such
problems easily become NP-hard, scalable solvers exist only for a basic
error-free version of this problem, namely minimally decomposing a network flow
into weighted paths.
  The main result of this paper is to show that we can achieve speedups of two
orders of magnitude also for path-finding problems in the realistic setting
(i.e., the weights do not induce a flow). We obtain these by employing the
safety information that is encoded in the graph structure inside Integer Linear
Programming (ILP) solvers for these problems. We first characterize the paths
that appear in all path covers of the DAG, generalizing a graph reduction
commonly used in the error-free setting (e.g. by Kloster et al. [ALENEX~2018]).
Secondly, following the work of Ma, Zheng and Kingsford [RECOMB 2021], we
characterize the \emph{sequences} of arcs that appear in all path covers of the
DAG.
  We experiment with a path-finding ILP model (least squares) and with a more
recent and accurate one. We use a variety of datasets originally created by
Shao and Kingsford [TCBB, 2017], as well as graphs built from sequencing reads
by the state-of-the-art tool for long-read transcript discovery, IsoQuant
[Prjibelski et al., Nat.~Biotechnology~2023]. The ILPs armed with safe paths or
sequences exhibit significant speed-ups over the original ones. On graphs with
a large width, average speed-ups are in the range $50-160\times$ in the latter
ILP model and in the range $100-1000\times$ in the least squares model.
  Our scaling techniques apply to any ILP whose solution paths are a path cover
of the arcs of the DAG. As such, they can become a scalable building block of
practical RNA transcript assembly tools, avoiding heuristic trade-offs
currently needed on complex graphs.",448,2411.03871v2,cs.DS,"cs.DS,math.OC,q-bio.GN",biotechnology,2024-11-06,2024-12-23T21:07:10.166969
Tracking one-in-a-million: Large-scale benchmark for microbial single-cell tracking with experiment-aware robustness metrics,"Tracking the development of living cells in live-cell time-lapses reveals
crucial insights into single-cell behavior and presents tremendous potential
for biomedical and biotechnological applications. In microbial live-cell
imaging (MLCI), a few to thousands of cells have to be detected and tracked
within dozens of growing cell colonies. The challenge of tracking cells is
heavily influenced by the experiment parameters, namely the imaging interval
and maximal cell number. For now, tracking benchmarks are not widely available
in MLCI and the effect of these parameters on the tracking performance are not
yet known. Therefore, we present the largest publicly available and annotated
dataset for MLCI, containing more than 1.4 million cell instances, 29k cell
tracks, and 14k cell divisions. With this dataset at hand, we generalize
existing tracking metrics to incorporate relevant imaging and experiment
parameters into experiment-aware metrics. These metrics reveal that current
cell tracking methods crucially depend on the choice of the experiment
parameters, where their performance deteriorates at high imaging intervals and
large cell colonies. Thus, our new benchmark quantifies the influence of
experiment parameters on the tracking quality, and gives the opportunity to
develop new data-driven methods that generalize across imaging and experiment
parameters. The benchmark dataset is publicly available at
https://zenodo.org/doi/10.5281/zenodo.7260136.",295,2411.00552v1,cs.CV,cs.CV,biotechnology,2024-11-01,2024-12-23T21:07:10.167966
RNA-GPT: Multimodal Generative System for RNA Sequence Understanding,"RNAs are essential molecules that carry genetic information vital for life,
with profound implications for drug development and biotechnology. Despite this
importance, RNA research is often hindered by the vast literature available on
the topic. To streamline this process, we introduce RNA-GPT, a multi-modal RNA
chat model designed to simplify RNA discovery by leveraging extensive RNA
literature. RNA-GPT integrates RNA sequence encoders with linear projection
layers and state-of-the-art large language models (LLMs) for precise
representation alignment, enabling it to process user-uploaded RNA sequences
and deliver concise, accurate responses. Built on a scalable training pipeline,
RNA-GPT utilizes RNA-QA, an automated system that gathers RNA annotations from
RNACentral using a divide-and-conquer approach with GPT-4o and latent Dirichlet
allocation (LDA) to efficiently handle large datasets and generate
instruction-tuning samples. Our experiments indicate that RNA-GPT effectively
addresses complex RNA queries, thereby facilitating RNA research. Additionally,
we present RNA-QA, a dataset of 407,616 RNA samples for modality alignment and
instruction tuning, further advancing the potential of RNA research tools.",255,2411.08900v1,q-bio.GN,"q-bio.GN,cs.AI,cs.CE,cs.LG,q-bio.BM",biotechnology,2024-10-29,2024-12-23T21:07:10.167966
Characterizing RNA oligomers using Stochastic Titration Constant-pH Metadynamics simulations,"RNA molecules exhibit various biological functions intrinsically dependent on
their diverse ecosystem of highly flexible structures. This flexibility arises
from complex hydrogen-bonding networks defined by canonical and non-canonical
base pairs that require protonation events to stabilize or perturb these
interactions. Constant pH molecular dynamics (CpHMD) methods provide a reliable
framework to explore the conformational and protonation space of dynamic
structures and for robust calculations of pH-dependent properties, such as the
pK$_\mathrm{a}$ of titrable sites. Despite growing biological evidence
concerning pH regulation of certain motifs and in biotechnological
applications, pH-sensitive in silico methods have rarely been applied to
nucleic acids. In this work, we extended the stochastic titration CpHMD method
to include RNA parameters from the standard $\chi$OL3 AMBER force field and
highlighted its capability to depict titration events of nucleotides in
single-stranded RNAs. We validated the method using trimers and pentamers with
a single central titrable site while integrating a well-tempered metadynamics
approach into the st-CpHMD methodology (CpH-MetaD) using PLUMED. This approach
enhanced the convergence of the conformational landscape and enabled more
efficient sampling of protonation-conformation coupling. Our pK$_\mathrm{a}$
estimates agree with experimental data, validating the method's ability to
reproduce electrostatic changes around a titrable nucleobase in single-stranded
RNA. These findings provided molecular insight into intramolecular phenomena,
such as nucleobase stacking and phosphate interactions, that dictate the
experimentally observed pK$_\mathrm{a}$ shifts between different strands.
Overall, this work validates both the st-CpHMD and the metadynamics integration
as reliable tools for studying biologically relevant RNA systems.",397,2410.16064v2,q-bio.BM,"q-bio.BM,physics.bio-ph,physics.chem-ph",biotechnology,2024-10-21,2024-12-23T21:07:10.168964
Privacy-Preserving Synthetically Augmented Knowledge Graphs with Semantic Utility,"Knowledge Graphs (KGs) have recently gained relevant attention in many
application domains, from healthcare to biotechnology, from logistics to
finance. Financial organisations, central banks, economic research entities,
and national supervision authorities apply ontological reasoning on KGs to
address crucial business tasks, such as economic policymaking, banking
supervision, anti-money laundering, and economic research. Reasoning allows for
the generation of derived knowledge capturing complex business semantics and
the set up of effective business processes. A major obstacle in KGs sharing is
represented by privacy considerations since the identity of the data subjects
and their sensitive or company-confidential information may be improperly
exposed.
  In this paper, we propose a novel framework to enable KGs sharing while
ensuring that information that should remain private is not directly released
nor indirectly exposed via derived knowledge, while maintaining the embedded
knowledge of the KGs to support business downstream tasks. Our approach
produces a privacy-preserving synthetic KG as an augmentation of the input one
via the introduction of structural anonymisation. We introduce a novel privacy
measure for KGs, which considers derived knowledge and a new utility metric
that captures the business semantics we want to preserve, and propose two novel
anonymization algorithms. Our extensive experimental evaluation, with both
synthetic graphs and real-world datasets, confirms the effectiveness of our
approach achieving up to a 70% improvement in the privacy of entities compared
to existing methods not specifically designed for KGs.",288,2410.12418v1,cs.DB,"cs.DB,cs.AI,cs.CR",biotechnology,2024-10-16,2024-12-23T21:07:10.169961
Prediction by Machine Learning Analysis of Genomic Data Phenotypic Frost Tolerance in Perccottus glenii,"Analysis of the genome sequence of Perccottus glenii, the only fish known to
possess freeze tolerance, holds significant importance for understanding how
organisms adapt to extreme environments, Traditional biological analysis
methods are time-consuming and have limited accuracy, To address these issues,
we will employ machine learning techniques to analyze the gene sequences of
Perccottus glenii, with Neodontobutis hainanens as a comparative group,
Firstly, we have proposed five gene sequence vectorization methods and a method
for handling ultra-long gene sequences, We conducted a comparative study on the
three vectorization methods: ordinal encoding, One-Hot encoding, and K-mer
encoding, to identify the optimal encoding method, Secondly, we constructed
four classification models: Random Forest, LightGBM, XGBoost, and Decision
Tree, The dataset used by these classification models was extracted from the
National Center for Biotechnology Information database, and we vectorized the
sequence matrices using the optimal encoding method, K-mer, The Random Forest
model, which is the optimal model, achieved a classification accuracy of up to
99, 98 , Lastly, we utilized SHAP values to conduct an interpretable analysis
of the optimal classification model, Through ten-fold cross-validation and the
AUC metric, we identified the top 10 features that contribute the most to the
model's classification accuracy, This demonstrates that machine learning
methods can effectively replace traditional manual analysis in identifying
genes associated with the freeze tolerance phenotype in Perccottus glenii.",307,2410.08867v1,cs.LG,cs.LG,biotechnology,2024-10-11,2024-12-23T21:07:10.170958
Mechanical Interactions Govern Self-Organized Ordering in Two-Dimensional Bacterial Colonies,"We investigate the influence of mechanical interactions on the growth and
self-organization of rod-shaped bacteria confined within square and circular
geometries. Our results reveal that mechanical interactions are crucial in
constraining cell growth and shaping the degree of ordering within bacterial
colonies. Colonies with larger aspect ratios exhibit higher levels of nematic
ordering, while the geometry of confinements significantly impacts colony
organization. Compression influences force magnitudes within colonies, with
larger division lengths encountering greater forces. These findings enhance our
understanding of bacterial self-organization in confined environments, with
implications for microbial ecology and biotechnology.",118,2410.00898v2,cond-mat.soft,"cond-mat.soft,physics.bio-ph",biotechnology,2024-10-01,2024-12-23T21:07:10.170958
EnzymeFlow: Generating Reaction-specific Enzyme Catalytic Pockets through Flow Matching and Co-Evolutionary Dynamics,"Enzyme design is a critical area in biotechnology, with applications ranging
from drug development to synthetic biology. Traditional methods for enzyme
function prediction or protein binding pocket design often fall short in
capturing the dynamic and complex nature of enzyme-substrate interactions,
particularly in catalytic processes. To address the challenges, we introduce
EnzymeFlow, a generative model that employs flow matching with hierarchical
pre-training and enzyme-reaction co-evolution to generate catalytic pockets for
specific substrates and catalytic reactions. Additionally, we introduce a
large-scale, curated, and validated dataset of enzyme-reaction pairs,
specifically designed for the catalytic pocket generation task, comprising a
total of $328,192$ pairs. By incorporating evolutionary dynamics and
reaction-specific adaptations, EnzymeFlow becomes a powerful model for
designing enzyme pockets, which is capable of catalyzing a wide range of
biochemical reactions. Experiments on the new dataset demonstrate the model's
effectiveness in designing high-quality, functional enzyme catalytic pockets,
paving the way for advancements in enzyme engineering and synthetic biology. We
provide EnzymeFlow code at https://github.com/WillHua127/EnzymeFlow with
notebook demonstration at
https://github.com/WillHua127/EnzymeFlow/blob/main/enzymeflow_demo.ipynb.",268,2410.00327v1,cs.LG,"cs.LG,cs.AI,cs.CE,q-bio.QM",biotechnology,2024-10-01,2024-12-23T21:07:10.171955
Normalized topological indices discriminate between architectures of branched macromolecules,"Branching architecture characterizes numerous systems, ranging from synthetic
(hyper)branched polymers and biomolecules such as lignin, amylopectin, and
nucleic acids to tracheal and neuronal networks. Its ubiquity reflects the many
favourable properties that arise because of it. For instance, branched
macromolecules are spatially compact and have a high surface functionality,
which impacts their phase characteristics and self-assembly behaviour, among
others. The relationship between branching and physical properties has been
studied by mapping macromolecules to mathematical trees whose architecture can
be characterized using topological indices. These indices, however, do not
allow for a comparison of macromolecules that map to trees of different size,
be it due to different mapping procedures or differences in their molecular
weight. To alleviate this, we introduce a novel normalization of topological
indices using estimates of their probability density functions. We determine
two optimal normalized topological indices and construct a phase space that
enables a robust discrimination between different architectures of branched
macromolecules. We demonstrate the necessity of such a phase space on two
practical applications, one being ribonucleic acid (RNA) molecules with various
branching topologies and the other different methods of coarse-graining
branched macromolecules. Our approach can be applied to any type of branched
molecules and extended as needed to other topological indices, making it useful
across a wide range of fields where branched molecules play an important role,
including polymer physics, green chemistry, bioengineering, biotechnology, and
medicine.",320,2409.16007v1,cond-mat.soft,cond-mat.soft,biotechnology,2024-09-24,2024-12-23T21:07:10.172953
SOWAHA as a Cancer Suppressor Gene Influence Metabolic Reprogramming,"SOWAHA is a protein-coding gene, also known as ANKRD43. Studies have
indicated that SOWAHA can serve as a prognostic biomarker in colorectal cancer
and pancreatic cancer. However, there are few reports about SOWAHA in other
types of cancer and the specific mechanism of action of SOWAHA in cancer is
also not clear. Based on National Center for Biotechnology Information (NCBI),
The Cancer Genome Atlas (TCGA), Genotype-Tissue Expression Project (GTEx),
cBioPortal, Human Protein Atlas (HPA), etc., we adopted bioinformatics methods
to uncover the potential tumor genomic features of SOWAHA, including the
correlation with prognosis, gene mutation, immune cell infiltration, and DNA
methylation in different tumors and evaluated the association with tumor
heterogeneity, stemness, chemokines chemokine receptors, and immunomodulators
in pan-cancer. Besides, we knocked down SOWAHA in SW620 cells and performed
RNA-seq analysis, then we conducted functional enrichment to uncover the
biological significance of the gene set. SOWAHA has early diagnostic potential,
and low expression of SOWAHA was associated with poor prognosis in was
associated with poor prognosis in GBMLGG, PAAD, READ, etc. SOWAHA is associated
with most tumor immune-infiltrating cells in pan-cancer. SOWAHA correlates with
DNA methylation, tumor heterogeneity, and stemness in many epithelial
carcinomas. Furthermore, SOWAHA is involved in many enzyme activity and
metabolic pathways, mainly metabolic programming pathways in cancer.
Additionally, we identified two potential transcription factors of SOWAHA,
TBX4, and FOXP2, which are dysregulated in SW620 cells. Besides, the cell
proliferation and viability in siSOWAHA groups are better than in siNC
groups.SOWAHA, identified as a suppressor gene, and its role in the progression
of colorectal cancer is primarily mediated through metabolic reprogramming
mechanisms.",447,2409.16332v2,q-bio.QM,q-bio.QM,biotechnology,2024-09-24,2024-12-23T21:07:10.173950
Democratising Artificial Intelligence for Pandemic Preparedness and Global Governance in Latin American and Caribbean Countries,"Infectious diseases, transmitted directly or indirectly, are among the
leading causes of epidemics and pandemics. Consequently, several open
challenges exist in predicting epidemic outbreaks, detecting variants, tracing
contacts, discovering new drugs, and fighting misinformation. Artificial
Intelligence (AI) can provide tools to deal with these scenarios, demonstrating
promising results in the fight against the COVID-19 pandemic. AI is becoming
increasingly integrated into various aspects of society. However, ensuring that
AI benefits are distributed equitably and that they are used responsibly is
crucial. Multiple countries are creating regulations to address these concerns,
but the borderless nature of AI requires global cooperation to define
regulatory and guideline consensus. Considering this, The Global South AI for
Pandemic & Epidemic Preparedness & Response Network (AI4PEP) has developed an
initiative comprising 16 projects across 16 countries in the Global South,
seeking to strengthen equitable and responsive public health systems that
leverage Southern-led responsible AI solutions to improve prevention,
preparedness, and response to emerging and re-emerging infectious disease
outbreaks. This opinion introduces our branches in Latin American and Caribbean
(LAC) countries and discusses AI governance in LAC in the light of
biotechnology. Our network in LAC has high potential to help fight infectious
diseases, particularly in low- and middle-income countries, generating
opportunities for the widespread use of AI techniques to improve the health and
well-being of their communities.",292,2409.14181v1,cs.AI,cs.AI,biotechnology,2024-09-21,2024-12-23T21:07:10.174947
Simulation of charged nanotubes self-assembly during evaporation of a sessile droplet on a substrate,"The ability to control the morphology of the nanotube deposit formed during
the evaporation of a sessile droplet on a substrate is of theoretical and
practical interest. Such deposits is required for various applications
including nanotechnology, medicine, biotechnology, and optronics. In the
experiment of Zhao et al. [J. Colloid Interface Sci. 440, 68 (2015)], an
annular deposit was formed near the contact line. The deposition geometry is
caused by the coffee-ring effect. This deposit is unusual in its morphology. It
changes gradually in space from a disordered structure in the inner part of the
ring to an aligned structure of nanotubes close to the periphery. To understand
the mechanisms that lead to this, we have developed a mathematical model that
takes into account the effects of advection, diffusion, and electrostatic
interactions on particle transport. Results of numerical calculations have
confirmed that all these factors together have an influence on the formation of
such a variable morphology. Qualitative agreement with the experiment is shown
for some values of the model parameters.",219,2409.12647v1,cond-mat.soft,cond-mat.soft,biotechnology,2024-09-19,2024-12-23T21:07:10.174947
The Future of Decoding Non-Standard Nucleotides: Leveraging Nanopore Sequencing for Expanded Genetic Codes,"Expanding genetic codes from natural standard nucleotides to artificial
non-standard nucleotides marks a significant advancement in synthetic biology,
with profound implications for biotechnology and medicine. Decoding the
biological information encoded in these non-standard nucleotides presents new
challenges, as traditional sequencing technologies are unable to recognize or
interpret novel base pairings. In this perspective, we explore the potential of
nanopore sequencing, which is uniquely suited to decipher both standard and
non-standard nucleotides by directly measuring the biophysical properties of
nucleic acids. Nanopore technology offers real-time, long-read sequencing
without the need for amplification or synthesis, making it particularly
advantageous for expanded genetic systems like Artificially Expanded Genetic
Information Systems (AEGIS). We discuss how the adaptability of nanopore
sequencing and advancements in data processing can unlock the potential of
these synthetic genomes and open new frontiers in understanding and utilizing
expanded genetic codes.",194,2409.09314v1,q-bio.GN,"q-bio.GN,q-bio.PE",biotechnology,2024-09-14,2024-12-23T21:07:10.175946
De novo design of high-affinity protein binders with AlphaProteo,"Computational design of protein-binding proteins is a fundamental capability
with broad utility in biomedical research and biotechnology. Recent methods
have made strides against some target proteins, but on-demand creation of
high-affinity binders without multiple rounds of experimental testing remains
an unsolved challenge. This technical report introduces AlphaProteo, a family
of machine learning models for protein design, and details its performance on
the de novo binder design problem. With AlphaProteo, we achieve 3- to 300-fold
better binding affinities and higher experimental success rates than the best
existing methods on seven target proteins. Our results suggest that AlphaProteo
can generate binders ""ready-to-use"" for many research applications using only
one round of medium-throughput screening and no further optimization.",160,2409.08022v1,q-bio.BM,q-bio.BM,biotechnology,2024-09-12,2024-12-23T21:07:10.176943
The microbiome science of composting and human excrement composting: a review,"Linear waste management systems are unsustainable and contribute to
environmental degradation, economic inequity, and health disparities. Among the
array of environmental challenges stemming from anthropogenic impacts, the
management of human excrement (human feces and urine) stands as a significant
concern. Over two billion people do not have access to adequate sanitation
resulting in a global public health crisis.
  Composting is the microbial biotechnology aimed at cycling organic waste,
including human excrement, for improved public health, agricultural
productivity and safety, and environmental sustainability. Applications of
modern microbiome-omics and related technologies have vast capacity to support
continued advances in composting science and praxis. In this article, we review
literature focused on applications of microbiome technologies to study
composting systems and reactions. The studies we survey generally fall into the
categories of animal manure composting, food and landscaping waste composting,
biosolids composting, and human excrement composting. We review experiments
utilizing microbiome technologies to investigate strategies for enhancing
pathogen suppression and accelerating the biodegradation of organic matter.
Additionally, we explore studies focused on the bioengineering potential of
microbes as inoculants to facilitate degradation of toxins such as
pharmaceuticals or per- and polyfluoroalkyl substances (PFAS). The findings
from these studies underscore the importance of advancing our understanding of
composting processes through the integration of emerging microbiome-omics
technologies.
  We conclude that work to-date has demonstrated exciting basic and applied
science potential from studying compost microbiomes, with promising
implications for enhancing global environmental sustainability and public
health.",351,2409.07376v1,q-bio.QM,q-bio.QM,biotechnology,2024-09-11,2024-12-23T21:07:10.177940
Selecting Differential Splicing Methods: Practical Considerations,"Alternative splicing is crucial in gene regulation, with significant
implications in clinical settings and biotechnology. This review article
compiles bioinformatics RNA-seq tools for investigating differential splicing;
offering a detailed examination of their statistical methods, case
applications, and benefits. A total of 22 tools are categorised by their
statistical family (parametric, non-parametric, and probabilistic) and level of
analysis (transcript, exon, and event). The central challenges in quantifying
alternative splicing include correct splice site identification and accurate
isoform deconvolution of transcripts. Benchmarking studies show no consensus on
tool performance, revealing considerable variability across different
scenarios. Tools with high citation frequency and continued developer
maintenance, such as DEXSeq and rMATS, are recommended for prospective
researchers. To aid in tool selection, a guide schematic is proposed based on
variations in data input and the required level of analysis. Additionally,
advancements in long-read RNA sequencing are expected to drive the evolution of
differential splicing tools, reducing the need for isoform deconvolution and
prompting further innovation.",239,2409.05458v1,q-bio.GN,q-bio.GN,biotechnology,2024-09-09,2024-12-23T21:07:10.177940
Microfinance in Thailand: Navigating Challenges and Unlocking Opportunities,"This review article explores the challenges and opportunities faced by the
Bank for Agriculture and Agricultural Cooperatives (BAAC) in Thailand from a
microfinance perspective. It examines the role of BAAC as a specialized
financial institution in assisting underprivileged households and small
businesses in accessing financial services. The study emphasizes the challenges
and opportunities faced by BAAC in promoting sustainable development. It also
explores BAAC's role in advancing the BCG Model policy, which fosters
sustainability in the agricultural sector through Bio Economy Credit, Circular
Economy Credit, and Green Credit. These initiatives support investments in
biotechnology, waste reduction (Zero Waste), organic farming, and safe food
production, all aimed at enhancing farmers' quality of life, stimulating growth
in agriculture, and preserving the environment. Moreover, BAAC remains
committed to upholding transparency, fairness, and operational standards.",174,2409.03157v2,econ.GN,"econ.GN,q-fin.EC",biotechnology,2024-09-05,2024-12-23T21:07:10.178937
A method for site-specifically tethering the enzyme urease to DNA origami with sustained activity,"Attaching enzymes to nanostructures has proven useful to the study of enzyme
functionality under controlled conditions and has led to new technologies.
Often, the utility and interest of enzyme-tethered nanostructures lie in how
the enzymatic activity is affected by how the enzymes are arranged in space.
Therefore, being able to conjugate enzymes to nanostructures while preserving
the enzymatic activity is essential. In this paper, we present a method to
conjugate single-stranded DNA to the enzyme urease while maintaining enzymatic
activity. We show evidence of successful conjugation and quantify the variables
that affect the conjugation yield. We also show that the enzymatic activity is
unchanged after conjugation compared to the enzyme in its native state.
Finally, we demonstrate the tethering of urease to nanostructures made using
DNA origami with high site-specificity. Decorating nanostructures with
enzymatically-active urease may prove to be useful in studying, or even
utilizing, the functionality of urease in disciplines ranging from
biotechnology to soft-matter physics. The techniques we present in this paper
will enable researchers across these fields to modify enzymes without
disrupting their functionality, thus allowing for more insightful studies into
their behavior and utility.",273,2409.03040v1,physics.bio-ph,physics.bio-ph,biotechnology,2024-09-04,2024-12-23T21:07:10.179934
The overlooked need for Ethics in Complexity Science: Why it matters,"Complexity science, despite its broad scope and potential impact, has not
kept pace with fields like artificial intelligence, biotechnology and social
sciences in addressing ethical concerns. The field lacks a comprehensive
ethical framework, leaving us, as a community, vulnerable to ethical challenges
and dilemmas. Other areas have gone through similar experiences and created,
with discussions and working groups, their guides, policies and
recommendations. Therefore, here we highlight the critical absence of formal
guidelines, dedicated ethical committees, and widespread discussions on ethics
within the complexity science community. Drawing on insights from the
disciplines mentioned earlier, we propose a roadmap to enhance ethical
awareness and action. Our recommendations include (i) initiating supportive
mechanisms to develop ethical guidelines specific to complex systems research,
(ii) creating open-access resources, and (iii) fostering inclusive dialogues to
ensure that complexity science can responsibly tackle societal challenges and
achieve a more inclusive environment. By initiating this dialogue, we aim to
encourage a necessary shift in how ethics is integrated into complexity
research, positioning the field to address contemporary challenges more
effectively.",214,2409.02002v1,physics.soc-ph,"physics.soc-ph,cs.CY",biotechnology,2024-09-03,2024-12-23T21:07:10.179934
Leveraging Deep Generative Model For Computational Protein Design And Optimization,"Proteins are the fundamental macromolecules that play diverse and crucial
roles in all living matter and have tremendous implications in healthcare,
manufacturing, and biotechnology. Their functions are largely determined by the
sequences of amino acids that compose them and their unique three-dimensional
structures when folded. The recent surge in highly accurate computational
protein structure prediction tools has equipped scientists with the means to
derive preliminary structural insights without the onerous costs of
experimental structure determination. These breakthroughs hold profound promise
for building robust and efficient in silico protein design systems.
  While the prospect of designing de novo proteins with precise computational
accuracy remains a grand challenge in biochemical engineering, conventional
assembly-based and rational design methods often grapple with the expansive
design space, resulting in suboptimal design success rates. Despite recently
emerged deep learning-based models have shown promise in improving the
efficiency of the computational protein design process, a significant gap
persists between current design paradigms and their experimental realization.
This thesis will investigate the potential of deep generative models in
refining protein structure and sequence design methods, aiming to develop
frameworks capable of crafting novel protein sequences with predetermined
structures or specific functionalities. By harnessing extensive protein
databases and cutting-edge neural architectures, this research aims to enhance
precision and robustness in current protein design paradigms, potentially
paving the way for advancements across various scientific fields.",278,2408.17241v2,q-bio.BM,q-bio.BM,biotechnology,2024-08-30,2024-12-23T21:07:10.180931
ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding,"Understanding biological processes, drug development, and biotechnological
advancements requires detailed analysis of protein structures and sequences, a
task in protein research that is inherently complex and time-consuming when
performed manually. To streamline this process, we introduce ProteinGPT, a
state-of-the-art multi-modal protein chat system, that allows users to upload
protein sequences and/or structures for comprehensive protein analysis and
responsive inquiries. ProteinGPT seamlessly integrates protein sequence and
structure encoders with linear projection layers for precise representation
adaptation, coupled with a large language model (LLM) to generate accurate and
contextually relevant responses. To train ProteinGPT, we construct a
large-scale dataset of 132,092 proteins with annotations, and optimize the
instruction-tuning process using GPT-4o. This innovative system ensures
accurate alignment between the user-uploaded data and prompts, simplifying
protein analysis. Experiments show that ProteinGPT can produce promising
responses to proteins and their corresponding questions.",212,2408.11363v1,cs.AI,"cs.AI,cs.CE,cs.LG,q-bio.BM",biotechnology,2024-08-21,2024-12-23T21:07:10.181929
Exploring Latent Space for Generating Peptide Analogs Using Protein Language Models,"Generating peptides with desired properties is crucial for drug discovery and
biotechnology. Traditional sequence-based and structure-based methods often
require extensive datasets, which limits their effectiveness. In this study, we
proposed a novel method that utilized autoencoder shaped models to explore the
protein embedding space, and generate novel peptide analogs by leveraging
protein language models. The proposed method requires only a single sequence of
interest, avoiding the need for large datasets. Our results show significant
improvements over baseline models in similarity indicators of peptide
structures, descriptors and bioactivities. The proposed method validated
through Molecular Dynamics simulations on TIGIT inhibitors, demonstrates that
our method produces peptide analogs with similar yet distinct properties,
highlighting its potential to enhance peptide screening processes.",157,2408.08341v1,q-bio.QM,"q-bio.QM,cs.AI,cs.LG",biotechnology,2024-08-15,2024-12-23T21:07:10.181929
DOME Registry: Implementing community-wide recommendations for reporting supervised machine learning in biology,"Supervised machine learning (ML) is used extensively in biology and deserves
closer scrutiny. The DOME recommendations aim to enhance the validation and
reproducibility of ML research by establishing standards for key aspects such
as data handling and processing, optimization, evaluation, and model
interpretability. The recommendations help to ensure that key details are
reported transparently by providing a structured set of questions. Here, we
introduce the DOME Registry (URL: registry.dome-ml.org), a database that allows
scientists to manage and access comprehensive DOME-related information on
published ML studies. The registry uses external resources like ORCID, APICURON
and the Data Stewardship Wizard to streamline the annotation process and ensure
comprehensive documentation. By assigning unique identifiers and DOME scores to
publications, the registry fosters a standardized evaluation of ML methods.
Future plans include continuing to grow the registry through community
curation, improving the DOME score definition and encouraging publishers to
adopt DOME standards, promoting transparency and reproducibility of ML in the
life sciences.",211,2408.07721v2,q-bio.OT,q-bio.OT,biotechnology,2024-08-14,2024-12-23T21:07:10.182926
Fragmentation and aggregation of cyanobacterial colonies,"Fluid flow has a major effect on the aggregation and fragmentation of
bacterial colonies. Yet, a generic framework to understand and predict how
hydrodynamics affects colony size remains elusive. This study investigates how
fluid flow affects the formation and maintenance of large colonial structures
in cyanobacteria. We performed experiments on laboratory cultures and lake
samples of the cyanobacterium Microcystis, while their colony size distribution
was measured simultaneously by direct microscopic imaging. We demonstrate that
EPS-embedded cells formed by cell division exhibit significant mechanical
resistance to shear forces. However, at elevated hydrodynamic stress levels
(exceeding those typically generated by surface wind mixing) these colonies
experience fragmentation through an erosion process. We also show that single
cells can aggregate into small colonies due to fluid flow. However, the
structural integrity of these flow-induced colonies is weaker than that of
colonies formed by cell division. We provide a mathematical analysis to support
the experiments and demonstrate that a population model with two categories of
colonies describes the measured size distributions. Our results shed light on
the specific conditions wherein flow-induced fragmentation and aggregation of
cyanobacteria are decisive and indicate that colony formation under natural
conditions is mainly driven by cell division, although flow-induced aggregation
could play a role in dense bloom events. These findings can be used to improve
prediction models and mitigation strategies for toxic cyanobacterial blooms and
also offer potential applications in other areas such as algal biotechnology or
medical settings where the dynamics of biological aggregates play a significant
role.",306,2407.21115v1,cond-mat.soft,"cond-mat.soft,physics.bio-ph",biotechnology,2024-07-30,2024-12-23T21:07:10.183924
ToF-SIMS data analysis of Shewanella oneidensis MR-1 biofilms,"Analysis of bacterial biofilms is particularly challenging and important with
diverse applications from systems biology to biotechnology. Among the variety
of techniques that have been applied, time-of-flight secondary ion mass
spectrometry (ToF-SIMS) has many promising features in studying the surface
characteristics of biofilms. ToF-SIMS offers high spatial resolution and high
mass accuracy, which permit surface sensitive analysis of biofilm components.
Thus, ToF-SIMS provides a powerful solution to addressing the challenge of
bacterial biofilm analysis. This dataset covers ToF-SIMS analysis of Shewanella
oneidensis MR-1 isolated from freshwater lake sediment in New York state. The
MR-1 strain is known to have metal and sulfur reducing properties and it can be
used for bioremediation and wastewater treatment. There is a current need to
identify small molecules and fragments produced from bacterial biofilms. Static
ToF-SIMS spectra of MR-1 were obtained using an IONTOF TOF.SIMS V instrument
equipped with a 25 keV Bi3+ metal ion gun. Identified molecules and molecular
fragments are compared against known biological databases and the reported
peaks have at least 65 ppm mass accuracy. These molecules range from lipids and
fatty acids to flavonoids, quinolones, and other naturally occurring organic
compounds. It is anticipated that the spectral identification of key peaks will
assist detection of metabolites, extracellular polymeric substance molecules
like polysaccharides, and biologically relevant small molecules using ToF-SIMS
in future surface and interface research.",318,2407.20414v1,q-bio.BM,q-bio.BM,biotechnology,2024-07-29,2024-12-23T21:07:10.184921
"Enhanced Piezoelectricity in Sustainable-by-design Chitosan Nanocomposite Elastomers for Prosthetics, Robotics, and Circular Electronics","Piezoelectricity, the generation of electric charge in response to mechanical
stress, is a key property in both natural and synthetic materials. This study
significantly boosts the piezoelectric response of chitosan, a biodegradable
biopolymer, by integrating chitin/chitosan nanocrystals into natural
chitosan-based thin film elastomers. The resulting materials achieve d$_{33}$
values of 15-19 pmV$^{-1}$, a marked improvement over the 5-9 pmV$^{-1}$
observed in pure chitosan films thanks to increased crystallinity from the
nanocrystals. We utilize piezoresponse force microscopy (PFM) to accurately
measure the d$_{33}$ coefficient, employing an engineered extraction method
that eliminates the electrostatic contribution, which can overestimate the
piezoelectric response. The resulting chitosan elastomers exhibit elastic
deformation up to 40\% strain and a Young's modulus of approximately 100 MPa,
similar to soft tissues. These properties, along with the fact that the
employed materials can be entirely crafted from upcycled biowaste, make these
elastomers ideal for prosthetics, wearable devices, energy harvesters, and
sustainable transducers. Our findings underscore the potential of
chitosan-based piezoelectric materials for advanced applications in
biotechnology, soft robotics, and the green Internet of Things.",317,2407.18585v1,cond-mat.other,cond-mat.other,biotechnology,2024-07-26,2024-12-23T21:07:10.185918
FoodMem: Near Real-time and Precise Food Video Segmentation,"Food segmentation, including in videos, is vital for addressing real-world
health, agriculture, and food biotechnology issues. Current limitations lead to
inaccurate nutritional analysis, inefficient crop management, and suboptimal
food processing, impacting food security and public health. Improving
segmentation techniques can enhance dietary assessments, agricultural
productivity, and the food production process. This study introduces the
development of a robust framework for high-quality, near-real-time segmentation
and tracking of food items in videos, using minimal hardware resources. We
present FoodMem, a novel framework designed to segment food items from video
sequences of 360-degree unbounded scenes. FoodMem can consistently generate
masks of food portions in a video sequence, overcoming the limitations of
existing semantic segmentation models, such as flickering and prohibitive
inference speeds in video processing contexts. To address these issues, FoodMem
leverages a two-phase solution: a transformer segmentation phase to create
initial segmentation masks and a memory-based tracking phase to monitor food
masks in complex scenes. Our framework outperforms current state-of-the-art
food segmentation models, yielding superior performance across various
conditions, such as camera angles, lighting, reflections, scene complexity, and
food diversity. This results in reduced segmentation noise, elimination of
artifacts, and completion of missing segments. Here, we also introduce a new
annotated food dataset encompassing challenging scenarios absent in previous
benchmarks. Extensive experiments conducted on Nutrition5k and Vegetables &
Fruits datasets demonstrate that FoodMem enhances the state-of-the-art by 2.5%
mean average precision in food video segmentation and is 58 x faster on
average.",348,2407.12121v1,cs.CV,cs.CV,biotechnology,2024-07-16,2024-12-23T21:07:10.186915
Understanding and Diagnosing Deep Reinforcement Learning,"Deep neural policies have recently been installed in a diverse range of
settings, from biotechnology to automated financial systems. However, the
utilization of deep neural networks to approximate the value function leads to
concerns on the decision boundary stability, in particular, with regard to the
sensitivity of policy decision making to indiscernible, non-robust features due
to highly non-convex and complex deep neural manifolds. These concerns
constitute an obstruction to understanding the reasoning made by deep neural
policies, and their foundational limitations. Hence, it is crucial to develop
techniques that aim to understand the sensitivities in the learnt
representations of neural network policies. To achieve this we introduce a
theoretically founded method that provides a systematic analysis of the
unstable directions in the deep neural policy decision boundary across both
time and space. Through experiments in the Arcade Learning Environment (ALE),
we demonstrate the effectiveness of our technique for identifying correlated
directions of instability, and for measuring how sample shifts remold the set
of sensitive directions in the neural policy landscape. Most importantly, we
demonstrate that state-of-the-art robust training techniques yield learning of
disjoint unstable directions, with dramatically larger oscillations over time,
when compared to standard training. We believe our results reveal the
fundamental properties of the decision process made by reinforcement learning
policies, and can help in constructing reliable and robust deep neural
policies.",276,2406.16979v1,cs.LG,"cs.LG,cs.AI",biotechnology,2024-06-23,2024-12-23T21:07:10.186915
A comprehensive approach to incorporating intermolecular dispersion into the openCOSMO-RS model. Part 1: Halocarbons,"The COSMO-RS (Conductor-like Screening Model for Real Solvents) is a
predictive thermodynamic model that has found diverse applications in various
domains like chemical engineering, environmental chemistry, nanotechnology,
material science, and biotechnology. Its core concept involves calculating the
screening charge density on the surface of each molecule and letting these
surface patches interact with each other to calculate thermodynamic properties.
In this study, we aim to enhance the performance of the open-source
implementation openCOSMO-RS by incorporating dispersive interactions between
the paired segments. Several parametrizations were systematically evaluated
through the extensive regression analysis using a comprehensive database of
Vapor-Liquid Equilibrium (VLE), Liquid-Liquid Equilibrium (LLE) and Infinite
Dilution Activity Coefficients (IDACs). Furthermore, the influence of different
combinatorial terms on the model performance was investigated. Our findings
indicate that incorporating dispersive interactions significantly improves the
accuracy of phase equilibrium predictions for halocarbons and refrigerant
mixtures.",210,2406.05244v1,cond-mat.soft,"cond-mat.soft,physics.chem-ph,physics.comp-ph",biotechnology,2024-06-07,2024-12-23T21:07:10.187913
Generalized Inverse Optimal Control and its Application in Biology,"Living organisms exhibit remarkable adaptations across all scales, from
molecules to ecosystems. We believe that many of these adaptations correspond
to optimal solutions driven by evolution, training, and underlying physical and
chemical laws and constraints. While some argue against such optimality
principles due to their potential ambiguity, we propose generalized inverse
optimal control to infer them directly from data. This novel approach
incorporates multi-criteria optimality, nestedness of objective functions on
different scales, the presence of active constraints, the possibility of
switches of optimality principles during the observed time horizon,
maximization of robustness, and minimization of time as important special
cases, as well as uncertainties involved with the mathematical modeling of
biological systems. This data-driven approach ensures that optimality
principles are not merely theoretical constructs but are firmly rooted in
experimental observations. Furthermore, the inferred principles can be used in
forward optimal control to predict and manipulate biological systems, with
possible applications in bio-medicine, biotechnology, and agriculture. As
discussed and illustrated, the well-posed problem formulation and the inference
are challenging and require a substantial interdisciplinary effort in the
development of theory and robust numerical methods.",231,2405.20747v1,q-bio.QM,"q-bio.QM,math.OC",biotechnology,2024-05-31,2024-12-23T21:07:10.188910
Deep Learning for Protein-Ligand Docking: Are We There Yet?,"The effects of ligand binding on protein structures and their in vivo
functions carry numerous implications for modern biomedical research and
biotechnology development efforts such as drug discovery. Although several deep
learning (DL) methods and benchmarks designed for protein-ligand docking have
recently been introduced, to date no prior works have systematically studied
the behavior of docking methods within the broadly applicable context of (1)
using predicted (apo) protein structures for docking (e.g., for applicability
to unknown structures); (2) docking multiple ligands concurrently to a given
target protein (e.g., for enzyme design); and (3) having no prior knowledge of
binding pockets (e.g., for unknown pocket generalization). To enable a deeper
understanding of docking methods' real-world utility, we introduce PoseBench,
the first comprehensive benchmark for broadly applicable protein-ligand
docking. PoseBench enables researchers to rigorously and systematically
evaluate DL docking methods for apo-to-holo protein-ligand docking and
protein-ligand structure generation using both single and multi-ligand
benchmark datasets, the latter of which we introduce for the first time to the
DL community. Empirically, using PoseBench, we find that (1) DL methods
consistently outperform conventional docking algorithms; (2) most recent DL
docking methods fail to generalize to multi-ligand protein targets; and (3)
training DL methods with physics-informed loss functions on diverse clusters of
protein-ligand complexes is a promising direction for future work. Code, data,
tutorials, and benchmark results are available at
https://github.com/BioinfoMachineLearning/PoseBench.",350,2405.14108v4,cs.LG,"cs.LG,cs.AI,q-bio.BM,q-bio.QM,I.2.1; J.3",biotechnology,2024-05-23,2024-12-23T21:07:10.189907
Nano-gap electrode dielectrophoresis for tether-free trapping and interferometric-scattering detection of single 20 nm particles,"Accurate detection and characterization of nanoparticles within confined
spaces is crucial for applications ranging from nanofluidics to biotechnology.
We present a novel approach that combines interferometric scattering (iSCAT)
detection with trapping by dielectrophoresis (DEP) to achieve label-free
detection of nanoparticles that are trapped and/or actuated between nano-gap
electrodes. DEP utilizes the interaction between the induced dipole of the
particle and the applied electric field to create a trapping potential. We
demonstrate our method by trapping and label-free detection of down to 20 nm
polystyrene nanoparticles. Additionally, we demonstrate that the
signal-to-noise ratio of our detection can be boosted up to 20-fold by periodic
actuation of the nanoparticle in the trap. This is done by a digital lock-in
detection scheme on the modulated scattering signal. Our method holds promise
for various applications, including assembly of nanoparticles, single-particle
property analysis, and nanofluidic devices.",215,2405.12338v1,physics.optics,"physics.optics,physics.chem-ph",biotechnology,2024-05-20,2024-12-23T21:07:10.190905
Molecular techniques employed in CTG(Ser1) and CTG(Ala) D-xylose metabolizing yeast clades for strain design and industrial applications,"D-xylose is the second most abundant monosaccharide found in lignocellulose
and is of biotechnological importance for producing second-generation ethanol
and other high-value chemical compounds. D-xylose conversion to ethanol is
promoted by microbial fermentation, mainly by bacteria, yeasts, or filamentous
fungi. Considering yeasts, species belonging to the CTG(Ser1) or CTG(Ala) clade
display a remarkable ability to ferment D-xylose to ethanol and other
compounds; however, these yeasts are not employed on an industrial scale due to
the poor fermentative performance compared to conventional yeasts, like
Saccharomyces cerevisiae, and also due to the lack of a molecular toolbox for
development of new strains tailored to fermentation stress tolerance and
performance. Thus, the purpose of this review is to evaluate the major
molecular tools (e.g., transformation markers and techniques, vectors,
regulatory sequences, and gene editing techniques) available for the most
studied yeasts of CTG(Ser1) clade, like Scheffersomyces, Spathaspora, Candida
and Yamadazyma species, and the CTG(Ala) clade representative Pachysolen
tannophilus. Furthermore, we synthesized the current state-of-the-art molecular
developments and perspectives for D-xylose fermenting yeast strain design.",303,2405.11010v2,q-bio.QM,q-bio.QM,biotechnology,2024-05-17,2024-12-23T21:07:10.190905
Representing Information on DNA using Patterns Induced by Enzymatic Labeling,"Enzymatic DNA labeling is a powerful tool with applications in biochemistry,
molecular biology, biotechnology, medical science, and genomic research. This
paper contributes to the evolving field of DNA-based data storage by presenting
a formal framework for modeling DNA labeling in strings, specifically tailored
for data storage purposes. Our approach involves a known DNA molecule as a
template for labeling, employing patterns induced by a set of designed labels
to represent information. One hypothetical implementation can use CRISPR-Cas9
and gRNA reagents for labeling. Various aspects of the general labeling
channel, including fixed-length labels, are explored, and upper bounds on the
maximal size of the corresponding codes are given. The study includes the
development of an efficient encoder-decoder pair that is proven optimal in
terms of maximum code size under specific conditions.",166,2405.08475v1,cs.IT,"cs.IT,math.IT",biotechnology,2024-05-14,2024-12-23T21:07:10.191904
DNA Calorimetric Force Spectroscopy at Single Base Pair Resolution,"DNA hybridization is a fundamental reaction with wide-ranging applications in
biotechnology. The nearest-neighbor (NN) model provides the most reliable
description of the energetics of duplex formation. Most DNA thermodynamics
studies have been done in melting experiments in bulk, of limited resolution
due to ensemble averaging. In contrast, single-molecule methods have reached
the maturity to derive DNA thermodynamics with unprecedented accuracy. We
combine single-DNA mechanical unzipping experiments using a temperature jump
optical trap with machine learning methods and derive the temperature-dependent
DNA energy parameters of the NN model. In particular, we measure the previously
unknown ten heat-capacity change parameters $\Delta C_p$, relevant for
thermodynamical predictions throughout the DNA stability range. Calorimetric
force spectroscopy establishes a groundbreaking methodology to accurately study
nucleic acids, from chemically modified DNA to RNA and DNA/RNA hybrid
structures.",190,2404.18785v1,q-bio.BM,"q-bio.BM,physics.bio-ph",biotechnology,2024-04-29,2024-12-23T21:07:10.192901
The light quantum mechanism of PCR efficiency oscillation with gold nanoparticle concentration,"The widespread application of nanomaterials in polymerase chain reaction
(PCR) technology has opened new avenues for improving detection methods in the
biomedical field. Recent experiments (Chem. Eur. J. 2023, e202203513) have
revealed oscillatory behavior between PCR efficiency and the concentration of
gold nanoparticles in the pM range, potentially linked to the long-range
Coulomb interactions among charged colloidal particles and the quantum size
effect of nanoparticle electronic states. Through Monte Carlo simulation, we
discovered that the radial distribution function of gold nanoparticles in
solution gradually exhibits peak characteristics with increasing charge,
triggering coherent photon behavior in Rayleigh scattering within the solution,
thereby influencing the efficiency of reusing released photons in the PCR chain
reaction. The study demonstrates that the oscillation period aligns with the
wavelength of downstream reaction photons, while their energy matches the width
of energy levels near the Fermi level of gold nanoparticles. The latter can
absorb and store electron states internally, promoting upstream PCR reactions
through subsequent re-release, and compensating for energy deficiencies through
the Boltzmann distribution of electrons. This work is poised to advance the
application of PCR-specific precise detection methods in the field of quantum
biotechnology.",269,2404.12153v1,physics.bio-ph,"physics.bio-ph,q-bio.QM",biotechnology,2024-04-16,2024-12-23T21:07:10.192901
Latent Chemical Space Searching for Plug-in Multi-objective Molecule Generation,"Molecular generation, an essential method for identifying new drug
structures, has been supported by advancements in machine learning and
computational technology. However, challenges remain in multi-objective
generation, model adaptability, and practical application in drug discovery. In
this study, we developed a versatile 'plug-in' molecular generation model that
incorporates multiple objectives related to target affinity, drug-likeness, and
synthesizability, facilitating its application in various drug development
contexts. We improved the Particle Swarm Optimization (PSO) in the context of
drug discoveries, and identified PSO-ENP as the optimal variant for
multi-objective molecular generation and optimization through comparative
experiments. The model also incorporates a novel target-ligand affinity
predictor, enhancing the model's utility by supporting three-dimensional
information and improving synthetic feasibility. Case studies focused on
generating and optimizing drug-like big marine natural products were performed,
underscoring PSO-ENP's effectiveness and demonstrating its considerable
potential for practical drug discovery applications.",203,2404.06691v1,q-bio.BM,"q-bio.BM,cs.LG,cs.NE",biotechnology,2024-04-10,2024-12-23T21:07:10.193898
In silico bioactivity prediction of proteins interacting with graphene-based nanomaterials guides rational design of biosensor,"Graphene based nanomaterials have attracted significant attention for their
potentials in biomedical and biotechnology applications in recent years, owing
to the outstanding physical and chemical properties. However, the interaction
mechanism and impact on biological activity of macro and micro biomolecules
still require more concerns and further research in order to enhance their
applicability in biosensors, etc. Herein, an integrated method has been
developed to predict the protein bioactivity performance when interacting with
nanomaterials for protein based biosensor. Molecular dynamics simulation and
molecular docking technique were consolidated to investigate several
nanomaterials C60 fullerene, single walled carbon nanotube, pristine graphene
and graphene oxide, and their effect when interacting with protein. The
adsorption behavior, secondary structure changes and protein bioactivity
changes were simulated, and the results of protein activity simulation were
verified in combination with atomic force spectrum, circular dichroism spectrum
fluorescence and electrochemical experiments. The best quantification alignment
between bioactivity obtained by simulation and experiment measurements was
further explored. The two proteins, RNase A and Exonuclease III, were regarded
as analysis model for the proof of concept, and the prediction accuracy of
protein bioactivty could reach up to 0.98.",263,2404.05329v1,q-bio.BM,q-bio.BM,biotechnology,2024-04-08,2024-12-23T21:07:10.194894
Transmission IR Microscopy for the Quantitation of Biomolecular Mass In Live Cells,"Absolute quantity imaging of biomolecules on a single cell level is critical
for measurement assurance in biosciences and bioindustries. While infrared (IR)
transmission microscopy is a powerful label-free imaging modality capable of
chemical quantification, its applicability to hydrated biological samples
remains challenging due to the strong water absorption. We overcome this
challenge by applying a solvent absorption compensation (SAC) technique to a
home-built quantum cascade laser IR microscope. SAC-IR microscopy improves the
chemical sensitivity considerably by adjusting the incident light intensity to
pre-compensate the IR absorption by water while retaining the full dynamic
range. We demonstrate the label-free chemical imaging of key biomolecules of a
cell, such as protein, fatty acid, and nucleic acid, with sub-cellular spatial
resolution. By imaging live fibroblast cells over twelve hours, we monitor the
mass change of the three molecular species of single cells at various phases,
including cell division. While the current live-cell imaging demonstration
involved three wavenumbers, more wavenumber images could measure more
biomolecules in live cells with higher accuracy. As a label-free method to
measure absolute quantities of various molecules in a cell, SAC-IR microscopy
can potentially become a standard chemical characterization tool for live cells
in biology, medicine, and biotechnology.",274,2403.18881v1,q-bio.QM,"q-bio.QM,physics.optics",biotechnology,2024-03-27,2024-12-23T21:07:10.195892
Investigation of Genomic Effect of Zirconium Oxide Nanoparticles in Escherichia coli Bacteria,"Due to the concerns of the society about the increase of antibiotic resistant
infections, many studies and research have been done on nanoparticles and
applications of nano-biotechnology. Zirconium Oxide ($\text{ZrO}_{2}$) in which
called zirconia, is a white oxide of zirconium metal that its diameter is 20
nm. The colloidal size of these particles is often smaller than bacterial and
eukaryotic cells. The main intention of this paper is to investigate the effect
of different doses of $\text{ZrO}_{2}$ NPs on the sequences changes for the
$\textit{Escherichia coli}$ ($\textit{E. coli}$) genome. At the first step,
$\textit{E. coli}$ was cultured in eosin methylene blue agar and brain heart
broth (BHB) mediums, respectively. Then, bacteria were treated with
$\text{ZrO}_{2}$ NPs at concentrations of 100, 250, and 350 $\mu$g/ml. After
treatment, the growth of bacteria was evaluated by utilizing spectrophotometry
at 600 nm after incubation times including 2, 4, 6, 8, and 24 hours at 37
$^{\circ}$C. At the second step, the extraction of DNA was performed by using
control and treated samples. Then, the changes in the sequence of bacterial
genome were investigated using RAPD markers. Finally, NTSYS-PC platform was
employed in order to analyze of the results extracted by electrophoresis of
products on agarose gel. In this paper, it was observed that $\text{ZrO}_{2}$
NPs can inhibit the growth of bacteria at concentrations of 250 and 350
$\mu$g/ml after 8 hours of treatment. It was also found that the
$\text{ZrO}_{2}$ NPs at different concentrations have not changed the genome
sequence of $\textit{E. coli}$. Furthermore, it was concluded that the
$\text{ZrO}_{2}$ NPs with the concentration of 350 $\mu$g/ml had the highest
inhibitory properties without significant changing in the genomic sequence of
$\textit{E. coli}$.",502,2403.14728v1,q-bio.OT,q-bio.OT,biotechnology,2024-03-21,2024-12-23T21:07:10.196889
From Explainable to Interpretable Deep Learning for Natural Language Processing in Healthcare: How Far from Reality?,"Deep learning (DL) has substantially enhanced natural language processing
(NLP) in healthcare research. However, the increasing complexity of DL-based
NLP necessitates transparent model interpretability, or at least
explainability, for reliable decision-making. This work presents a thorough
scoping review of explainable and interpretable DL in healthcare NLP. The term
""eXplainable and Interpretable Artificial Intelligence"" (XIAI) is introduced to
distinguish XAI from IAI. Different models are further categorized based on
their functionality (model-, input-, output-based) and scope (local, global).
Our analysis shows that attention mechanisms are the most prevalent emerging
IAI technique. The use of IAI is growing, distinguishing it from XAI. The major
challenges identified are that most XIAI does not explore ""global"" modelling
processes, the lack of best practices, and the lack of systematic evaluation
and benchmarks. One important opportunity is to use attention mechanisms to
enhance multi-modal XIAI for personalized medicine. Additionally, combining DL
with causal logic holds promise. Our discussion encourages the integration of
XIAI in Large Language Models (LLMs) and domain-specific smaller models. In
conclusion, XIAI adoption in healthcare requires dedicated in-house expertise.
Collaboration with domain experts, end-users, and policymakers can lead to
ready-to-use XIAI methods across NLP and medical tasks. While challenges exist,
XIAI techniques offer a valuable foundation for interpretable NLP algorithms in
healthcare.",308,2403.11894v4,cs.CL,"cs.CL,cs.AI,cs.LG",biotechnology,2024-03-18,2024-12-23T21:07:10.196889
HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding,"The rapid advance of Large Language Models (LLMs) has catalyzed the
development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid
modality-specific encoders, offer a promising alternative to the compositional
ones but face the challenge of inferior performance. Most existing monolithic
VLMs require tuning pre-trained LLMs to acquire vision abilities, which may
degrade their language capabilities. To address this dilemma, this paper
presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs
have been shown capable of interpreting images, when image embeddings are
aligned with text embeddings. The challenge for current monolithic VLMs
actually lies in the lack of a holistic embedding module for both vision and
language inputs. Therefore, HoVLE introduces a holistic embedding module that
converts visual and textual inputs into a shared space, allowing LLMs to
process images in the same way as texts. Furthermore, a multi-stage training
strategy is carefully designed to empower the holistic embedding module. It is
first trained to distill visual features from a pre-trained vision encoder and
text embeddings from the LLM, enabling large-scale training with unpaired
random images and text tokens. The whole model further undergoes next-token
prediction on multi-modal data to align the embeddings. Finally, an
instruction-tuning stage is incorporated. Our experiments show that HoVLE
achieves performance close to leading compositional models on various
benchmarks, outperforming previous monolithic models by a large margin. Model
available at https://huggingface.co/OpenGVLab/HoVLE.",362,2412.16158v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.106090
Personalized Representation from Personalized Generation,"Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.",211,2412.16156v1,cs.CV,"cs.CV,cs.LG",computational biology,2024-12-20,2024-12-23T21:07:11.107088
Can Generative Video Models Help Pose Estimation?,"Pairwise pose estimation from images with little or no overlap is an open
challenge in computer vision. Existing methods, even those trained on
large-scale datasets, struggle in these scenarios due to the lack of
identifiable correspondences or visual overlap. Inspired by the human ability
to infer spatial relationships from diverse scenes, we propose a novel
approach, InterPose, that leverages the rich priors encoded within pre-trained
generative video models. We propose to use a video model to hallucinate
intermediate frames between two input images, effectively creating a dense,
visual transition, which significantly simplifies the problem of pose
estimation. Since current video models can still produce implausible motion or
inconsistent geometry, we introduce a self-consistency score that evaluates the
consistency of pose predictions from sampled videos. We demonstrate that our
approach generalizes among three state-of-the-art video models and show
consistent improvements over the state-of-the-art DUSt3R on four diverse
datasets encompassing indoor, outdoor, and object-centric scenes. Our findings
suggest a promising avenue for improving pose estimation models by leveraging
large generative models trained on vast amounts of video data, which is more
readily available than 3D data. See our project page for results:
https://inter-pose.github.io/.",271,2412.16155v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.108084
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",computational biology,2024-12-20,2024-12-23T21:07:11.109083
Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training,"Vision Language Models (VLMs) can be trained more efficiently if training
sets can be reduced in size. Recent work has shown the benefits of masking text
during VLM training using a variety of approaches: truncation, random masking,
block masking and syntax masking. In this paper, we show that the best masking
strategy changes over training epochs and that, given sufficient training
epochs, word frequency information is what you need to achieve the best
performance. Experiments on a large range of data sets demonstrate the
advantages of our approach, called Contrastive Language-Image Pre-training with
word Frequency Masking (CLIPF). The benefits are particularly evident as the
number of input tokens decreases. We analyze the impact of CLIPF vs. other
masking approaches on word frequency balance and discuss the apparently
critical contribution of CLIPF in maintaining word frequency balance across POS
categories.",183,2412.16148v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.109083
SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild,"Seagrass meadows play a crucial role in marine ecosystems, providing
important services such as carbon sequestration, water quality improvement, and
habitat provision. Monitoring the distribution and abundance of seagrass is
essential for environmental impact assessments and conservation efforts.
However, the current manual methods of analyzing underwater video transects to
assess seagrass coverage are time-consuming and subjective. This work explores
the use of deep learning models to automate the process of seagrass detection
and coverage estimation from underwater video data. A dataset of over 8,300
annotated underwater images was created, and several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer, were evaluated for the task of binary classification of ``Eelgrass
Present'' and ``Eelgrass Absent'' images. The results demonstrate that deep
learning models, particularly the Vision Transformer, can achieve high
performance in predicting eelgrass presence, with AUROC scores exceeding 0.95
on the final test dataset. The use of transfer learning and the application of
the Deep WaveNet underwater image enhancement model further improved the
models' capabilities. The proposed methodology allows for the efficient
processing of large volumes of video data, enabling the acquisition of much
more detailed information on seagrass distributions compared to current manual
methods. This information is crucial for environmental impact assessments and
monitoring programs, as seagrasses are important indicators of coastal
ecosystem health. Overall, this project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.",309,2412.16147v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.110080
Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks,"State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset.",207,2412.16146v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.111077
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",computational biology,2024-12-20,2024-12-23T21:07:11.112074
FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph Attention Networks,"Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.",308,2412.16144v1,cs.LG,"cs.LG,cs.DC",computational biology,2024-12-20,2024-12-23T21:07:11.113071
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.113071
Cross-sectional Topology Optimization of Slender Soft Pneumatic Actuators using Genetic Algorithms and Geometrically Exact Beam Models,"The design of soft robots is still commonly driven by manual trial-and-error
approaches, requiring the manufacturing of multiple physical prototypes, which
in the end, is time-consuming and requires significant expertise. To reduce the
number of manual interventions in this process, topology optimization can be
used to assist the design process. The design is then guided by simulations and
numerous prototypes can be tested in simulation rather than being evaluated
through laborious experiments. To implement this simulation-driven design
process, the possible design space of a slender soft pneumatic actuator is
generalized to the design of the circular cross-section. We perform a black-box
topology optimization using genetic algorithms to obtain a cross-sectional
design of a soft pneumatic actuator that is capable of reaching a target
workspace defined by the end-effector positions at different pressure values.
This design method is evaluated for three different case studies and target
workspaces, which were either randomly generated or specified by the operator
of the design assistant. The black-box topology optimization based on genetic
algorithms proves to be capable of finding good designs under given plausible
target workspaces. We considered a simplified simulation model to verify the
efficacy of the employed method. An experimental validation has not yet been
performed. It can be concluded that the employed black-box topology
optimization can assist in the design process for slender soft pneumatic
actuators. It supports at searching for possible design prototypes that reach
points specified by corresponding actuation pressures. This helps reduce the
trial-and-error driven iterative manual design process and enables the operator
to focus on prototypes that already offer a good viable solution.",330,2412.16138v1,cs.RO,"cs.RO,physics.comp-ph",computational biology,2024-12-20,2024-12-23T21:07:11.114069
Camera-Based Localization and Enhanced Normalized Mutual Information,"Robust and fine localization algorithms are crucial for autonomous driving.
For the production of such vehicles as a commodity, affordable sensing
solutions and reliable localization algorithms must be designed. This work
considers scenarios where the sensor data comes from images captured by an
inexpensive camera mounted on the vehicle and where the vehicle contains a fine
global map. Such localization algorithms typically involve finding the section
in the global map that best matches the captured image. In harsh environments,
both the global map and the captured image can be noisy. Because of physical
constraints on camera placement, the image captured by the camera can be viewed
as a noisy perspective transformed version of the road in the global map. Thus,
an optimal algorithm should take into account the unequal noise power in
various regions of the captured image, and the intrinsic uncertainty in the
global map due to environmental variations. This article briefly reviews two
matching methods: (i) standard inner product (SIP) and (ii) normalized mutual
information (NMI). It then proposes novel and principled modifications to
improve the performance of these algorithms significantly in noisy
environments. These enhancements are inspired by the physical constraints
associated with autonomous vehicles. They are grounded in statistical signal
processing and, in some context, are provably better. Numerical simulations
demonstrate the effectiveness of such modifications.",259,2412.16137v1,cs.CV,"cs.CV,eess.SP,stat.AP",computational biology,2024-12-20,2024-12-23T21:07:11.115066
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",computational biology,2024-12-20,2024-12-23T21:07:11.116063
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",computational biology,2024-12-20,2024-12-23T21:07:11.117061
Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information,"We study mechanism design when agents hold private information about both
their preferences and a common payoff-relevant state. We show that standard
message-driven mechanisms cannot implement socially efficient allocations when
agents have multidimensional types, even under favorable conditions. To
overcome this limitation, we propose data-driven mechanisms that leverage
additional post-allocation information, modeled as an estimator of the
payoff-relevant state. Our data-driven mechanisms extend the classic
Vickrey-Clarke-Groves class. We show that they achieve exact implementation in
posterior equilibrium when the state is either fully revealed or the utility is
linear in an unbiased estimator. We also show that they achieve approximate
implementation with a consistent estimator, converging to exact implementation
as the estimator converges, and present bounds on the convergence rate. We
demonstrate applications to digital advertising auctions and large language
model (LLM)-based mechanisms, where user engagement naturally reveals relevant
information.",198,2412.16132v1,econ.TH,"econ.TH,cs.GT",computational biology,2024-12-20,2024-12-23T21:07:11.117061
LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical Analysis of Diffeomorphism,"Image registration is a core task in computational anatomy that establishes
correspondences between images. Invertible deformable registration, which
computes a deformation field and handles complex, non-linear transformation, is
essential for tracking anatomical variations, especially in neuroimaging
applications where inter-subject differences and longitudinal changes are key.
Analyzing the deformation fields is challenging due to their non-linearity,
limiting statistical analysis. However, traditional approaches for analyzing
deformation fields are computationally expensive, sensitive to initialization,
and prone to numerical errors, especially when the deformation is far from the
identity. To address these limitations, we propose the Log-Euclidean
Diffeomorphic Autoencoder (LEDA), an innovative framework designed to compute
the principal logarithm of deformation fields by efficiently predicting
consecutive square roots. LEDA operates within a linearized latent space that
adheres to the diffeomorphisms group action laws, enhancing our model's
robustness and applicability. We also introduce a loss function to enforce
inverse consistency, ensuring accurate latent representations of deformation
fields. Extensive experiments with the OASIS-1 dataset demonstrate the
effectiveness of LEDA in accurately modeling and analyzing complex non-linear
deformations while maintaining inverse consistency. Additionally, we evaluate
its ability to capture and incorporate clinical variables, enhancing its
relevance for clinical applications.",271,2412.16129v1,cs.CV,"cs.CV,cs.LG",computational biology,2024-12-20,2024-12-23T21:07:11.118058
Multi-scale reconstruction of large supply networks,"The structure of the supply chain network has important implications for
modelling economic systems, from growth trajectories to responses to shocks or
natural disasters. However, reconstructing firm-to-firm networks from available
information poses several practical and theoretical challenges: the lack of
publicly available data, the complexity of meso-scale structures, and the high
level of heterogeneity of firms. With this work we contribute to the literature
on economic network reconstruction by proposing a novel methodology based on a
recently developed multi-scale model. This approach has three main advantages
over other methods: its parameters are defined to maintain statistical
consistency at different scales of node aggregation, it can be applied in a
multi-scale setting, and it is computationally more tractable for very large
graphs. The consistency at different scales of aggregation, inherent to the
model definition, is preserved for any hierarchy of coarse-grainings. The
arbitrariness of the aggregation allows us to work across different scales,
making it possible to estimate model parameters even when node information is
inconsistent, such as when some nodes are firms while others are countries or
regions. Finally, the model can be fitted at an aggregate scale with lower
computational requirements, since the parameters are invariant to the grouping
of nodes. We assess the advantages and limitations of this approach by testing
it on two complementary datasets of Dutch firms constructed from inter-client
transactions on the bank accounts of two major Dutch banking institutions. We
show that the model reliably predicts important topological properties of the
observed network in several scenarios of practical interest and is therefore a
suitable candidate for reconstructing firm-to-firm networks at scale.",333,2412.16122v1,physics.soc-ph,"physics.soc-ph,econ.GN,q-fin.EC",computational biology,2024-12-20,2024-12-23T21:07:11.119055
PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics,"Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.",226,2412.16120v1,cs.CL,cs.CL,computational biology,2024-12-20,2024-12-23T21:07:11.119055
Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts,"This study investigates the potential of Large Language Models (LLMs),
particularly GPT-4o, for Optical Character Recognition (OCR) in low-resource
scripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.
Using a meticulously curated dataset of 2,520 images incorporating controlled
variations in text length, font size, background color, and blur, the research
simulates diverse real-world challenges. Results emphasize the limitations of
zero-shot LLM-based OCR, particularly for linguistically complex scripts,
highlighting the need for annotated datasets and fine-tuned models. This work
underscores the urgency of addressing accessibility gaps in text digitization,
paving the way for inclusive and robust OCR solutions for underserved
languages.",164,2412.16119v1,cs.LG,"cs.LG,cs.CV,eess.IV",computational biology,2024-12-20,2024-12-23T21:07:11.120052
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",computational biology,2024-12-20,2024-12-23T21:07:11.121050
PruneVid: Visual Token Pruning for Efficient Video Large Language Models,"In this paper, we introduce PruneVid, a visual token pruning method designed
to enhance the efficiency of multi-modal video understanding. Large Language
Models (LLMs) have shown promising performance in video tasks due to their
extended capabilities in comprehending visual modalities. However, the
substantial redundancy in video data presents significant computational
challenges for LLMs. To address this issue, we introduce a training-free method
that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)
leverages LLMs' reasoning capabilities to selectively prune visual features
relevant to question tokens, enhancing model efficiency. We validate our method
across multiple video benchmarks, which demonstrate that PruneVid can prune
over 80% of tokens while maintaining competitive performance combined with
different model networks. This highlights its superior effectiveness and
efficiency compared to existing pruning methods. Code:
https://github.com/Visual-AI/PruneVid.",204,2412.16117v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.121050
The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse,"There is an ongoing debate about how to moderate toxic speech on social media
and how content moderation affects online discourse. We propose and validate a
methodology for measuring the content-moderation-induced distortions in online
discourse using text embeddings from computational linguistics. We test our
measure on a representative dataset of 5 million US political Tweets and find
that removing toxic Tweets distorts online content. This finding is consistent
across different embedding models, toxicity metrics, and samples. Importantly,
we demonstrate that content-moderation-induced distortions are not caused by
the toxic language. Instead, we show that, as a side effect, content moderation
shifts the mean and variance of the embedding space, distorting the topic
composition of online content. Finally, we propose an alternative approach to
content moderation that uses generative Large Language Models to rephrase toxic
Tweets to preserve their salvageable content rather than removing them
entirely. We demonstrate that this rephrasing strategy reduces toxicity while
minimizing distortions in online content.",220,2412.16114v1,cs.SI,cs.SI,computational biology,2024-12-20,2024-12-23T21:07:11.122047
CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up,"Diffusion Transformers (DiT) have become a leading architecture in image
generation. However, the quadratic complexity of attention mechanisms, which
are responsible for modeling token-wise relationships, results in significant
latency when generating high-resolution images. To address this issue, we aim
at a linear attention mechanism in this paper that reduces the complexity of
pre-trained DiTs to linear. We begin our exploration with a comprehensive
summary of existing efficient attention mechanisms and identify four key
factors crucial for successful linearization of pre-trained DiTs: locality,
formulation consistency, high-rank attention maps, and feature integrity. Based
on these insights, we introduce a convolution-like local attention strategy
termed CLEAR, which limits feature interactions to a local window around each
query token, and thus achieves linear complexity. Our experiments indicate
that, by fine-tuning the attention layer on merely 10K self-generated samples
for 10K iterations, we can effectively transfer knowledge from a pre-trained
DiT to a student model with linear complexity, yielding results comparable to
the teacher model. Simultaneously, it reduces attention computations by 99.5%
and accelerates generation by 6.3 times for generating 8K-resolution images.
Furthermore, we investigate favorable properties in the distilled attention
layers, such as zero-shot generalization cross various models and plugins, and
improved support for multi-GPU parallel inference. Models and codes are
available here: https://github.com/Huage001/CLEAR.",308,2412.16112v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.123044
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",computational biology,2024-12-20,2024-12-23T21:07:11.123044
Logical Consistency of Large Language Models in Fact-checking,"In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses -- a meaning-preserving change in the input query
results in an inconsistent response and attributes to vulnerabilities of LLMs
such as hallucination, jailbreaking, etc. Consequently, existing research
focuses on simple paraphrasing-based consistency assessment of LLMs, and
ignores complex queries that necessitates an even better understanding of
logical reasoning by an LLM. Our work therefore addresses the logical
inconsistency of LLMs under complex logical queries with primitive logical
operators, e.g., negation, conjunction, and disjunction. As a test bed, we
consider retrieval-augmented LLMs on a fact-checking task involving
propositional logic queries from real-world knowledge graphs (KGs). Our
contributions are three-fold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries as input and demonstrate that existing LLMs lack
logical consistency, specially on complex queries. Improvement: We employ
supervised fine-tuning to improve the logical consistency of LLMs on the
complex fact-checking task with KG contexts.",309,2412.16100v1,cs.CL,cs.CL,computational biology,2024-12-20,2024-12-23T21:07:11.124042
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",computational biology,2024-12-20,2024-12-23T21:07:11.125038
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",computational biology,2024-12-20,2024-12-23T21:07:11.126037
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,computational biology,2024-12-20,2024-12-23T21:07:11.127033
Spiral waves speed up cell cycle oscillations in the frog cytoplasm,"Spiral waves are a well-known phenomenon in excitable media, playing critical
roles in biological systems such as cardiac tissues, where they are involved in
arrhythmias, and in slime molds, where they guide collective cell migration.
However, their presence in the cytoplasm of cells has not been reported to
date. In this study, we present the observation of spiral waves in a Xenopus
laevis frog egg extract reconstituting periodic cell cycle transitions. We find
that the emergence of these spiral waves accelerates the cell division cycle
nearly twofold. Using two distinct computational models, we demonstrate that
this behavior arises from generic principles and is driven primarily by
time-scale separation in the cell cycle oscillator. Additionally, we
investigate the interplay between these spiral waves and the more commonly
observed target pattern waves in the frog cytoplasm, providing new insights
into their dynamic interactions.",190,2412.16094v1,nlin.PS,"nlin.PS,physics.bio-ph,q-bio.CB",computational biology,2024-12-20,2024-12-23T21:07:11.127033
Social Group Human-Robot Interaction: A Scoping Review of Computational Challenges,"Group interactions are a natural part of our daily life, and as robots become
more integrated into society, they must be able to socially interact with
multiple people at the same time. However, group human-robot interaction (HRI)
poses unique computational challenges often overlooked in the current HRI
literature. We conducted a scoping review including 44 group HRI papers from
the last decade (2015-2024). From these papers, we extracted variables related
to perception and behaviour generation challenges, as well as factors related
to the environment, group, and robot capabilities that influence these
challenges. Our findings show that key computational challenges in perception
included detection of groups, engagement, and conversation information, while
challenges in behaviour generation involved developing approaching and
conversational behaviours. We also identified research gaps, such as improving
detection of subgroups and interpersonal relationships, and recommended future
work in group HRI to help researchers address these computational challenges",185,2412.16093v1,cs.RO,cs.RO,computational biology,2024-12-20,2024-12-23T21:07:11.128030
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,computational biology,2024-12-20,2024-12-23T21:07:11.129028
Decision algorithms for fragments of real analysis.\ II. A theory of differentiable functions with convexity and concavity predicates,"We address the decision problem for a fragment of real analysis involving
differentiable functions with continuous first derivatives. The proposed
theory, besides the operators of Tarski's theory of reals, includes predicates
for comparisons, monotonicity, convexity, and derivative of functions over
bounded closed intervals or unbounded intervals.
  Our decision algorithm is obtained by showing that satisfiable formulae of
our theory admit canonical models in which functional variables are interpreted
as piecewise exponential functions. These can be implicitly described within
the decidable Tarski's theory of reals.
  Our satisfiability test generalizes previous decidability results not
involving derivative operators.",137,2412.16091v1,cs.LO,"cs.LO,03B25, 26A99",computational biology,2024-12-20,2024-12-23T21:07:11.130025
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",computational biology,2024-12-20,2024-12-23T21:07:11.130025
Efficient MedSAMs: Segment Anything in Medical Images on Laptop,"Promptable segmentation foundation models have emerged as a transformative
approach to addressing the diverse needs in medical images, but most existing
models require expensive computing, posing a big barrier to their adoption in
clinical practice. In this work, we organized the first international
competition dedicated to promptable medical image segmentation, featuring a
large-scale dataset spanning nine common imaging modalities from over 20
different institutions. The top teams developed lightweight segmentation
foundation models and implemented an efficient inference pipeline that
substantially reduced computational requirements while maintaining
state-of-the-art segmentation accuracy. Moreover, the post-challenge phase
advanced the algorithms through the design of performance booster and
reproducibility tasks, resulting in improved algorithms and validated
reproducibility of the winning solution. Furthermore, the best-performing
algorithms have been incorporated into the open-source software with a
user-friendly interface to facilitate clinical adoption. The data and code are
publicly available to foster the further development of medical image
segmentation foundation models and pave the way for impactful real-world
applications.",213,2412.16085v1,eess.IV,"eess.IV,cs.CV",computational biology,2024-12-20,2024-12-23T21:07:11.131022
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",computational biology,2024-12-20,2024-12-23T21:07:11.132019
Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg Evolutionary Game,"Decentralised learning enables the training of deep learning algorithms
without centralising data sets, resulting in benefits such as improved data
privacy, operational efficiency and the fostering of data ownership policies.
However, significant data imbalances pose a challenge in this framework.
Participants with smaller datasets in distributed learning environments often
achieve poorer results than participants with larger datasets. Data imbalances
are particularly pronounced in medical fields and are caused by different
patient populations, technological inequalities and divergent data collection
practices.
  In this paper, we consider distributed learning as an Stackelberg
evolutionary game. We present two algorithms for setting the weights of each
node's contribution to the global model in each training round: the
Deterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg
Weighting Model (ASWM). We use three medical datasets to highlight the impact
of dynamic weighting on underrepresented nodes in distributed learning. Our
results show that the ASWM significantly favours underrepresented nodes by
improving their performance by 2.713% in AUC. Meanwhile, nodes with larger
datasets experience only a modest average performance decrease of 0.441%.",250,2412.16079v1,cs.LG,"cs.LG,cs.CV,cs.GT,cs.NE",computational biology,2024-12-20,2024-12-23T21:07:11.133017
SegCol Challenge: Semantic Segmentation for Tools and Fold Edges in Colonoscopy data,"Colorectal cancer (CRC) remains a leading cause of cancer-related deaths
worldwide, with polyp removal being an effective early screening method.
However, navigating the colon for thorough polyp detection poses significant
challenges. To advance camera navigation in colonoscopy, we propose the
Semantic Segmentation for Tools and Fold Edges in Colonoscopy (SegCol)
Challenge. This challenge introduces a dataset from the EndoMapper repository,
featuring manually annotated, pixel-level semantic labels for colon folds and
endoscopic tools across selected frames from 96 colonoscopy videos. By
providing fold edges as anatomical landmarks and depth discontinuity
information from both fold and tool labels, the dataset is aimed to improve
depth perception and localization methods. Hosted as part of the Endovis
Challenge at MICCAI 2024, SegCol aims to drive innovation in colonoscopy
navigation systems. Details are available at
https://www.synapse.org/Synapse:syn54124209/wiki/626563, and code resources at
https://github.com/surgical-vision/segcol_challenge .",249,2412.16078v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.134014
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,computational biology,2024-12-20,2024-12-23T21:07:11.134014
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,computational biology,2024-12-20,2024-12-23T21:07:11.135011
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",computational biology,2024-12-20,2024-12-23T21:07:11.136009
Motif Caller: Sequence Reconstruction for Motif-Based DNA Storage,"DNA data storage is rapidly gaining traction as a long-term data archival
solution, primarily due to its exceptional durability. Retrieving stored data
relies on DNA sequencing, which involves a process called basecalling -- a
typically costly and slow task that uses machine learning to map raw sequencing
signals back to individual DNA bases (which are then translated into digital
bits to recover the data). Current models for basecalling have been optimized
for reading individual bases. However, with the advent of novel DNA synthesis
methods tailored for data storage, there is significant potential for
optimizing the reading process. In this paper, we focus on Motif-based DNA
synthesis, where sequences are constructed from motifs -- groups of bases --
rather than individual bases. To enable efficient reading of data stored in DNA
using Motif-based DNA synthesis, we designed Motif Caller, a machine learning
model built to detect entire motifs within a DNA sequence, rather than
individual bases. Motifs can also be detected from individually identified
bases using a basecaller and then searching for motifs, however, such an
approach is unnecessarily complex and slow. Building a machine learning model
that directly identifies motifs allows to avoid the additional step of
searching for motifs. It also makes use of the greater amount of features per
motif, thus enabling finding the motifs with higher accuracy. Motif Caller
significantly enhances the efficiency and accuracy of data retrieval in DNA
storage based on Motif-Based DNA synthesis.",295,2412.16074v1,cs.OH,"cs.OH,q-bio.GN",computational biology,2024-12-20,2024-12-23T21:07:11.137006
Correct implied volatility shapes and reliable pricing in the rough Heston model,"We use modifications of the Adams method and very fast and accurate
sinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and
Levendorski\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for
options of moderate and long maturities and strikes not very far from the spot,
thousands of prices can be calculated in several msec. with relative errors of
the order of 0.5\% and smaller running Matlab on a Mac with moderate
characteristics. We demonstrate that for the calibrated set of parameters in
Euch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility
surface is significantly flatter and fits the data very poorly, hence, the
calibration results in op.cit. is an example of the {\em ghost calibration}
(M.Boyarchenko and Levendorki\u{i}, Quantitative Finance 2015, v. 15): the
errors of the model and numerical method almost cancel one another. We explain
how calibration errors of this sort are generated by each of popular versions
of numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)
with prefixed parameters of a numerical method, resulting in spurious
volatility smiles and skews. We suggest a general {\em Conformal Bootstrap
principle} which allows one to avoid ghost calibration errors. We outline
schemes of application of Conformal Bootstrap principle and the method of the
paper to the design of accurate and fast calibration procedures.",339,2412.16067v1,q-fin.MF,"q-fin.MF,q-fin.CP,60-08, 60E10, 60G10, 60G22, 65C20, 65D30, 65G20, 91G20, 91G60",computational biology,2024-12-20,2024-12-23T21:07:11.137006
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",computational biology,2024-12-20,2024-12-23T21:07:11.138003
On the Impact of 3D Visualization of Repository Metrics in Software Engineering Education,"Context: Software development is a complex socio-technical process requiring
a deep understanding of various aspects. In order to support practitioners in
understanding such a complex activity, repository process metrics, like number
of pull requests and issues, emerged as crucial for evaluating CI/CD workflows
and guiding informed decision-making. The research community proposed different
ways to visualize these metrics to increase their impact on developers' process
comprehension: VR is a promising one. Nevertheless, despite such promising
results, the role of VR, especially in educational settings, has received
limited research attention. Objective: This study aims to address this gap by
exploring how VR-based repository metrics visualization can support the
teaching of process comprehension. Method: The registered report proposes the
execution of a controlled experiment where VR and non-VR approaches will be
compared, with the final aim to assess whether repository metrics in VR's
impact on learning experience and software process comprehension. By immersing
students in an intuitive environment, this research hypothesizes that VR can
foster essential analytical skills, thus preparing software engineering
students more effectively for industry requirements and equipping them to
navigate complex software development tasks with enhanced comprehension and
critical thinking abilities.",243,2412.16061v1,cs.CY,"cs.CY,cs.SE",computational biology,2024-12-20,2024-12-23T21:07:11.139001
Adaptable TeaStore,"Adaptability is a fundamental requirement for modern Cloud software
architectures to ensure robust performance in the face of diverse known and
unforeseen events inherent to distributed systems. State-of-the-art Cloud
systems frequently adopt microservices or serverless architectures. Among
these, TeaStore is a recognised microservice reference architecture that offers
a benchmarking framework for modelling and resource management techniques.
However, TeaStore's original configuration lacks the flexibility necessary to
address the varied scenarios encountered in real-world applications. To
overcome this limitation, we propose an enhanced variant of TeaStore that
distinguishes between mandatory and optional services while incorporating
third-party service integration. Core services such as WebUI, Image Provider,
and Persistence are designated as mandatory to maintain essential
functionality, whereas optional services, such as Recommender and Auth, extend
the architecture's feature set. We outline the design and configuration
possibilities of this adaptable TeaStore variant, aimed at enabling a broader
spectrum of configurability and operational resilience.",213,2412.16060v1,cs.DC,cs.DC,computational biology,2024-12-20,2024-12-23T21:07:11.139001
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",computational biology,2024-12-20,2024-12-23T21:07:11.139998
SAT Solving for Variants of First-Order Subsumption,"Automated reasoners, such as SAT/SMT solvers and first-order provers, are
becoming the backbones of rigorous systems engineering, being used for example
in applications of system verification, program synthesis, and cybersecurity.
Automation in these domains crucially depends on the efficiency of the
underlying reasoners towards finding proofs and/or counterexamples of the task
to be enforced. In order to gain efficiency, automated reasoners use dedicated
proof rules to keep proof search tractable. To this end, (variants of)
subsumption is one of the most important proof rules used by automated
reasoners, ranging from SAT solvers to first-order theorem provers and beyond.
  It is common that millions of subsumption checks are performed during proof
search, necessitating efficient implementations. However, in contrast to
propositional subsumption as used by SAT solvers and implemented using
sophisticated polynomial algorithms, first-order subsumption in first-order
theorem provers involves NP-complete search queries, turning the efficient use
of first-order subsumption into a huge practical burden.
  In this paper we argue that the integration of a dedicated SAT solver opens
up new venues for efficient implementations of first-order subsumption and
related rules. We show that, by using a flexible learning approach to choose
between various SAT encodings of subsumption variants, we greatly improve the
scalability of first-order theorem proving. Our experimental results
demonstrate that, by using a tailored SAT solver within first-order reasoning,
we gain a large speedup in solving state-of-the-art benchmarks.",331,2412.16058v1,cs.LO,cs.LO,computational biology,2024-12-20,2024-12-23T21:07:11.140995
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",computational biology,2024-12-20,2024-12-23T21:07:11.141994
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",computational biology,2024-12-20,2024-12-23T21:07:11.141994
Segmentation of arbitrary features in very high resolution remote sensing imagery,"Very high resolution (VHR) mapping through remote sensing (RS) imagery
presents a new opportunity to inform decision-making and sustainable practices
in countless domains. Efficient processing of big VHR data requires automated
tools applicable to numerous geographic regions and features. Contemporary RS
studies address this challenge by employing deep learning (DL) models for
specific datasets or features, which limits their applicability across
contexts.
  The present research aims to overcome this limitation by introducing
EcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.
EcoMapper fully automates processing of geospatial data, DL model training, and
inference. Models trained with EcoMapper successfully segmented two distinct
features in a real-world UAV dataset, achieving scores competitive with prior
studies which employed context-specific models.
  To evaluate EcoMapper, many additional models were trained on permutations of
principal field survey characteristics (FSCs). A relationship was discovered
allowing derivation of optimal ground sampling distance from feature size,
termed Cording Index (CI). A comprehensive methodology for field surveys was
developed to ensure DL methods can be applied effectively to collected data.
  The EcoMapper code accompanying this work is available at
https://github.com/hcording/ecomapper .",264,2412.16046v1,cs.CV,cs.CV,computational biology,2024-12-20,2024-12-23T21:07:11.142991
Stochastic Analysis of Entanglement-assisted Quantum Communication Channels,"In this paper, we present a queueing model for quantum communication
networks, a rapidly growing field of research inspired by its technological
promise and recent experimental successes. The model consists of a primary
queue and a service queue where Bell pairs are formed and stored. The Bell
pairs are by nature extremely short-lived rendering the service queue (the
quantum queue) much faster than the primary queue. We study the asymptotic
behaviour of this multi-scale queueing system utilizing the theory of
stochastic averaging principle. We prove a Functional Law of Large Numbers
(FLLN) and a Functional Central Limit Theorem (FCLT) for the standard queue
averaging the dynamics of the fast service queue. Our proofs are probablistic
and rely on the stochastic analysis of Stochastic Differential Equations (SDEs)
driven by Poisson Random Measures.",172,2412.16157v1,math.PR,"math.PR,cs.NI,quant-ph,60K25, 68M20, 60F17, 60F05",quantum materials,2024-12-20,2024-12-23T21:07:11.939940
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",quantum materials,2024-12-20,2024-12-23T21:07:11.940938
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",quantum materials,2024-12-20,2024-12-23T21:07:11.940938
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",quantum materials,2024-12-20,2024-12-23T21:07:11.941936
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",quantum materials,2024-12-20,2024-12-23T21:07:11.941936
Role of the ratio of tangential to normal stiffness coefficient on the behaviour of vibrofluidised particles,"The selection of parameters in the contact law for inter-particle
interactions affects the results of simulations of flowing granular materials.
The present study aims to understand the effect of the ratio of tangential to
normal spring stiffness coefficient ($\kappa$) on inter-particle contact
behaviour in terms of the rotational coefficient of restitution determined
using data obtained from multi-particle simulations. The effect of $\kappa$ on
the profiles of the micro- and macroscopic properties of particles in a
vibrofluidised bed is also investigated. The Discrete Element Method (DEM) is
used to simulate a vertically vibrated fluidised bed using the open-source
software LAMMPS. The inter-particle and wall-particle contact forces are
determined using the linear spring-dashpot (LSD) model. The distribution of the
mean co-ordination number, force during the contact, contact regimes, and
rotational coefficient of restitution are determined from the data obtained
from simulations. It was shown that $\kappa$ plays a significant role in the
distribution of inter-particle contacts between different regimes and, thereby,
the velocity distribution and profiles of statistically averaged properties of
the vibrofluidised particles. Our results show that for particles with surface
friction coefficient $\mu>0.1$, the commonly used value $\kappa=\frac{2}{7}$
results in quantitatively different results from those obtained using $0.67 \le
\kappa < 1$, a range consistent with the realistic values of Poisson ratios for
simple materials.",315,2412.16133v1,cond-mat.soft,cond-mat.soft,quantum materials,2024-12-20,2024-12-23T21:07:11.942933
Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods,"We present a combined magnetometry, muon spin relaxation ($\mu$SR), and
neutron scattering study of the insulating spin glass Zn$_{0.5}$Mn$_{0.5}$Te,
for which magnetic Mn$^{2+}$ and nonmagnetic Zn$^{2+}$ ions are randomly
distributed on a face-centered cubic lattice. Using magnetic pair distribution
function (mPDF) analysis and reverse Monte Carlo (RMC) modeling of the diffuse
magnetic scattering, we show that the spin-glass ground state exhibits
short-range type-III antiferromagnetic order with a locally ordered moment of
3.4 $\mu_{\mathrm{B}}$ between nearest-neighbor spins, which decays as a
function of spin separation distance with a correlation length of approximately
5 {\AA}. The diffuse magnetic scattering and corresponding mPDF show no
significant changes across the spin-glass freezing temperature $T_f = 22$ K,
indicating that the dynamically fluctuating short-range spin correlations in
the paramagnetic state retain the same basic type-III configuration that
characterizes the spin-glass state; the only change apparent from the neutron
scattering data is a gradual reduction of the correlation length and locally
ordered moment with increasing temperature. The $\mu$SR results demonstrate
that fluctuation rate of the short-range spin correlations decreases gradually
and somewhat inhomogeneously through the sample volume as the temperature
decreases toward $T_f$. Taken together, these results provide a unique and
detailed picture of the local magnetic structure and dynamics in a concentrated
spin glass. In addition, this work showcases a new statistical method for
extracting diffuse scattering signals from neutron powder diffraction data,
which we developed to facilitate the mPDF and RMC analysis of the neutron data.
This method has the potential to be broadly useful for neutron powder
diffraction experiments on a variety of materials with short-range atomic or
magnetic order.",418,2412.16130v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,quantum materials,2024-12-20,2024-12-23T21:07:11.943931
Kramers-protected hardware-efficient error correction with Andreev spin qubits,"We propose an architecture for bit flip error correction of Andreev spins
that is protected by Kramers' degeneracy. Specifically, we show that a coupling
network of linear inductors results in a static Hamiltonian composed of the
stabilizers of a bit flip code. Thereby, without detuning from the Kramers'
point, reflectometry off a single coupled resonator accomplishes a projective
measurement of multiple stabilizers. We further show how circuit-mediated spin
couplings enable error correction operations and a complete set of logical
quantum gates. The concept is experimentally feasible.",120,2412.16116v1,quant-ph,"quant-ph,cond-mat.mes-hall,cond-mat.supr-con",quantum materials,2024-12-20,2024-12-23T21:07:11.944927
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",quantum materials,2024-12-20,2024-12-23T21:07:11.944927
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",quantum materials,2024-12-20,2024-12-23T21:07:11.946921
Integration of Quantum Key Distribution in a 20-km 32-user Coherent Passive Optical Network with Single Feeder Fiber,"We demonstrate for the first time the integration of O-band
polarization-encoding decoy-state BB84 QKD into a C-band 20-km single-feeder
fiber 32-user coherent PON running at carrier-grade power levels without
modifying existing PON infrastructures.",61,2412.16104v1,quant-ph,"quant-ph,cs.CR",quantum materials,2024-12-20,2024-12-23T21:07:11.946921
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",quantum materials,2024-12-20,2024-12-23T21:07:11.946921
Engineering high-Q superconducting tantalum microwave coplanar waveguide resonators for compact coherent quantum circuits,"Tantalum (Ta) has recently received considerable attention in manufacturing
robust superconducting quantum circuits. Ta offers low microwave loss, high
kinetic inductance compared to aluminium (Al) and niobium (Nb), and good
compatibility with complementary metal-oxide-semiconductor (CMOS) technology,
which is essential for quantum computing applications. Here, we demonstrate the
fabrication engineering of thickness-dependent high quality factor (high-Q_i)
Ta superconducting microwave coplanar waveguide resonators. All films are
deposited on high-resistivity silicon substrates at room temperature without
additional substrate heating. Before Ta deposition, a niobium (Nb) seed layer
is used to ensure a body-centred cubic lattice ({\alpha}-Ta) formation. We
further engineer the kinetic inductance (L_K) resonators by varying Ta film
thicknesses. High L_K is a key advantage for applications because it
facilitates the realisation of high-impedance, compact quantum circuits with
enhanced coupling to qubits. The maximum internal quality factor Q_i of ~ 3.6 *
10^6 is achieved at the high power regime for 100 nm Ta, while the highest
kinetic inductance is obtained to be 0.6 pH/sq for the thinnest film, which is
40 nm. This combination of high Q_i and high L_K highlights the potential of Ta
microwave circuits for high-fidelity operations of compact quantum circuits.",305,2412.16099v1,quant-ph,"quant-ph,cond-mat.supr-con,cs.SY,eess.SY,physics.app-ph",quantum materials,2024-12-20,2024-12-23T21:07:11.947919
Sparse Non-Markovian Noise Modeling of Transmon-Based Multi-Qubit Operations,"The influence of noise on quantum dynamics is one of the main factors
preventing current quantum processors from performing accurate quantum
computations. Sufficient noise characterization and modeling can provide key
insights into the effect of noise on quantum algorithms and inform the design
of targeted error protection protocols. However, constructing effective noise
models that are sparse in model parameters, yet predictive can be challenging.
In this work, we present an approach for effective noise modeling of
multi-qubit operations on transmon-based devices. Through a comprehensive
characterization of seven devices offered by the IBM Quantum Platform, we show
that the model can capture and predict a wide range of single- and two-qubit
behaviors, including non-Markovian effects resulting from spatio-temporally
correlated noise sources. The model's predictive power is further highlighted
through multi-qubit dynamical decoupling demonstrations and an implementation
of the variational quantum eigensolver. As a training proxy for the hardware,
we show that the model can predict expectation values within a relative error
of 0.5%; this is a 7$\times$ improvement over default hardware noise models.
Through these demonstrations, we highlight key error sources in superconducting
qubits and illustrate the utility of reduced noise models for predicting
hardware dynamics.",257,2412.16092v1,quant-ph,quant-ph,quantum materials,2024-12-20,2024-12-23T21:07:11.948916
Bounds on concatenated entanglement-assisted quantum error-correcting codes,"Entanglement-assisted quantum error-correcting codes (EAQECCs) make use of
pre-shared entanglement to enhance the rate of error correction and
communication. We study the concatenation of EAQECCs, in specific showing how
the order of concatenation affects the number of ebits consumed, the logical
error probability, the pseudo-threshold, and the violation of the quantum
Hamming bound. We find that if the quaternary code from which an EAQECC is
derived saturates the Griesmer (resp., Plotkin) bound, then the derived code
will saturate the Griesmer (resp., linear Plotkin) bound for EAQECCs. We
present families of concatenated EAQECCs that saturate the quantum Singleton,
Griesmer, and linear Plotkin bounds for EAQECCs.",186,2412.16082v1,quant-ph,quant-ph,quantum materials,2024-12-20,2024-12-23T21:07:11.948916
Error-corrected fermionic quantum processors with neutral atoms,"Many-body fermionic systems can be simulated in a hardware-efficient manner
using a fermionic quantum processor. Neutral atoms trapped in optical
potentials can realize such processors, where non-local fermionic statistics
are guaranteed at the hardware level. Implementing quantum error correction in
this setup is however challenging, due to the atom-number superselection
present in atomic systems, that is, the impossibility of creating coherent
superpositions of different particle numbers. In this work, we overcome this
constraint and present a blueprint for an error-corrected fermionic quantum
computer that can be implemented using current experimental capabilities. To
achieve this, we first consider an ancillary set of fermionic modes and design
a fermionic reference, which we then use to construct superpositions of
different numbers of referenced fermions. This allows us to build logical
fermionic modes that can be error corrected using standard atomic operations.
Here, we focus on phase errors, which we expect to be a dominant source of
errors in neutral-atom quantum processors. We then construct logical fermionic
gates, and show their implementation for the logical particle-number conserving
processes relevant for quantum simulation. Finally, our protocol is illustrated
using a minimal fermionic circuit, where it leads to a quadratic suppression of
the logical error rate.",272,2412.16081v1,quant-ph,"quant-ph,cond-mat.quant-gas,physics.atom-ph",quantum materials,2024-12-20,2024-12-23T21:07:11.949914
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",quantum materials,2024-12-20,2024-12-23T21:07:11.950911
Comparing effective-one-body and Mathisson-Papapetrou-Dixon results for a spinning test particle on circular equatorial orbits around a Kerr black hole,"We consider a spinning test particle around a rotating black hole and compare
the Mathisson-Papapetrou-Dixon (MPD) formalism under the Tulczyjew-Dixon spin
supplementary condition to the test-mass limit of the effective-one-body (EOB)
Hamiltonian of [Phys. Rev. D.90, 044018(2014)], with enhanced spin-orbit
sector. We focus on circular equatorial orbits: we first compare the constants
of motion at their linear in secondary spin approximation and then we compute
the gravitational-wave (GW) fluxes using a frequency domain Teukolsky equation
solver. We find no difference between the EOB and MPD fluxes when the
background spacetime is Schwarzschild, while the difference for a Kerr
background is maximum for large, positive spins. Our work could be considered
as a first step to improve the radiation reaction of the EOB model, in view of
the needs of the next-generation of GW detectors.",209,2412.16077v1,gr-qc,gr-qc,quantum materials,2024-12-20,2024-12-23T21:07:11.951909
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",quantum materials,2024-12-20,2024-12-23T21:07:11.951909
A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification,"We propose BayesPIM, a Bayesian prevalence-incidence mixture model for
estimating time- and covariate-dependent disease incidence from screening and
surveillance data. The method is particularly suited to settings where some
individuals may have the disease at baseline, baseline tests may be missing or
incomplete, and the screening test has imperfect sensitivity. Building on the
existing PIMixture framework, which assumes perfect sensitivity, BayesPIM
accommodates uncertain test accuracy by incorporating informative priors. By
including covariates, the model can quantify heterogeneity in disease risk,
thereby informing personalized screening strategies. We motivate the model
using data from high-risk familial colorectal cancer (CRC) surveillance through
colonoscopy, where adenomas - precursors of CRC - may already be present at
baseline and remain undetected due to imperfect test sensitivity. We show that
conditioning incidence and prevalence estimates on covariates explains
substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs
sampler and data augmentation, BayesPIM robustly recovers incidence times while
handling latent prevalence. Informative priors on the test sensitivity
stabilize estimation and mitigate non-convergence issues. Model fit can be
assessed using information criteria and validated against a non-parametric
estimator. In this way, BayesPIM enhances estimation accuracy and supports the
development of more effective, patient-centered screening policies.",308,2412.16065v1,stat.ME,"stat.ME,stat.CO,62N02",quantum materials,2024-12-20,2024-12-23T21:07:11.952906
Multipartite entanglement structure of monitored quantum circuits,"Monitored quantum circuits have attracted significant interest as an example
of synthetic quantum matter, intrinsically defined by their quantum information
content. Here, we propose a multipartite entanglement perspective on monitored
phases through the lens of quantum Fisher information. Our findings reveal that
unstructured monitored random circuits fail to exhibit divergent multipartite
entanglement even at criticality, highlighting their departure from standard
quantum critical behavior. However, we demonstrate that genuinely multipartite
entangled phases can be realized through two-site measurements, provided a
protection mechanism is in place. This work positions multipartite entanglement
as a valuable perspective for the study of interacting monitored circuits and
broader frameworks of noisy quantum dynamics.",142,2412.16062v1,quant-ph,"quant-ph,cond-mat.stat-mech",quantum materials,2024-12-20,2024-12-23T21:07:11.953904
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",quantum materials,2024-12-20,2024-12-23T21:07:11.953904
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",quantum materials,2024-12-20,2024-12-23T21:07:11.954901
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,quantum materials,2024-12-20,2024-12-23T21:07:11.954901
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",quantum materials,2024-12-20,2024-12-23T21:07:11.955898
Millikelvin Nb nanoSQUID-embedded tuneable resonator fabricated with a neon focused-ion-beam,"SQUID-embedded superconducting resonators are of great interest due to their
potential for coupling highly scalable superconducting circuits with quantum
memories based on solid-state spin ensembles. Such an application requires a
high-$Q$, frequency-tuneable resonator which is both resilient to magnetic
field, and able to operate at millikelvin temperatures. These requirements
motivate the use of a higher $H_{c}$ metal such as niobium, however the
challenge then becomes to sufficiently reduce the operating temperature. We
address this by presenting a monolithic Nb nanoSQUID-embedded resonator, where
neon focused-ion-beam fabrication of the nanoSQUID results in a device
displaying frequency tuneability at $T = 16$ mK. In order to assess the
applicability of the device for coupling to small spin clusters, we
characterise the flux sensitivity as a function of microwave drive power and
externally applied magnetic field, and find that the noise is dominated by
dielectric noise in the resonator. Finally, we discuss improvements to the
device design which can dramatically improve the flux sensitivity, which
highlights the promise of Nb SQUID-embedded resonators for hybrid
superconductor-spin applications.",261,2412.16045v1,quant-ph,"quant-ph,cond-mat.supr-con",quantum materials,2024-12-20,2024-12-23T21:07:11.956896
A two-dimensional 10-qubit array in germanium with robust and localised qubit control,"Quantum computers require the systematic operation of qubits with high
fidelity. For holes in germanium, the spin-orbit interaction allows for
\textit{in situ} electric fast and high-fidelity qubit gates. However, the
interaction also causes a large qubit variability due to strong g-tensor
anisotropy and dependence on the environment. Here, we leverage advances in
material growth, device fabrication, and qubit control to realise a
two-dimensional 10-spin qubit array, with qubits coupled up to four neighbours
that can be controlled with high fidelity. By exploring the large parameter
space of gate voltages and quantum dot occupancies, we demonstrate that plunger
gate driving in the three-hole occupation enhances electric-dipole spin
resonance (EDSR), creating a highly localised qubit drive. Our findings,
confirmed with analytical and numerical models, highlight the crucial role of
intradot Coulomb interaction and magnetic field direction. Furthermore, the
ability to engineer qubits for robust control is a key asset for further
scaling.",219,2412.16044v1,cond-mat.mes-hall,"cond-mat.mes-hall,quant-ph",quantum materials,2024-12-20,2024-12-23T21:07:11.957893
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",quantum materials,2024-12-20,2024-12-23T21:07:11.957893
Integrability versus chaos in the steady state of many-body open quantum systems,"The Lindblad description of an open quantum system gives rise to two types of
integrability, since the nonequilibrium steady state can be integrable
independently of the Liouvillian. Taking boundary-driven and dephasing spin
chains as a representative example, we discriminate Liouvillian and
steady-state chaos by combining level spacing statistics and an extension of
the eigenstate thermalization hypothesis to open quantum systems. Moreover, we
analyze the structure of the steady states by expanding it in the basis of
Pauli strings and comparing the weight of strings of different lengths. We show
that the natural expectation that integrable steady states are ""simple"" (i.e.,
built from few-body local operators) does not hold: the steady states of both
chaotic and integrable models have relevant contributions coming from Pauli
strings of all possible lengths, including long-range and many-body
interactions. Nevertheless, we show that one can effectively use the
operator-size distribution to distinguish chaotic and integrable steady states.",217,2412.16041v1,cond-mat.stat-mech,"cond-mat.stat-mech,cond-mat.mes-hall,nlin.CD,quant-ph",quantum materials,2024-12-20,2024-12-23T21:07:11.958893
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",quantum materials,2024-12-20,2024-12-23T21:07:11.959889
Integral representation for a relaxed optimal design problem for non-simple grade two materials,"A measure representation result for a functional modelling optimal design
problems for plastic deformations, under linear growth conditions, is obtained.
  Departing from an energy with a bulk term depending on the second gradient,
as well as a perimeter term, the functional in question corresponds to the
relaxation of this energy with respect to a pair $(\chi,u)$, where $\chi$ is
the characteristic function of a set of finite perimeter and $u$ is a function
of bounded hessian.",99,2412.16027v1,math.AP,"math.AP,math.OC,49J45, 49Q20, 26B25",quantum materials,2024-12-20,2024-12-23T21:07:11.959889
Entropy maximizers for kinetic wave equations set on tori,"We consider the kinetic wave equation, or phonon Boltzmann equation, set on
the torus (physical system set on the lattice). We describe entropy maximizers
for fixed mass and energy; our framework is very general, being valid in any
dimension, for any dispersion relation, and even including the quantum kinetic
wave equation. Of particular interest is the presence of condensation in
certain regimes which we characterize.",89,2412.16026v1,math.AP,"math.AP,math-ph,math.MP",quantum materials,2024-12-20,2024-12-23T21:07:11.960885
Knowledge-dependent optimal Gaussian strategies for phase estimation,"When estimating an unknown phase rotation of a continuous-variable system
with homodyne detection, the optimal probe state strongly depends on the value
of the estimated parameter. In this article, we identify the optimal pure
single-mode Gaussian probe states depending on the knowledge of the estimated
phase parameter before the measurement. We find that for a large prior
uncertainty, the optimal probe states are close to coherent states, a result in
line with findings from noisy parameter estimation. But with increasingly
precise estimates of the parameter it becomes beneficial to put more of the
available energy into the squeezing of the probe state. Surprisingly, there is
a clear jump, where the optimal probe state changes abruptly to a squeezed
vacuum state, which maximizes the Fisher information for this estimation task.
We use our results to study repeated measurements and compare different methods
to adapt the probe state based on the changing knowledge of the parameter
according to the previous findings.",184,2412.16023v1,quant-ph,quant-ph,quantum materials,2024-12-20,2024-12-23T21:07:11.960885
QUANTUM ESPRESSO implementation of the RPA-based functional,"We detail our implementation of the random-phase-approximation based
functional (RPAF) derived in our previous publication [Phys. Rev. B 110, 195151
(2024)] for the QUANTUM ESPRESSO (QE) package. We also make available the
source files required in order to apply this functional within QE. We also
provide the corresponding RPAF projector augmented wave (PAW) and ultrasolf
pseudopotentials for most elements. Lastly, we benchmark the performance of the
RPAF by calculating the equilibrium lattice constant and bulk modulus of a set
of the same 60 crystals used by other authors to benchmark other functionals
for both PAW and ultrasoft pseudopotentials. We find that the RPAF performs
better overall as compared to the other most popular functionals.",170,2412.16017v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.str-el",quantum materials,2024-12-20,2024-12-23T21:07:11.961883
Hole burning experiments and modeling in erbium-doped silica glass fibers down to millikelvin temperatures: evidence for ultra-long population storage,"We use spectral hole burning to investigate spin dynamics within the
electronic Zeeman sublevels of the ground state of the erbium ions in
erbium-doped fibers (EDF). Conducted at ultra-low temperatures and under
varying magnetic fields, our study reveals distinct changes in spin relaxation
dynamics across different conditions. We identified three decay components at
approximately 7 mK, with one achieving spin lifetimes of over 9 hours under
optimal conditions, while two components were observed at higher temperatures.
The fairly stable relative weights of the decay components across conditions
suggest distinct ion populations contributing to the observed relaxation
dynamics. While earlier studies struggled to account for all decay components
at higher temperatures, our approach successfully models spin dynamics across
all observed decay components, using a consistent set of underlying mechanisms,
including spin flip-flop interactions, direct coupling to two-level systems,
and Raman-type processes, and distinguishes the decay components by the
strengths with which these mechanisms contribute. These results suggest EDFs'
potential as a promising candidate for quantum memory applications, with
further room for optimization.",214,2412.16013v1,quant-ph,quant-ph,quantum materials,2024-12-20,2024-12-23T21:07:11.961883
Discovery of a non-Hermitian phase transition in a bulk condensed-matter system,"Phase transitions are fundamental in nature. A small parameter change near a
critical point leads to a qualitative change in system properties. Across a
regular phase transition, the system remains in thermal equilibrium and,
therefore, experiences a change of static properties, like the emergence of a
magnetisation upon cooling a ferromagnet below the Curie temperature. When
driving a system far from equilibrium, novel, otherwise inaccessible quantum
states of matter may arise. Such states are typically non-Hermitian, that is,
their dynamics break time-reversal symmetry, a basic law of equilibrium
physics. Phase transitions in non-Hermitian systems are of fundamentally new
nature in that the dynamical behaviour rather than static properties may
undergo a qualitative change at a critical, here called exceptional point. Here
we experimentally realize a non-Hermitian phase transition in a bulk
condensed-matter system. Optical excitation creates charge carriers in the
ferromagnetic semiconductor EuO. In a temperature-dependent interplay with the
Hermitian transition to ferromagnetic order, a non-Hermitian change of the
relaxation dynamics occurs, manifesting in our time-resolved reflection data as
a transition from bi-exponential real to single-exponential complex decay. Our
theory models this behavior and predicts non-Hermitian phase transitions for a
large class of condensed-matter systems, where they may be exploited to
sensitively control bulk-dynamic properties.",289,2412.16012v1,cond-mat.str-el,cond-mat.str-el,quantum materials,2024-12-20,2024-12-23T21:07:11.962880
Fuzzy-Space Engineering,"The techniques developed for matrix models and fuzzy geometry are powerful
tools for representing strings and membranes in quantum physics. We study the
representation of fuzzy surfaces using these techniques. This involves
constructing graphs and writing their coordinates and connectivity into
matrices. To construct arbitrary graphs and quickly change them, we use 3D
software. A script generates the three matrices from the graphs. These matrices
are then processed in Wolfram Mathematica to calculate the zero modes of the
Dirac operator. Our first result shows the quantization of a two-dimensional
Trefoil knot. Additional examples illustrate various properties and behaviors
of this process. This helps us to gain a deeper understanding of fuzzy spaces
and zero-mode surfaces. This work contributes to advancing the understanding of
visualization aspects in fuzzy geometry.",153,2412.16011v1,hep-th,hep-th,quantum materials,2024-12-20,2024-12-23T21:07:11.963877
Advantages and limitations of channel multiplexing for discrete-variable quantum key distribution,"Typically practical realizations of discrete-variable quantum key
distribution (QKD) protocols, based on exchanging single-photon signals between
the trusted parties, can provide its users with only very low key generation
rates. One of the potential solutions for this problem, that can be adapted for
the case of entanglement-based QKD schemes using broadband photon-pair sources,
is to utilize wavelength-division-multiplexing (WDM) modules in order to split
the photons with different wavelengths to separate detection channels and
generate multiple keys in parallel. Here, we theoretically investigate this
idea in the case of pulsed laser used to pump spontaneous parametric
down-conversion source. We optimize the effective phase-matching function width
of the nonlinear crystal, and the intensity and duration of the pump laser
pulses in order to maximize the advantage of the overall key generation rate
provided by the WDM-based QKD scheme over the traditional no-WDM scenario. The
results of our analysis show that the considered method can significantly
accelerate the production of cryptographic keys, but proper optimization of the
photon-pair source is needed to exploit its full potential.",231,2412.16007v1,quant-ph,quant-ph,quantum materials,2024-12-20,2024-12-23T21:07:11.964875
Single-shot all-optical magnetization switching in in-plane magnetized magnetic tunnel junction,"Single pulse All Optical Helicity-Independent Switching is demonstrated in an
in-plane magnetized magnetic tunnel junction. A toggle switching of the 2nm
thick Co40Fe40B20 soft layer could be achieved by exchange coupling the
Co40Fe40B20 with a 10nm thick Co85Gd15 layer monitored by measuring the Tunnel
magneto resistance of the device. The use of in plane magnetized electrodes
relaxes the constrains linked to perpendicular magnetic anisotropy systems
while achieving a tunneling magnetoresistance (TMR) ratio exceeding 100%. The
influence of the upper electrical electrode, which is opaque to the laser beam
in this study, is also discussed.",146,2412.16005v1,cond-mat.mtrl-sci,"cond-mat.mtrl-sci,cond-mat.mes-hall",quantum materials,2024-12-20,2024-12-23T21:07:11.964875
Presentations for small reflection equation algebras of type A,"We give presentations, in terms of the generators and relations, for the
reflection equation algebras of type $GL_n$ and $SL_n$, i.e., the covariantized
algebras of the dual Hopf algebras of the small quantum groups of
$\mathfrak{gl}_n$ and $\mathfrak{sl}_n$. Our presentations display these
algebras as quotients of the infinite-dimensional reflection equation algebras
of types $GL_n$ and $SL_n$ by identifying additional relations that correspond
to twisting the nilpotency and unipotency relations of the finite-dimensional
quantum function algebras. The presentations are valid for appropriately
defined integral forms of these algebras.",161,2412.16004v1,math.QA,"math.QA,math.CT,math.RA,17B37 (Primary) 16T05, 18M05, 18M15 (Secondary)",quantum materials,2024-12-20,2024-12-23T21:07:11.965872
Collective single-photon emission and energy transfer in thin-layer dielectric and plasmonic systems,"We study the collective photon decay of multiple quantum emitters embedded in
a thin high-index dielectric layer such as hexagonal boron nitride (hBN), with
and without a metal substrate. We first explore the significant role that
guided modes including surface plasmon modes play in the collective decay of
identical singlephoton emitters (super- and subradiance). Surprisingly, on
distances relevant for collective emission, the guided or surface-plasmon modes
do not always enhance the collective emission. We identify configurations with
inhibition, and others with enhancement of the dipole interaction due to the
guided modes. We interpret our results in terms of local and cross densities of
optical states. In the same structure, we show a remarkably favorable
configuration for enhanced F\""orster resonance energy transfer between a donor
and acceptor in the dielectric layer on a metallic substrate. We compare our
results to theoretical limits for energy transfer efficiency.",195,2412.16000v1,physics.optics,"physics.optics,cond-mat.mes-hall,quant-ph",quantum materials,2024-12-20,2024-12-23T21:07:11.965872
"Ti and Spi, Carrollian extended boundaries at timelike and spatial infinity","The goal of this paper is to provide a definition for a notion of extended
boundary at time and space-like infinity which, following
Figueroa-O'Farril--Have--Prohazka--Salzer, we refer to as Ti and Spi. This
definition applies to asymptotically flat spacetime in the sense of
Ashtekar--Romano and we wish to demonstrate, by example, its pertinence in a
number of situations. The definition is invariant, is constructed solely from
the asymptotic data of the metric and is such that automorphisms of the
extended boundaries are canonically identified with asymptotic symmetries.
Furthermore, scattering data for massive fields are realised as functions on Ti
and a geometric identification of cuts of Ti with points of Minkowksi then
produces an integral formula of Kirchhoff type. Finally, Ti and Spi are both
naturally equipped with (strong) Carrollian geometries which, under mild
assumptions, enable to reduce the symmetry group down to the BMS group, or to
Poincar\'e in the flat case. In particular, Strominger's matching conditions
are naturally realised by restricting to Carrollian geometries compatible with
a discrete symmetry of Spi.",262,2412.15996v1,gr-qc,"gr-qc,hep-th,math-ph,math.MP",quantum materials,2024-12-20,2024-12-23T21:07:11.966869
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",quantum materials,2024-12-20,2024-12-23T21:07:11.967371
Quantum Cohomology of a Fano Quiver Moduli Space,"We consider a prime Fano 6-fold $Y$ of index 3, which is a fine quiver moduli
space and a blow down of $\mathrm{Hilb}^3(\mathds{P}^2)$. We calculate the
quantum cohomology ring of $Y$ and obtain Quantum Chevalley formulas for the
Schubert type subvarieties. The famous Dubrovin's Conjecture relating the
quantum cohomology and the derived category is verified for $Y$.",106,2412.15987v1,math.AG,math.AG,quantum materials,2024-12-20,2024-12-23T21:07:11.967371
Identifying and quantifying Su-Schrieffer-Heeger-like interactions with RIXS,"Su-Schrieffer-Heeger (SSH)-like electron-phonon (e-ph) interactions can drive
the formation of light (bi)polarons and several novel states of matter. It is,
therefore, prudent to develop experimental protocols for identifying such
couplings in real materials and quantifying their strength. Here, we
investigate how resonant inelastic x-ray scattering (RIXS) probes e-ph
interactions in the one-dimensional half-filled Hubbard-SSH model with onsite
phonons. Using the density matrix renormalization group method, we compute the
full RIXS response and find that the lattice excitations generated during the
scattering process inevitably couple to the system's charge and magnetic
sectors, resulting in combined multi-particle excitations that cannot be easily
disentangled from one another. While this aspect complicates the interpretation
of RIXS experiments, we outline how it can be leveraged to identify and
quantify SSH-like interactions in quantum materials.",220,2412.15981v1,cond-mat.str-el,cond-mat.str-el,quantum materials,2024-12-20,2024-12-23T21:07:11.968370
A quantum dual logarithmic barrier method for linear optimization,"Quantum computing has the potential to speed up some optimization methods.
One can use quantum computers to solve linear systems via Quantum Linear System
Algorithms (QLSAs). QLSAs can be used as a subroutine for algorithms that
require solving linear systems, such as the dual logarithmic barrier method
(DLBM) for solving linear optimization (LO) problems. In this paper, we use a
QLSA to solve the linear systems arising in each iteration of the DLBM. To use
the QLSA in a hybrid setting, we read out quantum states via a tomography
procedure which introduces considerable error and noise. Thus, this paper first
proposes an inexact-feasible variant of DLBM for LO problems and then extends
it to a quantum version. Our quantum approach has quadratic convergence toward
the central path with inexact directions and we show that this method has the
best-known $\mathcal{O}(\sqrt{n} \log (n \mu_0 /\zeta))$ iteration complexity,
where $n$ is the number of variables, $\mu_0$ is the initial duality gap, and
$\zeta$ is the desired accuracy. We further use iterative refinement to improve
the time complexity dependence on accuracy. For LO problems with quadratically
more constraints than variables, the quantum complexity of our method has a
sublinear dependence on dimension.",291,2412.15977v1,math.OC,"math.OC,90C05, 90C51, 81P68",quantum materials,2024-12-20,2024-12-23T21:07:11.969367
Exact correlation functions at finite temperatures in Tomonaga-Luttinger liquid with an open end,"The paradigmatic state of a 1D collective metal, the Tomonaga-Luttinger
liquid (TLL), offers us an exact analytic solution for a strongly interacting
quantum system not only for infinite systems at zero temperature but also at
finite temperature and with a boundary. Potentially, these results are of high
relevance for technology as they could lay the foundation for a many-body
description of various nanostructures. For this to happen, we need expressions
for local (i.e., spatially resolved) correlations as a function of frequency.
In this study, we find such expressions and study their outcome. Based on our
analytic expressions we are able to identify two distinct cases of TLL which we
call Coulomb metal and Hund metal, respectively. We argue that these two cases
span all the situations possible in nanotubes made out of p-block elements.
From an applications viewpoint, it is crucial to capture the fact that the end
of the 1D system can be coupled with the external environment and emit
electrons into it. We discuss such coupling on two levels for both Coulomb and
Hund metals: i) in the zeroth order approximation, the coupling modifies the 1D
system's boundary conditions; ii) stronger coupling, when the environment can
self-consistently modify the 1D system, we introduce spatially dependent TLL
parameters. In case ii) we were able to capture the presence of
plasmon-polariton particles, thus building a link between TLL and the field of
nano-optics.",317,2412.15963v1,cond-mat.str-el,cond-mat.str-el,quantum materials,2024-12-20,2024-12-23T21:07:11.970364
Effective Metric Description of 2+1 Dimensional Quantum Black Holes,"We develop an effective metric description of 2+1 dimensional black holes
describing deviations from the classical Ba\~nados-Teitelboim-Zanelli (BTZ)
black hole. The latter is a classical 2+1 dimensional rotating black hole with
constant negative curvature. The effective metric is constrained by imposing
the black hole symmetries and asymptotic classical behavior. The deformed
metric is parametrized in terms of a physical quantity that we choose to be a
physical distance. The latter can be solved for in three main regions of
interest, the one around the horizon, origin, and spatial infinity. The
finiteness of physical quantities at the horizon, such as the Ricci and
Kretschmann scalars, leads to universal constraints on the physical parameters
of the metric around the horizon. This allows us to further derive the general
form of the corrected Hawking temperature in terms of the physical parameters
of the effective metric. Assuming that the approach can be generalized to the
interior of the black hole, we further develop an effective metric description
near the origin. To illustrate the approach, we show how to recast the
information encoded in a specific model of quantum BTZ known as quBTZ black
hole in terms of the effective metric coefficients.",256,2412.15960v1,gr-qc,"gr-qc,hep-ph,hep-th",quantum materials,2024-12-20,2024-12-23T21:07:11.971362
Extraordinary oxidation behavior of W-Zr thin-film metallic glasses: A route for tailoring functional properties of W-Zr-O films,"The oxidation behavior of W-Zr thin-film metallic glasses (TFMGs) with 32, 48
and 61 at.% Zr, prepared by dc magnetron co-sputtering, was comprehensively
studied after annealing in synthetic air. The study focuses on the effect of
the annealing temperature (up to 600{\deg}C) on the oxidation process, oxygen
saturation, structure evolution, and their subsequent impact on electrical,
optical and mechanical properties. The findings reveal that controlled
oxidation transforms W-Zr TFMGs into amorphous ceramic W-Zr-O films with
substoichiometric compositions. This is a consequence of an oxidation process
that does not proceed through the formation of a stoichiometric oxide layer on
the surface of W-Zr TFMGs, acting as a diffusion barrier against fast
oxidation, but leads to a gradual incorporation of oxygen across the film
volume due to thermodynamics factors. Higher Zr content accelerates the oxygen
incorporation and its depth uniformity in the films. As a result, the
mechanical properties are significantly enhanced achieving hardness values of
up to 17.5 GPa at approximately 50% oxygen saturation. Simultaneously, the
electrical and optical properties are finely tuned with the resistivity and the
extinction coefficient (measured at 550 nm) ranging from 1.7 to 95.7x10-4
Ohm.cm and 0.28 to 1.06, respectively.",297,2412.15943v1,cond-mat.mtrl-sci,cond-mat.mtrl-sci,quantum materials,2024-12-20,2024-12-23T21:07:11.971362
Chaotic orbital dynamics of pulsating stars around black holes surrounded by dark matter halos,"We analyze the orbital dynamics of spherical test bodies in ``black hole
surrounded by dark matter halo'' spherically symmetric spacetimes. When the
test body pulsates periodically (such as a variable star), altering its
quadrupole tensor, Melnikov's method shows that its orbital dynamics presents
homoclinic chaos near the corresponding unstable circular orbits however small
the oscillation amplitude is. Since for supermassive black holes the period of
revolution of a star near the innermost stable circular orbit roughly spans
time intervals from minutes to hours, the formalism can be applied in principle
to the astrophysical scenario of a pulsating (variable) star inspiraling into a
supermassive black hole, including the black hole SgrA* at the center of our
Galaxy. The chaotic nature of its orbit, due to pulsation, is imprinted in the
redshift time series of the emitted light and can, in principle, be observed in
the corresponding light curves and even in gravitational-wave signals detected
by future observatories such as the Laser Inteferometer Space Antenna. Also,
although periodic with respect to the star's proper time, the chaotic orbital
motion will produce an erratic light curve (and gravitational-wave signal) in
terms of observed, coordinate time. Although our results were obtained for a
specific exact solution, we argue that this phenomenon is generic for pulsating
bodies immersed in black hole spacetimes surrounded by self-gravitating fluids.",306,2412.15938v1,gr-qc,"gr-qc,astro-ph.HE,nlin.CD",quantum materials,2024-12-20,2024-12-23T21:07:11.972359
The Classical Super-Phaserotation Infrared Triangle,"The universality of the logarithmic soft photon theorem in four dimensions
can be traced to an infinite-dimensional asymptotic symmetry which acts as a
local phase rotation on matter as we have shown in 2403.13053. Here we extend
our earlier results for the charges associated to these superphaserotations to
all orders in the coupling and prove that their conservation is exactly the
classical logarithmic soft photon theorem discovered by Saha, Sahoo and Sen in
1912.06413. We furthermore generalize the formulae for the associated
electromagnetic displacement memory and its tail from particles to scalar
matter fields. This completes the classical superphaserotation infrared
triangle.",142,2412.16149v1,hep-th,hep-th,sustainable energy,2024-12-20,2024-12-23T21:07:12.801243
Quantitative classicality in cosmological interactions during inflation,"We examine the classical and quantum evolution of inflationary cosmological
perturbations from quantum initial conditions, using the on-shell and off-shell
contributions to correlators to investigate the signatures of interactions. In
particular, we calculate the Keldysh contributions to the leading order
bispectrum from past infinity, showing that the squeezed limit is dominated by
the on-shell evolution. By truncating the time integrals in the analytic
expressions for contributions to the bispectrum, we define a `quantum
interactivity' and quantitatively identify scales and times for which it is
sufficient to only assume classical evolution, given a fixed precision. In
contrast to common perceptions inspired by free two-point functions, we show
that common non-linear terms of inflationary perturbations can be
well-described by classical evolution even prior to horizon crossing. The
insights gained here can pave the way for quantitative criteria for justifying
the validity of numerically simulating the generation and evolution of quantum
fluctuations in inflation. In particular, we comment on the validity of using
stochastic inflation to reproduce known in-in perturbative results. An
extensive appendix provides a review of the Keldysh formulation of the in-in
formalism with the initial state set at a finite, as opposed to infinite past,
emphasizing the importance of considering temporal boundary terms and the
initial state for correctly obtaining the propagators. We also show how
stochastic dynamics can emerge as a sufficient approximation to the full
quantum evolution. This becomes particularly transparent in the Keldysh
description.",322,2412.16143v1,gr-qc,"gr-qc,hep-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.802241
The Classical Super-Rotation Infrared Triangle,"The universality of gravitational scattering at low energies and large
distances encoded in soft theorems and memory effects can be understood from
symmetries. In four-dimensional asymptotically flat spacetimes the infinite
enhancement of translations, extending the Poincar\'e group to the BMS group,
is the symmetry underlying Weinberg's soft graviton theorem and the
gravitational displacement memory effect. Beyond this leading infrared
triangle, loop corrections alter their nature by introducing logarithms in the
soft expansion and late time tails to the memory, and this persists in the
classical limit. In this work we give the first complete description of an
`infrared triangle' where the long-range nature of gravitational interactions
is accounted for. Building on earlier results in 2403.13053 where we derived a
novel conservation law associated to the infinite dimensional enhancement of
Lorentz transformations to superrotations, we prove here its validity to all
orders in the gravitational coupling and show that it implies the classical
logarithmic soft graviton theorem of Saha-Sahoo-Sen in 1912.06413. We
furthermore extend the formula for the displacement memory and its tail from
particles to fields, thus completing the classical superrotation infrared
triangle.",253,2412.16142v1,hep-th,"hep-th,gr-qc",sustainable energy,2024-12-20,2024-12-23T21:07:12.803237
NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems,"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.",194,2412.16141v1,cs.CV,cs.CV,sustainable energy,2024-12-20,2024-12-23T21:07:12.804235
Borel singularities and Stokes constants of the topological string free energy on one-parameter Calabi-Yau threefolds,"We study the Borel plane of the topological string free energy on all
hypergeometric one-parameter Calabi-Yau models close to singular points in
moduli space, focusing on the location of Borel singularities and the value of
the associated Stokes constants. We find in particular that in models which
exhibit massless D-branes at a singular point, the central charge of the
D-brane close to the singular point coincides with the location of the leading
Borel singularity, and the generalized Donaldson-Thomas invariant associated to
the charge of the D-brane, in as far as its value is known, coincides with the
Stokes constant associated to the Borel singularity.",144,2412.16140v1,hep-th,"hep-th,math.AG",sustainable energy,2024-12-20,2024-12-23T21:07:12.804235
Henneaux-Teitelboim Form of the Generalized Unimodular Gravity Action,"We present an alternative formulation of generalized unimodular gravity
(GUMG), extending the Henneaux-Teitelboim approach to unimodular gravity (UMG).
The central feature of this formulation is the consistent incorporation of time
reparameterization, which enhances the gauge structure and reveals a spatial
nonlocality hidden in the dynamics of the original formulation. We examine the
resulting dynamics, emphasizing the effects of spatial nonlocality, and outline
the constraint structure. In particular, we show that the gauge symmetry in the
gravitational sector is extended by a functionally incomplete symmetry, as
occurs in the unimodular gravity. Furthermore, we identify a subset of GUMG
models for which the alternative formulation preserves manifest locality.",155,2412.16139v1,hep-th,"hep-th,gr-qc",sustainable energy,2024-12-20,2024-12-23T21:07:12.805232
Asymptotic T-duality in three dimensions,"In (super)gravity theories, T-duality relates solutions with an exact
isometry which can have wildly different asymptotic behaviors: a well-known
example is the duality between BTZ black holes and (non-extremal)
three-dimensional black strings. Using this dual pair, we show how the
knowledge of a phase space which includes one set of solutions (here, BTZ black
holes embedded in the Brown-Henneaux phase space) allows to obtain a phase
space for the dual set via an asymptotic notion of T-duality. The resulting
asymptotic symmetry algebras can be very different. For our particular example,
we find a large algebra of symmetries for the black string phase space which
includes as subalgebras $\mathfrak{bms}_2$, $\mathfrak{bms}_3$, and a twisted
warped conformal algebra. On the way, we show that a chiral half of the
Brown-Henneaux boundary conditions are dual to the Comp\`ere-Song-Strominger
ones.",234,2412.16136v1,hep-th,"hep-th,gr-qc",sustainable energy,2024-12-20,2024-12-23T21:07:12.805232
Terbium under High Pressure: First-Principles Dynamical Mean-Field Theory Study,"Elemental rare-earth metals provide a playground for studying novel electron
correlation effects and complex magnetism. However, ab initio simulations of
these systems remain challenging. Here, we employ fully charge self-consistent
density functional theory and dynamical mean-field theory (DFT+DMFT) to
investigate terbium (Tb) metal under pressure. We show that Tb exhibits a
strong band renormalization due to correlation effects, with the calculated
electron density of states in good agreement with the experiments. At higher
pressures, the correlated electronic structures persist but with modulation in
the Hubbard gap, highlighting the tunability of effective Coulomb interactions
and kinetic energies. Our DFT+DMFT calculations further indicate a
ferromagnetic ground state of Tb at low pressure and low temperature, as well
as a transition from ferromagnetism to paramagnetism at elevated temperatures.
These ab initio results also align with the experiments. Our study paves the
way for exploring heavy lanthanides via advanced first-principles simulations.",212,2412.16125v1,cond-mat.str-el,cond-mat.str-el,sustainable energy,2024-12-20,2024-12-23T21:07:12.806229
Prospects for measurements of the longitudinal proton structure function $F_L$ at the Electron Ion Collider,"We explore the potential for extracting the longitudinal proton structure
function $F_{L}$ at the future Electron-Ion Collider (EIC) through a Rosenbluth
separation method. The impacts of differing assumptions on sample sizes,
systematic uncertainties and beam energy scenarios are investigated. With a
sufficiently large number of centre of mass energy configurations and
well-controlled systematics, the EIC will measure $F_{L}$ to an unprecedented
precision, even with relatively modest luminosities. The accessible kinematic
range complements both fixed target and HERA data. In the most optimistic
scenarios, the EIC data will be a highly competitive direct probe of the proton
gluon density.",146,2412.16123v1,hep-ph,hep-ph,sustainable energy,2024-12-20,2024-12-23T21:07:12.806229
Flavor Violations in $B$-Mesons within Non-Minimal SU(5),"Recent anomalies in $B$-meson decays, such as deviations in $R_{D^{(*)}}$ and
$B\to K\nu{\bar\nu}$, suggest possible lepton flavor universality violation and
new exotic interactions. In this work, we explore these anomalies within a
non-minimal SU(5) grand unified theory (GUT) framework, which introduces a
45-dimensional Higgs representation predicting exotic scalar particles,
including the leptoquark $R_2$ and diquark $S_6$. The $R_2$ leptoquark
addresses charged current anomalies in $b\to c\tau\nu$ transitions, the $S_6$
diquark contributes to nonleptonic neutral current processes, such as $B\to
K\pi$ while at the loop level, the exchange of a leptoquark and diquark
contributes to $B\to K\nu{\bar\nu}$ offering solutions to longstanding puzzles.",228,2412.16115v1,hep-ph,"hep-ph,hep-ex",sustainable energy,2024-12-20,2024-12-23T21:07:12.807227
Full S-matrices and Witten diagrams with (relative) L-infinity algebras,"The $L_\infty$-algebra approach to scattering amplitudes elegantly describes
the nontrivial part of the $S$-matrix but fails to take into account the
trivial part. We argue that the trivial contribution to the $S$-matrix should
be accounted for by another, complementary $L_\infty$-algebra, such that a
perturbative field theory is described by a cyclic relative $L_\infty$-algebra.
We further demonstrate that this construction reproduces Witten diagrams that
arise in AdS/CFT including, in particular, the trivial Witten diagrams
corresponding to CFT two-point functions. We also discuss Chern-Simons theory
and Yang-Mills theory on manifolds with boundaries using this approach.",161,2412.16106v1,hep-th,"hep-th,math-ph,math.MP,81T18 (Primary) 81T13, 81T35, 17B56, 17B81 (Secondary)",sustainable energy,2024-12-20,2024-12-23T21:07:12.807227
Quantifying the benefit of load uncertainty reduction for the design of district energy systems under grid constraints using the Value of Information,"Load uncertainty must be accounted for during design to ensure building
energy systems can meet energy demands during operation. Reducing building load
uncertainty allows for improved designs with less compromise to be identified,
reducing the cost of decarbonizing energy usage. However, the building
monitoring required to reduce load uncertainty is costly. This study quantifies
the economic benefit of practical building monitoring for supporting energy
system design decisions, to determine if its benefits outweigh its cost. Value
of Information analysis (VoI) is a numerical framework for quantifying the
benefit of uncertainty reduction to support decision making. An extension of
the framework, termed 'On-Policy' VoI, is proposed, which admits complex
decision making tasks where decision policies are required. This is applied to
a case study district energy system design problem, where a Linear Program
model is used to size solar-battery systems and grid connection capacity under
uncertain building loads, modelled using historic electricity metering data.
Load uncertainty is found to have a significant impact on both system operating
costs (\pm30%) and the optimal system design (\pm20%). However, using building
monitoring is found to reduce overall costs by less than 2% on average, less
than the cost of measurement, and is therefore not economically worthwhile.
This provides the first numerical evidence to support the sufficiency of using
standard building load profiles for energy system design. Further, reducing
only uncertainty in mean load is found to provide all available decision
support benefit, meaning using hourly measurement data provides no benefit for
energy retrofit design.",312,2412.16105v1,eess.SY,"eess.SY,cs.SY",sustainable energy,2024-12-20,2024-12-23T21:07:12.808224
Local structure and phonon states mediated by intercalation-driven doping in superconducting $Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$,"Intercalation of two-dimensional (2D) iron chalcogenides with molecular
species requires disentangling electronic and structural contributions to
understand the puzzling limit to superconducting transition temperature ($T_c$)
at the frontier of long interlayer separations. Here, synchrotron X-ray
absorption spectroscopy (XAS) at the Se K-edge sheds light on the impact of
carrier-doping on the local structure of the high-$T_c$ (~39 K)
$Li_{1.0}(C_5H_5N)_yFe_{2-z}Se_2$ phase. This material is derived by annealing
the structurally related as-made derivative ($T_c$~ 44 K), with layers being
primed apart by [alkali-molecule] guests. Metrics, such as, a reduced filling
of Se $4p$ orbitals and shorter Fe-Se bonds in the annealed phase, corroborate
to a lower electron doping level with respect to the as-made one. Analysis of
the metal-ligand thermal motion, based on the correlated Debye model, further
relates the higher $T_c$ intercalates with the softening of the local Fe-Se
bond. Beyond electronic effects, intercalation brings forth host-guest
interactions that mediate the dynamics of the bulk crystal structure. For this,
neutron time-of-flight spectroscopy on the annealed derivative, corroborates to
the Se-Fe-Se layer being sensitive to chemical pressure effects imposed by the
confined organic guests. This reflects in the phonon density of states, where
harder low-energy transverse acoustic matrix phonons and molecular vibrations
are witnessed, with respect to the pristine inorganic ($\beta$-FeSe) and
organic ($C_5D_5N$) counterparts. On cooling through $T_c$, these excitations
arrive without a collective magnetic-resonance mode - essential in
unconventional, spin-mediated mechanisms - enquiring about deviations from
optimal doping. The work highlights that when the Fe-square planes are tuned
far apart, carrier-doping leveraged by intercalation plays a key role in the
$T_c$ parametrization.",474,2412.16103v1,cond-mat.supr-con,"cond-mat.supr-con,cond-mat.mtrl-sci,cond-mat.str-el",sustainable energy,2024-12-20,2024-12-23T21:07:12.809223
High precision X-ray spectroscopy of kaonic neon,"The high-precision kaonic neon X-ray transitions measurement performed by the
SIDDHARTA-2 collaboration at the DA$\Phi$NE collider is reported. Both the
X-ray energies and yields for high-n transitions were measured, demonstrating
the feasibility of sub-eV Xray spectroscopy for kaonic atoms using low-Z
gaseous targets. The measurement provides valuable insights into the
de-excitation processes in kaonic atoms, providing new input data for the
refinement of the corresponding theoretical models, and a framework for testing
Quantum Electrodynamics in strange exotic atoms.",123,2412.16101v1,nucl-ex,"nucl-ex,hep-ex",sustainable energy,2024-12-20,2024-12-23T21:07:12.810219
Mixed QCD-EW corrections to the neutral-current Drell-Yan process,"We report on the complete computation of the mixed QCD-electroweak
corrections to the neutral-current Drell-Yan process. Our calculation holds in
the entire range of dilepton invariant masses. We present phenomenological
results for several kinematical distributions in the case of bare muons both in
the resonant region and for high invariant masses. We also consider the
forward-backward asymmetry, which is a key observable to measure the weak
mixing angle. We finally extend our calculation to dressed leptons and compare
our results in the massless limit to those available in the literature.",127,2412.16095v1,hep-ph,hep-ph,sustainable energy,2024-12-20,2024-12-23T21:07:12.810219
Electroweak corrections in the SMEFT: four-fermion operators at high energies,"In the Standard Model (SM), electroweak (EW) corrections become significant
at high energies, particularly at the tera-electronvolt scale and beyond, due
to the presence of Sudakov logarithms. At these energy scales, the Standard
Model Effective Field Theory (SMEFT) framework provides an enhanced sensitivity
to potential new physics effects. This motivates the inclusion of EW
corrections not only for SM predictions but also for analyses within SMEFT. In
this work, we compute EW corrections in the high-energy limit for a selected
set of dimension-six operators, specifically the class of four-fermion contact
interactions, in key hard-scattering processes relevant to both current and
future colliders: top-quark pair production at the Large Hadron Collider (LHC)
and in a muon collider scenario, as well as the Drell-Yan process at the LHC.
We first discuss the technical details and challenges associated with
evaluating EW Sudakov logarithms in SMEFT, contrasting them with the SM case.
We then present phenomenological results for the aforementioned processes,
highlighting the non-trivial effects introduced by EW corrections arising from
the insertion of dimension-six, four-fermion operators. Importantly, the
resulting $K$-factors exhibit significant deviations from their SM
counterparts, with dependencies not only on the process but also on the
specific operators considered. Finally, we explore the potential to lift flat
directions in the SMEFT parameter space by incorporating higher-order
corrections, using Fisher information techniques.",327,2412.16076v1,hep-ph,hep-ph,sustainable energy,2024-12-20,2024-12-23T21:07:12.811216
Cosmological Zoom-In Simulations of Milky Way Host Size Dark Matter Halos with a Blue-Tilted Primordial Power Spectrum,"Recent observations from the James Webb Space Telescope revealed a
surprisingly large number of galaxies formed at high redshift. Along with
strong lensing studies and nearby galaxy observations, these could challenge
the standard Lambda Cold Dark Matter cosmology with a power-law primordial
power spectrum. In this study, we conduct high-resolution cosmological zoom-in
dark matter-only simulations of Milky Way host size halos with a blue, tilted
primordial power spectrum ($P(k)\propto k^{m_s}$ with $m_s>1$ at small scales
$>1~{\rm Mpc}^{-1}$). We find that the blue-tilted subhalo mass functions can
be enhanced by more than a factor of two for subhalo masses $M_{\rm sub}
\lesssim 10^{10}~ M_{\odot}$, whereas the subhalo $V_{\rm max}$ functions can
be enhanced by a factor of four for maximum circular velocities $V_{\rm
max}\lesssim 30 ~{\rm km/s}$. The blue-tilted scaled cumulative substructure
fraction can be an order of magnitude higher at $\sim$10\% of the virial
radius. The blue-tilted subhalos also have higher central densities, since the
blue-tilted subhalos reach the same $V_{\rm max}$ at a smaller distance $R_{\rm
max}$ from the center. We have also verified these findings with
higher-resolution simulations.",343,2412.16072v1,astro-ph.CO,"astro-ph.CO,astro-ph.GA,gr-qc,hep-ph",sustainable energy,2024-12-20,2024-12-23T21:07:12.812214
Fully heavy asymmetric scalar tetraquarks,"The scalar tetraquarks $T_{b}$ and $T_{c}$ with asymmetric contents $bb
\overline{b}\overline{c}$ and $cc \overline{c}\overline{b}$ are explored using
the QCD sum rule method. These states are modeled as the diquark-antidiquarks
composed of the axial-vector components. The masses and current couplings of
$T_{b}$ and $T_{c}$ are calculated using the two-point sum rule approach. The
predictions obtained for the masses of these four-quark mesons prove that they
are unstable against the strong two-meson fall-apart decays to conventional
mesons. In the case of the tetraquark $ T_{b}$ this is the decay
$T_{\mathrm{b}}\to \eta _{b}B_{c}^{-}$. The processes
$T_{\mathrm{c}}\rightarrow \eta _{c}B_{c}^{+}$ and $J/\psi B_{c}^{\ast +}$ are
kinematically allowed decay modes of the tetraquark $ T_{c}$. The widths of
corresponding processes are evaluated by employing the QCD three-point sum rule
approach which are necessary to estimate strong couplings at the
tetraquark-meson-meson vertices of interest. The mass $ m=(15697 \pm
95)~\mathrm{MeV}$ and width $\Gamma[T_b]=(36.0 \pm 10.2)~ \mathrm{MeV}$ of the
tetraquark $T_{b}$ as well as the parameters $ \widetilde{m}=(9680 \pm
102)~\mathrm{MeV}$ and $\Gamma[T_c]=(54.7 \pm 9.9)~ \mathrm{MeV}$ in the case
of $T_{c}$ provide useful information to search for and interpret new exotic
states.",474,2412.16068v1,hep-ph,"hep-ph,hep-ex,hep-lat",sustainable energy,2024-12-20,2024-12-23T21:07:12.813211
Phase structure of quark matter and in-medium properties of mesons from Callan-Symanzik flows,"We compute meson spectral functions at finite temperature and density in the
quark-meson model, supplemented with a computation of the phase diagram. In
particular, we provide a detailed analysis of the non-analytic structure of the
meson two-point functions which is of great relevance for phenomenological
applications, such as moat regimes and inhomogeneous phases. Furthermore, it is
also relevant from a field-theoretical standpoint as it provides an insight
into the applicability of derivative expansions of the effective action to
studies of general fermion-boson models, both at zero and finite chemical
potential. Our computation is based on a functional renormalization group setup
that preserves causality, all spacetime symmetries, and the Silver-Blaze
property. The combination of these properties can only be achieved by a
Callan-Symanzik regulator. Instead of momentum shell integrations,
renormalization group flows generated by such a regulator describe the change
of the theory induced by a change of the masses of the mesons and quarks. A
particular focus of our work lies on the construction of controlled
Callan-Symanzik flows in the presence of spontaneous and explicit chiral
symmetry breaking by means of chiral Ward-Takahashi identities.",258,2412.16059v1,hep-ph,"hep-ph,nucl-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.813211
One-loop corrections to near extremal Kerr thermodynamics from semiclassical Virasoro blocks,"We propose a method to perform an exact calculation of one-loop quantum
corrections to black hole entropy in terms of Virasoro semiclassical blocks. We
analyse in detail four-dimensional Kerr black hole and show that in the
near-extremal limit a branch of long-lived modes arises. We prove that the
contribution of these modes accounts for a $(s-1/2)\log T_{\text{Hawking}}$
correction to the entropy for massless particles of spin $s=1,2$. We show that
in the full calculation performed in the exact Kerr background the leading
contribution actually is sourced by the near-horizon region only, and as such
has a universal validity for any asymptotic behavior at infinity.",157,2412.16057v1,hep-th,"hep-th,gr-qc",sustainable energy,2024-12-20,2024-12-23T21:07:12.814208
Approximation of Schrödinger operators with point interactions on bounded domains,"We consider Schr\""odinger operators on a bounded domain $\Omega\subset
\mathbb{R}^3$, with homogeneous Robin or Dirichlet boundary conditions on
$\partial\Omega$ and a point (zero-range) interaction placed at an interior
point of $\Omega$. We show that, under suitable spectral assumptions, and by
means of an extension-restriction procedure which exploit the already known
result on the entire space, the singular interaction is approximated by
rescaled sequences of regular potentials. The result is missing in the
literature, and we also take the opportunity to point out some general issues
in the approximation of point interactions and the role of zero energy
resonances.",146,2412.16056v1,math-ph,"math-ph,math.MP",sustainable energy,2024-12-20,2024-12-23T21:07:12.814208
Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space,"Within the Functional Renormalisation Group (FRG) approach, we present a
fluid-dynamical approach to solving flow equations for models living in a
multi-dimensional field space. To this end, the underlying exact flow equation
of the effective potential is reformulated as a set of nonlinear
advection-diffusion-type equations which can be solved using the
Kurganov-Tadmor central scheme, a modern finite-volume discretization from
computational fluid dynamics (CFD). We demonstrate the effectiveness of our
approach by performing explicit benchmark tests using zero-dimensional models
with two discretized field space directions or two symmetry invariants. Our
techniques can be directly applied to flow equations of effective potentials of
general (fermion-)boson systems with multiple invariants or condensates, as we
also demonstrate for two concrete examples in three spacetime dimensions.",180,2412.16053v1,cond-mat.stat-mech,"cond-mat.stat-mech,hep-ph",sustainable energy,2024-12-20,2024-12-23T21:07:12.815205
Self-organized critical characteristics of TeV-photons from GRB 221009A,"The very high-energy afterglow in GRB 221009A, known as the `Brightest Of All
Time' (B.O.A.T.), has been thoroughly analyzed in previous studies. In this
paper, we conducted a statistical analysis of the waiting time behavior of 172
TeV photons from the B.O.A.T. observed by LHAASO-KM2A. The following results
were obtained: (I) The waiting time distribution (WTD) of these photons
deviates from the exponential distribution. (II) The behavior of these photons
exhibits characteristics resembling those of a self-organized critical system,
such as power-law distribution and scale-invariance features in the waiting
time distribution. The power-law distribution of waiting times is consistent
with the prediction of a non-stationary process. (III) The relationship between
the power-law slopes of the WTD and the scale-invariant characteristics of the
Tsallis q-Gaussian distribution deviates from existing theory. We suggest that
this deviation is due to the photons not being completely independent of each
other. In summary, the power-law and scale-free characteristics observed in
these photons imply a self-organized critical process in the generation of TeV
photons from GRB 221009A. Based on other relevant research, we propose that the
involvement of a partially magnetically dominated component and the continuous
energy injection from the central engine can lead to deviations in the
generation of TeV afterglow from the simple external shock-dominated process,
thereby exhibiting the self-organized critical characteristics mentioned above.",335,2412.16052v1,astro-ph.HE,astro-ph.HE,sustainable energy,2024-12-20,2024-12-23T21:07:12.816203
Functional renormalization of QCD in $1 + 1$ dimensions: four-fermion interactions from quark-gluon dynamics,"Quantum Chromodynamics in two spacetime dimensions is investigated with the
Functional Renormalization Group. We use a functional formulation with
covariant gauge fixing and derive Renormalization Group flow equations for the
gauge coupling, quark mass and an algebraically complete set of local
fermion-fermion interaction vertices. The flow, based on a convenient
Callan-Symanzik-type regularization, shows the expected behavior for a
super-renormalizable theory in the ultraviolet regime and leads to a strongly
coupled regime in the infrared. Through a detailed discussion of symmetry
implications, and variations in the gauge group and flavor numbers, the
analysis sets the stage for a more detailed investigation of the bound state
spectrum in future work.",154,2412.16051v1,hep-ph,"hep-ph,hep-th,nucl-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.816203
Generalized Wilson lines and the gravitational scattering of spinning bodies,"A generalization of Wilson line operators at subleading power in the soft
expansion has been recently introduced as an efficient building block of
gravitational scattering amplitudes for non-spinning objects. The classical
limit in this picture corresponds to the strict Regge limit, where the
Post-Minkowskian (PM) expansion corresponds to the soft expansion, interpreted
as a sum over correlations of soft emissions. Building on the well-studied
worldline model with ${\cal N}=1$ supersymmetry, in this work we extend the
generalized Wilson line (GWL) approach to the case of spinning gravitating
bodies. Specifically, at the quantum level we derive from first-principles a
representation for the spin $1/2$ GWL that is relevant for the all-order
factorization of next-to-soft gravitons with fermionic matter, thus
generalizing the exponentiation of single-emission next-to-soft theorems. At
the classical level, we identity the suitable generalization of Wilson line
operators that enables the generation of classical spin observables at linear
order in spin. Thanks to the crucial role played by the soft expansion, the map
from Grassmann variables to classical spin is manifest. We also comment on the
relation between the GWL approach and the Worldline Quantum Field Theory as
well as the Heavy Mass Effective Theory formalism. We validate the approach by
rederiving known results in the conservative sector at 2PM order.",302,2412.16049v1,hep-th,hep-th,sustainable energy,2024-12-20,2024-12-23T21:07:12.817201
Discriminating between different modified dispersion relations from gamma-ray observations,"The fact that the standard dispersion relation for photons in vacuum could be
modified because of their interaction with the quantum nature of spacetime has
been proposed more than two decades ago. A quantitative model [Jacob \& Piran,
JCAP 01, 031 (2008)], has been tested extensively using distant highly
energetic astrophysical sources, searching for energy-dependent time delays in
photon arrival times. Since no delay was firmly measured, lower limits were set
on the energy scale $\Lambda$ related to these effects. In recent years,
however, different but equally well-grounded expressions beyond the Jacob \&
Piran model were obtained for the photon dispersion relation, leading to
different expressions for the dependence of lag versus redshift. This article
introduces a general parameterization of modified dispersion relations in
cosmological symmetry, which directly leads to a general parameterized lag
versus redshift dependence encompassing both existing and new models. This
parameterization could be used in the future to compare the predicted time lags
of the different models and test them against observations. To investigate this
possibility, realistic data sets are simulated, mimicking different types of
extragalactic sources as detected by current and future instruments. When no
lag is injected in the simulated data, each lag-redshift model leads, as
expected, to a different value for the limit on $\Lambda$, and the Jacob \&
Piran model gives the most stringent bound. When a lag at $\Lambda \sim E_P$ in
the Jacob \& Piran model is injected, it is detected for all the other
lag-redshift relations considered, although leading to different values.
Finally, the possibility to discriminate between several lag-redshift models is
investigated, emphasizing the importance of an evenly distributed sample of
sources across a wide range of redshifts.",388,2412.16048v1,astro-ph.HE,"astro-ph.HE,gr-qc",sustainable energy,2024-12-20,2024-12-23T21:07:12.818198
Segmentation of arbitrary features in very high resolution remote sensing imagery,"Very high resolution (VHR) mapping through remote sensing (RS) imagery
presents a new opportunity to inform decision-making and sustainable practices
in countless domains. Efficient processing of big VHR data requires automated
tools applicable to numerous geographic regions and features. Contemporary RS
studies address this challenge by employing deep learning (DL) models for
specific datasets or features, which limits their applicability across
contexts.
  The present research aims to overcome this limitation by introducing
EcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.
EcoMapper fully automates processing of geospatial data, DL model training, and
inference. Models trained with EcoMapper successfully segmented two distinct
features in a real-world UAV dataset, achieving scores competitive with prior
studies which employed context-specific models.
  To evaluate EcoMapper, many additional models were trained on permutations of
principal field survey characteristics (FSCs). A relationship was discovered
allowing derivation of optimal ground sampling distance from feature size,
termed Cording Index (CI). A comprehensive methodology for field surveys was
developed to ensure DL methods can be applied effectively to collected data.
  The EcoMapper code accompanying this work is available at
https://github.com/hcording/ecomapper .",264,2412.16046v1,cs.CV,cs.CV,sustainable energy,2024-12-20,2024-12-23T21:07:12.819195
Twist-tuned quantum criticality in moiré bilayer graphene,"We argue that moir\'e bilayer graphene at charge neutrality hosts a
continuous semimetal-to-insulator quantum phase transition that can be accessed
experimentally by tuning the twist angle between the two layers. For small
twist angles near the first magic angle, the system realizes a Kramers
intervalley-coherent insulator, characterized by circulating currents and
spontaneously broken time reversal and U(1) valley symmetries. For larger twist
angles above a critical value, the spectrum remains gapless down to the lowest
temperatures, with a fully symmetric Dirac semimetal ground state. Using
self-consistent Hartree-Fock theory applied to a realistic model of twisted
bilayer graphene, based on the Bistritzer-MacDonald Hamiltonian augmented by
screened Coulomb interactions, we find that the twist-tuned quantum phase
transition is continuous. We argue that the quantum critical behavior belongs
to the relativistic Gross-Neveu-XY universality class, and we characterize it
through an effective field theory analysis. Our theoretical predictions can be
directly tested using current experimental setups incorporating the recently
developed quantum twisting microscope.",232,2412.16042v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.str-el,hep-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.820192
Full Parity-Violating Trispectrum in Axion Inflation: Reduction to Low-D Integrals,"Recent measurements of the galaxy 4-Point Correlation Function (4PCF) have
seemingly detected non-zero parity-odd modes at high significance. Since
gravity, the primary driver of galaxy formation and evolution is parity-even,
any parity violation, if genuine, is likely to have been produced by some new
parity-violating mechanism in the early Universe. Here we investigate an
inflationary model with a Chern-Simons interaction between an axion and a
$U(1)$ gauge field, where the axion itself is the inflaton field. Evaluating
the trispectrum (Fourier-space analog of the 4PCF) of the primordial curvature
perturbations is an involved calculation with very high-dimensional loop
integrals. We demonstrate how to simplify these integrals and perform all
angular integrations analytically by reducing the integrals to convolutions and
exploiting the Convolution Theorem. This leaves us with low-dimensional radial
integrals that are much more amenable to efficient numerical evaluation. This
paper is the first in a series in which we will use these results to compute
the full late-time 4PCF for axion inflation, thence enabling constraints from
upcoming 3D spectroscopic surveys such as Dark Energy Spectroscopic Instrument
(DESI), Euclid, or Roman.",277,2412.16037v1,astro-ph.CO,"astro-ph.CO,gr-qc,hep-ph,hep-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.820192
X-ray polarization of the magnetar 1E 1841-045 in outburst,"We report on IXPE and NuSTAR observations that began forty days following the
onset of the 2024 outburst of the magnetar 1E 1841-045, marking the first ever
IXPE observation of a magnetar in an enhanced state. Our spectropolarimetric
analysis indicates that a non-thermal double power-law (PL) spectral model can
fit the phase-averaged intensity data well, with the soft and hard components
dominating below and above around 5 keV, respectively. We find that the soft PL
exhibits a polarization degree (PD) of about 20% while the hard X-ray PL
displays a PD of about 50%; both components have a polarization angle (PA)
compatible with 0 degree. These results are supported through model-independent
polarization analysis which shows an increasing PD from about 15% to 70% in the
2-3 keV and 6-8 keV ranges, respectively, while the PA remains consistent with
0 degree. We find marginal evidence for variability in the polarization
properties with pulse phase, namely a higher PD at spin phases coinciding with
the peak in the hard X-ray pulse. We compare the hard X-ray PL to the
expectation from direct resonant inverse Compton scattering (RICS) and
secondary pair cascade synchrotron radiation from primary high-energy RICS
photons, finding that both can provide reasonable spectropolarimetric agreement
with the data, yet, the latter more naturally. Finally, we suggest that the
soft power law X-ray component may be emission emanating from a Comptonized
corona in the inner magnetosphere.",327,2412.16036v1,astro-ph.HE,astro-ph.HE,sustainable energy,2024-12-20,2024-12-23T21:07:12.821190
Cosmological Non-Gaussianity from Neutrino Seesaw,"The neutrino mass generation via conventional seesaw mechanism is realized at
high scales around $O(10^{14})$GeV and probing new physics of the seesaw scale
poses a great challenge. A striking fact is that the neutrino seesaw scale is
typically around the cosmological inflation scale. In this work, we propose a
framework incorporating inflation and neutrino seesaw in which the inflaton
primarily decays into right-handed neutrinos after inflation. This decay
process is governed by the inflaton interaction with the right-handed neutrinos
that respects the shift symmetry. Under the neutrino seesaw mechanism,
fluctuations of the Higgs field can modulate the inflaton decays, contributing
to the curvature perturbation. We investigate the induced non-Gaussian
signatures and demonstrate that such signatures provides an important means to
probe the high-scale neutrino seesaw mechanism.",194,2412.16033v1,hep-ph,"hep-ph,astro-ph.CO,hep-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.822188
Integral representation for a relaxed optimal design problem for non-simple grade two materials,"A measure representation result for a functional modelling optimal design
problems for plastic deformations, under linear growth conditions, is obtained.
  Departing from an energy with a bulk term depending on the second gradient,
as well as a perimeter term, the functional in question corresponds to the
relaxation of this energy with respect to a pair $(\chi,u)$, where $\chi$ is
the characteristic function of a set of finite perimeter and $u$ is a function
of bounded hessian.",99,2412.16027v1,math.AP,"math.AP,math.OC,49J45, 49Q20, 26B25",sustainable energy,2024-12-20,2024-12-23T21:07:12.822188
Entropy maximizers for kinetic wave equations set on tori,"We consider the kinetic wave equation, or phonon Boltzmann equation, set on
the torus (physical system set on the lattice). We describe entropy maximizers
for fixed mass and energy; our framework is very general, being valid in any
dimension, for any dispersion relation, and even including the quantum kinetic
wave equation. Of particular interest is the presence of condensation in
certain regimes which we characterize.",89,2412.16026v1,math.AP,"math.AP,math-ph,math.MP",sustainable energy,2024-12-20,2024-12-23T21:07:12.822188
High-efficiency position resolved gamma ray detectors for 2D-measurements of the angular correlation of annihilation radiation,"The measurement of the 2D-Angular Correlation of Electron Positron
Annihilation Radiation (ACAR) provides unique information about the bulk
electronic structure of single crystals. We set up a new prototype for 2D-ACAR
measurements using two 24 x 24 (26.8 mm x 26.8 mm) pixelated LYSO scintillation
crystals in combination with a glass light guide and 8 x 8 (24 mm x 24 mm)
Multi Pixel Photon Counters (MPPCs). Compared to conventional Anger-cameras,
typically comprising large NaI(Tl) scintillators read out with photomultiplier
arrays a larger implementation of our prototype would drastically improve
resolution and count rate by taking advantage of the small pixel size of the
scintillator, its much higher attenuation coefficient for 511 keV
{\gamma}-quanta and faster digital readout. With our prototype we achieved a
detection efficiency of 45%, i.e. five times higher compared to NaI(Tl) used in
our Anger cameras, leading to a 25 (!) times higher coincidence count rate in
ACAR measurements. A spatial resolution of 1 mm was obtained, which is limited
by the pixel size of the scintillator. We demonstrate the high performance of
the setup by (i) imaging the local distribution of 22Na in a proton-irradiated
aluminum target and (ii) determining the Fermi energy of Cu from 2D-ACAR
spectra recorded for a polycrystalline copper sample.",311,2412.16024v1,physics.ins-det,physics.ins-det,sustainable energy,2024-12-20,2024-12-23T21:07:12.823184
Knowledge-dependent optimal Gaussian strategies for phase estimation,"When estimating an unknown phase rotation of a continuous-variable system
with homodyne detection, the optimal probe state strongly depends on the value
of the estimated parameter. In this article, we identify the optimal pure
single-mode Gaussian probe states depending on the knowledge of the estimated
phase parameter before the measurement. We find that for a large prior
uncertainty, the optimal probe states are close to coherent states, a result in
line with findings from noisy parameter estimation. But with increasingly
precise estimates of the parameter it becomes beneficial to put more of the
available energy into the squeezing of the probe state. Surprisingly, there is
a clear jump, where the optimal probe state changes abruptly to a squeezed
vacuum state, which maximizes the Fisher information for this estimation task.
We use our results to study repeated measurements and compare different methods
to adapt the probe state based on the changing knowledge of the parameter
according to the previous findings.",184,2412.16023v1,quant-ph,quant-ph,sustainable energy,2024-12-20,2024-12-23T21:07:12.824182
Dimension-8 operators in $W^+W^-$ production via gluon fusion,"We investigate the impact of dimension-8 operators on $W^+W^-$ production at
the LHC for the incoming gluon-gluon channel. To this end, we have identified
all dimension-8 CP-even operators contributing to the process in question, and
computed the corresponding tree-level helicity amplitudes for fully-leptonic
decays of the $W$ bosons. These are implemented in the program MCFM-RE, which
automatically incorporates the effect of a jet-veto to reduce the otherwise
overwhelming $t\bar t$ background. We find that, unless we break the hierarchy
of the effective field theory (EFT), the interference of the dimension-8
operators with the Standard Model is negligible across the considered
distributions. This justifies including the square of dimension-6 operators
when performing EFT fits with this channel. We then present new constraints on
CP-even and CP-odd dimension-6 operators within the EFT regime. Lastly, we
postulate a scenario in which the hierarchy of the EFT is broken, justified by
the strong constraints on dimension-6 operators from existing on-shell Higgs
data. In this scenario, we discuss the constraints that can be reasonably set
on CP-even dimension-8 operators with current and future data. We remark that
the effect of the jet-veto on the ability to constrain new physics in the
$W^+W^-$ channel is quite dramatic and must be properly taken into account.",309,2412.16020v1,hep-ph,"hep-ph,hep-ex",sustainable energy,2024-12-20,2024-12-23T21:07:12.825181
Distributed Beam Alignment in sub-THz D2D Networks,"Devices in a device-to-device (D2D) network operating in sub-THz frequencies
require knowledge of the spatial channel that connects them to their peers.
Acquiring such high dimensional channel state information entails large
overhead, which drastically increases with the number of network devices. In
this paper, we propose an accelerated method to achieve network-wide beam
alignment in an efficient way. To this aim, we consider compressed sensing
estimation enabled by a novel design of pilot sequences. Our designed pilots
have constant envelope to alleviate hardware requirements at the transmitters,
while they exhibit a ""comb-like""' spectrum that flexibly allocates energy only
on certain frequencies. This design enables multiple devices to transmit thier
pilots concurrently while remaining orthogonal in frequency, achieving
simultaneous alignment of multiple devices. Furthermore, we present a
sequential partitioning strategy into transmitters and receivers that results
in logarithmic scaling of the overhead with the number of devices, as opposed
to the conventional linear scaling. Finally, we show via accurate modeling of
the indoor propagation environment and ray tracing simulations that the
resulting sub-THz channels after successful beamforming are approximately
frequency flat, therefore suitable for efficient single carrier transmission
without equalization. We compare our results against an ""802.11ad inspired""
baseline and show that our method is capable to greatly reduce the number of
pilots required to achieve network-wide alignment.",277,2412.16015v1,eess.SP,eess.SP,sustainable energy,2024-12-20,2024-12-23T21:07:12.826179
Fuzzy-Space Engineering,"The techniques developed for matrix models and fuzzy geometry are powerful
tools for representing strings and membranes in quantum physics. We study the
representation of fuzzy surfaces using these techniques. This involves
constructing graphs and writing their coordinates and connectivity into
matrices. To construct arbitrary graphs and quickly change them, we use 3D
software. A script generates the three matrices from the graphs. These matrices
are then processed in Wolfram Mathematica to calculate the zero modes of the
Dirac operator. Our first result shows the quantization of a two-dimensional
Trefoil knot. Additional examples illustrate various properties and behaviors
of this process. This helps us to gain a deeper understanding of fuzzy spaces
and zero-mode surfaces. This work contributes to advancing the understanding of
visualization aspects in fuzzy geometry.",153,2412.16011v1,hep-th,hep-th,sustainable energy,2024-12-20,2024-12-23T21:07:12.826179
Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep Learning,"Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a
cornerstone to assessing the authenticity of the received information and
guaranteeing robust service delivery in several application domains. The
solutions available today for spoofing detection either rely on additional
communication systems, receivers, and antennas, or require mobile deployments.
Detection systems working at the Physical (PHY) layer of the satellite
communication link also require time-consuming and energy-hungry training
processes on all satellites of the constellation, and rely on the availability
of spoofed data, which are often challenging to collect. Moreover, none of such
contributions investigate the feasibility of aerial spoofing attacks launched
via drones operating at various altitudes. In this paper, we propose a new
spoofing detection technique for LEO satellite constellation systems, applying
anomaly detection on the received PHY signal via autoencoders. We validate our
solution through an extensive measurement campaign involving the deployment of
an actual spoofer (Software-Defined Radio) installed on a drone and injecting
rogue IRIDIUM messages while flying at different altitudes with various
movement patterns. Our results demonstrate that the proposed technique can
reliably detect LEO spoofing attacks launched at different altitudes, while
state-of-the-art competing approaches simply fail. We also release the
collected data as open source, fostering further research on satellite
security.",276,2412.16008v1,cs.CR,cs.CR,sustainable energy,2024-12-20,2024-12-23T21:07:12.827174
Collective single-photon emission and energy transfer in thin-layer dielectric and plasmonic systems,"We study the collective photon decay of multiple quantum emitters embedded in
a thin high-index dielectric layer such as hexagonal boron nitride (hBN), with
and without a metal substrate. We first explore the significant role that
guided modes including surface plasmon modes play in the collective decay of
identical singlephoton emitters (super- and subradiance). Surprisingly, on
distances relevant for collective emission, the guided or surface-plasmon modes
do not always enhance the collective emission. We identify configurations with
inhibition, and others with enhancement of the dipole interaction due to the
guided modes. We interpret our results in terms of local and cross densities of
optical states. In the same structure, we show a remarkably favorable
configuration for enhanced F\""orster resonance energy transfer between a donor
and acceptor in the dielectric layer on a metallic substrate. We compare our
results to theoretical limits for energy transfer efficiency.",195,2412.16000v1,physics.optics,"physics.optics,cond-mat.mes-hall,quant-ph",sustainable energy,2024-12-20,2024-12-23T21:07:12.828171
"Ti and Spi, Carrollian extended boundaries at timelike and spatial infinity","The goal of this paper is to provide a definition for a notion of extended
boundary at time and space-like infinity which, following
Figueroa-O'Farril--Have--Prohazka--Salzer, we refer to as Ti and Spi. This
definition applies to asymptotically flat spacetime in the sense of
Ashtekar--Romano and we wish to demonstrate, by example, its pertinence in a
number of situations. The definition is invariant, is constructed solely from
the asymptotic data of the metric and is such that automorphisms of the
extended boundaries are canonically identified with asymptotic symmetries.
Furthermore, scattering data for massive fields are realised as functions on Ti
and a geometric identification of cuts of Ti with points of Minkowksi then
produces an integral formula of Kirchhoff type. Finally, Ti and Spi are both
naturally equipped with (strong) Carrollian geometries which, under mild
assumptions, enable to reduce the symmetry group down to the BMS group, or to
Poincar\'e in the flat case. In particular, Strominger's matching conditions
are naturally realised by restricting to Carrollian geometries compatible with
a discrete symmetry of Spi.",262,2412.15996v1,gr-qc,"gr-qc,hep-th,math-ph,math.MP",sustainable energy,2024-12-20,2024-12-23T21:07:12.828171
From discrete to continuum in the helical XY-model: emergence of chirality transitions in the $S^1$ to $S^2$ limit,"We analyze the discrete-to-continuum limit of a frustrated
ferromagnetic/anti-ferromagnetic $\mathbb{S}^2$-valued spin system on the
lattice $\lambda_n\mathbb{Z}^2$ as $\lambda_n\to 0$. For $\mathbb{S}^2$ spin
systems close to the Landau-Lifschitz point (where the
helimagnetic/ferromagnetic transition occurs), it is well established that for
chirality transitions emerge with vanishing energy. Inspired by recent work on
the $N$-clock model, we consider a spin model where spins are constrained to
$k_n$ copies of $\mathbb{S}^1$ covering $\mathbb{S}^2$ as $n\to\infty$. We
identify a critical energy-scaling regime and a threshold for the divergence
rate of $k_n\to+\infty$, below which the $\Gamma$-limit of the discrete
energies capture chirality transitions while retaining an $\mathbb{S}^2$-valued
energy description in the continuum limit.",256,2412.15994v1,math.AP,math.AP,sustainable energy,2024-12-20,2024-12-23T21:07:12.829168
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",sustainable energy,2024-12-20,2024-12-23T21:07:12.830165
Active Flow Control for Bluff Body under High Reynolds Number Turbulent Flow Conditions Using Deep Reinforcement Learning,"This study employs Deep Reinforcement Learning (DRL) for active flow control
in a turbulent flow field of high Reynolds numbers at $Re=274000$. That is, an
agent is trained to obtain a control strategy that can reduce the drag of a
cylinder while also minimizing the oscillations of the lift. Probes are placed
only around the surface of the cylinder, and a Proximal Policy Optimization
(PPO) agent controls nine zero-net mass flux jets on the downstream side of the
cylinder. The trained PPO agent effectively reduces drag by $29\%$ and
decreases lift oscillations by $18\%$ of amplitude, with the control effect
demonstrating good repeatability. Control tests of this agent within the
Reynolds number range of $Re=260000$ to $288000$ show the agent's control
strategy possesses a certain degree of robustness, with very similar drag
reduction effects under different Reynolds numbers. Analysis using power
spectral energy reveals that the agent learns specific flow frequencies in the
flow field and effectively suppresses low-frequency, large-scale structures.
Graphically visualizing the policy, combined with pressure, vorticity, and
turbulent kinetic energy contours, reveals the mechanism by which jets achieve
drag reduction by influencing reattachment vortices. This study successfully
implements robust active flow control in realistically significant high
Reynolds number turbulent flows, minimizing time costs (using two-dimensional
geometrical models and turbulence models) and maximally considering the
feasibility of future experimental implementation.",315,2412.15975v1,physics.flu-dyn,physics.flu-dyn,sustainable energy,2024-12-20,2024-12-23T21:07:12.830165
Adding interferometric lightning detection to the Pierre Auger Observatory,"The Pierre Auger Observatory has detected downward terrestrial gamma-ray
flashes (TGFs) with its Surface Detector. A key to understanding this
high-energy radiation in thunderstorms is to combine such measurements with
measurements of lightning processes in their earliest stages. With eleven
modified Auger Engineering Radio Array (AERA) stations we can build an
interferometric lightning detection array working in the bandwidth between 30 -
80 MHz inside the Surface Detector array to precisely measure lightning stepped
leaders in 3D. These measurements allow us to decipher the cause of TGFs and
clarify the reason for the observed high-energy particles in thunderstorms. We
will present the current status of the detection plans including the
configuration of the interferometric lightning detection array and the steps to
take as well as the reconstruction characteristics obtained with AERA.",166,2412.15972v1,astro-ph.IM,"astro-ph.IM,astro-ph.HE",sustainable energy,2024-12-20,2024-12-23T21:07:12.831163
Optimization of Beyond Diagonal RIS: A Universal Framework Applicable to Arbitrary Architectures,"Reconfigurable intelligent surfaces (RISs) are envisioned as a promising
technology for future wireless communication systems due to their ability to
control the propagation environment in a hardware- and energy-efficient way.
Recently, the concept of RISs has been extended to beyond diagonal RISs
(BD-RISs), which unlock the full potential of RISs thanks to the presence of
tunable interconnections between RIS elements. While various algorithms have
been proposed for specific BD-RIS architectures, a universal optimization
framework applicable to arbitrary architectures is still lacking. In this
paper, we bridge this research gap by proposing an architecture-independent
framework for BD-RIS optimization, with the main focus on sum-rate maximization
and transmit power minimization in multiuser multi-input single-output
(MU-MISO) systems. Specifically, we first incorporate BD-RIS architectures into
the models by connecting the scattering matrix with the admittance matrix and
introducing appropriate constraints to the admittance matrix. The formulated
problems are then solved by our custom-designed partially proximal alternating
direction method of multipliers (pp-ADMM) algorithms. The pp-ADMM algorithms
are computationally efficient, with each subproblem either admitting a
closed-form solution or being easily solvable. We further explore the extension
of the proposed framework to general utility functions and multiuser
multi-input multi-output (MU-MIMO) systems. Simulation results demonstrate that
the proposed approaches achieve a better trade-off between performance and
computational efficiency compared to existing methods. We also compare the
performance of various BD-RIS architectures in MU-MISO systems using the
proposed approach, which has not been explored before due to the lack of an
architecture-independent framework.",366,2412.15965v1,eess.SP,"eess.SP,cs.IT,math.IT,math.OC",sustainable energy,2024-12-20,2024-12-23T21:07:12.832160
What shall we learn from a future supernova?,"Core-collapse supernovae constitute a unique laboratory for particle physics
and astrophysics. They are powerful neutrino sources of all flavors, emitting
essentially all the gravitational binding energy through neutrinos, at the end
of their life. I will highlight how crucial is the observation of the next
core-collapse supernova and of the diffuse supernova neutrino background, whose
discovery might be imminent.",84,2412.15964v1,astro-ph.SR,"astro-ph.SR,astro-ph.HE,hep-ph",sustainable energy,2024-12-20,2024-12-23T21:07:12.832160
Feynman Integral Reduction without Integration-By-Parts,"We present an interesting study of Feynman integral reduction that does not
employ integration-by-parts identities. Our approach proceeds by studying the
equivalence relations of integral contours in the Feynman parameterization. We
find that the integration contour can take a more general form than that given
by the Cheng-Wu theorem. We apply this idea to one-loop integrals, and derive
universal reduction formulas that can be used to efficiently reduce any
one-loop integral. We expect that this approach can be useful in the reduction
of multi-loop integrals as well.",117,2412.15962v1,hep-th,"hep-th,hep-ph",sustainable energy,2024-12-20,2024-12-23T21:07:12.833158
Effective Metric Description of 2+1 Dimensional Quantum Black Holes,"We develop an effective metric description of 2+1 dimensional black holes
describing deviations from the classical Ba\~nados-Teitelboim-Zanelli (BTZ)
black hole. The latter is a classical 2+1 dimensional rotating black hole with
constant negative curvature. The effective metric is constrained by imposing
the black hole symmetries and asymptotic classical behavior. The deformed
metric is parametrized in terms of a physical quantity that we choose to be a
physical distance. The latter can be solved for in three main regions of
interest, the one around the horizon, origin, and spatial infinity. The
finiteness of physical quantities at the horizon, such as the Ricci and
Kretschmann scalars, leads to universal constraints on the physical parameters
of the metric around the horizon. This allows us to further derive the general
form of the corrected Hawking temperature in terms of the physical parameters
of the effective metric. Assuming that the approach can be generalized to the
interior of the black hole, we further develop an effective metric description
near the origin. To illustrate the approach, we show how to recast the
information encoded in a specific model of quantum BTZ known as quBTZ black
hole in terms of the effective metric coefficients.",256,2412.15960v1,gr-qc,"gr-qc,hep-ph,hep-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.833158
Heavy-quark mass effects in off-light-cone distributions,"We compute the one-loop correction to the forward matrix element of an
off-light-cone bi-local quark correlator characterised by a space-like
separation $z^2$ in the presence of heavy quarks with mass $m$. This
calculation allows us to extract the one-loop matching kernel, necessary to
connect quasi and pseudo-distributions to collinear parton distribution
functions (PDFs), accounting for heavy-quark mass effects. Our result is exact
in that it includes all powers of $z^2m^2$ at one loop in $\alpha_s$. In the
limit $z^2m^2\rightarrow 0$, it consistently reduces to the known massless
result. We also carry out an implementation of our expression, which allows us
to compute the charm pseudo-distribution of the proton given its PDF. We
finally comment on the quantitative impact of heavy-quark mass corrections.",197,2412.15958v1,hep-ph,"hep-ph,hep-lat,nucl-th",sustainable energy,2024-12-20,2024-12-23T21:07:12.834155
MotiF: Making Text Count in Image Animation with Motion Focal Loss,"Text-Image-to-Video (TI2V) generation aims to generate a video from an image
following a text description, which is also referred to as text-guided image
animation. Most existing methods struggle to generate videos that align well
with the text prompts, particularly when motion is specified. To overcome this
limitation, we introduce MotiF, a simple yet effective approach that directs
the model's learning to the regions with more motion, thereby improving the
text alignment and motion generation. We use optical flow to generate a motion
heatmap and weight the loss according to the intensity of the motion. This
modified objective leads to noticeable improvements and complements existing
methods that utilize motion priors as model inputs. Additionally, due to the
lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V
Bench, a dataset consists of 320 image-text pairs for robust evaluation. We
present a human evaluation protocol that asks the annotators to select an
overall preference between two videos followed by their justifications. Through
a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced
models, achieving an average preference of 72%. The TI2V Bench is released in
https://wang-sj16.github.io/motif/.",263,2412.16153v1,cs.CV,"cs.CV,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.534863
Offline Reinforcement Learning for LLM Multi-Step Reasoning,"Improving the multi-step reasoning ability of large language models (LLMs)
with offline reinforcement learning (RL) is essential for quickly adapting them
to complex tasks. While Direct Preference Optimization (DPO) has shown promise
in aligning LLMs with human preferences, it is less suitable for multi-step
reasoning tasks because (1) DPO relies on paired preference data, which is not
readily available for multi-step reasoning tasks, and (2) it treats all tokens
uniformly, making it ineffective for credit assignment in multi-step reasoning
tasks, which often come with sparse reward. In this work, we propose OREO
(Offline Reasoning Optimization), an offline RL method for enhancing LLM
multi-step reasoning. Building on insights from previous works of maximum
entropy reinforcement learning, it jointly learns a policy model and value
function by optimizing the soft Bellman Equation. We show in principle that it
reduces the need to collect pairwise data and enables better credit assignment.
Empirically, OREO surpasses existing offline learning methods on multi-step
reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and
embodied agent control (ALFWorld). The approach can be extended to a
multi-iteration framework when additional resources are available. Furthermore,
the learned value function can be leveraged to guide the tree search for free,
which can further boost performance during test time.",288,2412.16145v1,cs.LG,"cs.LG,cs.AI,cs.CL",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.535861
Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation,"Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk. The
source code can be found at the following GitHub link:
https://github.com/mohammadi-ali/MetamorphASM.",348,2412.16135v1,cs.CR,"cs.CR,cs.AI,cs.CL",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.536858
Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy,"Focused ultrasound (FUS) therapy is a promising tool for optimally targeted
treatment of spinal cord injuries (SCI), offering submillimeter precision to
enhance blood flow at injury sites while minimizing impact on surrounding
tissues. However, its efficacy is highly sensitive to the placement of the
ultrasound source, as the spinal cord's complex geometry and acoustic
heterogeneity distort and attenuate the FUS signal. Current approaches rely on
computer simulations to solve the governing wave propagation equations and
compute patient-specific pressure maps using ultrasound images of the spinal
cord anatomy. While accurate, these high-fidelity simulations are
computationally intensive, taking up to hours to complete parameter sweeps,
which is impractical for real-time surgical decision-making. To address this
bottleneck, we propose a convolutional deep operator network (DeepONet) to
rapidly predict FUS pressure fields in patient spinal cords. Unlike
conventional neural networks, DeepONets are well equipped to approximate the
solution operator of the parametric partial differential equations (PDEs) that
govern the behavior of FUS waves with varying initial and boundary conditions
(i.e., new transducer locations or spinal cord geometries) without requiring
extensive simulations. Trained on simulated pressure maps across diverse
patient anatomies, this surrogate model achieves real-time predictions with
only a 2% loss on the test set, significantly accelerating the modeling of
nonlinear physical systems in heterogeneous domains. By facilitating rapid
parameter sweeps in surgical settings, this work provides a crucial step toward
precise and individualized solutions in neurosurgical treatments.",328,2412.16118v1,physics.med-ph,"physics.med-ph,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.537855
Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring,"The integration of Large Vision-Language Models (LVLMs) such as OpenAI's
GPT-4 Vision into various sectors has marked a significant evolution in the
field of artificial intelligence, particularly in the analysis and
interpretation of visual data. This paper explores the practical application of
GPT-4 Vision in the construction industry, focusing on its capabilities in
monitoring and tracking the progress of construction projects. Utilizing
high-resolution aerial imagery of construction sites, the study examines how
GPT-4 Vision performs detailed scene analysis and tracks developmental changes
over time. The findings demonstrate that while GPT-4 Vision is proficient in
identifying construction stages, materials, and machinery, it faces challenges
with precise object localization and segmentation. Despite these limitations,
the potential for future advancements in this technology is considerable. This
research not only highlights the current state and opportunities of using LVLMs
in construction but also discusses future directions for enhancing the model's
utility through domain-specific training and integration with other computer
vision techniques and digital twins.",209,2412.16108v1,cs.CV,"cs.CV,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.537855
Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis,"Detecting and analyzing complex patterns in multivariate time-series data is
crucial for decision-making in urban and environmental system operations.
However, challenges arise from the high dimensionality, intricate complexity,
and interconnected nature of complex patterns, which hinder the understanding
of their underlying physical processes. Existing AI methods often face
limitations in interpretability, computational efficiency, and scalability,
reducing their applicability in real-world scenarios. This paper proposes a
novel visual analytics framework that integrates two generative AI models, Time
Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex
patterns into lower-dimensional latent spaces and visualize them in 2D using
dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN.
These visualizations, presented through coordinated and interactive views and
tailored glyphs, enable intuitive exploration of complex multivariate temporal
patterns, identifying patterns' similarities and uncover their potential
correlations for a better interpretability of the AI outputs. The framework is
demonstrated through a case study on power grid signal data, where it
identifies multi-label grid event signatures, including faults and anomalies
with diverse root causes. Additionally, novel metrics and visualizations are
introduced to validate the models and evaluate the performance, efficiency, and
consistency of latent maps generated by TFT and VAE under different
configurations. These analyses provide actionable insights for model parameter
tuning and reliability improvements. Comparative results highlight that TFT
achieves shorter run times and superior scalability to diverse time-series data
shapes compared to VAE. This work advances fault diagnosis in multivariate time
series, fostering explainable AI to support critical system operations.",349,2412.16098v1,cs.LG,"cs.LG,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.538853
Dual-Polarized Beyond Diagonal RIS,"Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a family of
RIS architectures more flexible than conventional RIS. While BD-RIS has been
primarily analyzed assuming uni-polarized systems, modern wireless deployments
are dual-polarized. To address this gap, this paper investigates the
fundamental limits of dual-polarized BD-RIS-aided systems. We derive the
scaling laws governing the performance of BD-RIS and the Pareto frontier of the
trade-off between performance and circuit complexity enabled by BD-RIS.
Theoretical results show that the group-connected RIS with group size 2
provides remarkable gains over conventional RIS in both Rayleigh and
line-of-sight (LoS) channels, while maintaining a reduced circuit complexity.",166,2412.16097v1,cs.IT,"cs.IT,eess.SP,math.IT",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.539850
The Evolution of LLM Adoption in Industry Data Curation Practices,"As large language models (LLMs) grow increasingly adept at processing
unstructured text data, they offer new opportunities to enhance data curation
workflows. This paper explores the evolution of LLM adoption among
practitioners at a large technology company, evaluating the impact of LLMs in
data curation tasks through participants' perceptions, integration strategies,
and reported usage scenarios. Through a series of surveys, interviews, and user
studies, we provide a timely snapshot of how organizations are navigating a
pivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess
LLM adoption in industry for development tasks (N=84), and facilitated expert
interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we
explored practitioners' current and anticipated LLM usage through a user study
involving two LLM-based prototypes (N=12). While each study addressed distinct
research goals, they revealed a broader narrative about evolving LLM usage in
aggregate. We discovered an emerging shift in data understanding from
heuristic-first, bottom-up approaches to insights-first, top-down workflows
supported by LLMs. Furthermore, to respond to a more complex data landscape,
data practitioners now supplement traditional subject-expert-created 'golden
datasets' with LLM-generated 'silver' datasets and rigorously validated 'super
golden' datasets curated by diverse experts. This research sheds light on the
transformative role of LLMs in large-scale analysis of unstructured data and
highlights opportunities for further tool development.",328,2412.16089v1,cs.HC,"cs.HC,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.540847
Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG,"Deep learning has advanced medical image classification, but interpretability
challenges hinder its clinical adoption. This study enhances interpretability
in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)
and a multi-agent Retrieval-Augmented Generation (RAG) system for report
generation. By modeling relationships between visual features and clinical
concepts, we create interpretable concept vectors that guide a multi-agent RAG
system to generate radiology reports, enhancing clinical relevance,
explainability, and transparency. Evaluation of the generated reports using an
LLM-as-a-judge confirmed the interpretability and clinical utility of our
model's outputs. On the COVID-QU dataset, our model achieved 81% classification
accuracy and demonstrated robust report generation performance, with five key
metrics ranging between 84% and 90%. This interpretable multi-agent framework
bridges the gap between high-performance AI and the explainability required for
reliable AI-driven CXR analysis in clinical settings.",202,2412.16086v1,cs.IR,"cs.IR,cs.AI,cs.CL,cs.CV,eess.IV",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.540847
Benchmarking stochasticity behind reproducibility: denoising strategies in Ta$_2$O$_5$ memristors,"Reproducibility, endurance, driftless data retention, and fine resolution of
the programmable conductance weights are key technological requirements against
memristive artificial synapses in neural network applications. However, the
inherent fluctuations in the active volume impose severe constraints on the
weight resolution. In order to understand and push these limits, a
comprehensive noise benchmarking and noise reduction protocol is introduced.
Our approach goes beyond the measurement of steady-state readout noise levels
and tracks the voltage-dependent noise characteristics all along the resistive
switching $I(V)$ curves. Furthermore, we investigate the tunability of the
noise level by dedicated voltage cycling schemes in our filamentary Ta$_2$O$_5$
memristors. This analysis highlights a broad, order-of-magnitude variability of
the possible noise levels behind seemingly reproducible switching cycles. Our
nonlinear noise spectroscopy measurements identify a subthreshold voltage
region with voltage-boosted fluctuations. This voltage range enables the
reconfiguration of the fluctuators without resistive switching, yielding a
highly denoised state within a few subthreshold cycles.",235,2412.16080v1,cond-mat.mes-hall,"cond-mat.mes-hall,cond-mat.mtrl-sci",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.541846
Formal Mathematical Reasoning: A New Frontier in AI,"AI for Mathematics (AI4Math) is not only intriguing intellectually but also
crucial for AI-driven discovery in science, engineering, and beyond. Extensive
efforts on AI4Math have mirrored techniques in NLP, in particular, training
large language models on carefully curated math datasets in text form. As a
complementary yet less explored avenue, formal mathematical reasoning is
grounded in formal systems such as proof assistants, which can verify the
correctness of reasoning and provide automatic feedback. In this position
paper, we advocate for formal mathematical reasoning and argue that it is
indispensable for advancing AI4Math to the next level. In recent years, we have
seen steady progress in using AI to perform formal reasoning, including core
tasks such as theorem proving and autoformalization, as well as emerging
applications such as verifiable generation of code and hardware designs.
However, significant challenges remain to be solved for AI to truly master
mathematics and achieve broader impact. We summarize existing progress, discuss
open challenges, and envision critical milestones to measure future success. At
this inflection point for formal mathematical reasoning, we call on the
research community to come together to drive transformative advancements in
this field.",249,2412.16075v1,cs.AI,"cs.AI,cs.LG,cs.LO",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.542845
Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy,"The accurate segmentation of guidewires in interventional cardiac fluoroscopy
videos is crucial for computer-aided navigation tasks. Although deep learning
methods have demonstrated high accuracy and robustness in wire segmentation,
they require substantial annotated datasets for generalizability, underscoring
the need for extensive labeled data to enhance model performance. To address
this challenge, we propose the Segmentation-guided Frame-consistency Video
Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy
videos, augmenting the training data for wire segmentation networks. SF-VD
leverages videos with limited annotations by independently modeling scene
distribution and motion distribution. It first samples the scene distribution
by generating 2D fluoroscopy images with wires positioned according to a
specified input mask, and then samples the motion distribution by progressively
generating subsequent frames, ensuring frame-to-frame coherence through a
frame-consistency strategy. A segmentation-guided mechanism further refines the
process by adjusting wire contrast, ensuring a diverse range of visibility in
the synthesized image. Evaluation on a fluoroscopy dataset confirms the
superior quality of the generated videos and shows significant improvements in
guidewire segmentation.",247,2412.16050v1,cs.CV,"cs.CV,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.543840
Applying Predictive Analytics to Occupational Health and Safety in India,"Predictive analytics is revolutionizing occupational health and safety (OHS).
It offers evidence-based insights. These insights enable proactive risk
management and informed, data-driven decision-making in organizational
settings. This paper explores the key components of predictive analytics in
OHS, beginning with data collection, management, and preparation, and moving
through to advanced predictive modelling techniques. We emphasize the
importance of data integrity through processes such as missing value
imputation, anomaly detection, and feature engineering to ensure accurate model
predictions. Risk prioritization identifies and ranks hazards across various
factors, including employee behaviours, organizational policies, environmental
conditions, and operational practices. We posit that insights derived from
predictive models must be effectively interpreted and implemented. These
insights guide organizations to focus on high-impact areas for accident
prevention and resource optimization. The integration of predictive analytics
in OHS brings notable benefits, including enhanced decision-making, greater
operational efficiency, cost savings, and improved compliance with safety
standards. We examine applications of predictive analytics in OHS in Indian
settings. India has the largest workforce in the world, and the predominance of
it is in the informal sector - a sector largely unprotected by the already
inadequate OHS laws. Ethical considerations, data privacy concerns, and the
risk of overdependence on predictive models are discussed. We conclude with a
discussion on the potential for predictive analytics to create a data-oriented,
adaptive approach to OHS in India. We posit that, using predictive analytics,
India can develop high safety standards while traversing the complexities of
its workforce setting.",330,2412.16038v1,cs.CY,"cs.CY,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.544837
A Framework for Streaming Event-Log Prediction in Business Processes,"We present a Python-based framework for event-log prediction in streaming
mode, enabling predictions while data is being generated by a business process.
The framework allows for easy integration of streaming algorithms, including
language models like n-grams and LSTMs, and for combining these predictors
using ensemble methods.
  Using our framework, we conducted experiments on various well-known
process-mining data sets and compared classical batch with streaming mode.
Though, in batch mode, LSTMs generally achieve the best performance, there is
often an n-gram whose accuracy comes very close. Combining basic models in
ensemble methods can even outperform LSTMs. The value of basic models with
respect to LSTMs becomes even more apparent in streaming mode, where LSTMs
generally lack accuracy in the early stages of a prediction run, while basic
methods make sensible predictions immediately.",173,2412.16032v1,cs.AI,"cs.AI,cs.LG",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.545834
The Only Way is Ethics: A Guide to Ethical Research with Large Language Models,"There is a significant body of work looking at the ethical considerations of
large language models (LLMs): critiquing tools to measure performance and
harms; proposing toolkits to aid in ideation; discussing the risks to workers;
considering legislation around privacy and security etc. As yet there is no
work that integrates these resources into a single practical guide that focuses
on LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper',
which we provide as an open and living resource for NLP practitioners, and
those tasked with evaluating the ethical implications of others' work. Our goal
is to translate ethics literature into concrete recommendations and
provocations for thinking with clear first steps, aimed at computer scientists.
'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's
and Don'ts, which we present also in this paper. We likewise identify useful
toolkits to support ethical work. We refer the interested reader to the full
LLM Ethics Whitepaper, which provides a succinct discussion of ethical
considerations at each stage in a project lifecycle, as well as citations for
the hundreds of papers from which we drew our recommendations. The present
paper can be thought of as a pocket guide to conducting ethical research with
LLMs.",263,2412.16022v1,cs.CL,"cs.CL,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.545834
Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition,"Explaining machine learning (ML) models using eXplainable AI (XAI) techniques
has become essential to make them more transparent and trustworthy. This is
especially important in high-stakes domains like healthcare, where
understanding model decisions is critical to ensure ethical, sound, and
trustworthy outcome predictions. However, users are often confused about which
explanability method to choose for their specific use case. We present a
comparative analysis of widely used explainability methods, Shapley Additive
Explanations (SHAP) and Gradient-weighted Class Activation Mapping (GradCAM),
within the domain of human activity recognition (HAR) utilizing graph
convolutional networks (GCNs). By evaluating these methods on skeleton-based
data from two real-world datasets, including a healthcare-critical cerebral
palsy (CP) case, this study provides vital insights into both approaches'
strengths, limitations, and differences, offering a roadmap for selecting the
most appropriate explanation method based on specific models and applications.
We quantitatively and quantitatively compare these methods, focusing on feature
importance ranking, interpretability, and model sensitivity through
perturbation experiments. While SHAP provides detailed input feature
attribution, GradCAM delivers faster, spatially oriented explanations, making
both methods complementary depending on the application's requirements. Given
the importance of XAI in enhancing trust and transparency in ML models,
particularly in sensitive environments like healthcare, our research
demonstrates how SHAP and GradCAM could complement each other to provide more
interpretable and actionable model explanations.",315,2412.16003v1,cs.LG,"cs.LG,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.546831
CNN-LSTM Hybrid Deep Learning Model for Remaining Useful Life Estimation,"Remaining Useful Life (RUL) of a component or a system is defined as the
length from the current time to the end of the useful life. Accurate RUL
estimation plays a crucial role in Predictive Maintenance applications.
Traditional regression methods, both linear and non-linear, have struggled to
achieve high accuracy in this domain. While Convolutional Neural Networks
(CNNs) have shown improved accuracy, they often overlook the sequential nature
of the data, relying instead on features derived from sliding windows. Since
RUL prediction inherently involves multivariate time series analysis, robust
sequence learning is essential. In this work, we propose a hybrid approach
combining Convolutional Neural Networks with Long Short-Term Memory (LSTM)
networks for RUL estimation. Although CNN-based LSTM models have been applied
to sequence prediction tasks in financial forecasting, this is the first
attempt to adopt this approach for RUL estimation in prognostics. In this
approach, CNN is first employed to efficiently extract features from the data,
followed by LSTM, which uses these extracted features to predict RUL. This
method effectively leverages sensor sequence information, uncovering hidden
patterns within the data, even under multiple operating conditions and fault
scenarios. Our results demonstrate that the hybrid CNN-LSTM model achieves the
highest accuracy, offering a superior score compared to the other methods.",281,2412.15998v1,cs.LG,"cs.LG,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.547829
Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling,"Conversational assistants are increasingly popular across diverse real-world
applications, highlighting the need for advanced multimodal speech modeling.
Speech, as a natural mode of communication, encodes rich user-specific
characteristics such as speaking rate and pitch, making it critical for
effective interaction. Our work introduces a data-centric customization
approach for efficiently enhancing multimodal understanding in conversational
speech modeling. Central to our contributions is a novel multi-task learning
paradigm that involves designing auxiliary tasks to utilize a small amount of
speech data. Our approach achieves state-of-the-art performance on the
Spoken-SQuAD benchmark, using only 10% of the training data with open-weight
models, establishing a robust and efficient framework for audio-centric
conversational modeling. We also introduce ASK-QA, the first dataset for
multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation
inputs. Code and data forthcoming.",187,2412.15995v1,cs.CL,"cs.CL,cs.AI,cs.SD,eess.AS",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.548826
APIRL: Deep Reinforcement Learning for REST API Fuzzing,"REST APIs have become key components of web services. However, they often
contain logic flaws resulting in server side errors or security
vulnerabilities. HTTP requests are used as test cases to find and mitigate such
issues. Existing methods to modify requests, including those using deep
learning, suffer from limited performance and precision, relying on undirected
search or making limited usage of the contextual information. In this paper we
propose APIRL, a fully automated deep reinforcement learning tool for testing
REST APIs. A key novelty of our approach is the use of feedback from a
transformer module pre-trained on JSON-structured data, akin to that used in
API responses. This allows APIRL to learn the subtleties relating to test
outcomes, and generalise to unseen API endpoints. We show APIRL can find
significantly more bugs than the state-of-the-art in real world REST APIs while
minimising the number of required test cases. We also study how reward
functions, and other key design choices, affect learnt policies in a thorough
ablation study.",218,2412.15991v1,cs.SE,"cs.SE,cs.AI,cs.NI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.548826
Feedback Regulated Opto-Mechanical Soft Robotic Actuators,"Natural organisms can convert environmental stimuli into sensory feedback to
regulate their body and realize active adaptivity. However, realizing such a
feedback-regulation mechanism in synthetic material systems remains a grand
challenge. It is believed that achieving complex feedback mechanisms in
responsive materials will pave the way toward autonomous, intelligent structure
and actuation without complex electronics. Inspired by living systems, we
report a general principle to design and construct such feedback loops in
light-responsive materials. Specifically, we design a baffle-actuator mechanism
to incorporate programmed feedback into the opto-mechanical responsiveness. By
simply addressing the baffle position with respect to the incident light beam,
positive and negative feedback are programmed. We demonstrate the
transformation of a light-bending strip into a switcher, where the intensity of
light determines the energy barrier under positive feedback, realizing
multi-stable shape-morphing. By leveraging the negative feedback and associated
homeostasis, we demonstrate two soft robots, i.e., a locomotor and a swimmer.
Furthermore, we unveil the ubiquity of feedback in light-responsive materials,
which provides new insight into self-regulated robotic matters.",236,2412.15990v1,cs.RO,"cs.RO,cond-mat.mtrl-sci",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.549823
Never Reset Again: A Mathematical Framework for Continual Inference in Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are widely used for sequential processing
but face fundamental limitations with continual inference due to state
saturation, requiring disruptive hidden state resets. However, reset-based
methods impose synchronization requirements with input boundaries and increase
computational costs at inference. To address this, we propose an adaptive loss
function that eliminates the need for resets during inference while preserving
high accuracy over extended sequences. By combining cross-entropy and
Kullback-Leibler divergence, the loss dynamically modulates the gradient based
on input informativeness, allowing the network to differentiate meaningful data
from noise and maintain stable representations over time. Experimental results
demonstrate that our reset-free approach outperforms traditional reset-based
methods when applied to a variety of RNNs, particularly in continual tasks,
enhancing both the theoretical and practical capabilities of RNNs for streaming
applications.",178,2412.15983v1,cs.LG,"cs.LG,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.550821
Self-Supervised Radiograph Anatomical Region Classification -- How Clean Is Your Real-World Data?,"Modern deep learning-based clinical imaging workflows rely on accurate labels
of the examined anatomical region. Knowing the anatomical region is required to
select applicable downstream models and to effectively generate cohorts of high
quality data for future medical and machine learning research efforts. However,
this information may not be available in externally sourced data or generally
contain data entry errors. To address this problem, we show the effectiveness
of self-supervised methods such as SimCLR and BYOL as well as supervised
contrastive deep learning methods in assigning one of 14 anatomical region
classes in our in-house dataset of 48,434 skeletal radiographs. We achieve a
strong linear evaluation accuracy of 96.6% with a single model and 97.7% using
an ensemble approach. Furthermore, only a few labeled instances (1% of the
training set) suffice to achieve an accuracy of 92.2%, enabling usage in
low-label and thus low-resource scenarios. Our model can be used to correct
data entry mistakes: a follow-up analysis of the test set errors of our
best-performing single model by an expert radiologist identified 35% incorrect
labels and 11% out-of-domain images. When accounted for, the radiograph
anatomical region labelling performance increased -- without and with an
ensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%.",282,2412.15967v1,cs.CV,"cs.CV,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.550821
Monkey Transfer Learning Can Improve Human Pose Estimation,"In this study, we investigated whether transfer learning from macaque monkeys
could improve human pose estimation. Current state-of-the-art pose estimation
techniques, often employing deep neural networks, can match human annotation in
non-clinical datasets. However, they underperform in novel situations, limiting
their generalisability to clinical populations with pathological movement
patterns. Clinical datasets are not widely available for AI training due to
ethical challenges and a lack of data collection. We observe that data from
other species may be able to bridge this gap by exposing the network to a
broader range of motion cues. We found that utilising data from other species
and undertaking transfer learning improved human pose estimation in terms of
precision and recall compared to the benchmark, which was trained on humans
only. Compared to the benchmark, fewer human training examples were needed for
the transfer learning approach (1,000 vs 19,185). These results suggest that
macaque pose estimation can improve human pose estimation in clinical
situations. Future work should further explore the utility of pose estimation
trained with monkey data in clinical populations.",226,2412.15966v1,cs.CV,cs.CV,artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.551818
Optimization of Beyond Diagonal RIS: A Universal Framework Applicable to Arbitrary Architectures,"Reconfigurable intelligent surfaces (RISs) are envisioned as a promising
technology for future wireless communication systems due to their ability to
control the propagation environment in a hardware- and energy-efficient way.
Recently, the concept of RISs has been extended to beyond diagonal RISs
(BD-RISs), which unlock the full potential of RISs thanks to the presence of
tunable interconnections between RIS elements. While various algorithms have
been proposed for specific BD-RIS architectures, a universal optimization
framework applicable to arbitrary architectures is still lacking. In this
paper, we bridge this research gap by proposing an architecture-independent
framework for BD-RIS optimization, with the main focus on sum-rate maximization
and transmit power minimization in multiuser multi-input single-output
(MU-MISO) systems. Specifically, we first incorporate BD-RIS architectures into
the models by connecting the scattering matrix with the admittance matrix and
introducing appropriate constraints to the admittance matrix. The formulated
problems are then solved by our custom-designed partially proximal alternating
direction method of multipliers (pp-ADMM) algorithms. The pp-ADMM algorithms
are computationally efficient, with each subproblem either admitting a
closed-form solution or being easily solvable. We further explore the extension
of the proposed framework to general utility functions and multiuser
multi-input multi-output (MU-MIMO) systems. Simulation results demonstrate that
the proposed approaches achieve a better trade-off between performance and
computational efficiency compared to existing methods. We also compare the
performance of various BD-RIS architectures in MU-MISO systems using the
proposed approach, which has not been explored before due to the lack of an
architecture-independent framework.",366,2412.15965v1,eess.SP,"eess.SP,cs.IT,math.IT,math.OC",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.552815
From General to Specific: Tailoring Large Language Models for Personalized Healthcare,"The rapid development of large language models (LLMs) has transformed many
industries, including healthcare. However, previous medical LLMs have largely
focused on leveraging general medical knowledge to provide responses, without
accounting for patient variability and lacking true personalization at the
individual level. To address this, we propose a novel method called
personalized medical language model (PMLM), which explores and optimizes
personalized LLMs through recommendation systems and reinforcement learning
(RL). Specifically, by utilizing self-informed and peer-informed
personalization, PMLM captures changes in behaviors and preferences to design
initial personalized prompts tailored to individual needs. We further refine
these initial personalized prompts through RL, ultimately enhancing the
precision of LLM guidance. Notably, the personalized prompt are hard prompt,
which grants PMLM high adaptability and reusability, allowing it to directly
leverage high-quality proprietary LLMs. We evaluate PMLM using real-world
obstetrics and gynecology data, and the experimental results demonstrate that
PMLM achieves personalized responses, and it provides more refined and
individualized services, offering a potential way for personalized medical
LLMs.",239,2412.15957v1,cs.CL,"cs.CL,cs.AI,cs.IR",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.553813
Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring,"In the software industry, the drive to add new features often overshadows the
need to improve existing code. Large Language Models (LLMs) offer a new
approach to improving codebases at an unprecedented scale through AI-assisted
refactoring. However, LLMs come with inherent risks such as braking changes and
the introduction of security vulnerabilities. We advocate for encapsulating the
interaction with the models in IDEs and validating refactoring attempts using
trustworthy safeguards. However, equally important for the uptake of AI
refactoring is research on trust development. In this position paper, we
position our future work based on established models from research on human
factors in automation. We outline action research within CodeScene on
development of 1) novel LLM safeguards and 2) user interaction that conveys an
appropriate level of trust. The industry collaboration enables large-scale
repository analysis and A/B testing to continuously guide the design of our
research interventions.",201,2412.15948v1,cs.SE,"cs.SE,cs.AI,cs.HC",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.553813
pyRheo: An open-source Python package for complex rheology,"Mathematical modeling is a powerful tool in rheology, and we present pyRheo,
an open-source package for Python designed to streamline the analysis of creep,
stress relaxation, oscillation, and rotation tests. pyRheo contains a
comprehensive selection of viscoelastic models, including fractional order
approaches. It integrates model selection and fitting features and employs
machine intelligence to suggest a model to describe a given dataset. The
package fits the suggested model or one chosen by the user. An advantage of
using pyRheo is that it addresses challenges associated with sensitivity to
initial guesses in parameter optimization. It allows the user to iteratively
search for the best initial guesses, avoiding convergence to local minima. We
discuss the capabilities of pyRheo and compare them to other tools for
rheological modeling of biological matter. We demonstrate that pyRheo
significantly reduces the computation time required to fit high-performance
viscoelastic models.",198,2412.15941v1,cond-mat.soft,cond-mat.soft,artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.554810
Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation,"The rise of the generative models quality during the past years enabled the
generation of edited variations of images at an important scale. To counter the
harmful effects of such technology, the Image Difference Captioning (IDC) task
aims to describe the differences between two images. While this task is
successfully handled for simple 3D rendered images, it struggles on real-world
images. The reason is twofold: the training data-scarcity, and the difficulty
to capture fine-grained differences between complex images. To address those
issues, we propose in this paper a simple yet effective framework to both adapt
existing image captioning models to the IDC task and augment IDC datasets. We
introduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational
cost, and show it outperforms two-streams approaches by a significant margin on
real-world IDC datasets. We also propose to use synthetic augmentation to
improve the performance of IDC models in an agnostic fashion. We show that our
synthetic augmentation strategy provides high quality data, leading to a
challenging new dataset well-suited for IDC named Syned1.",244,2412.15939v1,cs.CV,"cs.CV,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.554810
Watertox: The Art of Simplicity in Universal Attacks A Cross-Model Framework for Robust Adversarial Generation,"Contemporary adversarial attack methods face significant limitations in
cross-model transferability and practical applicability. We present Watertox,
an elegant adversarial attack framework achieving remarkable effectiveness
through architectural diversity and precision-controlled perturbations. Our
two-stage Fast Gradient Sign Method combines uniform baseline perturbations
($\epsilon_1 = 0.1$) with targeted enhancements ($\epsilon_2 = 0.4$). The
framework leverages an ensemble of complementary architectures, from VGG to
ConvNeXt, synthesizing diverse perspectives through an innovative voting
mechanism. Against state-of-the-art architectures, Watertox reduces model
accuracy from 70.6% to 16.0%, with zero-shot attacks achieving up to 98.8%
accuracy reduction against unseen architectures. These results establish
Watertox as a significant advancement in adversarial methodologies, with
promising applications in visual security systems and CAPTCHA generation.",205,2412.15924v1,cs.CV,"cs.CV,cs.AI,cs.CR",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.555808
Less is More: Towards Green Code Large Language Models via Unified Structural Pruning,"The extensive application of Large Language Models (LLMs) in generative
coding tasks has raised concerns due to their high computational demands and
energy consumption. Unlike previous structural pruning methods designed for
classification models that deal with lowdimensional classification logits,
generative Code LLMs produce high-dimensional token logit sequences, making
traditional pruning objectives inherently limited. Moreover, existing single
component pruning approaches further constrain the effectiveness when applied
to generative Code LLMs. In response, we propose Flab-Pruner, an innovative
unified structural pruning method that combines vocabulary, layer, and
Feed-Forward Network (FFN) pruning. This approach effectively reduces model
parameters while maintaining performance. Additionally, we introduce a
customized code instruction data strategy for coding tasks to enhance the
performance recovery efficiency of the pruned model. Through extensive
evaluations on three state-of-the-art Code LLMs across multiple generative
coding tasks, the results demonstrate that Flab-Pruner retains 97% of the
original performance after pruning 22% of the parameters and achieves the same
or even better performance after post-training. The pruned models exhibit
significant improvements in storage, GPU usage, computational efficiency, and
environmental impact, while maintaining well robustness. Our research provides
a sustainable solution for green software engineering and promotes the
efficient deployment of LLMs in real-world generative coding intelligence
applications.",294,2412.15921v1,cs.SE,"cs.SE,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.556806
Data Preparation for Fairness-Performance Trade-Offs: A Practitioner-Friendly Alternative?,"As machine learning (ML) systems are increasingly adopted across industries,
addressing fairness and bias has become essential. While many solutions focus
on ethical challenges in ML, recent studies highlight that data itself is a
major source of bias. Pre-processing techniques, which mitigate bias before
training, are effective but may impact model performance and pose integration
difficulties. In contrast, fairness-aware Data Preparation practices are both
familiar to practitioners and easier to implement, providing a more accessible
approach to reducing bias. Objective. This registered report proposes an
empirical evaluation of how optimally selected fairness-aware practices,
applied in early ML lifecycle stages, can enhance both fairness and
performance, potentially outperforming standard pre-processing bias mitigation
methods. Method. To this end, we will introduce FATE, an optimization technique
for selecting 'Data Preparation' pipelines that optimize fairness and
performance. Using FATE, we will analyze the fairness-performance trade-off,
comparing pipelines selected by FATE with results by pre-processing bias
mitigation techniques.",209,2412.15920v1,cs.SE,"cs.SE,cs.LG",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.556806
Speedup Techniques for Switchable Temporal Plan Graph Optimization,"Multi-Agent Path Finding (MAPF) focuses on planning collision-free paths for
multiple agents. However, during the execution of a MAPF plan, agents may
encounter unexpected delays, which can lead to inefficiencies, deadlocks, or
even collisions. To address these issues, the Switchable Temporal Plan Graph
provides a framework for finding an acyclic Temporal Plan Graph with the
minimum execution cost under delays, ensuring deadlock- and collision-free
execution. Unfortunately, existing optimal algorithms, such as Mixed Integer
Linear Programming and Graph-Based Switchable Edge Search (GSES), are often too
slow for practical use. This paper introduces Improved GSES, which
significantly accelerates GSES through four speedup techniques: stronger
admissible heuristics, edge grouping, prioritized branching, and incremental
implementation. Experiments conducted on four different map types with varying
numbers of agents demonstrate that Improved GSES consistently achieves over
twice the success rate of GSES and delivers up to a 30-fold speedup on
instances where both methods successfully find solutions.",220,2412.15908v1,cs.MA,"cs.MA,cs.AI,cs.RO",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.557803
Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model,"Background: Recent advances in large language models highlight the need for
high-quality multilingual medical datasets. While Japan leads globally in CT
scanner deployment and utilization, the lack of large-scale Japanese radiology
datasets has hindered the development of specialized language models for
medical imaging analysis. Objective: To develop a comprehensive Japanese CT
report dataset through machine translation and establish a specialized language
model for structured finding classification. Additionally, to create a
rigorously validated evaluation dataset through expert radiologist review.
Methods: We translated the CT-RATE dataset (24,283 CT reports from 21,304
patients) into Japanese using GPT-4o mini. The training dataset consisted of
22,778 machine-translated reports, while the validation dataset included 150
radiologist-revised reports. We developed CT-BERT-JPN based on
""tohoku-nlp/bert-base-japanese-v3"" architecture for extracting 18 structured
findings from Japanese radiology reports. Results: Translation metrics showed
strong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores
ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression
sections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in
11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular
septal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1
scores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in
four conditions. Conclusions: Our study establishes a robust Japanese CT report
dataset and demonstrates the effectiveness of a specialized language model for
structured finding classification. The hybrid approach of machine translation
and expert validation enables the creation of large-scale medical datasets
while maintaining high quality.",398,2412.15907v1,cs.CL,"cs.CL,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.558801
What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning,"Step-level reward models (SRMs) can significantly enhance mathematical
reasoning performance through process supervision or step-level preference
alignment based on reinforcement learning. The performance of SRMs is pivotal,
as they serve as critical guidelines, ensuring that each step in the reasoning
process is aligned with desired outcomes. Recently, AlphaZero-like methods,
where Monte Carlo Tree Search (MCTS) is employed for automatic step-level
preference annotation, have proven particularly effective. However, the precise
mechanisms behind the success of SRMs remain largely unexplored. To address
this gap, this study delves into the counterintuitive aspects of SRMs,
particularly focusing on MCTS-based approaches. Our findings reveal that the
removal of natural language descriptions of thought processes has minimal
impact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept
at assessing the complex logical coherence present in mathematical language
while having difficulty in natural language. These insights provide a nuanced
understanding of the core elements that drive effective step-level reward
modeling in mathematical reasoning. By shedding light on these mechanisms, this
study offers valuable guidance for developing more efficient and streamlined
SRMs, which can be achieved by focusing on the crucial parts of mathematical
reasoning.",253,2412.15904v1,cs.AI,"cs.AI,cs.LG",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.559799
On the Suitability of pre-trained foundational LLMs for Analysis in German Legal Education,"We show that current open-source foundational LLMs possess instruction
capability and German legal background knowledge that is sufficient for some
legal analysis in an educational context. However, model capability breaks down
in very specific tasks, such as the classification of ""Gutachtenstil"" appraisal
style components, or with complex contexts, such as complete legal opinions.
Even with extended context and effective prompting strategies, they cannot
match the Bag-of-Words baseline. To combat this, we introduce a Retrieval
Augmented Generation based prompt example selection method that substantially
improves predictions in high data availability scenarios. We further evaluate
the performance of pre-trained LLMs on two standard tasks for argument mining
and automated essay scoring and find it to be more adequate. Throughout,
pre-trained LLMs improve upon the baseline in scenarios with little or no
labeled data with Chain-of-Thought prompting further helping in the zero-shot
case.",183,2412.15902v1,cs.CL,"cs.CL,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.560796
"TelcoLM: collecting data, adapting, and benchmarking language models for the telecommunication domain","Despite outstanding processes in many tasks, Large Language Models (LLMs)
still lack accuracy when dealing with highly technical domains. Especially,
telecommunications (telco) is a particularly challenging domain due the large
amount of lexical, semantic and conceptual peculiarities. Yet, this domain
holds many valuable use cases, directly linked to industrial needs. Hence, this
paper studies how LLMs can be adapted to the telco domain. It reports our
effort to (i) collect a massive corpus of domain-specific data (800M tokens,
80K instructions), (ii) perform adaptation using various methodologies, and
(iii) benchmark them against larger generalist models in downstream tasks that
require extensive knowledge of telecommunications. Our experiments on
Llama-2-7b show that domain-adapted models can challenge the large generalist
models. They also suggest that adaptation can be restricted to a unique
instruction-tuning step, dicarding the need for any fine-tuning on raw texts
beforehand.",200,2412.15891v1,cs.CL,"cs.CL,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.561794
Approximate State Abstraction for Markov Games,"This paper introduces state abstraction for two-player zero-sum Markov games
(TZMGs), where the payoffs for the two players are determined by the state
representing the environment and their respective actions, with state
transitions following Markov decision processes. For example, in games like
soccer, the value of actions changes according to the state of play, and thus
such games should be described as Markov games. In TZMGs, as the number of
states increases, computing equilibria becomes more difficult. Therefore, we
consider state abstraction, which reduces the number of states by treating
multiple different states as a single state. There is a substantial body of
research on finding optimal policies for Markov decision processes using state
abstraction. However, in the multi-player setting, the game with state
abstraction may yield different equilibrium solutions from those of the ground
game. To evaluate the equilibrium solutions of the game with state abstraction,
we derived bounds on the duality gap, which represents the distance from the
equilibrium solutions of the ground game. Finally, we demonstrate our state
abstraction with Markov Soccer, compute equilibrium policies, and examine the
results.",232,2412.15877v1,cs.GT,"cs.GT,cs.AI,cs.MA",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.561794
AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI,"AI is the workhorse of modern data analytics and omnipresent across many
sectors. Large Language Models and multi-modal foundation models are today
capable of generating code, charts, visualizations, etc. How will these massive
developments of AI in data analytics shape future data visualizations and
visual analytics workflows? What is the potential of AI to reshape methodology
and design of future visual analytics applications? What will be our role as
visualization researchers in the future? What are opportunities, open
challenges and threats in the context of an increasingly powerful AI? This
Visualization Viewpoint discusses these questions in the special context of
biomedical data analytics as an example of a domain in which critical decisions
are taken based on complex and sensitive data, with high requirements on
transparency, efficiency, and reliability. We map recent trends and
developments in AI on the elements of interactive visualization and visual
analytics workflows and highlight the potential of AI to transform biomedical
visualization as a research field. Given that agency and responsibility have to
remain with human experts, we argue that it is helpful to keep the focus on
human-centered workflows, and to use visual analytics as a tool for integrating
``AI-in-the-loop''. This is in contrast to the more traditional term
``human-in-the-loop'', which focuses on incorporating human expertise into
AI-based systems.",282,2412.15876v1,cs.HC,"cs.HC,cs.AI,cs.GR,68U01,H.1.2; H.5.2; I.3.6; I.2.1; J.3; D.2.0",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.562791
Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback,"Reinforcement learning from human feedback (RLHF) has proven effective in
enhancing the instruction-following capabilities of large language models;
however, it remains underexplored in the cross-modality domain. As the number
of modalities increases, aligning all-modality models with human intentions --
such as instruction following -- becomes a pressing challenge. In this work, we
make the first attempt to fine-tune all-modality models (i.e. input and output
with any modality, also named any-to-any models) using human preference data
across all modalities (including text, image, audio, and video), ensuring its
behavior aligns with human intentions. This endeavor presents several
challenges. First, there is no large-scale all-modality human preference data
in existing open-source resources, as most datasets are limited to specific
modalities, predominantly text and image. Secondly, the effectiveness of binary
preferences in RLHF for post-training alignment in complex all-modality
scenarios remains an unexplored area. Finally, there is a lack of a systematic
framework to evaluate the capabilities of all-modality models, particularly
regarding modality selection and synergy. To address these challenges, we
propose the align-anything framework, which includes meticulously annotated
200k all-modality human preference data. Then, we introduce an alignment method
that learns from unified language feedback, effectively capturing complex
modality-specific human preferences and enhancing the model's
instruction-following capabilities. Furthermore, to assess performance
improvements in all-modality models after post-training alignment, we construct
a challenging all-modality capability evaluation framework -- eval-anything.
All data, models, and code frameworks have been open-sourced for the community.
For more details, please refer to
https://github.com/PKU-Alignment/align-anything.",399,2412.15838v1,cs.AI,"cs.AI,cs.CL",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.563787
Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis,"Complying with traffic rules is challenging for automated vehicles, as
numerous rules need to be considered simultaneously. If a planned trajectory
violates traffic rules, it is common to replan a new trajectory from scratch.
We instead propose a trajectory repair technique to save computation time. By
coupling satisfiability modulo theories with set-based reachability analysis,
we determine if and in what manner the initial trajectory can be repaired.
Experiments in high-fidelity simulators and in the real world demonstrate the
benefits of our proposed approach in various scenarios. Even in complex
environments with intricate rules, we efficiently and reliably repair
rule-violating trajectories, enabling automated vehicles to swiftly resume
legally safe operation in real-time.",146,2412.15837v1,cs.RO,"cs.RO,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.564784
Exploring the Effects of AI Nonverbal Emotional Cues on Human Decision Certainty in Moral Dilemmas,"Exploring moral dilemmas allows individuals to navigate moral complexity,
where a reversal in decision certainty, shifting toward the opposite of one's
initial choice, could reflect open-mindedness and less rigidity. This study
probes how nonverbal emotional cues from conversational agents could influence
decision certainty in moral dilemmas. While existing research heavily focused
on verbal aspects of human-agent interaction, we investigated the impact of
agents expressing anger and sadness towards the moral situations through
animated chat balloons. We compared these with a baseline where agents offered
same responses without nonverbal cues. Results show that agents displaying
anger significantly caused reversal shifts in decision certainty. The
interaction between participant gender and agents' nonverbal emotional cues
significantly affects participants' perception of AI's influence. These
findings reveal that even subtly altering agents' nonverbal cues may impact
human moral decisions, presenting both opportunities to leverage these effects
for positive outcomes and ethical risks for future human-AI systems.",192,2412.15834v1,cs.HC,cs.HC,artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.564784
AIFS-CRPS: Ensemble forecasting using a model trained with a loss function based on the Continuous Ranked Probability Score,"Over the last three decades, ensemble forecasts have become an integral part
of forecasting the weather. They provide users with more complete information
than single forecasts as they permit to estimate the probability of weather
events by representing the sources of uncertainties and accounting for the
day-to-day variability of error growth in the atmosphere. This paper presents a
novel approach to obtain a weather forecast model for ensemble forecasting with
machine-learning. AIFS-CRPS is a variant of the Artificial Intelligence
Forecasting System (AIFS) developed at ECMWF. Its loss function is based on a
proper score, the Continuous Ranked Probability Score (CRPS). For the loss, the
almost fair CRPS is introduced because it approximately removes the bias in the
score due to finite ensemble size yet avoids a degeneracy of the fair CRPS. The
trained model is stochastic and can generate as many exchangeable members as
desired and computationally feasible in inference. For medium-range forecasts
AIFS-CRPS outperforms the physics-based Integrated Forecasting System (IFS)
ensemble for the majority of variables and lead times. For subseasonal
forecasts, AIFS-CRPS outperforms the IFS ensemble before calibration and is
competitive with the IFS ensemble when forecasts are evaluated as anomalies to
remove the influence of model biases.",282,2412.15832v1,physics.ao-ph,physics.ao-ph,artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.565782
S$^2$DN: Learning to Denoise Unconvincing Knowledge for Inductive Knowledge Graph Completion,"Inductive Knowledge Graph Completion (KGC) aims to infer missing facts
between newly emerged entities within knowledge graphs (KGs), posing a
significant challenge. While recent studies have shown promising results in
inferring such entities through knowledge subgraph reasoning, they suffer from
(i) the semantic inconsistencies of similar relations, and (ii) noisy
interactions inherent in KGs due to the presence of unconvincing knowledge for
emerging entities. To address these challenges, we propose a Semantic
Structure-aware Denoising Network (S$^2$DN) for inductive KGC. Our goal is to
learn adaptable general semantics and reliable structures to distill consistent
semantic knowledge while preserving reliable interactions within KGs.
Specifically, we introduce a semantic smoothing module over the enclosing
subgraphs to retain the universal semantic knowledge of relations. We
incorporate a structure refining module to filter out unreliable interactions
and offer additional knowledge, retaining robust structure surrounding target
links. Extensive experiments conducted on three benchmark KGs demonstrate that
S$^2$DN surpasses the performance of state-of-the-art models. These results
demonstrate the effectiveness of S$^2$DN in preserving semantic consistency and
enhancing the robustness of filtering out unreliable interactions in
contaminated KGs.",265,2412.15822v1,cs.LG,"cs.LG,cs.AI,cs.CL",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.566779
$π$-yalli: un nouveau corpus pour le nahuatl,"The NAHU$^2$ project is a Franco-Mexican collaboration aimed at building the
$\pi$-YALLI corpus adapted to machine learning, which will subsequently be used
to develop computer resources for the Nahuatl language. Nahuatl is a language
with few computational resources, even though it is a living language spoken by
around 2 million people. We have decided to build $\pi$-YALLI, a corpus that
will enable to carry out research on Nahuatl in order to develop Language
Models (LM), whether dynamic or not, which will make it possible to in turn
enable the development of Natural Language Processing (NLP) tools such as: a) a
grapheme unifier, b) a word segmenter, c) a POS grammatical analyser, d) a
content-based Automatic Text Summarization; and possibly, e) a translator
translator (probabilistic or learning-based).",195,2412.15821v1,cs.CL,"cs.CL,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.567777
WebLLM: A High-Performance In-Browser LLM Inference Engine,"Advancements in large language models (LLMs) have unlocked remarkable
capabilities. While deploying these models typically requires server-grade GPUs
and cloud-based inference, the recent emergence of smaller open-source models
and increasingly powerful consumer devices have made on-device deployment
practical. The web browser as a platform for on-device deployment is
universally accessible, provides a natural agentic environment, and
conveniently abstracts out the different backends from diverse device vendors.
To address this opportunity, we introduce WebLLM, an open-source JavaScript
framework that enables high-performance LLM inference entirely within web
browsers. WebLLM provides an OpenAI-style API for seamless integration into web
applications, and leverages WebGPU for efficient local GPU acceleration and
WebAssembly for performant CPU computation. With machine learning compilers
MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming
the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM
can retain up to 80% native performance on the same device, with room to
further close the gap. WebLLM paves the way for universally accessible,
privacy-preserving, personalized, and locally powered LLM applications in web
browsers. The code is available at: https://github.com/mlc-ai/web-llm.",291,2412.15803v1,cs.LG,"cs.LG,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.567777
Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form,"Urban morphology, examining city spatial configurations, links urban design
to sustainability. Morphology metrics play a fundamental role in
performance-driven computational urban design (CUD) which integrates urban form
generation, performance evaluation and optimization. However, a critical gap
remains between performance evaluation and complex urban form generation,
caused by the disconnection between morphology metrics and urban form,
particularly in metric-to-form workflows. It prevents the application of
optimized metrics to generate improved urban form with enhanced urban
performance. Formulating morphology metrics that not only effectively
characterize complex urban forms but also enable the reconstruction of diverse
forms is of significant importance. This paper highlights the importance of
establishing a bi-directional mapping between morphology metrics and complex
urban form to enable the integration of urban form generation with performance
evaluation. We present an approach that can 1) formulate morphology metrics to
both characterize urban forms and in reverse, retrieve diverse similar 3D urban
forms, and 2) evaluate the effectiveness of morphology metrics in representing
3D urban form characteristics of blocks by comparison. We demonstrate the
methodology with 3D urban models of New York City, covering 14,248 blocks. We
use neural networks and information retrieval for morphology metric encoding,
urban form clustering and morphology metric evaluation. We identified an
effective set of morphology metrics for characterizing block-scale urban forms
through comparison. The proposed methodology tightly couples complex urban
forms with morphology metrics, hence it can enable a seamless and bidirectional
relationship between urban form generation and optimization in
performance-driven urban design towards sustainable urban design and planning.",321,2412.15801v1,cs.CE,"cs.CE,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.568774
GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning,"The integration of multi-omic data is pivotal for understanding complex
diseases, but its high dimensionality and noise present significant challenges.
Graph Neural Networks (GNNs) offer a robust framework for analyzing large-scale
signaling pathways and protein-protein interaction networks, yet they face
limitations in expressivity when capturing intricate biological relationships.
To address this, we propose Graph Sequence Language Model (GraphSeqLM), a
framework that enhances GNNs with biological sequence embeddings generated by
Large Language Models (LLMs). These embeddings encode structural and biological
properties of DNA, RNA, and proteins, augmenting GNNs with enriched features
for analyzing sample-specific multi-omic data. By integrating topological,
sequence-derived, and biological information, GraphSeqLM demonstrates superior
predictive accuracy and outperforms existing methods, paving the way for more
effective multi-omic data integration in precision medicine.",192,2412.15790v1,q-bio.QM,"q-bio.QM,cs.AI,cs.LG",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.569772
AI Apology: A Critical Review of Apology in AI Systems,"Apologies are a powerful tool used in human-human interactions to provide
affective support, regulate social processes, and exchange information
following a trust violation. The emerging field of AI apology investigates the
use of apologies by artificially intelligent systems, with recent research
suggesting how this tool may provide similar value in human-machine
interactions. Until recently, contributions to this area were sparse, and these
works have yet to be synthesised into a cohesive body of knowledge. This
article provides the first synthesis and critical analysis of the state of AI
apology research, focusing on studies published between 2020 and 2023. We
derive a framework of attributes to describe five core elements of apology:
outcome, interaction, offence, recipient, and offender. With this framework as
the basis for our critique, we show how apologies can be used to recover from
misalignment in human-AI interactions, and examine trends and inconsistencies
within the field. Among the observations, we outline the importance of curating
a human-aligned and cross-disciplinary perspective in this research, with
consideration for improved system capabilities and long-term outcomes.",222,2412.15787v1,cs.CY,cs.CY,artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.569772
Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease Detection based on Spontaneous Speech,"Alzheimer's Disease (AD) is a significant and growing public health concern.
Investigating alterations in speech and language patterns offers a promising
path towards cost-effective and non-invasive early detection of AD on a large
scale. Large language models (LLMs), such as GPT, have enabled powerful new
possibilities for semantic text analysis. In this study, we leverage GPT-4 to
extract five semantic features from transcripts of spontaneous patient speech.
The features capture known symptoms of AD, but they are difficult to quantify
effectively using traditional methods of computational linguistics. We
demonstrate the clinical significance of these features and further validate
one of them (""Word-Finding Difficulties"") against a proxy measure and human
raters. When combined with established linguistic features and a Random Forest
classifier, the GPT-derived features significantly improve the detection of AD.
Our approach proves effective for both manually transcribed and automatically
generated transcripts, representing a novel and impactful use of recent
advancements in LLMs for AD speech analysis.",206,2412.15772v1,cs.CL,"cs.CL,cs.AI",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.571271
Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models,"Background: Despite the current ubiquity of Large Language Models (LLMs)
across the medical domain, there is a surprising lack of studies which address
their reasoning behaviour. We emphasise the importance of understanding
reasoning behaviour as opposed to high-level prediction accuracies, since it is
equivalent to explainable AI (XAI) in this context. In particular, achieving
XAI in medical LLMs used in the clinical domain will have a significant impact
across the healthcare sector. Results: Therefore, we define the concept of
reasoning behaviour in the specific context of medical LLMs. We then categorise
and discuss the current state of the art of methods which evaluate reasoning
behaviour in medical LLMs. Finally, we propose theoretical frameworks which can
empower medical professionals or machine learning engineers to gain insight
into the low-level reasoning operations of these previously obscure models.
Conclusion: The subsequent increased transparency and trust in medical machine
learning models by clinicians as well as patients will accelerate the
integration, application as well as further development of medical AI for the
healthcare system as a whole",216,2412.15748v1,cs.CL,"cs.CL,cs.AI,cs.LG",artificial intelligence ethics,2024-12-20,2024-12-23T21:07:13.571271
