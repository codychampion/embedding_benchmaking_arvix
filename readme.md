# ğŸ† Academic Embedding Model Evaluator

**Author**: Cody Champion

## Current Model Leaderboard

Our sophisticated leaderboard system shows exactly how different AI models perform at understanding research papers:

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                    ğŸ† Model Leaderboard                                         â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“                  â”‚
â”‚ â”ƒ Model   â”ƒ Score â”ƒ Own Title-Abstract â”ƒ Same Field Separation â”ƒ Avg Std â”ƒ                  â”‚
â”‚ â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©                  â”‚
â”‚ â”‚ MPNet   â”‚ 0.466 â”‚ 0.718              â”‚ 0.393                 â”‚ 0.088   â”‚                  â”‚
â”‚ â”‚ MiniLM  â”‚ 0.466 â”‚ 0.718              â”‚ 0.393                 â”‚ 0.088   â”‚                  â”‚
â”‚ â”‚ E5-Base â”‚ 0.264 â”‚ 0.858              â”‚ 0.074                 â”‚ 0.025   â”‚                  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### Understanding the Metrics:
- **Score**: Overall performance score (higher is better)
- **Own Title-Abstract**: How well the model connects a paper's title with its content
- **Same Field Separation**: How accurately it distinguishes between related and unrelated papers
- **Avg Std**: Consistency of the model (lower means more reliable)

## ğŸ¯ Beautiful Real-Time Progress Tracking

Watch the analysis happen in real-time with our elegant console interface:

```
ğŸš€ Starting Academic Embedding Model Evaluation
==================================================
ğŸ“š Loaded configuration from config.yaml
ğŸ¤– Models to evaluate: 3
ğŸ”¬ Research fields: 3

ğŸ“Š Total papers collected: 9
  Fetching papers for: biology              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%
  Processing paper 9/9 for intfloat/e5-base â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“Š Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                            Model Comparison Results                                           â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“    â”‚
â”‚ â”ƒ Model   â”ƒ Title-â€¦ â”ƒ Title-â€¦ â”ƒ Title-â€¦ â”ƒ Title-â€¦ â”ƒ Title-â€¦ â”ƒ Title-â€¦ â”ƒ Abstraâ€¦ â”ƒ Abstraâ€¦ â”ƒ Abstraâ€¦ â”ƒ    â”‚
â”‚ â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©    â”‚
â”‚ â”‚ MPNet   â”‚ 0.718   â”‚ 0.084   â”‚ 0.361   â”‚ 0.059   â”‚ 0.036   â”‚ 0.087   â”‚ 0.465   â”‚ 0.087   â”‚ 0.072   â”‚    â”‚
â”‚ â”‚ MiniLM  â”‚ 0.718   â”‚ 0.084   â”‚ 0.361   â”‚ 0.059   â”‚ 0.036   â”‚ 0.087   â”‚ 0.465   â”‚ 0.087   â”‚ 0.072   â”‚    â”‚
â”‚ â”‚ E5-Base â”‚ 0.858   â”‚ 0.029   â”‚ 0.772   â”‚ 0.022   â”‚ 0.710   â”‚ 0.031   â”‚ 0.846   â”‚ 0.018   â”‚ 0.772   â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

## ğŸ“Š Professional Experiment Tracking

Every experiment is automatically documented with detailed statistics and configurations:

### ğŸ“ˆ Collection Statistics
```yaml
# collection_statistics.yaml
date_range:
  earliest: '2024-12-06'
  latest: '2024-12-18'
papers_per_field:
  artificial intelligence: 25
  biology: 25
  machine learning: 25
token_statistics:
  max: 407
  mean: 260.76
  median: 254.0
  min: 81
  std: 67.19
total_papers: 75
```

## ğŸš€ Key Features

- ğŸ“Š Comprehensive model evaluation across multiple academic fields
- ğŸ”„ Support for various embedding models (Hugging Face models supported)
- ğŸ“‘ Multiple comparison metrics
- ğŸ’¾ Efficient caching system
- ğŸ“ˆ Detailed performance leaderboard
- ğŸ¯ Beautiful progress tracking
- ğŸ“Š Professional experiment logging

## ğŸ”§ Hardware Acceleration & Cloud Integration

### ğŸ–¥ï¸ GPU Support (Optional)
Works efficiently on both CPU and GPU:
- Automatic hardware detection
- CPU-only setup works well for most cases
- Optional GPU acceleration for faster processing
- All models support both CPU and GPU execution

Requirements:
- Basic: Any modern CPU
- Optional GPU: CUDA-compatible GPU with PyTorch support

### â˜ï¸ AWS Bedrock Integration
Includes AWS Bedrock's Titan Embeddings model:
- State-of-the-art embedding quality
- Cloud-based processing
- No local compute requirements
- Production-ready capabilities

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone [repository-url]
cd embedding-benchmarking-arxiv
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up environment:
```bash
cp .env-example .env
# Add your HuggingFace token to .env
```

## âš™ï¸ Configuration

Create your config file with our comprehensive model selection:

```yaml
models:
  # Cloud Provider Models
  'Bedrock': 'Bedrock'  # Amazon's Titan
  
  # Scientific/Academic Specialized
  'Specter': 'allenai/specter'
  'SciBERT': 'allenai/scibert_scivocab_uncased'
  'BioLinkBERT': 'michiyasunaga/BioLinkBERT-base'
  
  # Strong General Purpose Models
  'MPNet': 'sentence-transformers/all-mpnet-base-v2'
  'MiniLM': 'sentence-transformers/all-MiniLM-L6-v2'
  
  # E5 Family
  'E5-Base': 'intfloat/e5-base'
  'E5-Large': 'intfloat/e5-large'

fields:
  # Computer & Information Science
  - "artificial intelligence"
  - "machine learning"
  - "computer vision"
  
  # Biological Sciences
  - "molecular biology"
  - "neuroscience"
  - "genomics"
  
  # Physical Sciences
  - "quantum physics"
  - "materials science"
  - "astronomical sciences"
```

## ğŸš€ Usage

### Basic Usage
```bash
python -m src.embedding_benchmarking.cli evaluate
```

### Advanced Options
```bash
python -m src.embedding_benchmarking.cli evaluate \
  --cache-dir embedding_cache \
  --max-papers 100 \
  --config custom_config.yaml \
  --output-dir results
```

## ğŸ“Š Output Files

1. `embedding_comparison_results.csv`: Detailed metrics
2. `model_leaderboard.csv`: Aggregated performance scores
3. Experiment directory with:
   - Leaderboard and detailed metrics comparisons 
   - Full statistical analysis
   - Configuration snapshots
   - Paper metadata
   - Detailed logs


## Project Structure

```
.
â”œâ”€â”€ src/embedding_benchmarking/  # Main package source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                  # Command line interface
â”‚   â”œâ”€â”€ config.py               # Configuration handling
â”‚   â”œâ”€â”€ data.py                 # Data processing
â”‚   â”œâ”€â”€ embedding_evaluator.py  # Core evaluation logic
â”‚   â”œâ”€â”€ evaluation.py           # Evaluation metrics
â”‚   â”œâ”€â”€ models.py              # Model implementations
â”‚   â””â”€â”€ utils.py               # Utility functions
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ config-example.yaml    # Example configuration
â”‚   â””â”€â”€ .env-example          # Environment variables example
â”‚
â”œâ”€â”€ experiments/               # Experiment results
â”‚   â””â”€â”€ experiment_20241219_111718/
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ license.md
â”‚
â””â”€â”€ requirements.txt          # Project dependencies
```



## ğŸ“š Citation

```bibtex
@software{embedding_benchmarking_arxiv,
  title = {Academic Embedding Model Evaluator},
  author = {Champion, Cody},
  year = {2024},
  description = {A tool for evaluating embedding models on academic paper similarity tasks}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

Ready to revolutionize your research paper analysis? Get started today! ğŸš€
